============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-21 06:30:48,777 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:30:48,781 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37449 instead
  warnings.warn(
2024-01-21 06:30:48,785 - distributed.scheduler - INFO - State start
2024-01-21 06:30:48,839 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:30:48,840 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-21 06:30:48,841 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37449/status
2024-01-21 06:30:48,841 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:30:48,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45951'
2024-01-21 06:30:48,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45031'
2024-01-21 06:30:48,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44491'
2024-01-21 06:30:49,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40749'
2024-01-21 06:30:49,471 - distributed.scheduler - INFO - Receive client connection: Client-9ca57d4f-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:30:49,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54040
2024-01-21 06:30:50,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:50,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:50,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:50,893 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35777
2024-01-21 06:30:50,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35777
2024-01-21 06:30:50,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41953
2024-01-21 06:30:50,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:30:50,893 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:50,893 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:30:50,893 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:30:50,894 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-cddfbo02
2024-01-21 06:30:50,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1485c62b-b4b6-4699-9a2d-23c2fe3dbe83
2024-01-21 06:30:50,894 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56f76958-26f7-4ffc-9fb8-7d2a83d58df3
2024-01-21 06:30:50,894 - distributed.worker - INFO - Starting Worker plugin PreImport-79142423-5d5f-4388-bb5f-6722b3fcc9e0
2024-01-21 06:30:50,894 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:50,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:50,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:50,942 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:50,943 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40267
2024-01-21 06:30:50,943 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40267
2024-01-21 06:30:50,943 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33659
2024-01-21 06:30:50,943 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:30:50,943 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:50,943 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:30:50,943 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:30:50,944 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-4rheze6k
2024-01-21 06:30:50,944 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac6fa325-e452-4cf8-afce-8d6d46e92e15
2024-01-21 06:30:50,944 - distributed.worker - INFO - Starting Worker plugin PreImport-0ae114a8-0cad-4713-bf86-167a57e8109e
2024-01-21 06:30:50,944 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f22c0a4d-364e-4f84-8fcf-613eb6a80d44
2024-01-21 06:30:50,944 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:50,994 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:50,994 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:50,998 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:50,999 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40577
2024-01-21 06:30:50,999 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40577
2024-01-21 06:30:50,999 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45915
2024-01-21 06:30:50,999 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:30:50,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:50,999 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:30:50,999 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:30:50,999 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mefd_jjz
2024-01-21 06:30:50,999 - distributed.worker - INFO - Starting Worker plugin PreImport-3921ec85-dcc3-4966-ba14-f3eb8f7d38ef
2024-01-21 06:30:50,999 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27ff123f-f63d-4187-b27e-f4a006507161
2024-01-21 06:30:50,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6d4829e-02cc-4b38-a9c2-a7dc3d3862c4
2024-01-21 06:30:51,000 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:51,015 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:51,015 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:51,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:51,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37777
2024-01-21 06:30:51,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37777
2024-01-21 06:30:51,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40983
2024-01-21 06:30:51,020 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:30:51,021 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:51,021 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:30:51,021 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:30:51,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-x2b5j__5
2024-01-21 06:30:51,021 - distributed.worker - INFO - Starting Worker plugin PreImport-d7fe8789-e14f-4b2f-aa38-7e9accf34797
2024-01-21 06:30:51,021 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3eb12f2d-4017-4189-8294-e703122e9c76
2024-01-21 06:30:51,021 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31b54362-2330-4d09-b072-7c4174555326
2024-01-21 06:30:51,021 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:51,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35777', status: init, memory: 0, processing: 0>
2024-01-21 06:30:51,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35777
2024-01-21 06:30:51,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58898
2024-01-21 06:30:51,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:30:51,334 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:30:51,334 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:51,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:30:51,865 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40267', status: init, memory: 0, processing: 0>
2024-01-21 06:30:51,865 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40267
2024-01-21 06:30:51,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58906
2024-01-21 06:30:51,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:30:51,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:30:51,867 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:51,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:30:52,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40577', status: init, memory: 0, processing: 0>
2024-01-21 06:30:52,153 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40577
2024-01-21 06:30:52,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58908
2024-01-21 06:30:52,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:30:52,154 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:30:52,154 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:52,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:30:52,429 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37777', status: init, memory: 0, processing: 0>
2024-01-21 06:30:52,429 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37777
2024-01-21 06:30:52,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58918
2024-01-21 06:30:52,431 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:30:52,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:30:52,432 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:52,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:30:52,481 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:30:52,481 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:30:52,482 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:30:52,482 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:30:52,487 - distributed.scheduler - INFO - Remove client Client-9ca57d4f-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:30:52,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54040; closing.
2024-01-21 06:30:52,487 - distributed.scheduler - INFO - Remove client Client-9ca57d4f-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:30:52,487 - distributed.scheduler - INFO - Close client connection: Client-9ca57d4f-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:30:52,488 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45951'. Reason: nanny-close
2024-01-21 06:30:52,489 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:30:52,489 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45031'. Reason: nanny-close
2024-01-21 06:30:52,489 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:30:52,490 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44491'. Reason: nanny-close
2024-01-21 06:30:52,490 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37777. Reason: nanny-close
2024-01-21 06:30:52,490 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:30:52,490 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40749'. Reason: nanny-close
2024-01-21 06:30:52,490 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35777. Reason: nanny-close
2024-01-21 06:30:52,490 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:30:52,491 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40267. Reason: nanny-close
2024-01-21 06:30:52,491 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40577. Reason: nanny-close
2024-01-21 06:30:52,491 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:30:52,491 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58918; closing.
2024-01-21 06:30:52,492 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818652.492168')
2024-01-21 06:30:52,492 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:30:52,492 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:30:52,493 - distributed.nanny - INFO - Worker closed
2024-01-21 06:30:52,493 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:30:52,493 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58906; closing.
2024-01-21 06:30:52,493 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58898; closing.
2024-01-21 06:30:52,493 - distributed.nanny - INFO - Worker closed
2024-01-21 06:30:52,494 - distributed.nanny - INFO - Worker closed
2024-01-21 06:30:52,494 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818652.494249')
2024-01-21 06:30:52,494 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58908; closing.
2024-01-21 06:30:52,494 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818652.494844')
2024-01-21 06:30:52,495 - distributed.nanny - INFO - Worker closed
2024-01-21 06:30:52,495 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40577', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818652.495268')
2024-01-21 06:30:52,495 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:30:53,404 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:30:53,405 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:30:53,405 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:30:53,407 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-21 06:30:53,407 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-21 06:30:55,893 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:30:55,898 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35729 instead
  warnings.warn(
2024-01-21 06:30:55,904 - distributed.scheduler - INFO - State start
2024-01-21 06:30:55,927 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:30:55,928 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:30:55,929 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35729/status
2024-01-21 06:30:55,930 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:30:56,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45569'
2024-01-21 06:30:56,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33189'
2024-01-21 06:30:56,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34281'
2024-01-21 06:30:56,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37983'
2024-01-21 06:30:56,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43009'
2024-01-21 06:30:56,221 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46105'
2024-01-21 06:30:56,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32905'
2024-01-21 06:30:56,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33963'
2024-01-21 06:30:57,224 - distributed.scheduler - INFO - Receive client connection: Client-a0b7eed8-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:30:57,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54774
2024-01-21 06:30:57,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:57,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:57,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:57,944 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36723
2024-01-21 06:30:57,944 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36723
2024-01-21 06:30:57,944 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43417
2024-01-21 06:30:57,945 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:57,945 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:57,945 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:57,945 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:57,945 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9yulb9hm
2024-01-21 06:30:57,945 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a8640db-7ef8-412f-94f2-2d1737400c7d
2024-01-21 06:30:57,945 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a32fc90-1d14-480f-81b4-c0d95e6981d8
2024-01-21 06:30:58,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,208 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,208 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35349
2024-01-21 06:30:58,208 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35349
2024-01-21 06:30:58,208 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37871
2024-01-21 06:30:58,209 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,209 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,209 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,209 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pi658muj
2024-01-21 06:30:58,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbdbe364-629c-4cc2-b2bd-7bbb61135ad4
2024-01-21 06:30:58,209 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43517
2024-01-21 06:30:58,209 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43517
2024-01-21 06:30:58,209 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42011
2024-01-21 06:30:58,209 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4fe1426-878f-467a-a89b-478c41aa1fad
2024-01-21 06:30:58,210 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,210 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,210 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,210 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c_vix2x6
2024-01-21 06:30:58,210 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e238872-77a7-4ada-8220-d8e8195047c3
2024-01-21 06:30:58,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:30:58,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:30:58,213 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,214 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46039
2024-01-21 06:30:58,214 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46039
2024-01-21 06:30:58,214 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44799
2024-01-21 06:30:58,214 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,214 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,215 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,215 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,215 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gs3gmndw
2024-01-21 06:30:58,215 - distributed.worker - INFO - Starting Worker plugin PreImport-4ff7de6f-3670-4789-a66a-d781bc4952e5
2024-01-21 06:30:58,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25d0cce3-803c-4a12-994e-504256d829ef
2024-01-21 06:30:58,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,216 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,216 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33495
2024-01-21 06:30:58,216 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33495
2024-01-21 06:30:58,216 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33395
2024-01-21 06:30:58,216 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,216 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,216 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,216 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,217 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ewshdhd
2024-01-21 06:30:58,217 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46095
2024-01-21 06:30:58,217 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a7e8cb6-c774-46c2-9ae7-6f8b98c99af5
2024-01-21 06:30:58,217 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46095
2024-01-21 06:30:58,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,217 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36227
2024-01-21 06:30:58,217 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,217 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,217 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,217 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,217 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0a44m3fe
2024-01-21 06:30:58,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:30:58,217 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40c3a66a-7227-4f59-b7a4-e5d23c65d574
2024-01-21 06:30:58,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39123
2024-01-21 06:30:58,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39123
2024-01-21 06:30:58,218 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40997
2024-01-21 06:30:58,218 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,218 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,218 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,218 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7b22254-817c-4e33-827e-2a1ca513b89b
2024-01-21 06:30:58,218 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a56i0v0i
2024-01-21 06:30:58,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43147
2024-01-21 06:30:58,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43147
2024-01-21 06:30:58,218 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35007
2024-01-21 06:30:58,218 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-797449a0-8e7b-44e3-a1c3-ccbdbd7326c9
2024-01-21 06:30:58,218 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,218 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ec322fc-ccb5-440e-bd74-b8ce9c01c03c
2024-01-21 06:30:58,218 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:30:58,218 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:30:58,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ngw6sja8
2024-01-21 06:30:58,219 - distributed.worker - INFO - Starting Worker plugin RMMSetup-532981a4-e5d9-4a00-9eea-eaf854d8dc52
2024-01-21 06:30:58,219 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7eb4b88b-59a8-47c7-ac54-b8ccaf5eea70
2024-01-21 06:30:58,520 - distributed.worker - INFO - Starting Worker plugin PreImport-89389d5c-95e5-4045-8366-374405a13161
2024-01-21 06:30:58,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36723', status: init, memory: 0, processing: 0>
2024-01-21 06:30:58,547 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36723
2024-01-21 06:30:58,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54802
2024-01-21 06:30:58,547 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:30:58,548 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:30:58,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:30:58,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdcce352-607e-4962-b687-25e1a9a15f7a
2024-01-21 06:31:00,546 - distributed.worker - INFO - Starting Worker plugin PreImport-aa90adb2-aa6a-489a-be88-bdd345bd9360
2024-01-21 06:31:00,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,563 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6f80619-ec40-4686-b9ce-9b9eca5e8f02
2024-01-21 06:31:00,564 - distributed.worker - INFO - Starting Worker plugin PreImport-19fd47b8-0509-4d43-9241-2b4f04f2fc68
2024-01-21 06:31:00,564 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,567 - distributed.worker - INFO - Starting Worker plugin PreImport-f78df494-0a65-4af7-acde-0b2d9f338266
2024-01-21 06:31:00,567 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,575 - distributed.worker - INFO - Starting Worker plugin PreImport-f4ef8450-a612-43a5-afe3-a2ea07715251
2024-01-21 06:31:00,577 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d25126a4-e689-4dee-ab6e-6ce26c1bf645
2024-01-21 06:31:00,582 - distributed.worker - INFO - Starting Worker plugin PreImport-84c9e6f6-45d2-41cf-9c90-2ddb6e70693e
2024-01-21 06:31:00,583 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43517', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43517
2024-01-21 06:31:00,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55106
2024-01-21 06:31:00,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,587 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,587 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,589 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43147', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43147
2024-01-21 06:31:00,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55116
2024-01-21 06:31:00,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,589 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,591 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39123', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,591 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39123
2024-01-21 06:31:00,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55128
2024-01-21 06:31:00,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,607 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33495', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33495
2024-01-21 06:31:00,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55144
2024-01-21 06:31:00,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,609 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,609 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,616 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46095', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,616 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46095
2024-01-21 06:31:00,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55138
2024-01-21 06:31:00,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,619 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,622 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46039', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46039
2024-01-21 06:31:00,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55152
2024-01-21 06:31:00,627 - distributed.worker - INFO - Starting Worker plugin PreImport-6c6b59b0-8046-42b6-8a71-9ffa136d0d05
2024-01-21 06:31:00,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,630 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,630 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,662 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35349', status: init, memory: 0, processing: 0>
2024-01-21 06:31:00,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35349
2024-01-21 06:31:00,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55156
2024-01-21 06:31:00,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:00,665 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:00,666 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:00,668 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:00,717 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,717 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,717 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,717 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,718 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,718 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,718 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,718 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:00,723 - distributed.scheduler - INFO - Remove client Client-a0b7eed8-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:00,723 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54774; closing.
2024-01-21 06:31:00,723 - distributed.scheduler - INFO - Remove client Client-a0b7eed8-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:00,723 - distributed.scheduler - INFO - Close client connection: Client-a0b7eed8-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:00,724 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45569'. Reason: nanny-close
2024-01-21 06:31:00,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33189'. Reason: nanny-close
2024-01-21 06:31:00,726 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34281'. Reason: nanny-close
2024-01-21 06:31:00,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36723. Reason: nanny-close
2024-01-21 06:31:00,726 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37983'. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35349. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43009'. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43147. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46105'. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43517. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32905'. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33963'. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46039. Reason: nanny-close
2024-01-21 06:31:00,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,729 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:00,729 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54802; closing.
2024-01-21 06:31:00,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46095. Reason: nanny-close
2024-01-21 06:31:00,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39123. Reason: nanny-close
2024-01-21 06:31:00,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,729 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36723', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7293258')
2024-01-21 06:31:00,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33495. Reason: nanny-close
2024-01-21 06:31:00,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,730 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55116; closing.
2024-01-21 06:31:00,730 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7314487')
2024-01-21 06:31:00,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,731 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,732 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55156; closing.
2024-01-21 06:31:00,732 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:00,732 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55106; closing.
2024-01-21 06:31:00,733 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,733 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7332714')
2024-01-21 06:31:00,733 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,733 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55152; closing.
2024-01-21 06:31:00,733 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:00,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55128; closing.
2024-01-21 06:31:00,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7344387')
2024-01-21 06:31:00,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7347758')
2024-01-21 06:31:00,735 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39123', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7350526')
2024-01-21 06:31:00,735 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55138; closing.
2024-01-21 06:31:00,735 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55144; closing.
2024-01-21 06:31:00,736 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.736042')
2024-01-21 06:31:00,736 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818660.7365243')
2024-01-21 06:31:00,736 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:01,841 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:01,841 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:01,842 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:01,843 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:01,844 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-21 06:31:03,998 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:04,003 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:31:04,006 - distributed.scheduler - INFO - State start
2024-01-21 06:31:04,026 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:04,027 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:04,028 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:31:04,028 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:04,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46327'
2024-01-21 06:31:04,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41749'
2024-01-21 06:31:04,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36417'
2024-01-21 06:31:04,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32845'
2024-01-21 06:31:04,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45373'
2024-01-21 06:31:04,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39213'
2024-01-21 06:31:04,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36497'
2024-01-21 06:31:04,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42777'
2024-01-21 06:31:04,946 - distributed.scheduler - INFO - Receive client connection: Client-a5b5c7d6-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:04,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55290
2024-01-21 06:31:06,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,039 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40023
2024-01-21 06:31:06,039 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40023
2024-01-21 06:31:06,039 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34843
2024-01-21 06:31:06,039 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38053
2024-01-21 06:31:06,039 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34843
2024-01-21 06:31:06,039 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,039 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46297
2024-01-21 06:31:06,039 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,039 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,039 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,039 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,039 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,039 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,039 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ma0up5q
2024-01-21 06:31:06,039 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,039 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6tl_ab5x
2024-01-21 06:31:06,039 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86a0ec58-7c47-4102-a6b8-1cc5625b492c
2024-01-21 06:31:06,039 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43a75a4e-841a-4fdb-9112-f54b26cfe5e7
2024-01-21 06:31:06,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,039 - distributed.worker - INFO - Starting Worker plugin RMMSetup-074e9455-09d3-4a27-953e-f8fadcefe0e7
2024-01-21 06:31:06,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39143
2024-01-21 06:31:06,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39143
2024-01-21 06:31:06,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34669
2024-01-21 06:31:06,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,040 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,040 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,041 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,041 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ucdd_bi_
2024-01-21 06:31:06,041 - distributed.worker - INFO - Starting Worker plugin PreImport-aeee36f8-b286-4f21-85a2-f7a2c83fe2f2
2024-01-21 06:31:06,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd1974b6-a3af-4515-b118-baadc0eca7be
2024-01-21 06:31:06,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-116b0dc0-984f-484f-b0e8-d8f04fa6eef9
2024-01-21 06:31:06,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38337
2024-01-21 06:31:06,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38337
2024-01-21 06:31:06,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44201
2024-01-21 06:31:06,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,084 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,084 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-afclwaf5
2024-01-21 06:31:06,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44483
2024-01-21 06:31:06,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44483
2024-01-21 06:31:06,084 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30860d4c-40b4-4b8f-907b-ac2924eea475
2024-01-21 06:31:06,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36065
2024-01-21 06:31:06,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,084 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,085 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ypkjp_m9
2024-01-21 06:31:06,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ecec7e1-c87e-42f2-a7a5-492a064bec0b
2024-01-21 06:31:06,087 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40499
2024-01-21 06:31:06,088 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40499
2024-01-21 06:31:06,088 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35835
2024-01-21 06:31:06,088 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,089 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fs3n58jd
2024-01-21 06:31:06,089 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7dcc9c5-318e-44ce-9f1d-24aac83b719a
2024-01-21 06:31:06,117 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,122 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36491
2024-01-21 06:31:06,122 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36491
2024-01-21 06:31:06,122 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36725
2024-01-21 06:31:06,122 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,122 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,122 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,122 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,122 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vij33kx9
2024-01-21 06:31:06,123 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4776773b-9d32-4a3c-bb34-87239b331600
2024-01-21 06:31:06,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:06,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:06,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:06,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44973
2024-01-21 06:31:06,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44973
2024-01-21 06:31:06,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39147
2024-01-21 06:31:06,171 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:06,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:06,171 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:06,172 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:06,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8hc9gp_y
2024-01-21 06:31:06,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04560172-3daf-4779-8502-8b936607c618
2024-01-21 06:31:08,015 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-634b8cf5-e7ad-479d-abf4-2f8c957cae68
2024-01-21 06:31:08,016 - distributed.worker - INFO - Starting Worker plugin PreImport-dbd7d2f9-9764-4bed-9799-d972d7b17046
2024-01-21 06:31:08,016 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,039 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34843', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,043 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34843
2024-01-21 06:31:08,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55316
2024-01-21 06:31:08,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,045 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,087 - distributed.worker - INFO - Starting Worker plugin PreImport-43e90021-e9bc-4309-87cd-2bbf91d269af
2024-01-21 06:31:08,087 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40023', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40023
2024-01-21 06:31:08,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55324
2024-01-21 06:31:08,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,113 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,131 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,171 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39143', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39143
2024-01-21 06:31:08,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55328
2024-01-21 06:31:08,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,174 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,231 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8cc94075-803e-4f55-b9de-e87ebb395758
2024-01-21 06:31:08,232 - distributed.worker - INFO - Starting Worker plugin PreImport-0e9653da-2842-4f58-968a-f672b90ecd35
2024-01-21 06:31:08,233 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,265 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44483', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,265 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44483
2024-01-21 06:31:08,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55336
2024-01-21 06:31:08,267 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,267 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,267 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19cfa4e3-1575-4421-b3de-a98dbc2fb285
2024-01-21 06:31:08,310 - distributed.worker - INFO - Starting Worker plugin PreImport-8ee78fee-f393-4f4d-bfa1-150e7aad4136
2024-01-21 06:31:08,310 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,316 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dc473c2-0ecb-4ea7-8aaf-20a69140fb29
2024-01-21 06:31:08,317 - distributed.worker - INFO - Starting Worker plugin PreImport-a59555e9-23b9-427b-b615-e1340f9d5b93
2024-01-21 06:31:08,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7cfb46bf-c045-480d-8f01-1a12dd68628a
2024-01-21 06:31:08,320 - distributed.worker - INFO - Starting Worker plugin PreImport-11df05f5-3eac-4b15-ba50-c5eef0c63694
2024-01-21 06:31:08,321 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40499', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40499
2024-01-21 06:31:08,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55340
2024-01-21 06:31:08,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,338 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,338 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ffbd25e-2ea3-40bd-80ae-4e62cae6da95
2024-01-21 06:31:08,341 - distributed.worker - INFO - Starting Worker plugin PreImport-ed019f2a-349c-4748-866b-9fd15390496a
2024-01-21 06:31:08,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,346 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36491', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,347 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36491
2024-01-21 06:31:08,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55354
2024-01-21 06:31:08,348 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,349 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,354 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38337', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38337
2024-01-21 06:31:08,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55344
2024-01-21 06:31:08,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,357 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,357 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,375 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44973', status: init, memory: 0, processing: 0>
2024-01-21 06:31:08,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44973
2024-01-21 06:31:08,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55368
2024-01-21 06:31:08,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:08,378 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:08,378 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:08,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:08,432 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,432 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,432 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,432 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,432 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,433 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,433 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,433 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:08,437 - distributed.scheduler - INFO - Remove client Client-a5b5c7d6-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:08,437 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55290; closing.
2024-01-21 06:31:08,438 - distributed.scheduler - INFO - Remove client Client-a5b5c7d6-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:08,438 - distributed.scheduler - INFO - Close client connection: Client-a5b5c7d6-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:08,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46327'. Reason: nanny-close
2024-01-21 06:31:08,439 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41749'. Reason: nanny-close
2024-01-21 06:31:08,440 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36417'. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39143. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32845'. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38337. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45373'. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40023. Reason: nanny-close
2024-01-21 06:31:08,441 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39213'. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34843. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36497'. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44483. Reason: nanny-close
2024-01-21 06:31:08,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42777'. Reason: nanny-close
2024-01-21 06:31:08,443 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:08,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44973. Reason: nanny-close
2024-01-21 06:31:08,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40499. Reason: nanny-close
2024-01-21 06:31:08,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,443 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55324; closing.
2024-01-21 06:31:08,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36491. Reason: nanny-close
2024-01-21 06:31:08,444 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.444032')
2024-01-21 06:31:08,444 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55344; closing.
2024-01-21 06:31:08,444 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55328; closing.
2024-01-21 06:31:08,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,445 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:08,445 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55336; closing.
2024-01-21 06:31:08,445 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.4456725')
2024-01-21 06:31:08,445 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,446 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.446047')
2024-01-21 06:31:08,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55316; closing.
2024-01-21 06:31:08,446 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,446 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,446 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,446 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:08,447 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44483', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.447174')
2024-01-21 06:31:08,447 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34843', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.447584')
2024-01-21 06:31:08,448 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55344>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-21 06:31:08,450 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55328>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-21 06:31:08,450 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55336>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55336>: Stream is closed
2024-01-21 06:31:08,450 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55368; closing.
2024-01-21 06:31:08,451 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55340; closing.
2024-01-21 06:31:08,451 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55354; closing.
2024-01-21 06:31:08,451 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.4514577')
2024-01-21 06:31:08,451 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40499', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.4518297')
2024-01-21 06:31:08,452 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818668.4521701')
2024-01-21 06:31:08,452 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:10,457 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:10,457 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:10,457 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:10,459 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:10,459 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-21 06:31:12,878 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:12,882 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:31:12,886 - distributed.scheduler - INFO - State start
2024-01-21 06:31:12,909 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:12,910 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:12,910 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:31:12,910 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:13,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42347'
2024-01-21 06:31:13,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35917'
2024-01-21 06:31:13,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41617'
2024-01-21 06:31:13,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45155'
2024-01-21 06:31:13,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37213'
2024-01-21 06:31:13,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41737'
2024-01-21 06:31:13,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38295'
2024-01-21 06:31:13,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44909'
2024-01-21 06:31:14,391 - distributed.scheduler - INFO - Receive client connection: Client-aadb8a42-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:14,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52024
2024-01-21 06:31:14,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:14,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:14,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:14,990 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38349
2024-01-21 06:31:14,990 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38349
2024-01-21 06:31:14,990 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33449
2024-01-21 06:31:14,990 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:14,990 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:14,991 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:14,991 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:14,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dj47dsxy
2024-01-21 06:31:14,991 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c7b2bdd-2745-403c-8ddb-0e6f3c2e5f32
2024-01-21 06:31:15,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,010 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,011 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33093
2024-01-21 06:31:15,011 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33093
2024-01-21 06:31:15,011 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34637
2024-01-21 06:31:15,011 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,011 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,011 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,011 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,011 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-11pgj3pz
2024-01-21 06:31:15,012 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcce1c60-83c1-47c5-9d5b-17bf68442bf2
2024-01-21 06:31:15,012 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a1d1b0d-7a91-4dc2-a38b-f75ded296bc2
2024-01-21 06:31:15,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,043 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,048 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36527
2024-01-21 06:31:15,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36527
2024-01-21 06:31:15,049 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36699
2024-01-21 06:31:15,049 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,049 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,049 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,049 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,049 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_vkpd1v
2024-01-21 06:31:15,049 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9ae5b44-6033-4cda-b5b3-945ba0775cb9
2024-01-21 06:31:15,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,051 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d48c5112-5559-4e12-8111-ce7b0c4f1b31
2024-01-21 06:31:15,052 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39141
2024-01-21 06:31:15,052 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39141
2024-01-21 06:31:15,052 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40707
2024-01-21 06:31:15,052 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,052 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,052 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,052 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,052 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rz0ds21o
2024-01-21 06:31:15,052 - distributed.worker - INFO - Starting Worker plugin PreImport-1028e47c-5f1a-4805-bba2-c8dd6c5392d2
2024-01-21 06:31:15,053 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-491fb6c0-9fcb-457c-9a71-5e72bb5c82e2
2024-01-21 06:31:15,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-449e3155-c5a3-4f79-804b-519f670315b0
2024-01-21 06:31:15,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,263 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46297
2024-01-21 06:31:15,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46297
2024-01-21 06:31:15,264 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42069
2024-01-21 06:31:15,264 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,264 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g5oou__r
2024-01-21 06:31:15,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35385
2024-01-21 06:31:15,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2b6111d-c80f-4725-bfa7-83af1ba0e4ce
2024-01-21 06:31:15,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35385
2024-01-21 06:31:15,265 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43357
2024-01-21 06:31:15,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,265 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,265 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,265 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ai41d23l
2024-01-21 06:31:15,265 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c19a120e-337f-4dde-ae25-19939901e5ab
2024-01-21 06:31:15,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:15,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:15,270 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,271 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40191
2024-01-21 06:31:15,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40191
2024-01-21 06:31:15,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44337
2024-01-21 06:31:15,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,271 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,271 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,271 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a4r3q99d
2024-01-21 06:31:15,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-819e1f9e-f778-4079-a25d-b47d9040a840
2024-01-21 06:31:15,272 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0bae8947-cc46-4ba0-a879-2aa8d6e3d643
2024-01-21 06:31:15,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:15,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39485
2024-01-21 06:31:15,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39485
2024-01-21 06:31:15,274 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42867
2024-01-21 06:31:15,274 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:15,274 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:15,274 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:15,274 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:15,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3raukkyj
2024-01-21 06:31:15,274 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54aea540-2222-4ea1-97eb-f90eb0378ee8
2024-01-21 06:31:16,997 - distributed.worker - INFO - Starting Worker plugin PreImport-352c9fd0-c74a-4845-8689-a2d63dae883d
2024-01-21 06:31:16,997 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32649b5b-471f-4b9f-8d0c-f6caae89161b
2024-01-21 06:31:16,998 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,038 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38349', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38349
2024-01-21 06:31:17,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52040
2024-01-21 06:31:17,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,042 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,042 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,132 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,154 - distributed.worker - INFO - Starting Worker plugin PreImport-7ac94aa2-e0b0-42d8-a7c4-17cdc46ae336
2024-01-21 06:31:17,156 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,158 - distributed.worker - INFO - Starting Worker plugin PreImport-9511cac0-a2ce-437c-a4cd-a094e69cbbf6
2024-01-21 06:31:17,159 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,164 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39141', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,165 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39141
2024-01-21 06:31:17,165 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52052
2024-01-21 06:31:17,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,168 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,168 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,188 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33093', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33093
2024-01-21 06:31:17,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52060
2024-01-21 06:31:17,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,191 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,191 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36527', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,195 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36527
2024-01-21 06:31:17,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52070
2024-01-21 06:31:17,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,197 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,197 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b5a7ec8-4f91-4f43-8236-d46e5129f1cc
2024-01-21 06:31:17,210 - distributed.worker - INFO - Starting Worker plugin PreImport-44c19edd-5173-4e47-aeb9-5a0267e2e75f
2024-01-21 06:31:17,211 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,243 - distributed.worker - INFO - Starting Worker plugin PreImport-0a287554-bff6-4fd5-a896-c5a3e660097f
2024-01-21 06:31:17,244 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,250 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9e4eff9-8d21-4ec5-b236-eda2a07c7044
2024-01-21 06:31:17,250 - distributed.worker - INFO - Starting Worker plugin PreImport-e9368ce9-a4e8-4d72-8fd2-39d084cf83c1
2024-01-21 06:31:17,251 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,251 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46297', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,252 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46297
2024-01-21 06:31:17,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52074
2024-01-21 06:31:17,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,253 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b5611a3-070b-449e-a64d-8b36c49c7fbf
2024-01-21 06:31:17,254 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,254 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,254 - distributed.worker - INFO - Starting Worker plugin PreImport-fd4773a5-ef8c-4eb9-9425-3b4a66a2e0d0
2024-01-21 06:31:17,255 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40191', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40191
2024-01-21 06:31:17,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52076
2024-01-21 06:31:17,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,275 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35385', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35385
2024-01-21 06:31:17,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52082
2024-01-21 06:31:17,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,282 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,282 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,285 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39485', status: init, memory: 0, processing: 0>
2024-01-21 06:31:17,286 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39485
2024-01-21 06:31:17,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52098
2024-01-21 06:31:17,286 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:17,287 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:17,287 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:17,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:17,309 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,309 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,310 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,310 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,310 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,310 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,310 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,311 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:17,320 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,320 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:17,329 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:17,331 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:17,333 - distributed.scheduler - INFO - Remove client Client-aadb8a42-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:17,333 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52024; closing.
2024-01-21 06:31:17,333 - distributed.scheduler - INFO - Remove client Client-aadb8a42-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:17,334 - distributed.scheduler - INFO - Close client connection: Client-aadb8a42-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:17,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42347'. Reason: nanny-close
2024-01-21 06:31:17,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,336 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35917'. Reason: nanny-close
2024-01-21 06:31:17,336 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,337 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38349. Reason: nanny-close
2024-01-21 06:31:17,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41617'. Reason: nanny-close
2024-01-21 06:31:17,337 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45155'. Reason: nanny-close
2024-01-21 06:31:17,337 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33093. Reason: nanny-close
2024-01-21 06:31:17,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,338 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46297. Reason: nanny-close
2024-01-21 06:31:17,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37213'. Reason: nanny-close
2024-01-21 06:31:17,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35385. Reason: nanny-close
2024-01-21 06:31:17,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41737'. Reason: nanny-close
2024-01-21 06:31:17,339 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,339 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38295'. Reason: nanny-close
2024-01-21 06:31:17,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52040; closing.
2024-01-21 06:31:17,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39141. Reason: nanny-close
2024-01-21 06:31:17,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.3400924')
2024-01-21 06:31:17,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44909'. Reason: nanny-close
2024-01-21 06:31:17,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36527. Reason: nanny-close
2024-01-21 06:31:17,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:17,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40191. Reason: nanny-close
2024-01-21 06:31:17,341 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52082; closing.
2024-01-21 06:31:17,342 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,342 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39485. Reason: nanny-close
2024-01-21 06:31:17,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52074; closing.
2024-01-21 06:31:17,342 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52060; closing.
2024-01-21 06:31:17,342 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.342902')
2024-01-21 06:31:17,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,343 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46297', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.343342')
2024-01-21 06:31:17,343 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33093', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.343703')
2024-01-21 06:31:17,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:17,344 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52052; closing.
2024-01-21 06:31:17,345 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52070; closing.
2024-01-21 06:31:17,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52076; closing.
2024-01-21 06:31:17,345 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39141', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.3457289')
2024-01-21 06:31:17,346 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.3460975')
2024-01-21 06:31:17,346 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:17,346 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.3465111')
2024-01-21 06:31:17,347 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52098; closing.
2024-01-21 06:31:17,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818677.347361')
2024-01-21 06:31:17,347 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:18,351 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:18,351 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:18,352 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:18,353 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:18,353 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-21 06:31:20,303 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:20,307 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34621 instead
  warnings.warn(
2024-01-21 06:31:20,311 - distributed.scheduler - INFO - State start
2024-01-21 06:31:20,332 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:20,333 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:20,334 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34621/status
2024-01-21 06:31:20,334 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:20,360 - distributed.scheduler - INFO - Receive client connection: Client-b0338449-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:20,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33922
2024-01-21 06:31:20,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41857'
2024-01-21 06:31:20,839 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43647'
2024-01-21 06:31:20,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34067'
2024-01-21 06:31:20,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45899'
2024-01-21 06:31:20,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32985'
2024-01-21 06:31:20,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34955'
2024-01-21 06:31:20,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42573'
2024-01-21 06:31:20,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45681'
2024-01-21 06:31:21,749 - distributed.scheduler - INFO - Receive client connection: Client-af7d8239-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:21,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34010
2024-01-21 06:31:22,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,573 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46165
2024-01-21 06:31:22,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46165
2024-01-21 06:31:22,573 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42653
2024-01-21 06:31:22,573 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,573 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,573 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,574 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,574 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2f3qp6yh
2024-01-21 06:31:22,574 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b761cb7-1890-4aa8-b3e3-3831012f57b3
2024-01-21 06:31:22,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,625 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38757
2024-01-21 06:31:22,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38757
2024-01-21 06:31:22,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43653
2024-01-21 06:31:22,626 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,626 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,626 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8lywoblk
2024-01-21 06:31:22,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13de15a0-bb35-41f7-8a02-960e3cd47e74
2024-01-21 06:31:22,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ac234f1-f1b4-47f5-9602-0cdd5c3a3266
2024-01-21 06:31:22,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44221
2024-01-21 06:31:22,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44221
2024-01-21 06:31:22,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40117
2024-01-21 06:31:22,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,833 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,833 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xsrhj61r
2024-01-21 06:31:22,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45fd20eb-4020-4231-a276-533a6769a250
2024-01-21 06:31:22,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,836 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39205
2024-01-21 06:31:22,837 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39205
2024-01-21 06:31:22,837 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36345
2024-01-21 06:31:22,837 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,837 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,837 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,837 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,837 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e8v585n4
2024-01-21 06:31:22,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9317baa4-aae9-4ba3-b81a-b1446848357a
2024-01-21 06:31:22,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33911
2024-01-21 06:31:22,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44163
2024-01-21 06:31:22,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33911
2024-01-21 06:31:22,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44163
2024-01-21 06:31:22,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45347
2024-01-21 06:31:22,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34245
2024-01-21 06:31:22,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,841 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,841 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,841 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,841 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,841 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tnx0o88d
2024-01-21 06:31:22,841 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5we30zz8
2024-01-21 06:31:22,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,841 - distributed.worker - INFO - Starting Worker plugin PreImport-159520e1-420f-4b68-b917-31c9112078fb
2024-01-21 06:31:22,841 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-711e5694-d171-43c3-a4cd-ea47ed18bd5a
2024-01-21 06:31:22,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04688a85-381b-4e54-b880-eb6b0f64d074
2024-01-21 06:31:22,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4bd8bba0-a32d-4e98-809d-604b473bd6d8
2024-01-21 06:31:22,842 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33801
2024-01-21 06:31:22,842 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33801
2024-01-21 06:31:22,842 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37669
2024-01-21 06:31:22,842 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,842 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,842 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,842 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u66kat4x
2024-01-21 06:31:22,843 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b66032c6-1681-4a02-af70-bbafeee93274
2024-01-21 06:31:22,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-724980aa-ff30-4d6a-9e63-3c24d4acbc5a
2024-01-21 06:31:22,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d41415f-e4ab-479f-8f76-6de4ab81afe6
2024-01-21 06:31:22,858 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:22,859 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:22,863 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:22,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33959
2024-01-21 06:31:22,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33959
2024-01-21 06:31:22,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35257
2024-01-21 06:31:22,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:22,864 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:22,864 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:22,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:22,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ye91kk9p
2024-01-21 06:31:22,864 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9f0f6d6-76a5-4f5f-8474-a1a5e92648f0
2024-01-21 06:31:23,261 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4b0984a-25b6-4915-9ca5-82a1f34f86e5
2024-01-21 06:31:23,261 - distributed.worker - INFO - Starting Worker plugin PreImport-39f0dc6e-6d6f-4237-8da7-0caaeefb5b6d
2024-01-21 06:31:23,261 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:23,289 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46165', status: init, memory: 0, processing: 0>
2024-01-21 06:31:23,290 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46165
2024-01-21 06:31:23,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34110
2024-01-21 06:31:23,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:23,292 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:23,292 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:23,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:24,912 - distributed.worker - INFO - Starting Worker plugin PreImport-3762f637-085b-4cb8-97dd-b47364510d56
2024-01-21 06:31:24,914 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,945 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38757', status: init, memory: 0, processing: 0>
2024-01-21 06:31:24,945 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38757
2024-01-21 06:31:24,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34134
2024-01-21 06:31:24,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:24,948 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,948 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b8d3d060-0612-4e73-838c-f5c5f7a2f745
2024-01-21 06:31:25,009 - distributed.worker - INFO - Starting Worker plugin PreImport-9f95458f-04f0-45d3-8606-421a5437dffd
2024-01-21 06:31:25,010 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,014 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,021 - distributed.worker - INFO - Starting Worker plugin PreImport-4d2d6803-5cc2-42ea-8de7-927492334890
2024-01-21 06:31:25,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb22f6da-d32a-44b9-bff6-830d63937ce3
2024-01-21 06:31:25,023 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,025 - distributed.worker - INFO - Starting Worker plugin PreImport-d52e4018-f370-439e-afaa-eeebd92fe5ba
2024-01-21 06:31:25,026 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d31a028-1456-49b8-ac68-e9047c4aedd5
2024-01-21 06:31:25,027 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,027 - distributed.worker - INFO - Starting Worker plugin PreImport-43b50bea-3750-4fc3-b9b4-fdeac4f977a7
2024-01-21 06:31:25,028 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,028 - distributed.worker - INFO - Starting Worker plugin PreImport-47a1281c-c070-400a-849f-b673cf6ceb79
2024-01-21 06:31:25,029 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,033 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39205', status: init, memory: 0, processing: 0>
2024-01-21 06:31:25,034 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39205
2024-01-21 06:31:25,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34136
2024-01-21 06:31:25,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:25,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:25,036 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,049 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33911', status: init, memory: 0, processing: 0>
2024-01-21 06:31:25,049 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33911
2024-01-21 06:31:25,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34152
2024-01-21 06:31:25,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:25,052 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:25,052 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,054 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33959', status: init, memory: 0, processing: 0>
2024-01-21 06:31:25,055 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33959
2024-01-21 06:31:25,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34172
2024-01-21 06:31:25,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:25,056 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33801', status: init, memory: 0, processing: 0>
2024-01-21 06:31:25,056 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33801
2024-01-21 06:31:25,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34178
2024-01-21 06:31:25,057 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:25,057 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,057 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:25,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,058 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:25,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,064 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44221', status: init, memory: 0, processing: 0>
2024-01-21 06:31:25,065 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44221
2024-01-21 06:31:25,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34158
2024-01-21 06:31:25,065 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44163', status: init, memory: 0, processing: 0>
2024-01-21 06:31:25,066 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44163
2024-01-21 06:31:25,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34180
2024-01-21 06:31:25,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:25,067 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:25,068 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:25,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:25,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:25,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:25,116 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,116 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,116 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:25,121 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,121 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,121 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,121 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,122 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,122 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,122 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,122 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,127 - distributed.scheduler - INFO - Remove client Client-b0338449-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:25,128 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33922; closing.
2024-01-21 06:31:25,128 - distributed.scheduler - INFO - Remove client Client-b0338449-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:25,128 - distributed.scheduler - INFO - Close client connection: Client-b0338449-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:25,137 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,137 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,137 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,138 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,138 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,138 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,138 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,138 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:31:25,145 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,147 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:25,149 - distributed.scheduler - INFO - Remove client Client-af7d8239-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:25,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34010; closing.
2024-01-21 06:31:25,150 - distributed.scheduler - INFO - Remove client Client-af7d8239-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:25,151 - distributed.scheduler - INFO - Close client connection: Client-af7d8239-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:25,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41857'. Reason: nanny-close
2024-01-21 06:31:25,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43647'. Reason: nanny-close
2024-01-21 06:31:25,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34067'. Reason: nanny-close
2024-01-21 06:31:25,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44221. Reason: nanny-close
2024-01-21 06:31:25,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45899'. Reason: nanny-close
2024-01-21 06:31:25,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38757. Reason: nanny-close
2024-01-21 06:31:25,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32985'. Reason: nanny-close
2024-01-21 06:31:25,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39205. Reason: nanny-close
2024-01-21 06:31:25,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,155 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46165. Reason: nanny-close
2024-01-21 06:31:25,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34955'. Reason: nanny-close
2024-01-21 06:31:25,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42573'. Reason: nanny-close
2024-01-21 06:31:25,155 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33911. Reason: nanny-close
2024-01-21 06:31:25,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34158; closing.
2024-01-21 06:31:25,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45681'. Reason: nanny-close
2024-01-21 06:31:25,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44163. Reason: nanny-close
2024-01-21 06:31:25,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:25,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.1564965')
2024-01-21 06:31:25,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33801. Reason: nanny-close
2024-01-21 06:31:25,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,157 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33959. Reason: nanny-close
2024-01-21 06:31:25,157 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34134; closing.
2024-01-21 06:31:25,157 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,158 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,158 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,158 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,158 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.1583745')
2024-01-21 06:31:25,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34136; closing.
2024-01-21 06:31:25,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34110; closing.
2024-01-21 06:31:25,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,159 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:25,159 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,159 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39205', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.1598608')
2024-01-21 06:31:25,160 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,160 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.1602423')
2024-01-21 06:31:25,160 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34152; closing.
2024-01-21 06:31:25,160 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,161 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:25,161 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33911', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.16116')
2024-01-21 06:31:25,161 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34180; closing.
2024-01-21 06:31:25,161 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34178; closing.
2024-01-21 06:31:25,162 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44163', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.1622043')
2024-01-21 06:31:25,162 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.162589')
2024-01-21 06:31:25,162 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34172; closing.
2024-01-21 06:31:25,163 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33959', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818685.163306')
2024-01-21 06:31:25,163 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:27,069 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:27,069 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:27,070 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:27,072 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:27,073 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-21 06:31:29,273 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:29,278 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39905 instead
  warnings.warn(
2024-01-21 06:31:29,282 - distributed.scheduler - INFO - State start
2024-01-21 06:31:29,304 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:29,305 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:29,306 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39905/status
2024-01-21 06:31:29,306 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:29,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43495'
2024-01-21 06:31:29,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40987'
2024-01-21 06:31:29,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41225'
2024-01-21 06:31:29,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42237'
2024-01-21 06:31:29,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38835'
2024-01-21 06:31:29,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39615'
2024-01-21 06:31:29,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41059'
2024-01-21 06:31:29,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35689'
2024-01-21 06:31:29,761 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39969', status: init, memory: 0, processing: 0>
2024-01-21 06:31:29,780 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39969
2024-01-21 06:31:29,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35014
2024-01-21 06:31:29,831 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35014; closing.
2024-01-21 06:31:29,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818689.8319228')
2024-01-21 06:31:29,832 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:31,231 - distributed.scheduler - INFO - Receive client connection: Client-b4b909fc-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:31,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50830
2024-01-21 06:31:31,376 - distributed.scheduler - INFO - Receive client connection: Client-b7023699-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:31,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50840
2024-01-21 06:31:31,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,386 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,389 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35791
2024-01-21 06:31:31,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35791
2024-01-21 06:31:31,389 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35029
2024-01-21 06:31:31,389 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,389 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,389 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l6_746e7
2024-01-21 06:31:31,389 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a71ccc3c-7f89-4697-988f-00886adf7b3a
2024-01-21 06:31:31,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11ab6bf7-59b6-46b6-81bc-c3a36630811a
2024-01-21 06:31:31,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44285
2024-01-21 06:31:31,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44285
2024-01-21 06:31:31,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37499
2024-01-21 06:31:31,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,392 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,392 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m7lem5rp
2024-01-21 06:31:31,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3b42c6e-3fb8-4376-9958-dc3bc1d43ed1
2024-01-21 06:31:31,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,455 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38875
2024-01-21 06:31:31,455 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38875
2024-01-21 06:31:31,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37481
2024-01-21 06:31:31,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,456 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,456 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,456 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,456 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bk1qgft5
2024-01-21 06:31:31,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-961f0031-1f4f-4255-8fa8-0d1143fce017
2024-01-21 06:31:31,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46237
2024-01-21 06:31:31,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46237
2024-01-21 06:31:31,619 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45403
2024-01-21 06:31:31,619 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,619 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,620 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ty830_2r
2024-01-21 06:31:31,620 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42455
2024-01-21 06:31:31,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42455
2024-01-21 06:31:31,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45013
2024-01-21 06:31:31,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,620 - distributed.worker - INFO - Starting Worker plugin PreImport-ce5b8550-d476-4270-95c0-634ce0e1bfdb
2024-01-21 06:31:31,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,620 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,620 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f3b07cd-ab81-4416-84b6-818f71edd1bf
2024-01-21 06:31:31,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zb8jjpiv
2024-01-21 06:31:31,620 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17c33f93-cd9d-4565-af00-98c0b838ca84
2024-01-21 06:31:31,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b457e437-e41e-442e-8148-efed4172f414
2024-01-21 06:31:31,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af9a7af5-4798-4c79-8e79-5b1850a1824d
2024-01-21 06:31:31,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,816 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:31,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:31,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45487
2024-01-21 06:31:31,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45487
2024-01-21 06:31:31,826 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41057
2024-01-21 06:31:31,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,826 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m9ran_g8
2024-01-21 06:31:31,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6075b9aa-2210-420c-9002-fa687405e3e3
2024-01-21 06:31:31,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3c24a92-2435-4997-904a-0743024be639
2024-01-21 06:31:31,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44447
2024-01-21 06:31:31,829 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44447
2024-01-21 06:31:31,829 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36283
2024-01-21 06:31:31,829 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,829 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,829 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,829 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f39b0fmb
2024-01-21 06:31:31,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae22bdfb-b7f0-4c7a-b973-2af6e400c878
2024-01-21 06:31:31,830 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:31,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45095
2024-01-21 06:31:31,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45095
2024-01-21 06:31:31,831 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33787
2024-01-21 06:31:31,831 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:31,831 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:31,831 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:31,831 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:31,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ap5vz6ga
2024-01-21 06:31:31,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd39a2e3-683e-42f3-b7b2-22ee17b0e1d6
2024-01-21 06:31:33,217 - distributed.worker - INFO - Starting Worker plugin PreImport-0a03b4fc-6878-452a-8ea5-6214eba4a03c
2024-01-21 06:31:33,217 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35791', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,248 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35791
2024-01-21 06:31:33,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50854
2024-01-21 06:31:33,249 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,249 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,250 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,301 - distributed.worker - INFO - Starting Worker plugin PreImport-69c067e1-6275-40e1-ad24-b9fed23dfdc3
2024-01-21 06:31:33,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-041449c0-a549-42d8-b9f5-4692738ddf77
2024-01-21 06:31:33,303 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44285', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,343 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44285
2024-01-21 06:31:33,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50870
2024-01-21 06:31:33,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,346 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98760503-0fb4-415c-9f79-5904a76d72ae
2024-01-21 06:31:33,462 - distributed.worker - INFO - Starting Worker plugin PreImport-cae31db1-a5cf-4968-a0b8-0f872cde8431
2024-01-21 06:31:33,462 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,488 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38875', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,488 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38875
2024-01-21 06:31:33,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50878
2024-01-21 06:31:33,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,521 - distributed.worker - INFO - Starting Worker plugin PreImport-80bd0e95-07f9-4c74-8258-9f41c2ea489d
2024-01-21 06:31:33,523 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,547 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b85b7971-b65f-47b7-98b4-0d54e0ee116a
2024-01-21 06:31:33,548 - distributed.worker - INFO - Starting Worker plugin PreImport-38d4c6ae-7893-4cb7-afa8-6ce828f23f1f
2024-01-21 06:31:33,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,556 - distributed.worker - INFO - Starting Worker plugin PreImport-c5b0f986-db06-4ed7-a95b-f329d411cede
2024-01-21 06:31:33,556 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,558 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42455', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,558 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42455
2024-01-21 06:31:33,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50886
2024-01-21 06:31:33,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,561 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,561 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,562 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7014cb6-4460-405d-9cc5-9d8f97b7f4a4
2024-01-21 06:31:33,565 - distributed.worker - INFO - Starting Worker plugin PreImport-00b48fbf-67aa-4536-92b3-8477ed34897d
2024-01-21 06:31:33,565 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,575 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45095', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,576 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45095
2024-01-21 06:31:33,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50902
2024-01-21 06:31:33,577 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,577 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,578 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45487', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45487
2024-01-21 06:31:33,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50906
2024-01-21 06:31:33,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,586 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,589 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44447', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,590 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44447
2024-01-21 06:31:33,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50994
2024-01-21 06:31:33,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,591 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46237', status: init, memory: 0, processing: 0>
2024-01-21 06:31:33,597 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46237
2024-01-21 06:31:33,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50988
2024-01-21 06:31:33,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:33,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:33,600 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:33,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:33,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,649 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,649 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,649 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,649 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,649 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,653 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,653 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,653 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:31:33,657 - distributed.scheduler - INFO - Remove client Client-b4b909fc-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:33,658 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50830; closing.
2024-01-21 06:31:33,658 - distributed.scheduler - INFO - Remove client Client-b4b909fc-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:33,658 - distributed.scheduler - INFO - Close client connection: Client-b4b909fc-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:33,660 - distributed.scheduler - INFO - Remove client Client-b7023699-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:33,660 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50840; closing.
2024-01-21 06:31:33,660 - distributed.scheduler - INFO - Remove client Client-b7023699-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:33,661 - distributed.scheduler - INFO - Close client connection: Client-b7023699-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:33,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43495'. Reason: nanny-close
2024-01-21 06:31:33,664 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40987'. Reason: nanny-close
2024-01-21 06:31:33,665 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,666 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41225'. Reason: nanny-close
2024-01-21 06:31:33,666 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44285. Reason: nanny-close
2024-01-21 06:31:33,666 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,666 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35791. Reason: nanny-close
2024-01-21 06:31:33,666 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42237'. Reason: nanny-close
2024-01-21 06:31:33,667 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,667 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38875. Reason: nanny-close
2024-01-21 06:31:33,667 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38835'. Reason: nanny-close
2024-01-21 06:31:33,668 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,668 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44447. Reason: nanny-close
2024-01-21 06:31:33,668 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39615'. Reason: nanny-close
2024-01-21 06:31:33,668 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,668 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,668 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50870; closing.
2024-01-21 06:31:33,668 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41059'. Reason: nanny-close
2024-01-21 06:31:33,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50854; closing.
2024-01-21 06:31:33,668 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46237. Reason: nanny-close
2024-01-21 06:31:33,669 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.66919')
2024-01-21 06:31:33,669 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,669 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35689'. Reason: nanny-close
2024-01-21 06:31:33,669 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42455. Reason: nanny-close
2024-01-21 06:31:33,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6696565')
2024-01-21 06:31:33,669 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:33,669 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45487. Reason: nanny-close
2024-01-21 06:31:33,670 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,670 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,670 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,670 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,671 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50994; closing.
2024-01-21 06:31:33,671 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,671 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50878; closing.
2024-01-21 06:31:33,671 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45095. Reason: nanny-close
2024-01-21 06:31:33,672 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6720457')
2024-01-21 06:31:33,672 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,672 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6723938')
2024-01-21 06:31:33,672 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50988; closing.
2024-01-21 06:31:33,672 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,673 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46237', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6730998')
2024-01-21 06:31:33,673 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,673 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,673 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50906; closing.
2024-01-21 06:31:33,674 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50886; closing.
2024-01-21 06:31:33,674 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:33,674 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45487', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6745965')
2024-01-21 06:31:33,674 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42455', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6749132')
2024-01-21 06:31:33,675 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:33,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50902; closing.
2024-01-21 06:31:33,675 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818693.6755424')
2024-01-21 06:31:33,675 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:33,675 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:34,775 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:34,776 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:34,777 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:34,779 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:34,779 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-21 06:31:37,132 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:37,136 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33885 instead
  warnings.warn(
2024-01-21 06:31:37,140 - distributed.scheduler - INFO - State start
2024-01-21 06:31:37,373 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:37,374 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:37,374 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33885/status
2024-01-21 06:31:37,375 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:37,408 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44341', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,422 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44341
2024-01-21 06:31:37,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51194
2024-01-21 06:31:37,452 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45601', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,453 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45601
2024-01-21 06:31:37,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51202
2024-01-21 06:31:37,459 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51194; closing.
2024-01-21 06:31:37,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39171'
2024-01-21 06:31:37,459 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44341', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.4594336')
2024-01-21 06:31:37,482 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37141', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37141
2024-01-21 06:31:37,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51214
2024-01-21 06:31:37,496 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40753', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40753
2024-01-21 06:31:37,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51238
2024-01-21 06:31:37,508 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51238; closing.
2024-01-21 06:31:37,509 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.5091248')
2024-01-21 06:31:37,510 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51202; closing.
2024-01-21 06:31:37,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.5104845')
2024-01-21 06:31:37,513 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51214; closing.
2024-01-21 06:31:37,513 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37141', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.5136685')
2024-01-21 06:31:37,513 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:37,522 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36199', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,523 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36199
2024-01-21 06:31:37,523 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51240
2024-01-21 06:31:37,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35991', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35991
2024-01-21 06:31:37,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51250
2024-01-21 06:31:37,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51240; closing.
2024-01-21 06:31:37,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51250; closing.
2024-01-21 06:31:37,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.567954')
2024-01-21 06:31:37,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.5685213')
2024-01-21 06:31:37,568 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:37,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39101', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,604 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39101
2024-01-21 06:31:37,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51252
2024-01-21 06:31:37,608 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35415', status: init, memory: 0, processing: 0>
2024-01-21 06:31:37,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35415
2024-01-21 06:31:37,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51266
2024-01-21 06:31:37,623 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51266; closing.
2024-01-21 06:31:37,624 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35415', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.6242273')
2024-01-21 06:31:37,669 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51252; closing.
2024-01-21 06:31:37,670 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818697.669976')
2024-01-21 06:31:37,670 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:38,653 - distributed.scheduler - INFO - Receive client connection: Client-bb58aa4c-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:38,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51280
2024-01-21 06:31:38,715 - distributed.scheduler - INFO - Receive client connection: Client-b95ede41-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:38,716 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51298
2024-01-21 06:31:39,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:39,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:39,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:39,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38713
2024-01-21 06:31:39,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38713
2024-01-21 06:31:39,841 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-21 06:31:39,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:39,842 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:39,842 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:39,842 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:31:39,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a8iem4gb
2024-01-21 06:31:39,842 - distributed.worker - INFO - Starting Worker plugin PreImport-96276eb8-35d5-462d-82b5-22756882134d
2024-01-21 06:31:39,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b246715-d46c-4569-88bb-274ac414b664
2024-01-21 06:31:39,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91699f19-c39b-46e4-bbbf-a83af7de4b05
2024-01-21 06:31:39,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:39,904 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38713', status: init, memory: 0, processing: 0>
2024-01-21 06:31:39,904 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38713
2024-01-21 06:31:39,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51322
2024-01-21 06:31:39,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:39,906 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:39,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:39,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:39,944 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:39,948 - distributed.scheduler - INFO - Remove client Client-b95ede41-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:39,948 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51298; closing.
2024-01-21 06:31:39,949 - distributed.scheduler - INFO - Remove client Client-b95ede41-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:39,949 - distributed.scheduler - INFO - Close client connection: Client-b95ede41-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:39,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39171'. Reason: nanny-close
2024-01-21 06:31:39,951 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:39,952 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38713. Reason: nanny-close
2024-01-21 06:31:39,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51322; closing.
2024-01-21 06:31:39,954 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:39,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38713', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818699.954774')
2024-01-21 06:31:39,955 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:39,956 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:40,966 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:40,967 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:40,967 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:40,970 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:40,970 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-21 06:31:45,345 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:45,350 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41185 instead
  warnings.warn(
2024-01-21 06:31:45,353 - distributed.scheduler - INFO - State start
2024-01-21 06:31:45,375 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:45,376 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:45,376 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41185/status
2024-01-21 06:31:45,377 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:45,524 - distributed.scheduler - INFO - Receive client connection: Client-be461bea-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:45,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56724
2024-01-21 06:31:45,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34815'
2024-01-21 06:31:47,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:47,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:47,698 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:47,699 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45891
2024-01-21 06:31:47,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45891
2024-01-21 06:31:47,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34475
2024-01-21 06:31:47,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:47,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,699 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:47,699 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:31:47,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p9qon4sy
2024-01-21 06:31:47,700 - distributed.worker - INFO - Starting Worker plugin PreImport-5fe85c1e-ace8-45c6-aa7c-0b1d238e8920
2024-01-21 06:31:47,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfb76acb-170f-4478-9a4b-5be216914487
2024-01-21 06:31:47,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51985e71-eb6e-43f3-8190-8e56c2021974
2024-01-21 06:31:47,701 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45891', status: init, memory: 0, processing: 0>
2024-01-21 06:31:47,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45891
2024-01-21 06:31:47,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56780
2024-01-21 06:31:47,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:47,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:47,859 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:47,887 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:47,890 - distributed.scheduler - INFO - Remove client Client-be461bea-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:47,890 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56724; closing.
2024-01-21 06:31:47,891 - distributed.scheduler - INFO - Remove client Client-be461bea-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:47,891 - distributed.scheduler - INFO - Close client connection: Client-be461bea-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:47,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34815'. Reason: nanny-close
2024-01-21 06:31:47,892 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:47,893 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45891. Reason: nanny-close
2024-01-21 06:31:47,895 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:47,895 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56780; closing.
2024-01-21 06:31:47,896 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818707.8961003')
2024-01-21 06:31:47,896 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:47,897 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:47,933 - distributed.scheduler - INFO - Receive client connection: Client-bb58aa4c-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:47,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56814
2024-01-21 06:31:47,963 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35471', status: init, memory: 0, processing: 0>
2024-01-21 06:31:47,963 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35471
2024-01-21 06:31:47,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56800
2024-01-21 06:31:48,036 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45995', status: init, memory: 0, processing: 0>
2024-01-21 06:31:48,037 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45995
2024-01-21 06:31:48,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56824
2024-01-21 06:31:48,657 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:48,658 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:48,658 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:48,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:56800 has been closed.
2024-01-21 06:31:48,664 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35471', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818708.6644967')
2024-01-21 06:31:48,665 - distributed.core - INFO - Connection to tcp://127.0.0.1:56824 has been closed.
2024-01-21 06:31:48,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45995', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818708.6655736')
2024-01-21 06:31:48,665 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:48,668 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:48,669 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-21 06:31:50,733 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:50,737 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35339 instead
  warnings.warn(
2024-01-21 06:31:50,741 - distributed.scheduler - INFO - State start
2024-01-21 06:31:50,763 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:50,764 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:50,765 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35339/status
2024-01-21 06:31:50,765 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:51,103 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36259', status: init, memory: 0, processing: 0>
2024-01-21 06:31:51,116 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36259
2024-01-21 06:31:51,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53046
2024-01-21 06:31:51,149 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53046; closing.
2024-01-21 06:31:51,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818711.149772')
2024-01-21 06:31:51,150 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:51,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41389', status: init, memory: 0, processing: 0>
2024-01-21 06:31:51,335 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41389
2024-01-21 06:31:51,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53068
2024-01-21 06:31:51,356 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53068; closing.
2024-01-21 06:31:51,356 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818711.3566966')
2024-01-21 06:31:51,356 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:51,808 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53060'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53060>: Stream is closed
2024-01-21 06:31:52,007 - distributed.scheduler - INFO - Receive client connection: Client-bb58aa4c-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:52,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53076
2024-01-21 06:31:52,841 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53038'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53038>: Stream is closed
2024-01-21 06:31:53,057 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:53,057 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:53,058 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:53,059 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:31:53,060 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-21 06:31:55,088 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:55,092 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35069 instead
  warnings.warn(
2024-01-21 06:31:55,096 - distributed.scheduler - INFO - State start
2024-01-21 06:31:55,117 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:55,118 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-21 06:31:55,119 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35069/status
2024-01-21 06:31:55,119 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:55,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40431'
2024-01-21 06:31:55,308 - distributed.scheduler - INFO - Receive client connection: Client-c420f402-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:55,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43542
2024-01-21 06:31:56,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:56,690 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:56,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:56,694 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38781
2024-01-21 06:31:56,694 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38781
2024-01-21 06:31:56,694 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42569
2024-01-21 06:31:56,694 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:31:56,694 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:56,694 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:56,694 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:31:56,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zixk20lf
2024-01-21 06:31:56,695 - distributed.worker - INFO - Starting Worker plugin RMMSetup-230c28b6-f05f-4675-a54e-5e563cc82e2f
2024-01-21 06:31:56,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ae061b2-6361-4e87-a11c-fe4d2254e0ee
2024-01-21 06:31:56,695 - distributed.worker - INFO - Starting Worker plugin PreImport-ee237eab-1f14-4008-9216-bef89ec74ce9
2024-01-21 06:31:56,695 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:56,743 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38781', status: init, memory: 0, processing: 0>
2024-01-21 06:31:56,744 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38781
2024-01-21 06:31:56,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43572
2024-01-21 06:31:56,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:56,746 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:31:56,746 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:56,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:31:56,749 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:31:56,751 - distributed.scheduler - INFO - Remove client Client-c420f402-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:56,751 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43542; closing.
2024-01-21 06:31:56,751 - distributed.scheduler - INFO - Remove client Client-c420f402-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:56,752 - distributed.scheduler - INFO - Close client connection: Client-c420f402-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:56,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40431'. Reason: nanny-close
2024-01-21 06:31:56,754 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:56,755 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38781. Reason: nanny-close
2024-01-21 06:31:56,756 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:31:56,756 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43572; closing.
2024-01-21 06:31:56,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818716.756826')
2024-01-21 06:31:56,757 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:56,757 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:57,167 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:57,167 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:57,168 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:57,169 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-21 06:31:57,169 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-21 06:31:59,010 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:59,014 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43903 instead
  warnings.warn(
2024-01-21 06:31:59,017 - distributed.scheduler - INFO - State start
2024-01-21 06:31:59,038 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:59,038 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:31:59,039 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43903/status
2024-01-21 06:31:59,039 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:59,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44307'
2024-01-21 06:31:59,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36257'
2024-01-21 06:31:59,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43357'
2024-01-21 06:31:59,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33617'
2024-01-21 06:31:59,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45305'
2024-01-21 06:31:59,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36837'
2024-01-21 06:31:59,296 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42527'
2024-01-21 06:31:59,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39711'
2024-01-21 06:31:59,750 - distributed.scheduler - INFO - Receive client connection: Client-c6910c1e-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:31:59,765 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53498
2024-01-21 06:32:00,630 - distributed.scheduler - INFO - Receive client connection: Client-c87205b4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:00,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35180
2024-01-21 06:32:01,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,054 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,054 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39213
2024-01-21 06:32:01,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39213
2024-01-21 06:32:01,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33871
2024-01-21 06:32:01,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33871
2024-01-21 06:32:01,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38859
2024-01-21 06:32:01,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32853
2024-01-21 06:32:01,055 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,055 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,055 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,055 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-80eiaycp
2024-01-21 06:32:01,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e7leowf7
2024-01-21 06:32:01,055 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3eba8d5-3e71-493d-9e77-d6714a95ba94
2024-01-21 06:32:01,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4afb631-8a29-457e-a176-22a9c8ae143c
2024-01-21 06:32:01,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-393adff2-aef6-4537-a036-8c2f1608f4a3
2024-01-21 06:32:01,057 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46353
2024-01-21 06:32:01,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46353
2024-01-21 06:32:01,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42279
2024-01-21 06:32:01,058 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,058 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,058 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mdoqwy4_
2024-01-21 06:32:01,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05684bb7-50ff-487f-9ce4-0624ecca4aee
2024-01-21 06:32:01,059 - distributed.worker - INFO - Starting Worker plugin PreImport-1c804f67-df5c-4ce3-a178-b622efe30b82
2024-01-21 06:32:01,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf637a87-56a7-4a2d-b66c-308e4c2ee0eb
2024-01-21 06:32:01,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32835
2024-01-21 06:32:01,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32835
2024-01-21 06:32:01,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44651
2024-01-21 06:32:01,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,095 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,095 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wrvv2vvt
2024-01-21 06:32:01,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05dca12b-561c-4626-851a-7945d3e23bec
2024-01-21 06:32:01,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,103 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34405
2024-01-21 06:32:01,104 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34405
2024-01-21 06:32:01,104 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46131
2024-01-21 06:32:01,104 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,104 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,104 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,104 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,104 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ubbpb3jq
2024-01-21 06:32:01,104 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5098d981-c566-4b51-93da-6c12999a532d
2024-01-21 06:32:01,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,135 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37581
2024-01-21 06:32:01,135 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37581
2024-01-21 06:32:01,135 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35683
2024-01-21 06:32:01,135 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,136 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,136 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hcprq1uc
2024-01-21 06:32:01,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51023625-d0c2-44e9-9593-eab110840cd2
2024-01-21 06:32:01,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,148 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,150 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40461
2024-01-21 06:32:01,150 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40461
2024-01-21 06:32:01,150 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44093
2024-01-21 06:32:01,150 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,150 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,150 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,150 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,150 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bs66wx5s
2024-01-21 06:32:01,151 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f6fd4c0-e0b8-4d43-a3be-0777cfbb011f
2024-01-21 06:32:01,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:01,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:01,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:01,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45159
2024-01-21 06:32:01,160 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45159
2024-01-21 06:32:01,160 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43391
2024-01-21 06:32:01,160 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:01,161 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:01,161 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:01,161 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:01,161 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-54qvcw6b
2024-01-21 06:32:01,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69655c55-bf98-44e8-8e65-1c5a75d2a48e
2024-01-21 06:32:03,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ceeb2efd-9fd6-4b18-91ee-9d074feb7b30
2024-01-21 06:32:03,061 - distributed.worker - INFO - Starting Worker plugin PreImport-2322f597-da62-4d13-bf9f-e34af78cf723
2024-01-21 06:32:03,062 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,090 - distributed.worker - INFO - Starting Worker plugin PreImport-4bf064ed-d535-47ba-a177-5a85f1ba6b2d
2024-01-21 06:32:03,093 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,100 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32835', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32835
2024-01-21 06:32:03,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35270
2024-01-21 06:32:03,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69340127-70ab-48c2-a384-2e9640a5db0b
2024-01-21 06:32:03,104 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,104 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,105 - distributed.worker - INFO - Starting Worker plugin PreImport-4e970b3a-0410-4424-874f-f6293d1ec654
2024-01-21 06:32:03,106 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,120 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60594fe0-e7c8-4c2c-953f-2ff5461afc80
2024-01-21 06:32:03,120 - distributed.worker - INFO - Starting Worker plugin PreImport-fbcb40bb-ab66-47b5-bffb-e50526adc745
2024-01-21 06:32:03,120 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,123 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,132 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39213', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,132 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39213
2024-01-21 06:32:03,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35272
2024-01-21 06:32:03,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,135 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,135 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,137 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,146 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33871', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,147 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33871
2024-01-21 06:32:03,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35292
2024-01-21 06:32:03,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,148 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46353', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,149 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,149 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,149 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46353
2024-01-21 06:32:03,149 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35296
2024-01-21 06:32:03,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5564334d-1823-4daa-b0bf-a24d0b07fe81
2024-01-21 06:32:03,151 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,151 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,151 - distributed.worker - INFO - Starting Worker plugin PreImport-c6e1e028-eaa3-40ab-9ce1-9be85167b8fc
2024-01-21 06:32:03,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,152 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,153 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34405', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,153 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34405
2024-01-21 06:32:03,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35286
2024-01-21 06:32:03,155 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4196b8f4-5caa-46da-a10c-68f8f5175f04
2024-01-21 06:32:03,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,156 - distributed.worker - INFO - Starting Worker plugin PreImport-3d5ee493-fc58-42f0-ab4d-ae9da17a1286
2024-01-21 06:32:03,156 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,157 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ae96b81-5954-4b5a-ad71-bfb94e916144
2024-01-21 06:32:03,163 - distributed.worker - INFO - Starting Worker plugin PreImport-02205642-8600-4c13-a08a-3b107d4809c2
2024-01-21 06:32:03,164 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,178 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37581', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37581
2024-01-21 06:32:03,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35318
2024-01-21 06:32:03,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,181 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,187 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45159', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,188 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45159
2024-01-21 06:32:03,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35326
2024-01-21 06:32:03,188 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,189 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40461', status: init, memory: 0, processing: 0>
2024-01-21 06:32:03,189 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,189 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40461
2024-01-21 06:32:03,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35310
2024-01-21 06:32:03,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:03,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:03,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:03,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:03,281 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,282 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,282 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,282 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,282 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,283 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,283 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,283 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:03,289 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,289 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,290 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,290 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,290 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,291 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,291 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,291 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,297 - distributed.scheduler - INFO - Remove client Client-c87205b4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:03,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35180; closing.
2024-01-21 06:32:03,299 - distributed.scheduler - INFO - Remove client Client-c87205b4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:03,302 - distributed.scheduler - INFO - Close client connection: Client-c87205b4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:03,302 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,303 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:03,307 - distributed.scheduler - INFO - Remove client Client-c6910c1e-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:03,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53498; closing.
2024-01-21 06:32:03,307 - distributed.scheduler - INFO - Remove client Client-c6910c1e-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:03,308 - distributed.scheduler - INFO - Close client connection: Client-c6910c1e-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:03,309 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44307'. Reason: nanny-close
2024-01-21 06:32:03,309 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,310 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36257'. Reason: nanny-close
2024-01-21 06:32:03,310 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,310 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43357'. Reason: nanny-close
2024-01-21 06:32:03,310 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46353. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33617'. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33871. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45305'. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,311 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39213. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36837'. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42527'. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40461. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45159. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,312 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39711'. Reason: nanny-close
2024-01-21 06:32:03,312 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37581. Reason: nanny-close
2024-01-21 06:32:03,313 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,313 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:03,313 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34405. Reason: nanny-close
2024-01-21 06:32:03,313 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32835. Reason: nanny-close
2024-01-21 06:32:03,314 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,314 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,314 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,314 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35296; closing.
2024-01-21 06:32:03,314 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,315 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,315 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46353', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.3157666')
2024-01-21 06:32:03,315 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,316 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,316 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35292; closing.
2024-01-21 06:32:03,316 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:03,317 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35326; closing.
2024-01-21 06:32:03,317 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35272; closing.
2024-01-21 06:32:03,317 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,318 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,318 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,318 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.3186972')
2024-01-21 06:32:03,319 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:03,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45159', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.3192582')
2024-01-21 06:32:03,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39213', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.3197129')
2024-01-21 06:32:03,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35318; closing.
2024-01-21 06:32:03,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35310; closing.
2024-01-21 06:32:03,321 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.3215892')
2024-01-21 06:32:03,322 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.321994')
2024-01-21 06:32:03,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35270; closing.
2024-01-21 06:32:03,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35286; closing.
2024-01-21 06:32:03,322 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35292>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-21 06:32:03,325 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35272>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-21 06:32:03,325 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35326>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-21 06:32:03,325 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.3257308')
2024-01-21 06:32:03,326 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34405', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818723.32617')
2024-01-21 06:32:03,326 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:04,325 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:04,325 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:04,326 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:04,330 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:04,330 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-21 06:32:06,562 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:06,566 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34219 instead
  warnings.warn(
2024-01-21 06:32:06,570 - distributed.scheduler - INFO - State start
2024-01-21 06:32:06,845 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:06,846 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:06,846 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34219/status
2024-01-21 06:32:06,847 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:06,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46129'
2024-01-21 06:32:07,161 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39045', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,174 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39045
2024-01-21 06:32:07,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35468
2024-01-21 06:32:07,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46463', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46463
2024-01-21 06:32:07,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35470
2024-01-21 06:32:07,208 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35468; closing.
2024-01-21 06:32:07,209 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39045', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.2090752')
2024-01-21 06:32:07,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33783', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33783
2024-01-21 06:32:07,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35486
2024-01-21 06:32:07,226 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34641', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,226 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34641
2024-01-21 06:32:07,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35490
2024-01-21 06:32:07,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36113', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,248 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36113
2024-01-21 06:32:07,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35494
2024-01-21 06:32:07,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33819', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33819
2024-01-21 06:32:07,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35504
2024-01-21 06:32:07,255 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35470; closing.
2024-01-21 06:32:07,256 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.2563255')
2024-01-21 06:32:07,261 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35486; closing.
2024-01-21 06:32:07,261 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.261313')
2024-01-21 06:32:07,262 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35490; closing.
2024-01-21 06:32:07,263 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34641', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.263233')
2024-01-21 06:32:07,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42905', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42905
2024-01-21 06:32:07,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35512
2024-01-21 06:32:07,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35504; closing.
2024-01-21 06:32:07,307 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33819', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.3077419')
2024-01-21 06:32:07,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35494; closing.
2024-01-21 06:32:07,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.3099113')
2024-01-21 06:32:07,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35512; closing.
2024-01-21 06:32:07,311 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.3118224')
2024-01-21 06:32:07,312 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:07,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34261', status: init, memory: 0, processing: 0>
2024-01-21 06:32:07,416 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34261
2024-01-21 06:32:07,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35516
2024-01-21 06:32:07,457 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35516; closing.
2024-01-21 06:32:07,457 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34261', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818727.4575396')
2024-01-21 06:32:07,457 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:07,533 - distributed.scheduler - INFO - Receive client connection: Client-caebd447-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:07,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35522
2024-01-21 06:32:08,276 - distributed.scheduler - INFO - Receive client connection: Client-cd00b68d-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:08,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35550
2024-01-21 06:32:08,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:08,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:08,649 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:08,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45037
2024-01-21 06:32:08,651 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45037
2024-01-21 06:32:08,651 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37997
2024-01-21 06:32:08,652 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:08,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:08,652 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:08,652 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:08,652 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ppvujbi
2024-01-21 06:32:08,653 - distributed.worker - INFO - Starting Worker plugin PreImport-2ca6f016-9f4b-4ee7-8a4c-61799f9bb943
2024-01-21 06:32:08,653 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9af3a1fe-675c-407e-b61b-7f325a213e2d
2024-01-21 06:32:08,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7e39cea-69e8-4c87-8716-9f15313217a9
2024-01-21 06:32:08,977 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:09,085 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45037', status: init, memory: 0, processing: 0>
2024-01-21 06:32:09,085 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45037
2024-01-21 06:32:09,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35576
2024-01-21 06:32:09,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:09,087 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:09,087 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:09,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:09,103 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:32:09,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:09,109 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:09,111 - distributed.scheduler - INFO - Remove client Client-caebd447-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:09,111 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35522; closing.
2024-01-21 06:32:09,111 - distributed.scheduler - INFO - Remove client Client-caebd447-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:09,112 - distributed.scheduler - INFO - Close client connection: Client-caebd447-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:09,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46129'. Reason: nanny-close
2024-01-21 06:32:09,113 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:09,114 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45037. Reason: nanny-close
2024-01-21 06:32:09,116 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:09,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35576; closing.
2024-01-21 06:32:09,116 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818729.1168296')
2024-01-21 06:32:09,117 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:09,117 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:09,728 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:09,728 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:09,729 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:09,730 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:09,731 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-21 06:32:11,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41563'
2024-01-21 06:32:11,846 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:11,851 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46213 instead
  warnings.warn(
2024-01-21 06:32:11,855 - distributed.scheduler - INFO - State start
2024-01-21 06:32:11,876 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:11,877 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-21 06:32:11,878 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:11,879 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-21 06:32:12,875 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41563'. Reason: nanny-close
2024-01-21 06:32:13,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:13,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:13,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:13,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35189
2024-01-21 06:32:13,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35189
2024-01-21 06:32:13,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33893
2024-01-21 06:32:13,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:13,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:13,699 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:13,699 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:13,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bibuxa0e
2024-01-21 06:32:13,699 - distributed.worker - INFO - Starting Worker plugin PreImport-d8a73efe-45c3-41d8-91db-69571091b2c9
2024-01-21 06:32:13,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea4e89af-3093-4d5d-b5fc-0707d3367c35
2024-01-21 06:32:13,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8d1b4df-3e67-4b13-8ff2-1274a503a794
2024-01-21 06:32:14,417 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,500 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,500 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,529 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,531 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35189. Reason: nanny-close
2024-01-21 06:32:14,533 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,534 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45793 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36087 instead
  warnings.warn(
2024-01-21 06:33:14,635 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:48757 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-21 06:33:14,635 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:36805 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-21 06:33:14,636 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:36937 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-21 06:33:14,636 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:38665 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-21 06:33:14,636 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:46439 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36147 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33111 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46547 instead
  warnings.warn(
2024-01-21 06:34:25,226 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #015] ep: 0x7ff5cd4b70c0, tag: 0x5f7447caa3d94ff5, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #015] ep: 0x7ff5cd4b70c0, tag: 0x5f7447caa3d94ff5, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42517 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35073 instead
  warnings.warn(
[1705818905.322741] [dgx13:69527:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:54269) failed: Address already in use
2024-01-21 06:35:07,789 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-21 06:35:07,795 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-21 06:35:07,795 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-21 06:35:07,800 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucxx://10.33.225.163:44689', name: 4, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-21 06:35:07,810 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41605 instead
  warnings.warn(
2024-01-21 06:35:28,191 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #018] ep: 0x7fd1849250c0, tag: 0xb6af887c738b4a6d, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #018] ep: 0x7fd1849250c0, tag: 0xb6af887c738b4a6d, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46695 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44347 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45567 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43039 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45175 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39843 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41603 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38309 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37137 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41239 instead
  warnings.warn(
[1705819243.582226] [dgx13:75300:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:56554) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46301 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42681 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36355 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38799 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44005 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33085 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41791 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36333 instead
  warnings.warn(
[1705819518.425561] [dgx13:79686:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:46500) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35547 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41773 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] [1705819639.381036] [dgx13:81565:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:52146) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38223 instead
  warnings.warn(
[1705819668.858454] [dgx13:81992:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:40875) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38817 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46249 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39927 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45719 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40461 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38335 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40961 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34493 instead
  warnings.warn(
2024-01-21 06:51:29,704 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1152, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 290, in write
    raise CommClosedError("Endpoint is closed -- unable to send message")
distributed.comm.core.CommClosedError: Endpoint is closed -- unable to send message
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44837 instead
  warnings.warn(
[1705819908.346822] [dgx13:85208:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:48750) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37035 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44375 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37901 instead
  warnings.warn(
2024-01-21 06:53:43,873 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 406, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 505, in ep
    raise CommClosedError("UCX Endpoint is closed")
distributed.comm.core.CommClosedError: UCX Endpoint is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CommClosedError('UCX Endpoint is closed')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43931 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41945 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34811 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43987 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37297 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42363 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46159 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36581 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36727 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] [1705820224.862889] [dgx13:62961:0]            sock.c:481  UCX  ERROR bind(fd=253 addr=0.0.0.0:33252) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-21 06:57:24,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:24,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:24,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:24,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,839 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:24,871 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:24,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:24,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:24,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:25,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:57:25,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:57:25,323 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,324 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40989
2024-01-21 06:57:25,324 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40989
2024-01-21 06:57:25,324 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32967
2024-01-21 06:57:25,324 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,324 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,324 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b9fjdj5i
2024-01-21 06:57:25,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-18a34ae1-69f4-4116-be53-3fe91b368a9b
2024-01-21 06:57:25,324 - distributed.worker - INFO - Starting Worker plugin PreImport-f6f325c6-94c6-43cb-9723-0843d4445dc9
2024-01-21 06:57:25,325 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00f9a29a-e060-4aa7-94a2-31be080fbc07
2024-01-21 06:57:25,325 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35679
2024-01-21 06:57:25,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35679
2024-01-21 06:57:25,380 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38053
2024-01-21 06:57:25,380 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,380 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,380 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,380 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h78km814
2024-01-21 06:57:25,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1cc63993-5c4f-4f14-8c23-fd732d0ec49b
2024-01-21 06:57:25,383 - distributed.worker - INFO - Starting Worker plugin PreImport-333d3fd2-635d-4c41-a51a-27689800816a
2024-01-21 06:57:25,383 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bfb3dcaf-9768-46b9-914c-ef17c58612b8
2024-01-21 06:57:25,384 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,395 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,395 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,396 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,396 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,397 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45557
2024-01-21 06:57:25,398 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45557
2024-01-21 06:57:25,398 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36515
2024-01-21 06:57:25,398 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,398 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,398 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,398 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-073lg0dr
2024-01-21 06:57:25,398 - distributed.worker - INFO - Starting Worker plugin PreImport-c16b9f36-79b1-4584-b206-7f0dd626acc2
2024-01-21 06:57:25,398 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c88a38f-59cf-4986-ae48-1b92d3e61ed4
2024-01-21 06:57:25,398 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0ceb1ea-fa48-462c-873e-60c725be55d6
2024-01-21 06:57:25,399 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,494 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,494 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,495 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,496 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34303
2024-01-21 06:57:25,496 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34303
2024-01-21 06:57:25,497 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35579
2024-01-21 06:57:25,497 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,497 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,497 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,497 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zbrwnlag
2024-01-21 06:57:25,497 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a044c69-391a-45af-aad3-6a590bea4ae1
2024-01-21 06:57:25,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efccd785-f977-479a-8c7a-6a403b70a837
2024-01-21 06:57:25,497 - distributed.worker - INFO - Starting Worker plugin PreImport-6a6c4d3c-f0ea-4576-a109-3de06e1e2252
2024-01-21 06:57:25,497 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,500 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,500 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,566 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,566 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,587 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35231
2024-01-21 06:57:25,587 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35231
2024-01-21 06:57:25,587 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43405
2024-01-21 06:57:25,587 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,587 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,587 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eh7oue7w
2024-01-21 06:57:25,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f3d42d7-9c01-4e25-a6b9-f37e3ac73e9b
2024-01-21 06:57:25,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d67e7e75-3cf3-4d4a-83d5-c904fe506b5f
2024-01-21 06:57:25,588 - distributed.worker - INFO - Starting Worker plugin PreImport-195e8904-092f-431c-9702-3fa23d3d69a3
2024-01-21 06:57:25,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,615 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,616 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43269
2024-01-21 06:57:25,616 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43269
2024-01-21 06:57:25,616 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43713
2024-01-21 06:57:25,616 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,616 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,616 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,617 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fgpphpv8
2024-01-21 06:57:25,617 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ab02b2d-9d46-46d6-a847-8f5ef5f3b2b6
2024-01-21 06:57:25,617 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b191c31-d43a-4b2f-aee5-1986d4b1f0e8
2024-01-21 06:57:25,619 - distributed.worker - INFO - Starting Worker plugin PreImport-b1b0779d-a475-4acf-8d69-b7b05943deb7
2024-01-21 06:57:25,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,656 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,683 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34961
2024-01-21 06:57:25,683 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34961
2024-01-21 06:57:25,683 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44231
2024-01-21 06:57:25,684 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,684 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,684 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rw14u0kb
2024-01-21 06:57:25,684 - distributed.worker - INFO - Starting Worker plugin PreImport-d4bb2875-8e34-4977-a76e-dd49d82c26a9
2024-01-21 06:57:25,684 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89c41910-0df0-436f-9e74-8f7904274ecf
2024-01-21 06:57:25,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-311d07d0-235b-4b22-af48-14af58bab9b8
2024-01-21 06:57:25,685 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,700 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,700 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,767 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,767 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:25,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:57:25,892 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36807
2024-01-21 06:57:25,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36807
2024-01-21 06:57:25,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38213
2024-01-21 06:57:25,893 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,893 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,893 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:57:25,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2f7o1_0p
2024-01-21 06:57:25,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0926466b-8a31-4aa5-97ff-6aa9bb7d3ca5
2024-01-21 06:57:25,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6c17140-98e0-4cf0-93bb-f8f3784aa396
2024-01-21 06:57:25,894 - distributed.worker - INFO - Starting Worker plugin PreImport-7bafc4a6-8dc6-4a9e-9dec-4affac5b0bc2
2024-01-21 06:57:25,894 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:57:25,978 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44127
2024-01-21 06:57:25,978 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:57:25,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44127
2024-01-21 06:57:26,024 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,025 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:57:26,031 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45557. Reason: nanny-close
2024-01-21 06:57:26,032 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40989. Reason: nanny-close
2024-01-21 06:57:26,033 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35679. Reason: nanny-close
2024-01-21 06:57:26,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43269. Reason: nanny-close
2024-01-21 06:57:26,034 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34303. Reason: nanny-close
2024-01-21 06:57:26,035 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,035 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35231. Reason: nanny-close
2024-01-21 06:57:26,036 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36807. Reason: nanny-close
2024-01-21 06:57:26,036 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,036 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34961. Reason: nanny-close
2024-01-21 06:57:26,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,037 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,038 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,039 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,039 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,040 - distributed.nanny - INFO - Worker closed
2024-01-21 06:57:26,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:44127 has been closed.
2024-01-21 06:57:26,043 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed 2024-01-21 06:57:45,295 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:42902 remote=tcp://127.0.0.1:46241>: Stream is closed
2024-01-21 06:57:45,299 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:42920 remote=tcp://127.0.0.1:46241>: Stream is closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-21 06:58:13,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:13,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:13,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:13,782 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45313
2024-01-21 06:58:13,782 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45313
2024-01-21 06:58:13,782 - distributed.worker - INFO -           Worker name:                          0
2024-01-21 06:58:13,782 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32969
2024-01-21 06:58:13,782 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43935
2024-01-21 06:58:13,782 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:13,782 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:13,782 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:58:13,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cj_uh9xb
2024-01-21 06:58:13,782 - distributed.worker - INFO - Starting Worker plugin PreImport-38d380e8-c4c8-45c1-88fb-a18ffea3e41a
2024-01-21 06:58:13,787 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-21 06:58:13,787 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-697896d7-2443-4712-9f8a-c2ea74f9ba7b
2024-01-21 06:58:13,787 - distributed.worker - INFO - Starting Worker plugin RMMSetup-00af91c2-4cb3-4c25-ae30-c73dce7bf970
2024-01-21 06:58:13,788 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45313. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-21 06:58:13,788 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-21 06:58:13,790 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-21 06:58:18,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,469 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,469 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,522 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,522 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:18,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:18,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:19,026 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45957
2024-01-21 06:58:19,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45957
2024-01-21 06:58:19,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45565
2024-01-21 06:58:19,028 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,028 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,028 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-140e7kgr
2024-01-21 06:58:19,028 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81e9bad6-ee75-4d37-b49e-7615b504a7d9
2024-01-21 06:58:19,028 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a0e47da-394f-4969-b5c5-8a45615e51f1
2024-01-21 06:58:19,028 - distributed.worker - INFO - Starting Worker plugin PreImport-e9fe3d84-2156-4dd1-966d-10e493b700ea
2024-01-21 06:58:19,029 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,105 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,106 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,120 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34941
2024-01-21 06:58:19,120 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34941
2024-01-21 06:58:19,120 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43795
2024-01-21 06:58:19,120 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,120 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,120 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,120 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,120 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3nuzk1s9
2024-01-21 06:58:19,121 - distributed.worker - INFO - Starting Worker plugin PreImport-b29a8d00-3daa-4c96-8a6e-f91865b0edef
2024-01-21 06:58:19,121 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34747110-e904-40e4-8946-0ce72af7f513
2024-01-21 06:58:19,121 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2efb8eca-5c36-4c67-9602-f7318c3123d4
2024-01-21 06:58:19,121 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,142 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43249
2024-01-21 06:58:19,142 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43249
2024-01-21 06:58:19,142 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45987
2024-01-21 06:58:19,142 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,143 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,143 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,143 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gqzz1h0o
2024-01-21 06:58:19,143 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6971e53-18ab-43ac-8aca-0b30e20a8b01
2024-01-21 06:58:19,143 - distributed.worker - INFO - Starting Worker plugin PreImport-087b7f40-b8d8-4dbf-a311-3a74b4917366
2024-01-21 06:58:19,144 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ae32c27-eb08-451b-a498-b531717865ab
2024-01-21 06:58:19,144 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45453
2024-01-21 06:58:19,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45453
2024-01-21 06:58:19,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40759
2024-01-21 06:58:19,182 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,182 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,182 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,182 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j7bba_yc
2024-01-21 06:58:19,182 - distributed.worker - INFO - Starting Worker plugin PreImport-445a76a8-4be0-43b0-8819-a7cdf9231dd3
2024-01-21 06:58:19,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c390e5d4-9800-46d7-afa0-9e89870d9ca7
2024-01-21 06:58:19,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-897a5b6e-e879-446e-b10a-65e6a9d2307d
2024-01-21 06:58:19,182 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46079
2024-01-21 06:58:19,199 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46079
2024-01-21 06:58:19,199 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33821
2024-01-21 06:58:19,199 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,199 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,199 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,199 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eypvpwc0
2024-01-21 06:58:19,199 - distributed.worker - INFO - Starting Worker plugin PreImport-cbc750e3-5054-4942-bea1-965994b02f77
2024-01-21 06:58:19,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5f90ca5-c183-42b3-ad11-71450e069e44
2024-01-21 06:58:19,200 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bbe061e-d805-47d4-81cd-1b49e603a9b3
2024-01-21 06:58:19,200 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,207 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37107
2024-01-21 06:58:19,207 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37107
2024-01-21 06:58:19,207 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43281
2024-01-21 06:58:19,207 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,207 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,207 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,207 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,207 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x77pc16y
2024-01-21 06:58:19,207 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0aed4d80-f41f-4963-a055-7a2f43df4c4b
2024-01-21 06:58:19,208 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3f694b8-053a-499b-a711-c3c63a9ecfb4
2024-01-21 06:58:19,209 - distributed.worker - INFO - Starting Worker plugin PreImport-1f2c30ec-ffe2-45bf-b2e8-d86cd54c1227
2024-01-21 06:58:19,209 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,271 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,284 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,285 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35527
2024-01-21 06:58:19,285 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35527
2024-01-21 06:58:19,285 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34193
2024-01-21 06:58:19,285 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,285 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,285 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,285 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wy73cpmq
2024-01-21 06:58:19,285 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d03350c-05c3-4f4e-b2d3-a92f5a445bf4
2024-01-21 06:58:19,286 - distributed.worker - INFO - Starting Worker plugin PreImport-d9f5ebd6-1433-453a-8a72-47e2e4cbef94
2024-01-21 06:58:19,286 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0484eaf3-d156-4834-8497-f706cde9126a
2024-01-21 06:58:19,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,318 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,327 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:19,328 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34693
2024-01-21 06:58:19,328 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34693
2024-01-21 06:58:19,328 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40971
2024-01-21 06:58:19,328 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,328 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,329 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:19,329 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:58:19,329 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cg0rxk11
2024-01-21 06:58:19,329 - distributed.worker - INFO - Starting Worker plugin PreImport-ad5a6f41-2d34-4630-8831-323185d2acec
2024-01-21 06:58:19,329 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78b13003-1cea-43bb-9313-6efaffd77b00
2024-01-21 06:58:19,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-567d2d1f-436a-4edf-92dd-6a00a91cbc6c
2024-01-21 06:58:19,330 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,418 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,419 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,430 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,430 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,435 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,435 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,468 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,469 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,469 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:19,476 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37017
2024-01-21 06:58:19,476 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:19,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37017
2024-01-21 06:58:19,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45957. Reason: nanny-close
2024-01-21 06:58:19,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34941. Reason: nanny-close
2024-01-21 06:58:19,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43249. Reason: nanny-close
2024-01-21 06:58:19,509 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37107. Reason: nanny-close
2024-01-21 06:58:19,509 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45453. Reason: nanny-close
2024-01-21 06:58:19,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,510 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35527. Reason: nanny-close
2024-01-21 06:58:19,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,510 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34693. Reason: nanny-close
2024-01-21 06:58:19,511 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46079. Reason: nanny-close
2024-01-21 06:58:19,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,511 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,512 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,512 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,513 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,513 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,513 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,513 - distributed.core - INFO - Connection to tcp://127.0.0.1:37017 has been closed.
2024-01-21 06:58:19,514 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,514 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:19,514 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-01-21 06:58:42,165 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-21 06:58:42,170 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-01-21 06:59:33,400 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:46384 remote=tcp://127.0.0.1:42147>: Stream is closed
PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-01-21 06:59:42,403 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-0510f870e3f1398d4b018ddda308139b', 1)
Function:  subgraph_callable-68bcbbc5-ff2f-45a7-ae20-b278d756
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7f5bfadda310 of cudf.core.dataframe.DataFrame at 0x7f5bfadc2df0>, '_partitions', 'getitem-9d1eeab2dac9664ce3bf98396bd4334b', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-21 06:59:42,419 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('assign-0510f870e3f1398d4b018ddda308139b', 0))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-01-21 06:59:47,203 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-73cabdfebe1dd4d17c2f37ae549713da', 2)
Function:  subgraph_callable-58432c6b-29ff-46a3-ab7f-01ca21b6
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7f29de25f340 of cudf.core.dataframe.DataFrame at 0x7f29de247e20>, '_partitions', 'getitem-6bc36e7fdccbf44303bd588a4a4ec92f', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-21 06:59:47,223 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-73cabdfebe1dd4d17c2f37ae549713da', 1)
Function:  subgraph_callable-58432c6b-29ff-46a3-ab7f-01ca21b6
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7f29d6512e80 of cudf.core.dataframe.DataFrame at 0x7f29de25f130>, '_partitions', 'getitem-6bc36e7fdccbf44303bd588a4a4ec92f', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-21 06:59:55,730 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-21 06:59:55,733 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
