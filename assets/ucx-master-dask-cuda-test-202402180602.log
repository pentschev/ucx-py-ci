============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.0, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-18 06:56:14,591 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:14,596 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:56:14,600 - distributed.scheduler - INFO - State start
2024-02-18 06:56:14,623 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:14,625 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-18 06:56:14,626 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:56:14,626 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:56:14,787 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42465'
2024-02-18 06:56:14,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35953'
2024-02-18 06:56:14,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44521'
2024-02-18 06:56:14,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44445'
2024-02-18 06:56:15,280 - distributed.scheduler - INFO - Receive client connection: Client-cd674eb8-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:15,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58854
2024-02-18 06:56:16,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:16,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:16,719 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:16,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:16,723 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:16,723 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:16,723 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44019
2024-02-18 06:56:16,724 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44019
2024-02-18 06:56:16,724 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42915
2024-02-18 06:56:16,724 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,724 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,724 - distributed.worker - INFO -               Threads:                          4
2024-02-18 06:56:16,724 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-18 06:56:16,724 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nuoxsb_n
2024-02-18 06:56:16,724 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34557
2024-02-18 06:56:16,724 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34557
2024-02-18 06:56:16,724 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45255
2024-02-18 06:56:16,724 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bdab2f17-83f3-4e56-8cec-05e0172a1f01
2024-02-18 06:56:16,724 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,724 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,724 - distributed.worker - INFO - Starting Worker plugin PreImport-0defecfa-81b8-4a89-8db3-0792cf7259e2
2024-02-18 06:56:16,724 - distributed.worker - INFO -               Threads:                          4
2024-02-18 06:56:16,724 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-18 06:56:16,724 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba448302-1129-4cbe-804e-74968fd76713
2024-02-18 06:56:16,724 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-l8rfxh_0
2024-02-18 06:56:16,724 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15d9a4e8-6065-42b6-b581-5d776e72d525
2024-02-18 06:56:16,725 - distributed.worker - INFO - Starting Worker plugin PreImport-14d680e2-2a54-4a21-bd67-1a7dbcde327e
2024-02-18 06:56:16,725 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2cdf9f6d-4202-44b7-9888-8663a42d6007
2024-02-18 06:56:16,725 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:16,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:16,742 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:16,743 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43619
2024-02-18 06:56:16,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43619
2024-02-18 06:56:16,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44167
2024-02-18 06:56:16,743 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,743 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,743 - distributed.worker - INFO -               Threads:                          4
2024-02-18 06:56:16,743 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-18 06:56:16,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-51hb2ry9
2024-02-18 06:56:16,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87adfcf8-05c8-490d-ad94-de299c5dda60
2024-02-18 06:56:16,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d005445-04c3-4dc4-b64d-4bef2d6d1acb
2024-02-18 06:56:16,744 - distributed.worker - INFO - Starting Worker plugin PreImport-1a5d2d30-f6ba-4306-842f-2632b30f4997
2024-02-18 06:56:16,744 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:16,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:16,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:16,760 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42085
2024-02-18 06:56:16,760 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42085
2024-02-18 06:56:16,760 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41989
2024-02-18 06:56:16,760 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,761 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,761 - distributed.worker - INFO -               Threads:                          4
2024-02-18 06:56:16,761 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-18 06:56:16,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nai_t138
2024-02-18 06:56:16,761 - distributed.worker - INFO - Starting Worker plugin RMMSetup-095afbca-549b-4a18-bc29-64ad58a246ad
2024-02-18 06:56:16,761 - distributed.worker - INFO - Starting Worker plugin PreImport-a118f739-4ce2-4ac4-b23e-eeaa15dea76d
2024-02-18 06:56:16,761 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0273738-fffc-47a6-9207-765c07c25839
2024-02-18 06:56:16,761 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,832 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34557', status: init, memory: 0, processing: 0>
2024-02-18 06:56:16,833 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34557
2024-02-18 06:56:16,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58882
2024-02-18 06:56:16,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:16,835 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,835 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-18 06:56:16,839 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44019', status: init, memory: 0, processing: 0>
2024-02-18 06:56:16,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44019
2024-02-18 06:56:16,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58872
2024-02-18 06:56:16,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:16,841 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,842 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-18 06:56:16,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43619', status: init, memory: 0, processing: 0>
2024-02-18 06:56:16,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43619
2024-02-18 06:56:16,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58898
2024-02-18 06:56:16,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:16,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,860 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-18 06:56:16,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42085', status: init, memory: 0, processing: 0>
2024-02-18 06:56:16,864 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42085
2024-02-18 06:56:16,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58912
2024-02-18 06:56:16,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:16,865 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-18 06:56:16,865 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:16,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-18 06:56:16,932 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-18 06:56:16,932 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-18 06:56:16,933 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-18 06:56:16,933 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-18 06:56:16,938 - distributed.scheduler - INFO - Remove client Client-cd674eb8-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:16,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58854; closing.
2024-02-18 06:56:16,938 - distributed.scheduler - INFO - Remove client Client-cd674eb8-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:16,938 - distributed.scheduler - INFO - Close client connection: Client-cd674eb8-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:16,940 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42465'. Reason: nanny-close
2024-02-18 06:56:16,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:16,940 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35953'. Reason: nanny-close
2024-02-18 06:56:16,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:16,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44521'. Reason: nanny-close
2024-02-18 06:56:16,941 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44019. Reason: nanny-close
2024-02-18 06:56:16,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:16,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44445'. Reason: nanny-close
2024-02-18 06:56:16,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34557. Reason: nanny-close
2024-02-18 06:56:16,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:16,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43619. Reason: nanny-close
2024-02-18 06:56:16,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42085. Reason: nanny-close
2024-02-18 06:56:16,943 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-18 06:56:16,943 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58872; closing.
2024-02-18 06:56:16,943 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239376.9437578')
2024-02-18 06:56:16,943 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-18 06:56:16,944 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-18 06:56:16,944 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:16,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58912; closing.
2024-02-18 06:56:16,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-18 06:56:16,945 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:16,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58882; closing.
2024-02-18 06:56:16,945 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:16,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42085', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239376.9457738')
2024-02-18 06:56:16,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58898; closing.
2024-02-18 06:56:16,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34557', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239376.9463098')
2024-02-18 06:56:16,946 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:16,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239376.946712')
2024-02-18 06:56:16,946 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:56:17,605 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:56:17,605 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:56:17,605 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:56:17,606 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-18 06:56:17,607 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-18 06:56:19,899 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:19,904 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:56:19,907 - distributed.scheduler - INFO - State start
2024-02-18 06:56:19,930 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:19,930 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-18 06:56:19,931 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:56:19,932 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-18 06:56:20,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36259'
2024-02-18 06:56:20,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44659'
2024-02-18 06:56:20,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44457'
2024-02-18 06:56:20,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43355'
2024-02-18 06:56:20,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45129'
2024-02-18 06:56:20,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45921'
2024-02-18 06:56:20,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46013'
2024-02-18 06:56:20,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44963'
2024-02-18 06:56:22,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,089 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,090 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46819
2024-02-18 06:56:22,090 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46819
2024-02-18 06:56:22,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42589
2024-02-18 06:56:22,090 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,090 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36021
2024-02-18 06:56:22,090 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,091 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36021
2024-02-18 06:56:22,091 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,091 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,091 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35987
2024-02-18 06:56:22,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0r7kku95
2024-02-18 06:56:22,091 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,091 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,091 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,091 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q_dz6m0j
2024-02-18 06:56:22,091 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-194f5d53-71b3-43bf-8a01-3bab24f37a9e
2024-02-18 06:56:22,091 - distributed.worker - INFO - Starting Worker plugin PreImport-6ee761f7-e3d0-4a06-bdb4-09db66a789b4
2024-02-18 06:56:22,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e0b0e8c-0b8d-4a5c-9890-f0c2ea2c74e4
2024-02-18 06:56:22,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc74fdfd-df92-451e-afa0-44d689c41e97
2024-02-18 06:56:22,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35069
2024-02-18 06:56:22,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35069
2024-02-18 06:56:22,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36471
2024-02-18 06:56:22,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,093 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,093 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,093 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nfduh0p3
2024-02-18 06:56:22,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a17c8e39-c17b-40b8-8e9e-da263b4f2a94
2024-02-18 06:56:22,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,150 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,151 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41155
2024-02-18 06:56:22,151 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41155
2024-02-18 06:56:22,151 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37251
2024-02-18 06:56:22,152 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,152 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,152 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,152 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,152 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rfsaqsf0
2024-02-18 06:56:22,152 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7965afe-e74e-47f0-870e-f559f68cd93c
2024-02-18 06:56:22,152 - distributed.worker - INFO - Starting Worker plugin PreImport-a1e210dc-7a9b-4e52-9ddf-9c331fd37b85
2024-02-18 06:56:22,152 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c40328f4-f505-4fca-9773-acaa6fa06e39
2024-02-18 06:56:22,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,173 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42899
2024-02-18 06:56:22,174 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42899
2024-02-18 06:56:22,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44071
2024-02-18 06:56:22,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,174 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,175 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,175 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0w6ojeva
2024-02-18 06:56:22,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43d0780f-fdbe-4e7c-ab97-531a581b2853
2024-02-18 06:56:22,175 - distributed.worker - INFO - Starting Worker plugin PreImport-c35de4cf-31e1-4a32-a2bf-e094f20458ae
2024-02-18 06:56:22,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46e1caae-be12-4b87-9f0d-68d1fc563798
2024-02-18 06:56:22,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,178 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39483
2024-02-18 06:56:22,178 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39483
2024-02-18 06:56:22,179 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37093
2024-02-18 06:56:22,179 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,179 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,179 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,179 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,179 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xx2th2z8
2024-02-18 06:56:22,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa34322f-f696-4f6b-b50d-de4f1ab62cd8
2024-02-18 06:56:22,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97bdf057-30aa-48d2-9d44-b76b4c638fda
2024-02-18 06:56:22,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,192 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42121
2024-02-18 06:56:22,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:22,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42121
2024-02-18 06:56:22,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45009
2024-02-18 06:56:22,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:22,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,192 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,192 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1dg8xsyk
2024-02-18 06:56:22,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0898259-e78d-4211-9a22-775a588a138c
2024-02-18 06:56:22,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a14430dc-b2e0-4c2b-a169-d5412e8ad130
2024-02-18 06:56:22,198 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:22,200 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38099
2024-02-18 06:56:22,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38099
2024-02-18 06:56:22,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38649
2024-02-18 06:56:22,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:22,200 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:22,200 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:22,200 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:22,200 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mmz1aprb
2024-02-18 06:56:22,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b61db7be-2ae1-474e-852f-401c8cdc1698
2024-02-18 06:56:22,201 - distributed.worker - INFO - Starting Worker plugin PreImport-5defd886-a1bd-46a4-8924-1fa42d66d07c
2024-02-18 06:56:22,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-330db9c9-3d02-4ee6-a6af-280d963b104b
2024-02-18 06:56:24,341 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,374 - distributed.worker - INFO - Starting Worker plugin PreImport-273a9311-f979-47f0-9bf6-edabb0ad77f3
2024-02-18 06:56:24,375 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbb0da73-278a-4b7f-af4c-083b71003c84
2024-02-18 06:56:24,376 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,440 - distributed.worker - INFO - Starting Worker plugin PreImport-747df66a-3f09-448b-ae29-ea103df02d36
2024-02-18 06:56:24,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-349f8fc6-cb04-4396-929e-b37db8e03c69
2024-02-18 06:56:24,442 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,529 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,548 - distributed.worker - INFO - Starting Worker plugin PreImport-8876f36a-acea-45b8-ba66-2bab1f379d29
2024-02-18 06:56:24,548 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,555 - distributed.worker - INFO - Starting Worker plugin PreImport-8d25acd5-76dd-4c6c-ab48-7a0ee3e1ca64
2024-02-18 06:56:24,557 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:24,560 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:26,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:26,161 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:26,206 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36259'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,207 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,208 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36021. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,211 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:26,213 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:26,391 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:26,392 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:26,393 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:26,395 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:26,395 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:26,395 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:26,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43355'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,402 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,403 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45921'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38099. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,403 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,404 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39483. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,405 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:26,406 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:26,408 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:26,410 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:26,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:26,424 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:26,424 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:26,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44963'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,462 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,462 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41155. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,465 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:26,466 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:26,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:26,597 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:26,597 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:26,606 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46013'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,607 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,608 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42121. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,610 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:26,612 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:26,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:26,677 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:26,677 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:26,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:26,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45129'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,708 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42899. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-18 06:56:26,713 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:26,717 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:33444 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-18 06:56:26,722 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55388 parent=55191 started daemon>
2024-02-18 06:56:26,722 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55384 parent=55191 started daemon>
2024-02-18 06:56:26,722 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55380 parent=55191 started daemon>
2024-02-18 06:56:26,723 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55376 parent=55191 started daemon>
2024-02-18 06:56:26,723 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55371 parent=55191 started daemon>
2024-02-18 06:56:26,723 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55366 parent=55191 started daemon>
2024-02-18 06:56:26,723 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55361 parent=55191 started daemon>
2024-02-18 06:56:27,010 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 55361 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-18 06:56:42,824 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:42,829 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36153 instead
  warnings.warn(
2024-02-18 06:56:42,834 - distributed.scheduler - INFO - State start
2024-02-18 06:56:42,835 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0r7kku95', purging
2024-02-18 06:56:42,836 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nfduh0p3', purging
2024-02-18 06:56:42,883 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:42,885 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-18 06:56:42,885 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:56:42,886 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-18 06:56:43,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39419'
2024-02-18 06:56:43,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36893'
2024-02-18 06:56:43,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45509'
2024-02-18 06:56:43,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45241'
2024-02-18 06:56:43,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40749'
2024-02-18 06:56:43,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36207'
2024-02-18 06:56:43,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35081'
2024-02-18 06:56:43,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38089'
2024-02-18 06:56:45,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39419'. Reason: nanny-close
2024-02-18 06:56:45,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36893'. Reason: nanny-close
2024-02-18 06:56:45,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45509'. Reason: nanny-close
2024-02-18 06:56:45,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45241'. Reason: nanny-close
2024-02-18 06:56:45,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40749'. Reason: nanny-close
2024-02-18 06:56:45,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36207'. Reason: nanny-close
2024-02-18 06:56:45,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35081'. Reason: nanny-close
2024-02-18 06:56:45,229 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38089'. Reason: nanny-close
2024-02-18 06:56:45,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42307
2024-02-18 06:56:45,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46317
2024-02-18 06:56:45,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42307
2024-02-18 06:56:45,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46317
2024-02-18 06:56:45,310 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43855
2024-02-18 06:56:45,310 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43495
2024-02-18 06:56:45,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,310 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,310 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,310 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,310 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,310 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j1l8ohv_
2024-02-18 06:56:45,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wzhxeqbd
2024-02-18 06:56:45,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7932d364-a364-46d4-9e91-25ba5994f5d0
2024-02-18 06:56:45,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1cf8d25-b372-433f-983d-bf8c5e1ab2c7
2024-02-18 06:56:45,311 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37253
2024-02-18 06:56:45,311 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37253
2024-02-18 06:56:45,311 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33459
2024-02-18 06:56:45,311 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,311 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,311 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,311 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3yywnubj
2024-02-18 06:56:45,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac91eea3-d9c6-46b2-af3d-a5cd0fcd593c
2024-02-18 06:56:45,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33473
2024-02-18 06:56:45,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33473
2024-02-18 06:56:45,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43911
2024-02-18 06:56:45,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,312 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,313 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,313 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3nm3bynh
2024-02-18 06:56:45,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-216b074c-5162-4856-8fb2-800fdb97cf6c
2024-02-18 06:56:45,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,319 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38275
2024-02-18 06:56:45,320 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38275
2024-02-18 06:56:45,320 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42937
2024-02-18 06:56:45,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,320 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,320 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,320 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,320 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dtk77482
2024-02-18 06:56:45,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb8f481c-9c43-442b-8062-fefd9a650d26
2024-02-18 06:56:45,320 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b8c6ae2-f287-451f-b484-5f167e6731d3
2024-02-18 06:56:45,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39437
2024-02-18 06:56:45,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39437
2024-02-18 06:56:45,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39261
2024-02-18 06:56:45,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,378 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,378 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fwreg1y9
2024-02-18 06:56:45,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62b4d0eb-cf1c-4554-b6d7-d4aa372d340b
2024-02-18 06:56:45,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b41cfede-917f-45f8-ab34-28cab44c2d6c
2024-02-18 06:56:45,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33161
2024-02-18 06:56:45,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33161
2024-02-18 06:56:45,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43131
2024-02-18 06:56:45,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,386 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,386 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,386 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_633vhbi
2024-02-18 06:56:45,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e92c274b-4cef-4d09-8c74-b937be955a5f
2024-02-18 06:56:45,387 - distributed.worker - INFO - Starting Worker plugin PreImport-5b40fbb6-e025-4374-8d8f-c88a98fc2904
2024-02-18 06:56:45,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45037b03-b530-42dc-9004-cf58e3f8ab28
2024-02-18 06:56:45,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:45,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:45,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:45,424 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39723
2024-02-18 06:56:45,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39723
2024-02-18 06:56:45,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32901
2024-02-18 06:56:45,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:45,424 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:45,424 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:45,424 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:45,424 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4o1jwznx
2024-02-18 06:56:45,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dacad0b3-d48a-420c-ba7c-eb82e9e28b6c
2024-02-18 06:56:45,425 - distributed.worker - INFO - Starting Worker plugin PreImport-6504d201-c5fc-4715-a1b5-b7cf0d0f1910
2024-02-18 06:56:45,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c05dd207-d224-41e7-81da-8a3af37460a6
2024-02-18 06:56:47,710 - distributed.worker - INFO - Starting Worker plugin PreImport-17ad4d5e-416d-4a91-a0f2-3cc3b5cf5891
2024-02-18 06:56:47,711 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,725 - distributed.worker - INFO - Starting Worker plugin PreImport-87c0bb66-42b5-4516-a88b-2d9dcf135bc0
2024-02-18 06:56:47,726 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-407ca524-9cf7-4dbd-a3b3-beaea343183c
2024-02-18 06:56:47,726 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,739 - distributed.worker - INFO - Starting Worker plugin PreImport-f91f77d7-3d5d-45b7-89ba-5919ea09db1b
2024-02-18 06:56:47,739 - distributed.worker - INFO - Starting Worker plugin PreImport-97946ff8-5ef8-4692-896c-31831001d1c4
2024-02-18 06:56:47,740 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31d2d552-5b92-45dc-a609-f655a0aa3d3f
2024-02-18 06:56:47,740 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-317daebf-b2fd-4e9d-a7a9-0146a6341347
2024-02-18 06:56:47,741 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,744 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,748 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f511a60-ba76-41f5-a964-131c3565ce7e
2024-02-18 06:56:47,751 - distributed.worker - INFO - Starting Worker plugin PreImport-1f4070be-6997-43cd-a324-60de48c9b2c5
2024-02-18 06:56:47,753 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,825 - distributed.worker - INFO - Starting Worker plugin PreImport-5ef47486-8206-4e96-8713-3e6aca161723
2024-02-18 06:56:47,825 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,828 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:47,833 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:49,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:49,857 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:49,857 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:49,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:49,898 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:49,900 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39723. Reason: nanny-close
2024-02-18 06:56:49,902 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:49,903 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:49,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:49,922 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:49,922 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:49,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:49,949 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:49,950 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33161. Reason: nanny-close
2024-02-18 06:56:49,952 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:49,954 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:50,050 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:50,051 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:50,051 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:50,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:50,054 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:50,055 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42307. Reason: nanny-close
2024-02-18 06:56:50,057 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:50,059 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:50,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:50,139 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:50,139 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:50,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:50,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:50,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33473. Reason: nanny-close
2024-02-18 06:56:50,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:50,157 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:40452 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-18 06:56:50,259 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55667 parent=55470 started daemon>
2024-02-18 06:56:50,259 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55659 parent=55470 started daemon>
2024-02-18 06:56:50,260 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55655 parent=55470 started daemon>
2024-02-18 06:56:50,260 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55650 parent=55470 started daemon>
2024-02-18 06:56:50,260 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55645 parent=55470 started daemon>
2024-02-18 06:56:50,260 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55640 parent=55470 started daemon>
2024-02-18 06:56:50,260 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55636 parent=55470 started daemon>
2024-02-18 06:56:50,534 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 55655 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-18 06:56:52,955 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:52,964 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:56:52,968 - distributed.scheduler - INFO - State start
2024-02-18 06:56:52,970 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dtk77482', purging
2024-02-18 06:56:52,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wzhxeqbd', purging
2024-02-18 06:56:52,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3yywnubj', purging
2024-02-18 06:56:52,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fwreg1y9', purging
2024-02-18 06:56:52,994 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:56:52,995 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-18 06:56:52,996 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:56:52,996 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:56:53,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36167'
2024-02-18 06:56:53,142 - distributed.scheduler - INFO - Receive client connection: Client-e458aa70-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:53,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41127'
2024-02-18 06:56:53,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51736
2024-02-18 06:56:53,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40107'
2024-02-18 06:56:53,174 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43139'
2024-02-18 06:56:53,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38525'
2024-02-18 06:56:53,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41737'
2024-02-18 06:56:53,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40963'
2024-02-18 06:56:53,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45257'
2024-02-18 06:56:53,399 - distributed.scheduler - INFO - Receive client connection: Client-e5c57049-ce2a-11ee-9642-d8c49764f6bb
2024-02-18 06:56:53,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51744
2024-02-18 06:56:55,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,126 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,126 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35881
2024-02-18 06:56:55,127 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35881
2024-02-18 06:56:55,127 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34199
2024-02-18 06:56:55,127 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,127 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,127 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,127 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,127 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ax_j6v74
2024-02-18 06:56:55,127 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30516714-7bd4-4fdd-8198-286905d09855
2024-02-18 06:56:55,127 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f0e6d26-2d9a-4e1f-ba95-8963cf84b199
2024-02-18 06:56:55,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,169 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,170 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38873
2024-02-18 06:56:55,170 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38873
2024-02-18 06:56:55,170 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36923
2024-02-18 06:56:55,170 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,170 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,170 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,171 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-acl4fr2k
2024-02-18 06:56:55,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a34d72d-911d-4e24-a2a0-489b3482f65f
2024-02-18 06:56:55,171 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5afcac3-00b9-400e-b82d-7b760aea28da
2024-02-18 06:56:55,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,370 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,370 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35117
2024-02-18 06:56:55,371 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35117
2024-02-18 06:56:55,371 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38301
2024-02-18 06:56:55,371 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,371 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,371 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,371 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,371 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7l2zi3tf
2024-02-18 06:56:55,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1124b4a5-d48e-4af7-9e66-1dd6affcbf52
2024-02-18 06:56:55,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,372 - distributed.worker - INFO - Starting Worker plugin PreImport-766f17bd-eac4-488a-b98d-6fbcef5d176e
2024-02-18 06:56:55,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fbc4e9e-2a89-4a00-8359-561d44b8ec58
2024-02-18 06:56:55,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41799
2024-02-18 06:56:55,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41799
2024-02-18 06:56:55,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39309
2024-02-18 06:56:55,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,373 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,373 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rcez76n_
2024-02-18 06:56:55,373 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1d9fcad-25f6-4bd7-8446-e1882ce182be
2024-02-18 06:56:55,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,374 - distributed.worker - INFO - Starting Worker plugin PreImport-bcd6c9dd-abb3-4077-b7d6-e3560fc5b2a1
2024-02-18 06:56:55,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d5e9723-90d4-48ff-9357-5a95945ff54e
2024-02-18 06:56:55,375 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,376 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35663
2024-02-18 06:56:55,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35663
2024-02-18 06:56:55,377 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40267
2024-02-18 06:56:55,377 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,377 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,377 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,377 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8lr0p6yb
2024-02-18 06:56:55,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e001558f-688b-4322-b1b5-811bd56813b7
2024-02-18 06:56:55,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:56:55,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:56:55,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41653
2024-02-18 06:56:55,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41653
2024-02-18 06:56:55,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42427
2024-02-18 06:56:55,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,381 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,381 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8992rrm
2024-02-18 06:56:55,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2521d6f4-b478-41ac-a265-7a3fb946bf9a
2024-02-18 06:56:55,382 - distributed.worker - INFO - Starting Worker plugin PreImport-d35df982-d7f6-420f-866f-9fb9c1f49f7c
2024-02-18 06:56:55,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1bda43d3-9e0c-4168-8cb6-c0b8c7e3a54c
2024-02-18 06:56:55,383 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39673
2024-02-18 06:56:55,383 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39673
2024-02-18 06:56:55,383 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45993
2024-02-18 06:56:55,383 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,383 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,383 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,383 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-00kk7tfe
2024-02-18 06:56:55,384 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1d5f305-99f0-4568-859f-d67615fe5e03
2024-02-18 06:56:55,384 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0be7a582-060e-41ab-8329-030db3390455
2024-02-18 06:56:55,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:56:55,387 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46669
2024-02-18 06:56:55,387 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46669
2024-02-18 06:56:55,387 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42595
2024-02-18 06:56:55,387 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,387 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,388 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:56:55,388 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:56:55,388 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ewtcwsdw
2024-02-18 06:56:55,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37c90ae5-2e03-494f-97ca-1d5068162dd1
2024-02-18 06:56:55,388 - distributed.worker - INFO - Starting Worker plugin PreImport-fb87500f-a0ec-4dd2-b9dd-1c03757e037e
2024-02-18 06:56:55,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecf205d9-e5c7-4329-a934-a23dcdb67cae
2024-02-18 06:56:55,846 - distributed.worker - INFO - Starting Worker plugin PreImport-70402b6b-6543-42a9-8a30-cb102614484d
2024-02-18 06:56:55,847 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,876 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35881', status: init, memory: 0, processing: 0>
2024-02-18 06:56:55,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35881
2024-02-18 06:56:55,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51768
2024-02-18 06:56:55,878 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:55,879 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:55,880 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:55,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:55,952 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-18 06:56:56,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:56,250 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:56:56,252 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:56:56,255 - distributed.scheduler - INFO - Remove client Client-e5c57049-ce2a-11ee-9642-d8c49764f6bb
2024-02-18 06:56:56,255 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51744; closing.
2024-02-18 06:56:56,255 - distributed.scheduler - INFO - Remove client Client-e5c57049-ce2a-11ee-9642-d8c49764f6bb
2024-02-18 06:56:56,256 - distributed.scheduler - INFO - Close client connection: Client-e5c57049-ce2a-11ee-9642-d8c49764f6bb
2024-02-18 06:56:57,422 - distributed.worker - INFO - Starting Worker plugin PreImport-222e1750-22fc-4ac9-91c0-47ace5169973
2024-02-18 06:56:57,423 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,427 - distributed.worker - INFO - Starting Worker plugin PreImport-b39ac98a-0206-4869-950d-b809b2a73a66
2024-02-18 06:56:57,427 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95f02160-eaeb-491f-b1a7-15f5d69464ff
2024-02-18 06:56:57,428 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38873', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38873
2024-02-18 06:56:57,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51788
2024-02-18 06:56:57,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,446 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,452 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35663', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,453 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35663
2024-02-18 06:56:57,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51798
2024-02-18 06:56:57,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,454 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,455 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,468 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35117', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35117
2024-02-18 06:56:57,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51802
2024-02-18 06:56:57,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,471 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,471 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,472 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,479 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,481 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,484 - distributed.worker - INFO - Starting Worker plugin PreImport-67606e70-7e06-4f44-be6c-d095f4139b90
2024-02-18 06:56:57,486 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,504 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46669', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,505 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46669
2024-02-18 06:56:57,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51824
2024-02-18 06:56:57,506 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41653', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,506 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,506 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41653
2024-02-18 06:56:57,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51812
2024-02-18 06:56:57,507 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,507 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,509 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,509 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,512 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41799', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41799
2024-02-18 06:56:57,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51828
2024-02-18 06:56:57,515 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,516 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,516 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,518 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39673', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39673
2024-02-18 06:56:57,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51840
2024-02-18 06:56:57,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:56:57,521 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:56:57,521 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:56:57,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:56:57,547 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,547 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,549 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,549 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,560 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:56:57,569 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:56:57,570 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:56:57,572 - distributed.scheduler - INFO - Remove client Client-e458aa70-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:57,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51736; closing.
2024-02-18 06:56:57,573 - distributed.scheduler - INFO - Remove client Client-e458aa70-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:57,573 - distributed.scheduler - INFO - Close client connection: Client-e458aa70-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:56:57,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36167'. Reason: nanny-close
2024-02-18 06:56:57,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41127'. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40107'. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38873. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43139'. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35663. Reason: nanny-close
2024-02-18 06:56:57,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38525'. Reason: nanny-close
2024-02-18 06:56:57,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41653. Reason: nanny-close
2024-02-18 06:56:57,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41737'. Reason: nanny-close
2024-02-18 06:56:57,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41799. Reason: nanny-close
2024-02-18 06:56:57,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40963'. Reason: nanny-close
2024-02-18 06:56:57,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51788; closing.
2024-02-18 06:56:57,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46669. Reason: nanny-close
2024-02-18 06:56:57,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45257'. Reason: nanny-close
2024-02-18 06:56:57,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38873', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5784426')
2024-02-18 06:56:57,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35881. Reason: nanny-close
2024-02-18 06:56:57,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:56:57,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39673. Reason: nanny-close
2024-02-18 06:56:57,579 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35117. Reason: nanny-close
2024-02-18 06:56:57,579 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51798; closing.
2024-02-18 06:56:57,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,581 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35663', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5814233')
2024-02-18 06:56:57,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,581 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,581 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51812; closing.
2024-02-18 06:56:57,581 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,581 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51824; closing.
2024-02-18 06:56:57,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:56:57,582 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5828004')
2024-02-18 06:56:57,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46669', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.583097')
2024-02-18 06:56:57,583 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,583 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51828; closing.
2024-02-18 06:56:57,583 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,583 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51768; closing.
2024-02-18 06:56:57,584 - distributed.nanny - INFO - Worker closed
2024-02-18 06:56:57,584 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5844114')
2024-02-18 06:56:57,584 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5847173')
2024-02-18 06:56:57,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51840; closing.
2024-02-18 06:56:57,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51802; closing.
2024-02-18 06:56:57,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5856934')
2024-02-18 06:56:57,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.5861413')
2024-02-18 06:56:57,586 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:56:57,903 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41103', status: init, memory: 0, processing: 0>
2024-02-18 06:56:57,904 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41103
2024-02-18 06:56:57,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51856
2024-02-18 06:56:57,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51856; closing.
2024-02-18 06:56:57,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239417.9548185')
2024-02-18 06:56:57,955 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:56:58,541 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:56:58,541 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:56:58,542 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:56:58,543 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-18 06:56:58,544 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-18 06:57:00,949 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:00,953 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:57:00,957 - distributed.scheduler - INFO - State start
2024-02-18 06:57:00,979 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:00,980 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-18 06:57:00,980 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:57:00,980 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:57:01,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45703'
2024-02-18 06:57:01,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46819'
2024-02-18 06:57:01,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36455'
2024-02-18 06:57:01,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34475'
2024-02-18 06:57:01,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36139'
2024-02-18 06:57:01,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35469'
2024-02-18 06:57:01,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42765'
2024-02-18 06:57:01,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35099'
2024-02-18 06:57:02,683 - distributed.scheduler - INFO - Receive client connection: Client-e92fc2f5-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:02,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52746
2024-02-18 06:57:03,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,190 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46471
2024-02-18 06:57:03,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46471
2024-02-18 06:57:03,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45139
2024-02-18 06:57:03,190 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,190 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,190 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,190 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2n_u1e8v
2024-02-18 06:57:03,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bcadf881-bd36-4baf-867b-32d2ef96071f
2024-02-18 06:57:03,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,417 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,419 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,420 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43489
2024-02-18 06:57:03,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43489
2024-02-18 06:57:03,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41875
2024-02-18 06:57:03,421 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,421 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,421 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,421 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,421 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tkng05ns
2024-02-18 06:57:03,421 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34757
2024-02-18 06:57:03,421 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34757
2024-02-18 06:57:03,421 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33613
2024-02-18 06:57:03,421 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,421 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,421 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,421 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,421 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ed15907-e9da-4c3a-9ebb-4c8e51bd53a3
2024-02-18 06:57:03,421 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y31h7wq2
2024-02-18 06:57:03,421 - distributed.worker - INFO - Starting Worker plugin PreImport-769026b4-92c4-4297-a528-3fd0ff475d3c
2024-02-18 06:57:03,422 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b83c2043-2832-44b0-97ae-47f75a333db2
2024-02-18 06:57:03,422 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-162c6b18-eb65-4bf2-8309-ef05b11bd1e6
2024-02-18 06:57:03,422 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e3a9d0c-d6c8-4a53-a1af-878c895d8e8a
2024-02-18 06:57:03,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,424 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,424 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39887
2024-02-18 06:57:03,425 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39887
2024-02-18 06:57:03,425 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33021
2024-02-18 06:57:03,425 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,425 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,425 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,425 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lkajww0v
2024-02-18 06:57:03,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b773c0c8-cc93-4cdb-9c57-f5484fee8916
2024-02-18 06:57:03,425 - distributed.worker - INFO - Starting Worker plugin PreImport-0ca8c6bd-1c26-440e-9419-b89a66e3d815
2024-02-18 06:57:03,426 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ac9dd3c-4e7f-4f96-b702-d3c05b58f005
2024-02-18 06:57:03,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38885
2024-02-18 06:57:03,426 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38885
2024-02-18 06:57:03,426 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46841
2024-02-18 06:57:03,426 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,426 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,426 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,426 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tv0bqaoq
2024-02-18 06:57:03,426 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f899d840-ba83-4d6f-9862-c036a5239018
2024-02-18 06:57:03,427 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c073468-cc96-4cf0-a4f7-a1d65704a223
2024-02-18 06:57:03,430 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,431 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39823
2024-02-18 06:57:03,431 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39823
2024-02-18 06:57:03,431 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42233
2024-02-18 06:57:03,431 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,431 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,431 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,431 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,431 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wpg8c1jd
2024-02-18 06:57:03,432 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c265b696-e58e-4dc8-88ff-3bf33fc68fae
2024-02-18 06:57:03,433 - distributed.worker - INFO - Starting Worker plugin PreImport-7b63011c-56c2-4607-8e59-fb2d364d099c
2024-02-18 06:57:03,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-290b5b0b-1dfe-46e1-ab6f-720ebe9aae2d
2024-02-18 06:57:03,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:03,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:03,438 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:03,439 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36219
2024-02-18 06:57:03,439 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36219
2024-02-18 06:57:03,439 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39977
2024-02-18 06:57:03,439 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,439 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,440 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,440 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,440 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39959
2024-02-18 06:57:03,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ogv11jnt
2024-02-18 06:57:03,440 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39959
2024-02-18 06:57:03,440 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38289
2024-02-18 06:57:03,440 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,440 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,440 - distributed.worker - INFO - Starting Worker plugin PreImport-0c02a919-581b-4c4a-b06c-79e7ddb98c9e
2024-02-18 06:57:03,440 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:03,440 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:03,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e456f41-6808-41b1-bb6f-4079456ed719
2024-02-18 06:57:03,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qb006itj
2024-02-18 06:57:03,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20844610-c3d4-4ee2-8661-eaf2be7b05bd
2024-02-18 06:57:03,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6aebf64d-42d0-4b49-a712-f4b962dd9d58
2024-02-18 06:57:03,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4875f06a-0dff-4382-96d4-6eb522dab61e
2024-02-18 06:57:03,688 - distributed.worker - INFO - Starting Worker plugin PreImport-879fdc9e-29c1-47ad-a1c9-128fe952a5f3
2024-02-18 06:57:03,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27c0ebf6-4c7d-4a5a-8752-637ad1ee50a2
2024-02-18 06:57:03,689 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,725 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46471', status: init, memory: 0, processing: 0>
2024-02-18 06:57:03,726 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46471
2024-02-18 06:57:03,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52764
2024-02-18 06:57:03,727 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:03,729 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:03,729 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:03,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,451 - distributed.worker - INFO - Starting Worker plugin PreImport-521cace8-1ea8-407a-a54f-614f7b9da3bb
2024-02-18 06:57:05,453 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,457 - distributed.worker - INFO - Starting Worker plugin PreImport-a7ded785-06a7-4bd5-8e00-23a23115ea88
2024-02-18 06:57:05,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,465 - distributed.worker - INFO - Starting Worker plugin PreImport-3c346942-e543-4dbf-bb4c-2c05df34a0d8
2024-02-18 06:57:05,467 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,468 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,474 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,474 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39959', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,488 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39959
2024-02-18 06:57:05,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52794
2024-02-18 06:57:05,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,489 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,490 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,489 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,491 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34757', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,491 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34757
2024-02-18 06:57:05,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52780
2024-02-18 06:57:05,492 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43489', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,493 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43489
2024-02-18 06:57:05,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52808
2024-02-18 06:57:05,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,495 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,495 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,495 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,495 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39887', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39887
2024-02-18 06:57:05,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52822
2024-02-18 06:57:05,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,499 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,499 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,501 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36219', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,501 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36219
2024-02-18 06:57:05,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52834
2024-02-18 06:57:05,502 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38885', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,503 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38885
2024-02-18 06:57:05,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52814
2024-02-18 06:57:05,503 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,503 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,505 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,505 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,507 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,520 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39823', status: init, memory: 0, processing: 0>
2024-02-18 06:57:05,521 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39823
2024-02-18 06:57:05,521 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52844
2024-02-18 06:57:05,522 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:05,523 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:05,523 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:05,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:05,538 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,538 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,550 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,551 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:57:05,562 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,564 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:05,566 - distributed.scheduler - INFO - Remove client Client-e92fc2f5-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:05,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52746; closing.
2024-02-18 06:57:05,567 - distributed.scheduler - INFO - Remove client Client-e92fc2f5-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:05,567 - distributed.scheduler - INFO - Close client connection: Client-e92fc2f5-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:05,568 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45703'. Reason: nanny-close
2024-02-18 06:57:05,568 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46819'. Reason: nanny-close
2024-02-18 06:57:05,569 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36455'. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34757. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34475'. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46471. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36139'. Reason: nanny-close
2024-02-18 06:57:05,570 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39887. Reason: nanny-close
2024-02-18 06:57:05,571 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35469'. Reason: nanny-close
2024-02-18 06:57:05,571 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,571 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43489. Reason: nanny-close
2024-02-18 06:57:05,571 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42765'. Reason: nanny-close
2024-02-18 06:57:05,571 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,571 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35099'. Reason: nanny-close
2024-02-18 06:57:05,572 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38885. Reason: nanny-close
2024-02-18 06:57:05,572 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,572 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39959. Reason: nanny-close
2024-02-18 06:57:05,572 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,572 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52822; closing.
2024-02-18 06:57:05,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39887', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.573051')
2024-02-18 06:57:05,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,573 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36219. Reason: nanny-close
2024-02-18 06:57:05,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,573 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,574 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,574 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52808; closing.
2024-02-18 06:57:05,574 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,574 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52780; closing.
2024-02-18 06:57:05,574 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,575 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43489', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.5753858')
2024-02-18 06:57:05,575 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,575 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,575 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.5757928')
2024-02-18 06:57:05,575 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:05,576 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52814; closing.
2024-02-18 06:57:05,576 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,576 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52764; closing.
2024-02-18 06:57:05,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39823. Reason: nanny-close
2024-02-18 06:57:05,577 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38885', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.5770416')
2024-02-18 06:57:05,577 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52794; closing.
2024-02-18 06:57:05,577 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46471', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.5775776')
2024-02-18 06:57:05,577 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39959', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.5781796')
2024-02-18 06:57:05,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52834; closing.
2024-02-18 06:57:05,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:05,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.579406')
2024-02-18 06:57:05,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52844; closing.
2024-02-18 06:57:05,580 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39823', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239425.5803056')
2024-02-18 06:57:05,580 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:57:05,580 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:05,580 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52844>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-18 06:57:06,585 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:57:06,585 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:57:06,586 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:57:06,587 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-18 06:57:06,587 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-18 06:57:08,823 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:08,828 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44355 instead
  warnings.warn(
2024-02-18 06:57:08,832 - distributed.scheduler - INFO - State start
2024-02-18 06:57:08,855 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:08,856 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-18 06:57:08,856 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44355/status
2024-02-18 06:57:08,857 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:57:09,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38049'
2024-02-18 06:57:09,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41275'
2024-02-18 06:57:09,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45021'
2024-02-18 06:57:09,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32853'
2024-02-18 06:57:09,207 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45591'
2024-02-18 06:57:09,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32883'
2024-02-18 06:57:09,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38675'
2024-02-18 06:57:09,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45681'
2024-02-18 06:57:09,687 - distributed.scheduler - INFO - Receive client connection: Client-eddb4db3-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:09,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53152
2024-02-18 06:57:11,014 - distributed.scheduler - INFO - Receive client connection: Client-ede297de-ce2a-11ee-9550-d8c49764f6bb
2024-02-18 06:57:11,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49642
2024-02-18 06:57:11,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,071 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39109
2024-02-18 06:57:11,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39109
2024-02-18 06:57:11,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43707
2024-02-18 06:57:11,071 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,071 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,071 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,071 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z0b3k9yh
2024-02-18 06:57:11,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a7637f7-5997-43b4-8f67-9812601ca079
2024-02-18 06:57:11,071 - distributed.worker - INFO - Starting Worker plugin RMMSetup-95589290-a76c-4dea-b03f-a998d658cd18
2024-02-18 06:57:11,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,078 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,079 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42431
2024-02-18 06:57:11,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42431
2024-02-18 06:57:11,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45615
2024-02-18 06:57:11,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,079 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,079 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,079 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_l96zuxs
2024-02-18 06:57:11,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b504b956-079a-450c-ad1e-5bf886b80ec3
2024-02-18 06:57:11,115 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,120 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45245
2024-02-18 06:57:11,120 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45245
2024-02-18 06:57:11,120 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40487
2024-02-18 06:57:11,120 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,120 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,120 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,120 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,120 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kucw0x26
2024-02-18 06:57:11,121 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb4fe741-4255-4f9c-96c8-ffef93de60d7
2024-02-18 06:57:11,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,121 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ed3f60c-f5fa-4553-b766-999f10a691d6
2024-02-18 06:57:11,121 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34767
2024-02-18 06:57:11,121 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34767
2024-02-18 06:57:11,121 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38659
2024-02-18 06:57:11,122 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,122 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,122 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,122 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,122 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8b19f_2r
2024-02-18 06:57:11,122 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3b5c179-eb7a-4f22-903f-f8de269c8adb
2024-02-18 06:57:11,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,144 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,144 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,145 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36889
2024-02-18 06:57:11,145 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36889
2024-02-18 06:57:11,145 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35773
2024-02-18 06:57:11,145 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,145 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36719
2024-02-18 06:57:11,145 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,145 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36719
2024-02-18 06:57:11,145 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,145 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43769
2024-02-18 06:57:11,145 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,145 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,145 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yudao_93
2024-02-18 06:57:11,146 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,146 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,146 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,146 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1_nw8ebo
2024-02-18 06:57:11,146 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b362330-6285-4169-97fa-2925c4c0dc49
2024-02-18 06:57:11,146 - distributed.worker - INFO - Starting Worker plugin PreImport-44c85271-7725-4ae4-b69b-524ccfc2b613
2024-02-18 06:57:11,146 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e6b81b6-8806-4529-ad47-a174a9304cce
2024-02-18 06:57:11,146 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae7d9ad9-a8a4-4f9a-b46a-bead0370fb88
2024-02-18 06:57:11,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,378 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35685
2024-02-18 06:57:11,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35685
2024-02-18 06:57:11,379 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35437
2024-02-18 06:57:11,379 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,379 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,379 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-clk0cb09
2024-02-18 06:57:11,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db2fb2a1-c969-4e01-bb62-d8963f985790
2024-02-18 06:57:11,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:11,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:11,383 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0c4726f-cd42-4ef9-b123-556f34c32703
2024-02-18 06:57:11,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:11,387 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33865
2024-02-18 06:57:11,387 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33865
2024-02-18 06:57:11,387 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44283
2024-02-18 06:57:11,387 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:11,387 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:11,387 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:11,387 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:11,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rnyy1yd4
2024-02-18 06:57:11,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2fd5ea2e-2b30-4ee5-9672-23b0e7c7b46f
2024-02-18 06:57:13,307 - distributed.worker - INFO - Starting Worker plugin PreImport-3a2fca69-5046-4a94-9e44-61b47c4f6f7a
2024-02-18 06:57:13,308 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:13,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39109', status: init, memory: 0, processing: 0>
2024-02-18 06:57:13,333 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39109
2024-02-18 06:57:13,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49666
2024-02-18 06:57:13,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:13,335 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:13,335 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:13,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:14,734 - distributed.worker - INFO - Starting Worker plugin PreImport-7f58f48a-084b-4ec8-9158-0350e3fff54f
2024-02-18 06:57:14,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23097660-cb06-489d-bde7-e37405008d9d
2024-02-18 06:57:14,735 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:14,759 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42431', status: init, memory: 0, processing: 0>
2024-02-18 06:57:14,760 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42431
2024-02-18 06:57:14,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49684
2024-02-18 06:57:14,761 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:14,762 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:14,762 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:14,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,145 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,158 - distributed.worker - INFO - Starting Worker plugin PreImport-bd475d01-bc94-42f6-9f12-308121cbef55
2024-02-18 06:57:15,160 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,167 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36889', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,168 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36889
2024-02-18 06:57:15,168 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49686
2024-02-18 06:57:15,169 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:15,169 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:15,170 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,171 - distributed.worker - INFO - Starting Worker plugin PreImport-f081c822-0186-4fc1-b318-a1f500b44990
2024-02-18 06:57:15,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32f84b72-28db-463d-bbc4-80ce9115bc60
2024-02-18 06:57:15,173 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,191 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45245', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,191 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45245
2024-02-18 06:57:15,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49700
2024-02-18 06:57:15,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:15,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:15,194 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36719', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36719
2024-02-18 06:57:15,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49704
2024-02-18 06:57:15,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:15,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:15,201 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36705', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36705
2024-02-18 06:57:15,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49716
2024-02-18 06:57:15,229 - distributed.worker - INFO - Starting Worker plugin PreImport-531c1ed6-ffa8-4c1e-9240-e705703a4241
2024-02-18 06:57:15,230 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,243 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fc7163f-a489-4fff-af38-903f58f289f8
2024-02-18 06:57:15,244 - distributed.worker - INFO - Starting Worker plugin PreImport-ef8fcd2c-40e4-4de0-bfa1-79004a59400e
2024-02-18 06:57:15,245 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,245 - distributed.worker - INFO - Starting Worker plugin PreImport-9217a1d1-9cc9-4b23-8c9d-8d506c1f1a0b
2024-02-18 06:57:15,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-421e6a6a-dbe4-48e8-8906-225752cbe699
2024-02-18 06:57:15,247 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35685', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35685
2024-02-18 06:57:15,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49728
2024-02-18 06:57:15,263 - distributed.worker - WARNING - Mismatched versions found

+---------+---------------------------------------------+-----------+----------------------+
| Package | Worker-935fe832-9966-4554-9527-23242e95d218 | Scheduler | Workers              |
+---------+---------------------------------------------+-----------+----------------------+
| numpy   | 1.24.4                                      | 1.24.4    | {'1.24.4', '1.26.4'} |
| pandas  | 1.5.3                                       | 1.5.3     | {'1.5.3', '2.1.4'}   |
+---------+---------------------------------------------+-----------+----------------------+
2024-02-18 06:57:15,264 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:15,265 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:15,265 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33865', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,278 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33865
2024-02-18 06:57:15,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49732
2024-02-18 06:57:15,278 - distributed.worker - WARNING - Mismatched versions found

+---------+---------------------------------------------+-----------+----------------------+
| Package | Worker-94a30e24-3ec8-45b1-92d7-102163f9662f | Scheduler | Workers              |
+---------+---------------------------------------------+-----------+----------------------+
| numpy   | 1.24.4                                      | 1.24.4    | {'1.24.4', '1.26.4'} |
| pandas  | 1.5.3                                       | 1.5.3     | {'1.5.3', '2.1.4'}   |
+---------+---------------------------------------------+-----------+----------------------+
2024-02-18 06:57:15,279 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34767', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:15,280 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34767
2024-02-18 06:57:15,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49740
2024-02-18 06:57:15,281 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:15,280 - distributed.worker - WARNING - Mismatched versions found

+---------+---------------------------------------------+-----------+----------------------+
| Package | Worker-b71629fd-48d6-45d3-af6d-755d3e83e887 | Scheduler | Workers              |
+---------+---------------------------------------------+-----------+----------------------+
| numpy   | 1.24.4                                      | 1.24.4    | {'1.24.4', '1.26.4'} |
| pandas  | 1.5.3                                       | 1.5.3     | {'1.5.3', '2.1.4'}   |
+---------+---------------------------------------------+-----------+----------------------+
2024-02-18 06:57:15,281 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:15,282 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,283 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:15,283 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:15,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:15,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38701', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,331 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38701
2024-02-18 06:57:15,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49742
2024-02-18 06:57:15,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36207', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,357 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36207
2024-02-18 06:57:15,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49752
2024-02-18 06:57:15,389 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34399', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,389 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34399
2024-02-18 06:57:15,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49762
2024-02-18 06:57:15,401 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39349', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,401 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39349
2024-02-18 06:57:15,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49784
2024-02-18 06:57:15,402 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35221', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,403 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35221
2024-02-18 06:57:15,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49782
2024-02-18 06:57:15,404 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38019', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,405 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38019
2024-02-18 06:57:15,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49768
2024-02-18 06:57:15,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45077', status: init, memory: 0, processing: 0>
2024-02-18 06:57:15,413 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45077
2024-02-18 06:57:15,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49786
2024-02-18 06:57:25,730 - distributed.scheduler - INFO - Remove client Client-eddb4db3-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:25,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53152; closing.
2024-02-18 06:57:25,731 - distributed.scheduler - INFO - Remove client Client-eddb4db3-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:25,731 - distributed.scheduler - INFO - Close client connection: Client-eddb4db3-ce2a-11ee-960d-d8c49764f6bb
2024-02-18 06:57:25,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38049'. Reason: nanny-close
2024-02-18 06:57:25,733 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41275'. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45021'. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39109. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32853'. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,734 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42431. Reason: nanny-close
2024-02-18 06:57:25,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45591'. Reason: nanny-close
2024-02-18 06:57:25,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32883'. Reason: nanny-close
2024-02-18 06:57:25,735 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45245. Reason: nanny-close
2024-02-18 06:57:25,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,735 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34767. Reason: nanny-close
2024-02-18 06:57:25,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38675'. Reason: nanny-close
2024-02-18 06:57:25,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45681'. Reason: nanny-close
2024-02-18 06:57:25,736 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49666; closing.
2024-02-18 06:57:25,736 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36889. Reason: nanny-close
2024-02-18 06:57:25,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:25,736 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39109', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.7366877')
2024-02-18 06:57:25,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36719. Reason: nanny-close
2024-02-18 06:57:25,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33865. Reason: nanny-close
2024-02-18 06:57:25,737 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,737 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35685. Reason: nanny-close
2024-02-18 06:57:25,738 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,738 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,738 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,739 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,740 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49684; closing.
2024-02-18 06:57:25,740 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,740 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,740 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,741 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49700; closing.
2024-02-18 06:57:25,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49740; closing.
2024-02-18 06:57:25,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:25,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42431', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.742301')
2024-02-18 06:57:25,742 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49686; closing.
2024-02-18 06:57:25,743 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.7437115')
2024-02-18 06:57:25,744 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.7440555')
2024-02-18 06:57:25,744 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.7444036')
2024-02-18 06:57:25,744 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:25,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49732; closing.
2024-02-18 06:57:25,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49704; closing.
2024-02-18 06:57:25,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33865', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.745714')
2024-02-18 06:57:25,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36719', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.7461154')
2024-02-18 06:57:25,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49728; closing.
2024-02-18 06:57:25,747 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.7473285')
2024-02-18 06:57:25,748 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49728>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-18 06:57:25,849 - distributed.scheduler - INFO - Remove client Client-ede297de-ce2a-11ee-9550-d8c49764f6bb
2024-02-18 06:57:25,849 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49642; closing.
2024-02-18 06:57:25,850 - distributed.scheduler - INFO - Remove client Client-ede297de-ce2a-11ee-9550-d8c49764f6bb
2024-02-18 06:57:25,850 - distributed.scheduler - INFO - Close client connection: Client-ede297de-ce2a-11ee-9550-d8c49764f6bb
2024-02-18 06:57:25,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49752; closing.
2024-02-18 06:57:25,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49768; closing.
2024-02-18 06:57:25,857 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36207', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.8571393')
2024-02-18 06:57:25,857 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.8577163')
2024-02-18 06:57:25,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49782; closing.
2024-02-18 06:57:25,858 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.8589106')
2024-02-18 06:57:25,859 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49742; closing.
2024-02-18 06:57:25,860 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49784; closing.
2024-02-18 06:57:25,861 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.861091')
2024-02-18 06:57:25,861 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49716; closing.
2024-02-18 06:57:25,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49782>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-18 06:57:25,865 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.8649082')
2024-02-18 06:57:25,865 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36705', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.8653288')
2024-02-18 06:57:25,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49762; closing.
2024-02-18 06:57:25,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49786; closing.
2024-02-18 06:57:25,866 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.866372')
2024-02-18 06:57:25,866 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239445.866729')
2024-02-18 06:57:25,866 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:57:26,999 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:57:27,000 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:57:27,001 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:57:27,002 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-18 06:57:27,003 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-18 06:57:29,533 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:29,537 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40821 instead
  warnings.warn(
2024-02-18 06:57:29,542 - distributed.scheduler - INFO - State start
2024-02-18 06:57:29,565 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:29,565 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-18 06:57:29,566 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:57:29,567 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-18 06:57:29,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38891'
2024-02-18 06:57:31,621 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:31,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:32,240 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:32,241 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43157
2024-02-18 06:57:32,241 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43157
2024-02-18 06:57:32,241 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-18 06:57:32,241 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:32,241 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:32,241 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:32,241 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-18 06:57:32,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6g8tytqt
2024-02-18 06:57:32,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0754651-7b12-42e1-a65a-3d7deed2dc8c
2024-02-18 06:57:32,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ec3d350-27fc-4d43-89f8-ec2d0b01af8e
2024-02-18 06:57:32,242 - distributed.worker - INFO - Starting Worker plugin PreImport-7e79a8ae-06c2-4685-af24-7c2dc5d8fa04
2024-02-18 06:57:32,242 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:33,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38891'. Reason: nanny-close
2024-02-18 06:57:33,570 - distributed.worker - WARNING - Mismatched versions found

+---------+---------------------------------------------+-----------+----------------------+
| Package | Worker-4743cf89-2a3c-422a-9d50-2eec479a54b5 | Scheduler | Workers              |
+---------+---------------------------------------------+-----------+----------------------+
| numpy   | 1.24.4                                      | 1.26.4    | {'1.24.4', '1.26.4'} |
| pandas  | 1.5.3                                       | 2.1.4     | {'2.1.4', '1.5.3'}   |
+---------+---------------------------------------------+-----------+----------------------+
2024-02-18 06:57:33,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:33,571 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:33,571 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:33,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:33,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:33,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43157. Reason: nanny-close
2024-02-18 06:57:33,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:33,593 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-18 06:57:38,681 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:38,686 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37727 instead
  warnings.warn(
2024-02-18 06:57:38,691 - distributed.scheduler - INFO - State start
2024-02-18 06:57:38,715 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:38,716 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-18 06:57:38,717 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:57:38,718 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-18 06:57:38,866 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44389'
2024-02-18 06:57:41,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:41,112 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:41,814 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:41,815 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42245
2024-02-18 06:57:41,815 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42245
2024-02-18 06:57:41,815 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35763
2024-02-18 06:57:41,815 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:41,815 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:41,815 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:41,816 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-18 06:57:41,816 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x64x6hb_
2024-02-18 06:57:41,816 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc357ebd-8834-42be-a805-13f20bc56406
2024-02-18 06:57:41,816 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed5f9f94-1ffd-40fc-97dd-b94c30df42fe
2024-02-18 06:57:41,816 - distributed.worker - INFO - Starting Worker plugin PreImport-a6fc2702-71ae-4bda-afb3-a788899062c7
2024-02-18 06:57:41,817 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:42,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44389'. Reason: nanny-close
2024-02-18 06:57:42,297 - distributed.worker - WARNING - Mismatched versions found

+---------+---------------------------------------------+-----------+----------------------+
| Package | Worker-79fa0970-fda5-4a9d-adc6-76d55e2ff17a | Scheduler | Workers              |
+---------+---------------------------------------------+-----------+----------------------+
| numpy   | 1.24.4                                      | 1.26.4    | {'1.24.4', '1.26.4'} |
| pandas  | 1.5.3                                       | 2.1.4     | {'1.5.3', '2.1.4'}   |
+---------+---------------------------------------------+-----------+----------------------+
2024-02-18 06:57:42,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:42,298 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:42,298 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:42,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:57:42,307 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:42,309 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42245. Reason: nanny-close
2024-02-18 06:57:42,311 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:57:42,312 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-18 06:57:44,995 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:45,000 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:57:45,004 - distributed.scheduler - INFO - State start
2024-02-18 06:57:45,026 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:45,027 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-18 06:57:45,028 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:57:45,028 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:57:45,454 - distributed.scheduler - INFO - Receive client connection: Client-03dadd67-ce2b-11ee-9550-d8c49764f6bb
2024-02-18 06:57:45,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49892
2024-02-18 06:57:47,671 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:49884'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49884>: Stream is closed
2024-02-18 06:57:47,950 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:57:47,951 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:57:47,951 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:57:47,954 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-18 06:57:47,954 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-18 06:57:50,204 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:50,209 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:57:50,212 - distributed.scheduler - INFO - State start
2024-02-18 06:57:50,322 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:50,323 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-18 06:57:50,324 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:57:50,325 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:57:50,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45931'
2024-02-18 06:57:52,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:52,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:52,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:52,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45925
2024-02-18 06:57:52,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45925
2024-02-18 06:57:52,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39877
2024-02-18 06:57:52,344 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-18 06:57:52,344 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:52,344 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:52,344 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-18 06:57:52,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-wsu14iiu
2024-02-18 06:57:52,344 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17268f33-4fa4-451f-a227-1ba82467af02
2024-02-18 06:57:52,344 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3878915e-150a-4a02-b2c1-3cbb4b1add0c
2024-02-18 06:57:52,345 - distributed.worker - INFO - Starting Worker plugin PreImport-79434b3c-75ab-4ba0-a407-d641e3d2d0f2
2024-02-18 06:57:52,345 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:52,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45925', status: init, memory: 0, processing: 0>
2024-02-18 06:57:52,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45925
2024-02-18 06:57:52,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59104
2024-02-18 06:57:52,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:52,552 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-18 06:57:52,552 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:52,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-18 06:57:52,754 - distributed.scheduler - INFO - Receive client connection: Client-0683c548-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:57:52,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59106
2024-02-18 06:57:52,760 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:57:52,766 - distributed.scheduler - INFO - Remove client Client-0683c548-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:57:52,767 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59106; closing.
2024-02-18 06:57:52,767 - distributed.scheduler - INFO - Remove client Client-0683c548-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:57:52,767 - distributed.scheduler - INFO - Close client connection: Client-0683c548-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:57:52,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45931'. Reason: nanny-close
2024-02-18 06:57:52,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:57:52,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45925. Reason: nanny-close
2024-02-18 06:57:52,771 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59104; closing.
2024-02-18 06:57:52,771 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-18 06:57:52,772 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239472.772127')
2024-02-18 06:57:52,772 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:57:52,773 - distributed.nanny - INFO - Worker closed
2024-02-18 06:57:53,383 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:57:53,384 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:57:53,384 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:57:53,385 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-18 06:57:53,386 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-18 06:57:55,718 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:55,723 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:57:55,726 - distributed.scheduler - INFO - State start
2024-02-18 06:57:55,749 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:57:55,750 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-18 06:57:55,751 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:57:55,751 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:57:55,799 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37139', status: init, memory: 0, processing: 0>
2024-02-18 06:57:55,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37139
2024-02-18 06:57:55,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37864
2024-02-18 06:57:55,849 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35855', status: init, memory: 0, processing: 0>
2024-02-18 06:57:55,849 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35855
2024-02-18 06:57:55,849 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37880
2024-02-18 06:57:55,853 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37864; closing.
2024-02-18 06:57:55,853 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239475.8538582')
2024-02-18 06:57:55,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43077', status: init, memory: 0, processing: 0>
2024-02-18 06:57:55,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43077
2024-02-18 06:57:55,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37882
2024-02-18 06:57:55,900 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37882; closing.
2024-02-18 06:57:55,901 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239475.9011972')
2024-02-18 06:57:55,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37880; closing.
2024-02-18 06:57:55,902 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239475.9024148')
2024-02-18 06:57:55,902 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:57:55,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44583'
2024-02-18 06:57:55,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36467'
2024-02-18 06:57:55,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38255'
2024-02-18 06:57:55,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44247'
2024-02-18 06:57:55,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37409'
2024-02-18 06:57:55,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42041'
2024-02-18 06:57:55,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38487'
2024-02-18 06:57:55,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39011'
2024-02-18 06:57:56,116 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33901', status: init, memory: 0, processing: 0>
2024-02-18 06:57:56,116 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33901
2024-02-18 06:57:56,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37952
2024-02-18 06:57:56,157 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37952; closing.
2024-02-18 06:57:56,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33901', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239476.1577697')
2024-02-18 06:57:56,158 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:57:56,424 - distributed.scheduler - INFO - Receive client connection: Client-09c06f0e-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:57:56,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37972
2024-02-18 06:57:57,867 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,871 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39445
2024-02-18 06:57:57,872 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39445
2024-02-18 06:57:57,872 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40619
2024-02-18 06:57:57,872 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,872 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,872 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,872 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,872 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b5jcetvh
2024-02-18 06:57:57,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9c99678-13c5-4bcd-b8e0-16f77e85db21
2024-02-18 06:57:57,873 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b84a878-c7ac-4830-b321-601e173a9dd7
2024-02-18 06:57:57,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,896 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,901 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,902 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44881
2024-02-18 06:57:57,902 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44881
2024-02-18 06:57:57,902 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44111
2024-02-18 06:57:57,902 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,902 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,902 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,902 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,902 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5y4u8tt1
2024-02-18 06:57:57,902 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d61e6029-1ea7-4662-a491-fa377fb42b2a
2024-02-18 06:57:57,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,918 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34729
2024-02-18 06:57:57,919 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34729
2024-02-18 06:57:57,919 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40031
2024-02-18 06:57:57,919 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,919 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,919 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,919 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,919 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fwgki44a
2024-02-18 06:57:57,920 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ed6ded6-8956-4647-b64b-8107fb221c85
2024-02-18 06:57:57,920 - distributed.worker - INFO - Starting Worker plugin PreImport-19dc8a97-2db2-4533-9a30-322d9f396d8c
2024-02-18 06:57:57,920 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9603c8fc-fd8d-47bc-8e0d-1ebda87aa7d6
2024-02-18 06:57:57,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,926 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41193
2024-02-18 06:57:57,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41193
2024-02-18 06:57:57,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42721
2024-02-18 06:57:57,926 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,926 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,926 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,926 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xfbvcb7l
2024-02-18 06:57:57,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39353
2024-02-18 06:57:57,927 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39353
2024-02-18 06:57:57,927 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41275
2024-02-18 06:57:57,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b742613-e7c2-4313-8cb8-38b3e0f8d89e
2024-02-18 06:57:57,927 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,927 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,927 - distributed.worker - INFO - Starting Worker plugin PreImport-3cf2b3e2-a2b6-42e6-8c91-7850404fe7bf
2024-02-18 06:57:57,927 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,927 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,927 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f32bc42f-2ebe-45a7-90c4-e88a6e8f578c
2024-02-18 06:57:57,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ithx_tqv
2024-02-18 06:57:57,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59b398ba-47e1-486a-b397-cabfc0aec2d2
2024-02-18 06:57:57,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37817
2024-02-18 06:57:57,928 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37817
2024-02-18 06:57:57,928 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38643
2024-02-18 06:57:57,928 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,928 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,928 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,928 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,928 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-54hd4rso
2024-02-18 06:57:57,928 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ace4b65-8fa4-4404-9589-1233ec181ce7
2024-02-18 06:57:57,928 - distributed.worker - INFO - Starting Worker plugin PreImport-b790f94b-5a31-45c9-a7f3-b9eaf1280c11
2024-02-18 06:57:57,929 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bf2b9d2-c03f-4de4-baa9-e43a68534aa1
2024-02-18 06:57:57,931 - distributed.worker - INFO - Starting Worker plugin PreImport-38286272-c8fa-403c-86c2-91f1a9253b2f
2024-02-18 06:57:57,932 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7721203-5938-4d20-803b-0e0af91e0473
2024-02-18 06:57:57,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,937 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39039
2024-02-18 06:57:57,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39039
2024-02-18 06:57:57,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39575
2024-02-18 06:57:57,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,937 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,938 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,938 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,938 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3d3brde9
2024-02-18 06:57:57,938 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-477a1c4a-44a5-462a-bbe2-18a091001367
2024-02-18 06:57:57,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f03e9a93-73c1-43c9-ab26-04e145698b85
2024-02-18 06:57:57,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:57:57,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:57:57,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:57:57,970 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42895
2024-02-18 06:57:57,970 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42895
2024-02-18 06:57:57,970 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35259
2024-02-18 06:57:57,970 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:57:57,970 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:57,970 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:57:57,970 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-18 06:57:57,970 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2yost6s6
2024-02-18 06:57:57,970 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-249e80c9-77f2-4bb7-b2d9-d64e6fc5cc32
2024-02-18 06:57:57,971 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c41c105-4e9b-48c3-a69a-756fa99e1913
2024-02-18 06:57:59,953 - distributed.worker - INFO - Starting Worker plugin PreImport-8b46ac37-ebe5-4e0f-a1b6-0507dc61fac2
2024-02-18 06:57:59,955 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:59,987 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39445', status: init, memory: 0, processing: 0>
2024-02-18 06:57:59,988 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39445
2024-02-18 06:57:59,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41848
2024-02-18 06:57:59,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:57:59,991 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:57:59,991 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:57:59,993 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,089 - distributed.worker - INFO - Starting Worker plugin PreImport-ee99cf34-4548-4eb9-8663-389b784a4c27
2024-02-18 06:58:00,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb95597a-6285-4ceb-8791-4d62957a2029
2024-02-18 06:58:00,095 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44881', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44881
2024-02-18 06:58:00,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41856
2024-02-18 06:58:00,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,131 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,131 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,173 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,197 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41193', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,197 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41193
2024-02-18 06:58:00,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41864
2024-02-18 06:58:00,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,199 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,199 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,203 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,217 - distributed.worker - INFO - Starting Worker plugin PreImport-c5f38cc0-1097-4030-a51b-740e66ec4bc6
2024-02-18 06:58:00,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,222 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,225 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34729', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34729
2024-02-18 06:58:00,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41878
2024-02-18 06:58:00,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,226 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,227 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,227 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,236 - distributed.worker - INFO - Starting Worker plugin PreImport-69ccb74e-f756-4a4a-993d-cb8486e769e9
2024-02-18 06:58:00,237 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,248 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37817', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,249 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37817
2024-02-18 06:58:00,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41904
2024-02-18 06:58:00,250 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39039', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39039
2024-02-18 06:58:00,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41886
2024-02-18 06:58:00,251 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,251 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,253 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,253 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,257 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39353', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,257 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39353
2024-02-18 06:58:00,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41900
2024-02-18 06:58:00,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,260 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,260 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42895', status: init, memory: 0, processing: 0>
2024-02-18 06:58:00,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42895
2024-02-18 06:58:00,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41910
2024-02-18 06:58:00,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:00,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:00,264 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:00,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:00,348 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,348 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,348 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-18 06:58:00,353 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,353 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,353 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,354 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,354 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,354 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,354 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,354 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,365 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,367 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:00,371 - distributed.scheduler - INFO - Remove client Client-09c06f0e-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:00,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37972; closing.
2024-02-18 06:58:00,371 - distributed.scheduler - INFO - Remove client Client-09c06f0e-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:00,372 - distributed.scheduler - INFO - Close client connection: Client-09c06f0e-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:00,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44583'. Reason: nanny-close
2024-02-18 06:58:00,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36467'. Reason: nanny-close
2024-02-18 06:58:00,374 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38255'. Reason: nanny-close
2024-02-18 06:58:00,374 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39445. Reason: nanny-close
2024-02-18 06:58:00,374 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44247'. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44881. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37409'. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37817. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42041'. Reason: nanny-close
2024-02-18 06:58:00,375 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41193. Reason: nanny-close
2024-02-18 06:58:00,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38487'. Reason: nanny-close
2024-02-18 06:58:00,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39353. Reason: nanny-close
2024-02-18 06:58:00,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39011'. Reason: nanny-close
2024-02-18 06:58:00,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39039. Reason: nanny-close
2024-02-18 06:58:00,377 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41848; closing.
2024-02-18 06:58:00,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:00,377 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39445', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.3779185')
2024-02-18 06:58:00,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42895. Reason: nanny-close
2024-02-18 06:58:00,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,378 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41904; closing.
2024-02-18 06:58:00,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,378 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,378 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37817', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.3788865')
2024-02-18 06:58:00,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34729. Reason: nanny-close
2024-02-18 06:58:00,379 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,379 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41856; closing.
2024-02-18 06:58:00,380 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41864; closing.
2024-02-18 06:58:00,381 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,381 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,381 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:00,382 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:00,380 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41904>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-18 06:58:00,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.383709')
2024-02-18 06:58:00,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41900; closing.
2024-02-18 06:58:00,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41886; closing.
2024-02-18 06:58:00,384 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.3844755')
2024-02-18 06:58:00,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39353', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.385114')
2024-02-18 06:58:00,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.3854773')
2024-02-18 06:58:00,385 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41910; closing.
2024-02-18 06:58:00,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42895', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.3865116')
2024-02-18 06:58:00,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41878; closing.
2024-02-18 06:58:00,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239480.3872721')
2024-02-18 06:58:00,387 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:58:00,387 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41878>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-18 06:58:00,845 - distributed.scheduler - INFO - Receive client connection: Client-0df8c2f8-ce2b-11ee-9550-d8c49764f6bb
2024-02-18 06:58:00,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41924
2024-02-18 06:58:01,389 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:58:01,389 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:58:01,390 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:58:01,392 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-18 06:58:01,393 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-18 06:58:03,924 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:58:03,931 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39283 instead
  warnings.warn(
2024-02-18 06:58:03,937 - distributed.scheduler - INFO - State start
2024-02-18 06:58:04,271 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:58:04,273 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-18 06:58:04,274 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:58:04,275 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-18 06:58:04,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40849'
2024-02-18 06:58:06,414 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:58:06,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:58:06,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:58:06,428 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39407
2024-02-18 06:58:06,428 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39407
2024-02-18 06:58:06,428 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41031
2024-02-18 06:58:06,428 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:58:06,428 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:06,429 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:58:06,429 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-18 06:58:06,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w_gfoksi
2024-02-18 06:58:06,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc35806d-6789-4c3c-a9c0-1b38bd205f32
2024-02-18 06:58:08,161 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40849'. Reason: nanny-close
2024-02-18 06:58:08,496 - distributed.worker - INFO - Starting Worker plugin PreImport-843956cb-17b7-49fe-83b7-fc560bbecffc
2024-02-18 06:58:08,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dd53e3d-3cc3-421b-b3ca-fb79d43cbb39
2024-02-18 06:58:08,498 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:08,548 - distributed.worker - WARNING - Mismatched versions found

+---------+---------------------------------------------+-----------+----------------------+
| Package | Worker-5a0955e5-96d8-4363-84c8-7be12c7849ed | Scheduler | Workers              |
+---------+---------------------------------------------+-----------+----------------------+
| numpy   | 1.24.4                                      | 1.26.4    | {'1.24.4', '1.26.4'} |
| pandas  | 1.5.3                                       | 2.1.4     | {'2.1.4', '1.5.3'}   |
+---------+---------------------------------------------+-----------+----------------------+
2024-02-18 06:58:08,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:08,550 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:08,550 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:08,551 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:08,564 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:08,566 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39407. Reason: nanny-close
2024-02-18 06:58:08,568 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:08,569 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-18 06:58:11,492 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:58:11,497 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-18 06:58:11,501 - distributed.scheduler - INFO - State start
2024-02-18 06:58:11,524 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-18 06:58:11,525 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-18 06:58:11,526 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-18 06:58:11,526 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-18 06:58:11,607 - distributed.scheduler - INFO - Receive client connection: Client-1393b202-ce2b-11ee-9550-d8c49764f6bb
2024-02-18 06:58:11,620 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60010
2024-02-18 06:58:11,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37233'
2024-02-18 06:58:11,626 - distributed.scheduler - INFO - Receive client connection: Client-131cff1b-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:11,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60038
2024-02-18 06:58:13,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-18 06:58:13,472 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-18 06:58:13,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-18 06:58:13,477 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44677
2024-02-18 06:58:13,477 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44677
2024-02-18 06:58:13,477 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39763
2024-02-18 06:58:13,477 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-18 06:58:13,477 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:13,477 - distributed.worker - INFO -               Threads:                          1
2024-02-18 06:58:13,477 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-18 06:58:13,477 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d7mzraw6
2024-02-18 06:58:13,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-147658b0-180f-494e-ad2b-366e10844081
2024-02-18 06:58:13,957 - distributed.worker - INFO - Starting Worker plugin PreImport-352444da-bf6b-4486-8c65-c38d3318a8ce
2024-02-18 06:58:13,958 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c479b8a2-4e94-4f0f-8989-16acd128b251
2024-02-18 06:58:13,959 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:14,023 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44677', status: init, memory: 0, processing: 0>
2024-02-18 06:58:14,024 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44677
2024-02-18 06:58:14,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60070
2024-02-18 06:58:14,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-18 06:58:14,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-18 06:58:14,026 - distributed.worker - INFO - -------------------------------------------------
2024-02-18 06:58:14,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-18 06:58:14,108 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:14,110 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-18 06:58:14,112 - distributed.scheduler - INFO - Remove client Client-1393b202-ce2b-11ee-9550-d8c49764f6bb
2024-02-18 06:58:14,112 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60010; closing.
2024-02-18 06:58:14,113 - distributed.scheduler - INFO - Remove client Client-1393b202-ce2b-11ee-9550-d8c49764f6bb
2024-02-18 06:58:14,113 - distributed.scheduler - INFO - Close client connection: Client-1393b202-ce2b-11ee-9550-d8c49764f6bb
2024-02-18 06:58:14,115 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-18 06:58:14,119 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:14,120 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-18 06:58:14,123 - distributed.scheduler - INFO - Remove client Client-131cff1b-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:14,123 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60038; closing.
2024-02-18 06:58:14,123 - distributed.scheduler - INFO - Remove client Client-131cff1b-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:14,124 - distributed.scheduler - INFO - Close client connection: Client-131cff1b-ce2b-11ee-960d-d8c49764f6bb
2024-02-18 06:58:14,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37233'. Reason: nanny-close
2024-02-18 06:58:14,125 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-18 06:58:14,126 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44677. Reason: nanny-close
2024-02-18 06:58:14,128 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-18 06:58:14,128 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60070; closing.
2024-02-18 06:58:14,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44677', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708239494.1290092')
2024-02-18 06:58:14,129 - distributed.scheduler - INFO - Lost all workers
2024-02-18 06:58:14,130 - distributed.nanny - INFO - Worker closed
2024-02-18 06:58:14,790 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-18 06:58:14,790 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-18 06:58:14,791 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-18 06:58:14,792 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-18 06:58:14,793 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45611 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41507 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] 2024-02-18 06:59:17,591 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:53993 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7ff018165800 error: Connection reset by remote peer')
2024-02-18 06:59:17,631 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7ff018165800 error: Connection reset by remote peer')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46031 instead
  warnings.warn(
2024-02-18 07:00:00,619 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #012] ep: 0x7fc862efb0c0, tag: 0xb574078a2be2c155, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #012] ep: 0x7fc862efb0c0, tag: 0xb574078a2be2c155, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34473 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33055 instead
  warnings.warn(
2024-02-18 07:00:28,550 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-18 07:00:28,613 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-18 07:00:28,613 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-18 07:00:28,613 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-18 07:00:28,613 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1582' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33925 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41061 instead
  warnings.warn(
[1708239653.372650] [dgx13:61108:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:49512) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42593 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37241 instead
  warnings.warn(
[1708239685.399915] [dgx13:61638:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:37600) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35795 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39691 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39465 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38915 instead
  warnings.warn(
[1708240056.679420] [dgx13:66065:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:47286) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33549 instead
  warnings.warn(
[1708240090.677967] [dgx13:66680:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:37011) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37971 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36675 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42617 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33345 instead
  warnings.warn(
[1708240400.755022] [dgx13:71515:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:59889) failed: Address already in use
[1708240406.053953] [dgx13:71611:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:44662) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34439 instead
  warnings.warn(
[1708240421.596152] [dgx13:71956:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:59126) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33541 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44503 instead
  warnings.warn(
[1708240518.948380] [dgx13:73274:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:58910) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38209 instead
  warnings.warn(
[1708240558.972850] [dgx13:73723:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:34034) failed: Address already in use
[1708240565.207773] [dgx13:73812:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:58554) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34949 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33253 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41363 instead
  warnings.warn(
[1708240664.725794] [dgx13:75057:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:45194) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44363 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37213 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36383 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37647 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] [1708240770.364364] [dgx13:76711:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:33986) failed: Address already in use
[1708240770.687158] [dgx13:76711:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:44056) failed: Address already in use
2024-02-18 07:19:33,735 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35549 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33141 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38427 instead
  warnings.warn(
[1708240838.425462] [dgx13:77537:0]            sock.c:481  UCX  ERROR bind(fd=133 addr=0.0.0.0:55408) failed: Address already in use
2024-02-18 07:20:50,831 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-18 07:20:50,838 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-18 07:20:50,870 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:45598'.
2024-02-18 07:20:50,871 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:45598'. Shutting down.
2024-02-18 07:20:50,964 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc590cc68b0>>, <Task finished name='Task-21' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-21' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-18 07:20:51,596 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 3)
Function:  subgraph_callable-1b9f2c72-7de3-48ad-a5b8-8a5cd644
args:      (    key  payload1
24   24        24
25   25        25
26   26        26
27   27        27
28   28        28
29   29        29
30   30        30
31   31        31, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-18 07:20:51,602 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 1)
Function:  subgraph_callable-1b9f2c72-7de3-48ad-a5b8-8a5cd644
args:      (    key  payload1
8     8         8
9     9         9
10   10        10
11   11        11
12   12        12
13   13        13
14   14        14
15   15        15, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-18 07:20:51,671 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 4)
Function:  subgraph_callable-1b9f2c72-7de3-48ad-a5b8-8a5cd644
args:      (    key  payload1
32   32        32
33   33        33
34   34        34
35   35        35
36   36        36
37   37        37
38   38        38
39   39        39, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-18 07:20:52,267 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 2)
Function:  subgraph_callable-1b9f2c72-7de3-48ad-a5b8-8a5cd644
args:      (    key  payload1
16   16        16
17   17        17
18   18        18
19   19        19
20   20        20
21   21        21
22   22        22
23   23        23, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-18 07:20:52,802 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 0)
Function:  subgraph_callable-1b9f2c72-7de3-48ad-a5b8-8a5cd644
args:      (   key  payload1
0    0         0
1    1         1
2    2         2
3    3         3
4    4         4
5    5         5
6    6         6
7    7         7, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-18 07:20:52,967 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 21 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
