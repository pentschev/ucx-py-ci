============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-18 06:40:23,161 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:23,165 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36841 instead
  warnings.warn(
2023-12-18 06:40:23,170 - distributed.scheduler - INFO - State start
2023-12-18 06:40:23,981 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:23,982 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-18 06:40:23,983 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36841/status
2023-12-18 06:40:23,984 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:40:24,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44895'
2023-12-18 06:40:24,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40027'
2023-12-18 06:40:24,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37185'
2023-12-18 06:40:24,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38483'
2023-12-18 06:40:24,433 - distributed.scheduler - INFO - Receive client connection: Client-50d181f6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:24,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50780
2023-12-18 06:40:26,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:26,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:26,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:26,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:26,052 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:26,052 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:26,059 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:26,059 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:26,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:26,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:26,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-18 06:40:26,070 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44581
2023-12-18 06:40:26,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44581
2023-12-18 06:40:26,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38485
2023-12-18 06:40:26,071 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:26,071 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:26,071 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:26,071 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:26,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9pwvxgzd
2023-12-18 06:40:26,071 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40a9f6a5-0aed-4e48-988f-2d45e8a660a4
2023-12-18 06:40:26,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-103820bc-f055-4633-94c2-48901fcf5451
2023-12-18 06:40:26,071 - distributed.worker - INFO - Starting Worker plugin PreImport-3e195b15-1b80-477f-bc1a-83d16a5e2b5f
2023-12-18 06:40:26,072 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:26,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:26,292 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44581', status: init, memory: 0, processing: 0>
2023-12-18 06:40:26,293 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44581
2023-12-18 06:40:26,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50804
2023-12-18 06:40:26,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:26,295 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:26,295 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:26,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:27,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36173
2023-12-18 06:40:27,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36173
2023-12-18 06:40:27,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35337
2023-12-18 06:40:27,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:27,602 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,602 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:27,602 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:27,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-dfj_g2lt
2023-12-18 06:40:27,603 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-635ad3d5-7478-4611-b6ea-24bbda54c6ab
2023-12-18 06:40:27,603 - distributed.worker - INFO - Starting Worker plugin PreImport-f1407e7b-c30e-4266-b498-8ecbbb25abc6
2023-12-18 06:40:27,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08fec33b-6e5a-47ef-a42e-df343b661e59
2023-12-18 06:40:27,603 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,603 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41555
2023-12-18 06:40:27,604 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41555
2023-12-18 06:40:27,604 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44051
2023-12-18 06:40:27,604 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:27,604 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,604 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:27,604 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:27,604 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ynz6_h5p
2023-12-18 06:40:27,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fcbaa3da-6883-4fba-8e4c-77457f0661ba
2023-12-18 06:40:27,605 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e29707ce-96e2-4ac1-9387-112cfb7b52df
2023-12-18 06:40:27,606 - distributed.worker - INFO - Starting Worker plugin PreImport-320c44c3-4b41-492a-b5f9-04efe28560ff
2023-12-18 06:40:27,606 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,632 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36173', status: init, memory: 0, processing: 0>
2023-12-18 06:40:27,633 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36173
2023-12-18 06:40:27,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50834
2023-12-18 06:40:27,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:27,634 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:27,634 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:27,646 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41555', status: init, memory: 0, processing: 0>
2023-12-18 06:40:27,646 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41555
2023-12-18 06:40:27,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50844
2023-12-18 06:40:27,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:27,649 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:27,649 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:27,693 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46363
2023-12-18 06:40:27,694 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46363
2023-12-18 06:40:27,695 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37797
2023-12-18 06:40:27,695 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:27,695 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,695 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:27,695 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:27,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9rf5t_hd
2023-12-18 06:40:27,696 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12dcd064-c5ea-4dc0-84a7-aa54339f9efa
2023-12-18 06:40:27,696 - distributed.worker - INFO - Starting Worker plugin PreImport-5db66306-b520-4a82-95ae-769d30723699
2023-12-18 06:40:27,696 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e69e610-830c-4fa4-be20-f0680afb1e3d
2023-12-18 06:40:27,697 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,733 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46363', status: init, memory: 0, processing: 0>
2023-12-18 06:40:27,733 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46363
2023-12-18 06:40:27,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50850
2023-12-18 06:40:27,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:27,736 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:27,736 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:27,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:27,754 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:27,754 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:27,754 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:27,755 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:27,759 - distributed.scheduler - INFO - Remove client Client-50d181f6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:27,760 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50780; closing.
2023-12-18 06:40:27,760 - distributed.scheduler - INFO - Remove client Client-50d181f6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:27,760 - distributed.scheduler - INFO - Close client connection: Client-50d181f6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:27,761 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44895'. Reason: nanny-close
2023-12-18 06:40:27,762 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:27,762 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40027'. Reason: nanny-close
2023-12-18 06:40:27,762 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:27,763 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41555. Reason: nanny-close
2023-12-18 06:40:27,763 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37185'. Reason: nanny-close
2023-12-18 06:40:27,763 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38483'. Reason: nanny-close
2023-12-18 06:40:27,763 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36173. Reason: nanny-close
2023-12-18 06:40:27,763 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:27,764 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44581. Reason: nanny-close
2023-12-18 06:40:27,765 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:27,765 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50834; closing.
2023-12-18 06:40:27,765 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:27,765 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881627.7657259')
2023-12-18 06:40:27,766 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:27,766 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:27,767 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50804; closing.
2023-12-18 06:40:27,767 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:27,767 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50844; closing.
2023-12-18 06:40:27,767 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:27,767 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881627.7679033')
2023-12-18 06:40:27,768 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41555', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881627.7683518')
2023-12-18 06:40:27,768 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:50804>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:50804>: Stream is closed
2023-12-18 06:40:27,782 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:27,783 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46363. Reason: nanny-close
2023-12-18 06:40:27,785 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:27,785 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50850; closing.
2023-12-18 06:40:27,785 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881627.785759')
2023-12-18 06:40:27,786 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:27,787 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:29,179 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:40:29,179 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:40:29,179 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:40:29,180 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-18 06:40:29,181 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-18 06:40:31,571 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:31,576 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46319 instead
  warnings.warn(
2023-12-18 06:40:31,581 - distributed.scheduler - INFO - State start
2023-12-18 06:40:31,792 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:31,794 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:40:31,795 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46319/status
2023-12-18 06:40:31,795 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:40:32,717 - distributed.scheduler - INFO - Receive client connection: Client-55d39ad9-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:32,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50774
2023-12-18 06:40:33,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44619'
2023-12-18 06:40:33,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38569'
2023-12-18 06:40:33,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41593'
2023-12-18 06:40:33,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37329'
2023-12-18 06:40:33,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37987'
2023-12-18 06:40:33,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41239'
2023-12-18 06:40:33,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46293'
2023-12-18 06:40:33,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35853'
2023-12-18 06:40:35,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,058 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,058 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,061 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,106 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,120 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:35,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:35,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:35,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:37,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42859
2023-12-18 06:40:37,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42859
2023-12-18 06:40:37,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33699
2023-12-18 06:40:37,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:37,823 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:37,823 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:37,824 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:37,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8gc3h0_1
2023-12-18 06:40:37,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15a63f05-1711-418c-85a3-8d0c0e48aa22
2023-12-18 06:40:37,824 - distributed.worker - INFO - Starting Worker plugin PreImport-b3f86eb8-b37d-44b3-ab81-59266eb6207a
2023-12-18 06:40:37,824 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19c849a5-45a3-4218-b1a2-0842d074eb7e
2023-12-18 06:40:37,876 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:37,906 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42859', status: init, memory: 0, processing: 0>
2023-12-18 06:40:37,908 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42859
2023-12-18 06:40:37,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50864
2023-12-18 06:40:37,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:37,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:37,910 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:37,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,117 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35071
2023-12-18 06:40:38,118 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35071
2023-12-18 06:40:38,118 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43005
2023-12-18 06:40:38,118 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,118 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,118 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,118 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,119 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ex0r73bu
2023-12-18 06:40:38,119 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52363fc7-4fe1-4b8d-a4cb-bffae444986c
2023-12-18 06:40:38,119 - distributed.worker - INFO - Starting Worker plugin RMMSetup-949efcd9-60cd-44f5-9d94-cef423cd7bfb
2023-12-18 06:40:38,132 - distributed.worker - INFO - Starting Worker plugin PreImport-998445e0-add0-4c1c-ab08-298b6de4fe0c
2023-12-18 06:40:38,133 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,190 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35071', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,191 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35071
2023-12-18 06:40:38,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50872
2023-12-18 06:40:38,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,194 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41137
2023-12-18 06:40:38,363 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41137
2023-12-18 06:40:38,363 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42689
2023-12-18 06:40:38,363 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,363 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,363 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,363 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,364 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-077t2tzs
2023-12-18 06:40:38,363 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34741
2023-12-18 06:40:38,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca5d4577-dd0c-4e29-9ff1-580313c5790c
2023-12-18 06:40:38,364 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34741
2023-12-18 06:40:38,364 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37573
2023-12-18 06:40:38,364 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,364 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,364 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,365 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,363 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43601
2023-12-18 06:40:38,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7c9otq0j
2023-12-18 06:40:38,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43601
2023-12-18 06:40:38,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44431
2023-12-18 06:40:38,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,365 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,365 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,365 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79dd7b0c-454b-4152-8d00-c5cc752cdd39
2023-12-18 06:40:38,365 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tkvqafed
2023-12-18 06:40:38,365 - distributed.worker - INFO - Starting Worker plugin PreImport-95d0c378-bad6-43ff-b647-624792caee8a
2023-12-18 06:40:38,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45061
2023-12-18 06:40:38,365 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d72c0a5-46d3-4145-8906-ec1a011f34d4
2023-12-18 06:40:38,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45061
2023-12-18 06:40:38,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46637
2023-12-18 06:40:38,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,366 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28d1f4f6-fcae-4fe0-898d-a378097449f2
2023-12-18 06:40:38,366 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,366 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,366 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fbfw2lwq
2023-12-18 06:40:38,366 - distributed.worker - INFO - Starting Worker plugin PreImport-4e0c8ca6-8089-4970-91e9-b44d616309d0
2023-12-18 06:40:38,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-226ab0a1-f897-4e98-b9e4-4e667e38d042
2023-12-18 06:40:38,366 - distributed.worker - INFO - Starting Worker plugin PreImport-afc5515b-a868-4671-bee0-cc8f4af5401f
2023-12-18 06:40:38,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09e12add-d38a-4dd3-9e45-2a3464b52431
2023-12-18 06:40:38,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c423e585-3951-4447-b788-07ce5101fbb1
2023-12-18 06:40:38,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41511
2023-12-18 06:40:38,368 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41511
2023-12-18 06:40:38,368 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34663
2023-12-18 06:40:38,368 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,369 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,369 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bj0w6iq7
2023-12-18 06:40:38,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-298164f3-eb74-4473-ac0f-ed162b962df8
2023-12-18 06:40:38,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34419
2023-12-18 06:40:38,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34419
2023-12-18 06:40:38,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45737
2023-12-18 06:40:38,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,372 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,372 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:38,372 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:38,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gx9kqkw7
2023-12-18 06:40:38,373 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f709ef87-aab6-4866-b003-53cd770327dc
2023-12-18 06:40:38,374 - distributed.worker - INFO - Starting Worker plugin PreImport-49113475-82c9-4598-87ad-a3c58dab4ca2
2023-12-18 06:40:38,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c3675bb-c55c-4475-9276-10b537914c0e
2023-12-18 06:40:38,663 - distributed.worker - INFO - Starting Worker plugin PreImport-21ac3ff9-06a4-4767-8b9b-35de0c5f0ecc
2023-12-18 06:40:38,663 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3838c72b-a809-4f1e-862d-3aeb7321583c
2023-12-18 06:40:38,663 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,663 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06168d07-107c-4d06-bae8-97bca421e762
2023-12-18 06:40:38,664 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,667 - distributed.worker - INFO - Starting Worker plugin PreImport-acaaa826-d897-41ca-b9d1-4c20b98524c7
2023-12-18 06:40:38,668 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,672 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,673 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,673 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,685 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45061', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,686 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45061
2023-12-18 06:40:38,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50876
2023-12-18 06:40:38,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,688 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,688 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34741', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34741
2023-12-18 06:40:38,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50902
2023-12-18 06:40:38,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41137', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,697 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41137
2023-12-18 06:40:38,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50878
2023-12-18 06:40:38,697 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,697 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,698 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41511', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,698 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,698 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41511
2023-12-18 06:40:38,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50888
2023-12-18 06:40:38,699 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,699 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,701 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,701 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,706 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43601', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43601
2023-12-18 06:40:38,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50924
2023-12-18 06:40:38,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,707 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34419', status: init, memory: 0, processing: 0>
2023-12-18 06:40:38,708 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34419
2023-12-18 06:40:38,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50918
2023-12-18 06:40:38,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,709 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:38,710 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:38,710 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:38,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,729 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,730 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:38,734 - distributed.scheduler - INFO - Remove client Client-55d39ad9-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:38,734 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50774; closing.
2023-12-18 06:40:38,735 - distributed.scheduler - INFO - Remove client Client-55d39ad9-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:38,735 - distributed.scheduler - INFO - Close client connection: Client-55d39ad9-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:38,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44619'. Reason: nanny-close
2023-12-18 06:40:38,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38569'. Reason: nanny-close
2023-12-18 06:40:38,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41593'. Reason: nanny-close
2023-12-18 06:40:38,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37329'. Reason: nanny-close
2023-12-18 06:40:38,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37987'. Reason: nanny-close
2023-12-18 06:40:38,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42859. Reason: nanny-close
2023-12-18 06:40:38,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41239'. Reason: nanny-close
2023-12-18 06:40:38,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46293'. Reason: nanny-close
2023-12-18 06:40:38,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35071. Reason: nanny-close
2023-12-18 06:40:38,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35853'. Reason: nanny-close
2023-12-18 06:40:38,740 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,740 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50864; closing.
2023-12-18 06:40:38,740 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7407196')
2023-12-18 06:40:38,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,742 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50872; closing.
2023-12-18 06:40:38,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7430825')
2023-12-18 06:40:38,743 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,744 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,744 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,745 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,745 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41137. Reason: nanny-close
2023-12-18 06:40:38,745 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34741. Reason: nanny-close
2023-12-18 06:40:38,745 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,746 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43601. Reason: nanny-close
2023-12-18 06:40:38,746 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,746 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45061. Reason: nanny-close
2023-12-18 06:40:38,746 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:38,746 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34419. Reason: nanny-close
2023-12-18 06:40:38,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,747 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41511. Reason: nanny-close
2023-12-18 06:40:38,748 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,748 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50902; closing.
2023-12-18 06:40:38,748 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,748 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50924; closing.
2023-12-18 06:40:38,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50876; closing.
2023-12-18 06:40:38,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7494218')
2023-12-18 06:40:38,749 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:38,750 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7500684')
2023-12-18 06:40:38,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45061', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7504506')
2023-12-18 06:40:38,750 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,750 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50918; closing.
2023-12-18 06:40:38,751 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50878; closing.
2023-12-18 06:40:38,751 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,751 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34419', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7515974')
2023-12-18 06:40:38,751 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:38,752 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7520242')
2023-12-18 06:40:38,752 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50888; closing.
2023-12-18 06:40:38,752 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881638.7527516')
2023-12-18 06:40:38,753 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:40,554 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:40:40,554 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:40:40,555 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:40:40,556 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:40:40,556 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-18 06:40:42,753 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:42,758 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35621 instead
  warnings.warn(
2023-12-18 06:40:42,762 - distributed.scheduler - INFO - State start
2023-12-18 06:40:43,163 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:43,166 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:40:43,168 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35621/status
2023-12-18 06:40:43,168 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:40:43,807 - distributed.scheduler - INFO - Receive client connection: Client-5d9d573c-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:43,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43110
2023-12-18 06:40:43,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37229'
2023-12-18 06:40:43,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45675'
2023-12-18 06:40:43,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35249'
2023-12-18 06:40:43,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38977'
2023-12-18 06:40:43,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43519'
2023-12-18 06:40:43,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37025'
2023-12-18 06:40:43,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38403'
2023-12-18 06:40:43,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37689'
2023-12-18 06:40:44,578 - distributed.scheduler - INFO - Receive client connection: Client-5c94b382-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:44,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43220
2023-12-18 06:40:45,864 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,864 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,864 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,864 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,864 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,864 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,881 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,882 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,893 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,893 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:45,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:45,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:45,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:49,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42443
2023-12-18 06:40:49,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38033
2023-12-18 06:40:49,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42443
2023-12-18 06:40:49,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38033
2023-12-18 06:40:49,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45161
2023-12-18 06:40:49,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43093
2023-12-18 06:40:49,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:49,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:49,393 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,393 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,393 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:49,393 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:49,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:49,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:49,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t5n93wxc
2023-12-18 06:40:49,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sc8w4zin
2023-12-18 06:40:49,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f39e3cb-2d5b-410a-bac1-cbb2bc68db1d
2023-12-18 06:40:49,393 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f9a5b82-ce3b-4486-8d50-28ff9ba1fded
2023-12-18 06:40:49,393 - distributed.worker - INFO - Starting Worker plugin PreImport-9cc07a45-e219-4b09-b1b6-e2ec47000df9
2023-12-18 06:40:49,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dff4ea6c-4a78-4ee6-a6de-39973a9e3069
2023-12-18 06:40:49,425 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42369
2023-12-18 06:40:49,426 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42369
2023-12-18 06:40:49,426 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40455
2023-12-18 06:40:49,426 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:49,426 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,426 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:49,426 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:49,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rkk9a897
2023-12-18 06:40:49,427 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-119f39ed-1295-4a3a-827b-a2c3268eda5c
2023-12-18 06:40:49,427 - distributed.worker - INFO - Starting Worker plugin PreImport-d72e8236-7a38-4981-899a-f6dad7475d65
2023-12-18 06:40:49,427 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b466181c-fb84-4f35-a936-e1f4ec25e13e
2023-12-18 06:40:49,432 - distributed.worker - INFO - Starting Worker plugin PreImport-99e555f2-5d58-49c6-978a-c1939b725df1
2023-12-18 06:40:49,432 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eeb81e46-c398-45e5-82f2-2ddff0930ba4
2023-12-18 06:40:49,432 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,432 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42443', status: init, memory: 0, processing: 0>
2023-12-18 06:40:49,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42443
2023-12-18 06:40:49,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43314
2023-12-18 06:40:49,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38033', status: init, memory: 0, processing: 0>
2023-12-18 06:40:49,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:49,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38033
2023-12-18 06:40:49,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43328
2023-12-18 06:40:49,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:49,607 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:49,609 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:49,609 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:49,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:49,619 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,798 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42369', status: init, memory: 0, processing: 0>
2023-12-18 06:40:49,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42369
2023-12-18 06:40:49,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43332
2023-12-18 06:40:49,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:49,802 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:49,802 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:49,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:50,379 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40975
2023-12-18 06:40:50,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40975
2023-12-18 06:40:50,380 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38765
2023-12-18 06:40:50,380 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,380 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,380 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:50,380 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:50,380 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uur1av9t
2023-12-18 06:40:50,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82bc8f0a-5e74-43b8-a59b-9b6aaeb9a831
2023-12-18 06:40:50,381 - distributed.worker - INFO - Starting Worker plugin PreImport-473819b4-d8fb-4396-9b1b-133e2568d798
2023-12-18 06:40:50,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fea87471-ce16-4a23-ae5f-60ef11880cbe
2023-12-18 06:40:50,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38855
2023-12-18 06:40:50,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38855
2023-12-18 06:40:50,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33945
2023-12-18 06:40:50,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,393 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,393 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:50,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:50,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rhzqzkol
2023-12-18 06:40:50,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12baa192-24ff-4e12-9c61-1348a9a988ed
2023-12-18 06:40:50,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38631
2023-12-18 06:40:50,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38631
2023-12-18 06:40:50,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36715
2023-12-18 06:40:50,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,402 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,402 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:50,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:50,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mg0er94i
2023-12-18 06:40:50,403 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d78431e-b6f2-46bd-b42f-48172e9c61f1
2023-12-18 06:40:50,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45927
2023-12-18 06:40:50,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45927
2023-12-18 06:40:50,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35789
2023-12-18 06:40:50,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,406 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,406 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:50,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:50,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-isdpb1lc
2023-12-18 06:40:50,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44355
2023-12-18 06:40:50,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c51ce87-200f-41d9-9aa7-20614e38b6c8
2023-12-18 06:40:50,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44355
2023-12-18 06:40:50,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37799
2023-12-18 06:40:50,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,407 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,407 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:50,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:50,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a8ac603-4a80-482c-809d-4b650ee3eae0
2023-12-18 06:40:50,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9h_myypc
2023-12-18 06:40:50,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e956610f-dd60-4b92-8f62-a3d31478ae27
2023-12-18 06:40:50,408 - distributed.worker - INFO - Starting Worker plugin PreImport-52649380-efd7-4dd9-b951-be59802982ad
2023-12-18 06:40:50,409 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0d4c406-67cd-406e-845e-296ad5d5a9d5
2023-12-18 06:40:50,459 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,466 - distributed.worker - INFO - Starting Worker plugin PreImport-36cd9894-108b-4c0d-aad6-c5a57e01533a
2023-12-18 06:40:50,467 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec0cc9ff-c8f5-4dee-80f4-8d8396264dd0
2023-12-18 06:40:50,467 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6376508d-ac2b-4e6f-b61d-5bef6f613f9d
2023-12-18 06:40:50,470 - distributed.worker - INFO - Starting Worker plugin PreImport-a3796edc-e20e-4eb7-b33c-91c7d1b9ab3b
2023-12-18 06:40:50,471 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40975', status: init, memory: 0, processing: 0>
2023-12-18 06:40:50,624 - distributed.worker - INFO - Starting Worker plugin PreImport-90306aac-26f2-4b33-b3dc-f10958389089
2023-12-18 06:40:50,624 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,624 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40975
2023-12-18 06:40:50,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52196
2023-12-18 06:40:50,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38855', status: init, memory: 0, processing: 0>
2023-12-18 06:40:50,625 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:50,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38855
2023-12-18 06:40:50,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52198
2023-12-18 06:40:50,626 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,626 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,627 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38631', status: init, memory: 0, processing: 0>
2023-12-18 06:40:50,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:50,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38631
2023-12-18 06:40:50,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52204
2023-12-18 06:40:50,628 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,628 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,628 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:50,630 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,630 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:50,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:50,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:50,787 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44355', status: init, memory: 0, processing: 0>
2023-12-18 06:40:50,788 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44355
2023-12-18 06:40:50,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52248
2023-12-18 06:40:50,789 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45927', status: init, memory: 0, processing: 0>
2023-12-18 06:40:50,790 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45927
2023-12-18 06:40:50,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52244
2023-12-18 06:40:50,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:50,791 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,791 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:50,793 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:50,793 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:50,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:50,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:50,900 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,901 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,901 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,901 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,901 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,901 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,901 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:40:50,908 - distributed.scheduler - INFO - Remove client Client-5c94b382-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:50,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43220; closing.
2023-12-18 06:40:50,909 - distributed.scheduler - INFO - Remove client Client-5c94b382-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:50,909 - distributed.scheduler - INFO - Close client connection: Client-5c94b382-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:50,910 - distributed.scheduler - INFO - Remove client Client-5d9d573c-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:50,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43110; closing.
2023-12-18 06:40:50,911 - distributed.scheduler - INFO - Remove client Client-5d9d573c-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:50,911 - distributed.scheduler - INFO - Close client connection: Client-5d9d573c-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:50,975 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37229'. Reason: nanny-close
2023-12-18 06:40:50,976 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45675'. Reason: nanny-close
2023-12-18 06:40:50,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,977 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38631. Reason: nanny-close
2023-12-18 06:40:50,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35249'. Reason: nanny-close
2023-12-18 06:40:50,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42369. Reason: nanny-close
2023-12-18 06:40:50,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38977'. Reason: nanny-close
2023-12-18 06:40:50,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42443. Reason: nanny-close
2023-12-18 06:40:50,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43519'. Reason: nanny-close
2023-12-18 06:40:50,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38855. Reason: nanny-close
2023-12-18 06:40:50,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37025'. Reason: nanny-close
2023-12-18 06:40:50,979 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44355. Reason: nanny-close
2023-12-18 06:40:50,980 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52204; closing.
2023-12-18 06:40:50,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38403'. Reason: nanny-close
2023-12-18 06:40:50,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,980 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,980 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,980 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43314; closing.
2023-12-18 06:40:50,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45927. Reason: nanny-close
2023-12-18 06:40:50,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37689'. Reason: nanny-close
2023-12-18 06:40:50,980 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38631', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.9807007')
2023-12-18 06:40:50,980 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:50,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40975. Reason: nanny-close
2023-12-18 06:40:50,981 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38033. Reason: nanny-close
2023-12-18 06:40:50,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42443', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.98156')
2023-12-18 06:40:50,981 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,982 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43332; closing.
2023-12-18 06:40:50,982 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52198; closing.
2023-12-18 06:40:50,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.9834664')
2023-12-18 06:40:50,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:50,984 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,984 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.9846046')
2023-12-18 06:40:50,984 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52248; closing.
2023-12-18 06:40:50,985 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:50,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44355', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.9857032')
2023-12-18 06:40:50,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52244; closing.
2023-12-18 06:40:50,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52196; closing.
2023-12-18 06:40:50,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43328; closing.
2023-12-18 06:40:50,986 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.986681')
2023-12-18 06:40:50,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.9870281')
2023-12-18 06:40:50,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881650.9873817')
2023-12-18 06:40:50,987 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:52,978 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:40:52,979 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:40:52,980 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:40:52,982 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:40:52,983 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-18 06:40:55,355 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:55,359 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45709 instead
  warnings.warn(
2023-12-18 06:40:55,363 - distributed.scheduler - INFO - State start
2023-12-18 06:40:55,385 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:55,386 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:40:55,387 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45709/status
2023-12-18 06:40:55,387 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:40:55,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41049', status: init, memory: 0, processing: 0>
2023-12-18 06:40:55,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41049
2023-12-18 06:40:55,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52930
2023-12-18 06:40:55,505 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52930; closing.
2023-12-18 06:40:55,505 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881655.505741')
2023-12-18 06:40:55,506 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:55,513 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39343', status: init, memory: 0, processing: 0>
2023-12-18 06:40:55,514 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39343
2023-12-18 06:40:55,514 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52938
2023-12-18 06:40:55,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52938; closing.
2023-12-18 06:40:55,556 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39343', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881655.5564673')
2023-12-18 06:40:55,556 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:55,576 - distributed.scheduler - INFO - Receive client connection: Client-640538a1-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:40:55,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52940
2023-12-18 06:40:55,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45465'
2023-12-18 06:40:55,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40409'
2023-12-18 06:40:55,717 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44175', status: init, memory: 0, processing: 0>
2023-12-18 06:40:55,718 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44175
2023-12-18 06:40:55,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52952
2023-12-18 06:40:55,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40341'
2023-12-18 06:40:55,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38259'
2023-12-18 06:40:55,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40973'
2023-12-18 06:40:55,745 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43949', status: init, memory: 0, processing: 0>
2023-12-18 06:40:55,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35025'
2023-12-18 06:40:55,746 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43949
2023-12-18 06:40:55,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53040
2023-12-18 06:40:55,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32971'
2023-12-18 06:40:55,758 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53040; closing.
2023-12-18 06:40:55,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43949', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881655.7588103')
2023-12-18 06:40:55,759 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52952; closing.
2023-12-18 06:40:55,760 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881655.7602036')
2023-12-18 06:40:55,760 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:55,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39427'
2023-12-18 06:40:55,844 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40131', status: init, memory: 0, processing: 0>
2023-12-18 06:40:55,845 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40131
2023-12-18 06:40:55,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53042
2023-12-18 06:40:55,860 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53042; closing.
2023-12-18 06:40:55,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881655.8605304')
2023-12-18 06:40:55,860 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:56,096 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34355', status: init, memory: 0, processing: 0>
2023-12-18 06:40:56,097 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34355
2023-12-18 06:40:56,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53058
2023-12-18 06:40:56,112 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53058; closing.
2023-12-18 06:40:56,113 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34355', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881656.112927')
2023-12-18 06:40:56,113 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:57,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,641 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,641 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,657 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:57,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:57,737 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:57,797 - distributed.scheduler - INFO - Receive client connection: Client-6693ac42-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:57,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53086
2023-12-18 06:41:02,731 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46339
2023-12-18 06:41:02,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46339
2023-12-18 06:41:02,732 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38739
2023-12-18 06:41:02,732 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,732 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,732 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,733 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,733 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hpa6ts_o
2023-12-18 06:41:02,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99ff8f21-7140-4eb2-aa06-425bf67cb700
2023-12-18 06:41:02,733 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71f4a696-53e2-4c48-93c6-e7cbc3158c03
2023-12-18 06:41:02,812 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42077
2023-12-18 06:41:02,813 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42077
2023-12-18 06:41:02,814 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43567
2023-12-18 06:41:02,814 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,814 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,814 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,814 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,814 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2_v5vmyi
2023-12-18 06:41:02,815 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d3457c9-d63b-4094-81f8-9ada0dde623d
2023-12-18 06:41:02,815 - distributed.worker - INFO - Starting Worker plugin PreImport-8b2437a7-26e9-47ef-a6ad-c06429434e05
2023-12-18 06:41:02,815 - distributed.worker - INFO - Starting Worker plugin RMMSetup-75da980d-27cc-4c27-ace3-304e790d8b07
2023-12-18 06:41:02,885 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34625
2023-12-18 06:41:02,886 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34625
2023-12-18 06:41:02,886 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41671
2023-12-18 06:41:02,886 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,886 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,887 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,887 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,887 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a9idtpsp
2023-12-18 06:41:02,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-757901a9-1767-4212-a4de-c499f54f404c
2023-12-18 06:41:02,888 - distributed.worker - INFO - Starting Worker plugin PreImport-7407bef7-b9e2-4407-bb60-a8bc10bb8a5a
2023-12-18 06:41:02,888 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b53410e-e0fd-444c-9ab3-5497034c13e4
2023-12-18 06:41:02,912 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43945
2023-12-18 06:41:02,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43945
2023-12-18 06:41:02,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40323
2023-12-18 06:41:02,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,913 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,913 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ab3llm4y
2023-12-18 06:41:02,914 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5eb92f6d-458a-41ae-a9b2-a4261309097c
2023-12-18 06:41:02,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38667
2023-12-18 06:41:02,934 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38667
2023-12-18 06:41:02,934 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44229
2023-12-18 06:41:02,934 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,934 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,934 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,934 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,934 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d8eceh4y
2023-12-18 06:41:02,935 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41fcd5d8-06f2-4f1f-95ff-40028e8ece44
2023-12-18 06:41:02,935 - distributed.worker - INFO - Starting Worker plugin PreImport-aebf0c6d-d7df-44a8-818b-539e21facf6a
2023-12-18 06:41:02,935 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ea4bb76-0de8-49ee-9593-1cc3604d6a6d
2023-12-18 06:41:02,975 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34697
2023-12-18 06:41:02,976 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34697
2023-12-18 06:41:02,976 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36825
2023-12-18 06:41:02,976 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,976 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,976 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,976 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,976 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dobp7nm8
2023-12-18 06:41:02,977 - distributed.worker - INFO - Starting Worker plugin PreImport-09f48675-0c05-423e-bf3d-a1be76057c13
2023-12-18 06:41:02,977 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c5e6992-40df-436f-b385-17a2727a5d2c
2023-12-18 06:41:02,996 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46305
2023-12-18 06:41:02,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46305
2023-12-18 06:41:02,997 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45451
2023-12-18 06:41:02,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:02,997 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:02,997 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:02,997 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:02,997 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9pnznr9r
2023-12-18 06:41:02,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64bdae42-51b6-4c30-8af6-50ab89c69b32
2023-12-18 06:41:03,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34237
2023-12-18 06:41:03,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34237
2023-12-18 06:41:03,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33613
2023-12-18 06:41:03,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,008 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,008 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:03,008 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:03,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-md3mugas
2023-12-18 06:41:03,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-624c5e02-3e37-4780-8743-33cdb1ee9d85
2023-12-18 06:41:03,009 - distributed.worker - INFO - Starting Worker plugin PreImport-cb25d389-680d-4ddf-ad1a-0c1fafcc34f4
2023-12-18 06:41:03,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a34c670-17b6-46d6-bfa1-bd6ad7f88c47
2023-12-18 06:41:03,239 - distributed.worker - INFO - Starting Worker plugin PreImport-ac62dd86-41ac-4720-98e3-cfd242cad4e6
2023-12-18 06:41:03,239 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,247 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,257 - distributed.worker - INFO - Starting Worker plugin PreImport-63ae27e9-2d8d-4328-95ce-632514ee8edd
2023-12-18 06:41:03,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffd4460c-66dd-4b4a-823a-45472fdfc2c7
2023-12-18 06:41:03,258 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,262 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b2d1454-9207-411e-98a5-70af639c31e3
2023-12-18 06:41:03,263 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,264 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,266 - distributed.worker - INFO - Starting Worker plugin PreImport-b4870ff4-418a-42e0-9224-5123410915cd
2023-12-18 06:41:03,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6af8487b-4265-4a82-ab71-7e6c2be67adf
2023-12-18 06:41:03,266 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,267 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42077', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42077
2023-12-18 06:41:03,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44262
2023-12-18 06:41:03,275 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,275 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,281 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46339', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46339
2023-12-18 06:41:03,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44246
2023-12-18 06:41:03,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,285 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,285 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,292 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34237', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,293 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34237
2023-12-18 06:41:03,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44296
2023-12-18 06:41:03,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46305', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46305
2023-12-18 06:41:03,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44310
2023-12-18 06:41:03,295 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,295 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,296 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43945', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,299 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43945
2023-12-18 06:41:03,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44268
2023-12-18 06:41:03,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,300 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34625', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,300 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34625
2023-12-18 06:41:03,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44342
2023-12-18 06:41:03,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,301 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34697', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,301 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,301 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,301 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,301 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34697
2023-12-18 06:41:03,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44284
2023-12-18 06:41:03,302 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,302 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,303 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,304 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,304 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38667', status: init, memory: 0, processing: 0>
2023-12-18 06:41:03,316 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38667
2023-12-18 06:41:03,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44326
2023-12-18 06:41:03,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:03,319 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:03,319 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:03,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:03,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,364 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,364 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,364 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,370 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,370 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,370 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,370 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:03,376 - distributed.scheduler - INFO - Remove client Client-6693ac42-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:03,377 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53086; closing.
2023-12-18 06:41:03,379 - distributed.scheduler - INFO - Remove client Client-6693ac42-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:03,380 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,380 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,380 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,381 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,381 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,381 - distributed.scheduler - INFO - Close client connection: Client-6693ac42-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:03,381 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,381 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,381 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:03,387 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:03,390 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:03,393 - distributed.scheduler - INFO - Remove client Client-640538a1-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:03,393 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52940; closing.
2023-12-18 06:41:03,394 - distributed.scheduler - INFO - Remove client Client-640538a1-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:03,394 - distributed.scheduler - INFO - Close client connection: Client-640538a1-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:03,395 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45465'. Reason: nanny-close
2023-12-18 06:41:03,396 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,396 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40409'. Reason: nanny-close
2023-12-18 06:41:03,397 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,397 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34697. Reason: nanny-close
2023-12-18 06:41:03,397 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40341'. Reason: nanny-close
2023-12-18 06:41:03,397 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,398 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43945. Reason: nanny-close
2023-12-18 06:41:03,398 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38259'. Reason: nanny-close
2023-12-18 06:41:03,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,398 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34625. Reason: nanny-close
2023-12-18 06:41:03,398 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40973'. Reason: nanny-close
2023-12-18 06:41:03,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42077. Reason: nanny-close
2023-12-18 06:41:03,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35025'. Reason: nanny-close
2023-12-18 06:41:03,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,399 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46339. Reason: nanny-close
2023-12-18 06:41:03,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32971'. Reason: nanny-close
2023-12-18 06:41:03,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44284; closing.
2023-12-18 06:41:03,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38667. Reason: nanny-close
2023-12-18 06:41:03,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39427'. Reason: nanny-close
2023-12-18 06:41:03,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4003985')
2023-12-18 06:41:03,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:03,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46305. Reason: nanny-close
2023-12-18 06:41:03,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34237. Reason: nanny-close
2023-12-18 06:41:03,401 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,401 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44268; closing.
2023-12-18 06:41:03,402 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,402 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,402 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44262; closing.
2023-12-18 06:41:03,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43945', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4036386')
2023-12-18 06:41:03,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:03,404 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44342; closing.
2023-12-18 06:41:03,404 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,404 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,404 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,405 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:03,405 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4054697')
2023-12-18 06:41:03,406 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4059355')
2023-12-18 06:41:03,406 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44246; closing.
2023-12-18 06:41:03,407 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.407368')
2023-12-18 06:41:03,407 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44326; closing.
2023-12-18 06:41:03,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44310; closing.
2023-12-18 06:41:03,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44296; closing.
2023-12-18 06:41:03,408 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4086604')
2023-12-18 06:41:03,409 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4091692')
2023-12-18 06:41:03,409 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34237', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881663.4095476')
2023-12-18 06:41:03,409 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:06,115 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:41:06,115 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:41:06,116 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:06,118 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:41:06,119 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-18 06:41:08,540 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:08,544 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32769 instead
  warnings.warn(
2023-12-18 06:41:08,548 - distributed.scheduler - INFO - State start
2023-12-18 06:41:08,602 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:08,604 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:41:08,605 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32769/status
2023-12-18 06:41:08,605 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:41:08,770 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40763', status: init, memory: 0, processing: 0>
2023-12-18 06:41:08,785 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40763
2023-12-18 06:41:08,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45066
2023-12-18 06:41:08,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45066; closing.
2023-12-18 06:41:08,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40763', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881668.8484075')
2023-12-18 06:41:08,849 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:09,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43477'
2023-12-18 06:41:09,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38287'
2023-12-18 06:41:09,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39607'
2023-12-18 06:41:09,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43287'
2023-12-18 06:41:09,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45431'
2023-12-18 06:41:09,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35301'
2023-12-18 06:41:09,086 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40103'
2023-12-18 06:41:09,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37991'
2023-12-18 06:41:09,292 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39435', status: init, memory: 0, processing: 0>
2023-12-18 06:41:09,293 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39435
2023-12-18 06:41:09,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45148
2023-12-18 06:41:09,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33491', status: init, memory: 0, processing: 0>
2023-12-18 06:41:09,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33491
2023-12-18 06:41:09,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45142
2023-12-18 06:41:09,297 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39819', status: init, memory: 0, processing: 0>
2023-12-18 06:41:09,298 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39819
2023-12-18 06:41:09,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45158
2023-12-18 06:41:09,302 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45148; closing.
2023-12-18 06:41:09,302 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881669.302691')
2023-12-18 06:41:09,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45142; closing.
2023-12-18 06:41:09,303 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881669.3039029')
2023-12-18 06:41:09,353 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45158; closing.
2023-12-18 06:41:09,354 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39819', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881669.3542316')
2023-12-18 06:41:09,354 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:10,103 - distributed.scheduler - INFO - Receive client connection: Client-6bcfd110-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:10,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45180
2023-12-18 06:41:10,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:10,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:10,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,058 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,058 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,064 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,068 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,068 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,068 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,068 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:11,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:11,072 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,074 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:11,145 - distributed.scheduler - INFO - Receive client connection: Client-6e8880eb-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:11,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45192
2023-12-18 06:41:14,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44613
2023-12-18 06:41:14,137 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44613
2023-12-18 06:41:14,137 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37985
2023-12-18 06:41:14,137 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,137 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,137 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,137 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hufc228f
2023-12-18 06:41:14,138 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bd48d65-ba28-4432-8cb8-c984a835459c
2023-12-18 06:41:14,139 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7b81554-2950-4c07-a9ec-fef8f62e3728
2023-12-18 06:41:14,144 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40279
2023-12-18 06:41:14,145 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40279
2023-12-18 06:41:14,145 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41331
2023-12-18 06:41:14,145 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,145 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,145 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,145 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,145 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-quiitn91
2023-12-18 06:41:14,146 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e0dd651-ba07-42a6-95a8-1d32ad525a46
2023-12-18 06:41:14,146 - distributed.worker - INFO - Starting Worker plugin PreImport-7e01bdb5-f45b-4bf3-a030-b67401e22f30
2023-12-18 06:41:14,146 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad6830ff-9cb1-4e57-9de7-ae5af0594b51
2023-12-18 06:41:14,159 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35955
2023-12-18 06:41:14,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35955
2023-12-18 06:41:14,161 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44407
2023-12-18 06:41:14,161 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,161 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,161 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,161 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hy795yw4
2023-12-18 06:41:14,163 - distributed.worker - INFO - Starting Worker plugin PreImport-d6cd411e-beb1-4274-9254-6ceb53b6ef59
2023-12-18 06:41:14,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78bb7cc5-c7cd-4c10-9753-dc5cebe64cf2
2023-12-18 06:41:14,237 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46559
2023-12-18 06:41:14,238 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46559
2023-12-18 06:41:14,238 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37013
2023-12-18 06:41:14,238 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,238 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,238 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,238 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ge1xwma
2023-12-18 06:41:14,239 - distributed.worker - INFO - Starting Worker plugin RMMSetup-905451af-f40c-482e-94df-266b65681377
2023-12-18 06:41:14,239 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34699
2023-12-18 06:41:14,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34699
2023-12-18 06:41:14,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35619
2023-12-18 06:41:14,239 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40429
2023-12-18 06:41:14,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,240 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40429
2023-12-18 06:41:14,239 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35223
2023-12-18 06:41:14,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42887
2023-12-18 06:41:14,240 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35223
2023-12-18 06:41:14,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,240 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39095
2023-12-18 06:41:14,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kg08ee0w
2023-12-18 06:41:14,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,241 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,240 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,241 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hkug569l
2023-12-18 06:41:14,241 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,241 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u73mqip2
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fe9866f-4a66-4718-a21e-572354852e9b
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin PreImport-2b9746e8-120a-45ea-92b9-9f8ea5c97cb2
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3033d215-1cdf-40b4-be69-89086ed6cdeb
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44efa032-f857-4709-9bb2-6b4970797724
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9defb36-28a7-464c-8b2e-3f28d85586b9
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin PreImport-41396dcf-e5b3-4051-868c-47e42ed5e39b
2023-12-18 06:41:14,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3abbd12-768e-4b54-99b0-44b34ae99b1a
2023-12-18 06:41:14,241 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37115
2023-12-18 06:41:14,242 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37115
2023-12-18 06:41:14,242 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39189
2023-12-18 06:41:14,242 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,242 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,242 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:14,242 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:14,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mzrv8yna
2023-12-18 06:41:14,243 - distributed.worker - INFO - Starting Worker plugin PreImport-1bf0934e-1309-4dcf-b52c-0daf973922a3
2023-12-18 06:41:14,243 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27214536-f643-47e6-8b02-779107247952
2023-12-18 06:41:14,243 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c453f7b-fab6-47c6-85f7-5b7dff00c7da
2023-12-18 06:41:14,398 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bebf0f55-cc8a-46f1-a664-d26d95392c96
2023-12-18 06:41:14,399 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,404 - distributed.worker - INFO - Starting Worker plugin PreImport-72111887-c2db-41af-a63e-6d3987f1787b
2023-12-18 06:41:14,405 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,407 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,411 - distributed.worker - INFO - Starting Worker plugin PreImport-b3ba2aa3-35ad-422f-8e73-790e46499acb
2023-12-18 06:41:14,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-465b35ed-8fd5-43d5-b33d-9f1fc4895f2b
2023-12-18 06:41:14,411 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,418 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,424 - distributed.worker - INFO - Starting Worker plugin PreImport-3b46b353-b1df-4d02-aed6-c970413a78a4
2023-12-18 06:41:14,424 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87276aa1-cb6e-44a1-8095-89638df3fab6
2023-12-18 06:41:14,425 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,425 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,433 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40279', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,433 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40279
2023-12-18 06:41:14,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45328
2023-12-18 06:41:14,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,435 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35955', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,435 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,435 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35955
2023-12-18 06:41:14,435 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,435 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45306
2023-12-18 06:41:14,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,437 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,438 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46559', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,438 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46559
2023-12-18 06:41:14,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45338
2023-12-18 06:41:14,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,440 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34699', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,444 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34699
2023-12-18 06:41:14,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45354
2023-12-18 06:41:14,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,446 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,449 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44613', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,450 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44613
2023-12-18 06:41:14,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45322
2023-12-18 06:41:14,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,452 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,452 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,453 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35223', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,454 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35223
2023-12-18 06:41:14,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45368
2023-12-18 06:41:14,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,455 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,456 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40429', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,465 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40429
2023-12-18 06:41:14,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45372
2023-12-18 06:41:14,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,468 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37115', status: init, memory: 0, processing: 0>
2023-12-18 06:41:14,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37115
2023-12-18 06:41:14,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45382
2023-12-18 06:41:14,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:14,474 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:14,474 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:14,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:14,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:14,512 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,512 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,517 - distributed.scheduler - INFO - Remove client Client-6e8880eb-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:14,517 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45192; closing.
2023-12-18 06:41:14,518 - distributed.scheduler - INFO - Remove client Client-6e8880eb-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:14,518 - distributed.scheduler - INFO - Close client connection: Client-6e8880eb-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:14,526 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,526 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,526 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,526 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,526 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,526 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,527 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,527 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:41:14,534 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,536 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:14,538 - distributed.scheduler - INFO - Remove client Client-6bcfd110-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:14,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45180; closing.
2023-12-18 06:41:14,539 - distributed.scheduler - INFO - Remove client Client-6bcfd110-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:14,539 - distributed.scheduler - INFO - Close client connection: Client-6bcfd110-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:14,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43477'. Reason: nanny-close
2023-12-18 06:41:14,541 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38287'. Reason: nanny-close
2023-12-18 06:41:14,542 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39607'. Reason: nanny-close
2023-12-18 06:41:14,542 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35955. Reason: nanny-close
2023-12-18 06:41:14,543 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,543 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43287'. Reason: nanny-close
2023-12-18 06:41:14,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37115. Reason: nanny-close
2023-12-18 06:41:14,543 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35223. Reason: nanny-close
2023-12-18 06:41:14,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45431'. Reason: nanny-close
2023-12-18 06:41:14,544 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,544 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40279. Reason: nanny-close
2023-12-18 06:41:14,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35301'. Reason: nanny-close
2023-12-18 06:41:14,545 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,545 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44613. Reason: nanny-close
2023-12-18 06:41:14,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40103'. Reason: nanny-close
2023-12-18 06:41:14,545 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,545 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37991'. Reason: nanny-close
2023-12-18 06:41:14,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40429. Reason: nanny-close
2023-12-18 06:41:14,546 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:14,546 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45368; closing.
2023-12-18 06:41:14,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46559. Reason: nanny-close
2023-12-18 06:41:14,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35223', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5465274')
2023-12-18 06:41:14,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34699. Reason: nanny-close
2023-12-18 06:41:14,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,546 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,547 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45306; closing.
2023-12-18 06:41:14,547 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,548 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,548 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,548 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,548 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:14,549 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,549 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,549 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,549 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35955', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5494823')
2023-12-18 06:41:14,549 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45322; closing.
2023-12-18 06:41:14,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45328; closing.
2023-12-18 06:41:14,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45382; closing.
2023-12-18 06:41:14,550 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:14,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44613', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.551735')
2023-12-18 06:41:14,552 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40279', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5521572')
2023-12-18 06:41:14,552 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5526056')
2023-12-18 06:41:14,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45338; closing.
2023-12-18 06:41:14,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45372; closing.
2023-12-18 06:41:14,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5538247')
2023-12-18 06:41:14,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5541382')
2023-12-18 06:41:14,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45354; closing.
2023-12-18 06:41:14,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34699', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881674.5549362')
2023-12-18 06:41:14,555 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:16,259 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:41:16,259 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:41:16,259 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:16,262 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:41:16,262 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-18 06:41:18,548 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:18,553 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38901 instead
  warnings.warn(
2023-12-18 06:41:18,557 - distributed.scheduler - INFO - State start
2023-12-18 06:41:19,280 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:19,281 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:41:19,283 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38901/status
2023-12-18 06:41:19,283 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:41:19,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34181'
2023-12-18 06:41:19,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44233'
2023-12-18 06:41:19,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40747'
2023-12-18 06:41:19,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37607'
2023-12-18 06:41:19,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44247'
2023-12-18 06:41:19,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42297'
2023-12-18 06:41:19,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43241'
2023-12-18 06:41:19,752 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42379'
2023-12-18 06:41:20,381 - distributed.scheduler - INFO - Receive client connection: Client-71df7225-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:20,395 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40486
2023-12-18 06:41:21,027 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34567', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34567
2023-12-18 06:41:21,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40506
2023-12-18 06:41:21,085 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40506; closing.
2023-12-18 06:41:21,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.0857294')
2023-12-18 06:41:21,086 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:21,096 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45625', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,096 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45625
2023-12-18 06:41:21,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40508
2023-12-18 06:41:21,123 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40508; closing.
2023-12-18 06:41:21,124 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.1240785')
2023-12-18 06:41:21,124 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:21,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46283', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,208 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46283
2023-12-18 06:41:21,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40526
2023-12-18 06:41:21,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45697', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45697
2023-12-18 06:41:21,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40556
2023-12-18 06:41:21,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41193', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41193
2023-12-18 06:41:21,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40572
2023-12-18 06:41:21,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37611', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37611
2023-12-18 06:41:21,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40522
2023-12-18 06:41:21,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34247', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,220 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34247
2023-12-18 06:41:21,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40536
2023-12-18 06:41:21,221 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40219', status: init, memory: 0, processing: 0>
2023-12-18 06:41:21,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40219
2023-12-18 06:41:21,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40552
2023-12-18 06:41:21,225 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40526; closing.
2023-12-18 06:41:21,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46283', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.2257342')
2023-12-18 06:41:21,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40572; closing.
2023-12-18 06:41:21,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40556; closing.
2023-12-18 06:41:21,227 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.2277381')
2023-12-18 06:41:21,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.2280912')
2023-12-18 06:41:21,276 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40552; closing.
2023-12-18 06:41:21,276 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.2763522')
2023-12-18 06:41:21,277 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40536; closing.
2023-12-18 06:41:21,277 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.2776039')
2023-12-18 06:41:21,278 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40522; closing.
2023-12-18 06:41:21,278 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881681.278368')
2023-12-18 06:41:21,278 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:21,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:21,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:21,707 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:21,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:24,566 - distributed.scheduler - INFO - Receive client connection: Client-76885370-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:24,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40588
2023-12-18 06:41:25,057 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35271
2023-12-18 06:41:25,057 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35271
2023-12-18 06:41:25,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41231
2023-12-18 06:41:25,058 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,058 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,058 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,058 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g4cdnhip
2023-12-18 06:41:25,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa99ad55-bf04-4a61-aff9-c9392d51afe5
2023-12-18 06:41:25,070 - distributed.worker - INFO - Starting Worker plugin PreImport-05fbf972-dc54-41f9-a1c3-b1889dbad906
2023-12-18 06:41:25,070 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ff88015-5061-4896-b96a-7ab6dcb6b6d9
2023-12-18 06:41:25,071 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,119 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35271', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,120 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35271
2023-12-18 06:41:25,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40616
2023-12-18 06:41:25,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,123 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,123 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,251 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40393
2023-12-18 06:41:25,252 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40393
2023-12-18 06:41:25,252 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45671
2023-12-18 06:41:25,252 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,252 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,252 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,253 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cmpxwop_
2023-12-18 06:41:25,253 - distributed.worker - INFO - Starting Worker plugin PreImport-7f10fc8b-af1e-44e9-9fff-4249bc235ad0
2023-12-18 06:41:25,253 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3c3fb0a-90bd-4f2a-9026-1c2f1f8e9341
2023-12-18 06:41:25,254 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42797
2023-12-18 06:41:25,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42797
2023-12-18 06:41:25,255 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39659
2023-12-18 06:41:25,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,255 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,255 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,255 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ojwolyq6
2023-12-18 06:41:25,255 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51858d60-bafc-47fe-af15-01a3e36ea694
2023-12-18 06:41:25,256 - distributed.worker - INFO - Starting Worker plugin PreImport-1d6db94d-153f-4c80-91d7-1728d031a534
2023-12-18 06:41:25,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa526ab9-f13b-4683-8d5d-cd19bc44410c
2023-12-18 06:41:25,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39289
2023-12-18 06:41:25,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39289
2023-12-18 06:41:25,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33371
2023-12-18 06:41:25,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,257 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,257 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6dq2bksc
2023-12-18 06:41:25,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96a9704d-870a-4061-a17b-d515c4ced0aa
2023-12-18 06:41:25,259 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22c66bf0-2b48-4bad-a81c-720fcfc6fa06
2023-12-18 06:41:25,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38597
2023-12-18 06:41:25,337 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38597
2023-12-18 06:41:25,338 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44285
2023-12-18 06:41:25,338 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,338 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,338 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,338 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,338 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yepk8nos
2023-12-18 06:41:25,338 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33959
2023-12-18 06:41:25,339 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33959
2023-12-18 06:41:25,339 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41373
2023-12-18 06:41:25,339 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7578ca95-9dce-4b45-9cc5-52d0b3d937f0
2023-12-18 06:41:25,339 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,339 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,339 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,339 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,339 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3jcaaf6v
2023-12-18 06:41:25,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab126ece-e6db-45c1-82e7-dbe48d4c0d6b
2023-12-18 06:41:25,340 - distributed.worker - INFO - Starting Worker plugin PreImport-b81e3423-704f-4f31-999f-1a42c10a823f
2023-12-18 06:41:25,340 - distributed.worker - INFO - Starting Worker plugin RMMSetup-146c2599-e44b-4d51-a44c-712832fa29a5
2023-12-18 06:41:25,339 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43565
2023-12-18 06:41:25,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43565
2023-12-18 06:41:25,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33451
2023-12-18 06:41:25,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,341 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,341 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,341 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,341 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hi_b60k7
2023-12-18 06:41:25,341 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae77654d-48d9-47c4-adb6-018e62d4998f
2023-12-18 06:41:25,342 - distributed.worker - INFO - Starting Worker plugin PreImport-8aec040f-d966-4034-b941-47a2bab5eb44
2023-12-18 06:41:25,342 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20f813a1-01d5-4644-a3ca-13671a4912f9
2023-12-18 06:41:25,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37861
2023-12-18 06:41:25,346 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37861
2023-12-18 06:41:25,346 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40879
2023-12-18 06:41:25,346 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,346 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,346 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:25,347 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:25,347 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jyoto2yu
2023-12-18 06:41:25,348 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7eced67d-d7f8-4a65-bdc0-35bbe11fd9dd
2023-12-18 06:41:25,349 - distributed.worker - INFO - Starting Worker plugin PreImport-60172626-d512-43ff-b27a-ec9dda463f8d
2023-12-18 06:41:25,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c5ff7d7-b420-413f-8a0d-18f2d2c765e0
2023-12-18 06:41:25,373 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b4ed72a-3f7f-438e-84dc-8c38b6b81df5
2023-12-18 06:41:25,378 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,382 - distributed.worker - INFO - Starting Worker plugin PreImport-812ac3f7-1ba9-4363-9685-86f6e50cae8a
2023-12-18 06:41:25,382 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,394 - distributed.worker - INFO - Starting Worker plugin PreImport-29e2036c-830d-45c5-9f77-79c4ee796d32
2023-12-18 06:41:25,395 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24d910b2-ef99-49a9-b12a-762cdf983842
2023-12-18 06:41:25,395 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,398 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,406 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,406 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,409 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42797', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,410 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42797
2023-12-18 06:41:25,410 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40622
2023-12-18 06:41:25,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40393', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,412 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40393
2023-12-18 06:41:25,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40630
2023-12-18 06:41:25,413 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,413 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,415 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,415 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,417 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39289', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39289
2023-12-18 06:41:25,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40632
2023-12-18 06:41:25,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,420 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,420 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,425 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33959', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,426 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33959
2023-12-18 06:41:25,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40652
2023-12-18 06:41:25,427 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38597', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,427 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,428 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,428 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38597
2023-12-18 06:41:25,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40646
2023-12-18 06:41:25,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,430 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,430 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43565', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43565
2023-12-18 06:41:25,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40656
2023-12-18 06:41:25,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,434 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,434 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37861', status: init, memory: 0, processing: 0>
2023-12-18 06:41:25,441 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37861
2023-12-18 06:41:25,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40660
2023-12-18 06:41:25,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:25,443 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:25,443 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:25,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:25,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,492 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,492 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,492 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,492 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:25,500 - distributed.scheduler - INFO - Remove client Client-71df7225-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:25,500 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40486; closing.
2023-12-18 06:41:25,501 - distributed.scheduler - INFO - Remove client Client-71df7225-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:25,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34181'. Reason: nanny-close
2023-12-18 06:41:25,503 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44233'. Reason: nanny-close
2023-12-18 06:41:25,504 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,504 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40393. Reason: nanny-close
2023-12-18 06:41:25,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40747'. Reason: nanny-close
2023-12-18 06:41:25,505 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,505 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35271. Reason: nanny-close
2023-12-18 06:41:25,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37607'. Reason: nanny-close
2023-12-18 06:41:25,505 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,505 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33959. Reason: nanny-close
2023-12-18 06:41:25,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44247'. Reason: nanny-close
2023-12-18 06:41:25,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,506 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37861. Reason: nanny-close
2023-12-18 06:41:25,506 - distributed.scheduler - INFO - Close client connection: Client-71df7225-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:25,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42297'. Reason: nanny-close
2023-12-18 06:41:25,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,506 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39289. Reason: nanny-close
2023-12-18 06:41:25,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43241'. Reason: nanny-close
2023-12-18 06:41:25,507 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,507 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42379'. Reason: nanny-close
2023-12-18 06:41:25,507 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42797. Reason: nanny-close
2023-12-18 06:41:25,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:25,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43565. Reason: nanny-close
2023-12-18 06:41:25,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,509 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38597. Reason: nanny-close
2023-12-18 06:41:25,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40630; closing.
2023-12-18 06:41:25,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:25,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40616; closing.
2023-12-18 06:41:25,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40652; closing.
2023-12-18 06:41:25,509 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,510 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,510 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,510 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5108776')
2023-12-18 06:41:25,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,511 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5114067')
2023-12-18 06:41:25,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:25,511 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33959', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5118287')
2023-12-18 06:41:25,512 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,512 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,512 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,512 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:25,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40660; closing.
2023-12-18 06:41:25,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40632; closing.
2023-12-18 06:41:25,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40622; closing.
2023-12-18 06:41:25,518 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5188901')
2023-12-18 06:41:25,519 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5195298')
2023-12-18 06:41:25,520 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5200613')
2023-12-18 06:41:25,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40646; closing.
2023-12-18 06:41:25,521 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40656; closing.
2023-12-18 06:41:25,521 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40622>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40622>: Stream is closed
2023-12-18 06:41:25,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38597', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5242848')
2023-12-18 06:41:25,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43565', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881685.5248122')
2023-12-18 06:41:25,525 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:25,527 - distributed.scheduler - INFO - Remove client Client-76885370-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:25,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40588; closing.
2023-12-18 06:41:25,528 - distributed.scheduler - INFO - Remove client Client-76885370-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:25,528 - distributed.scheduler - INFO - Close client connection: Client-76885370-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:26,009 - distributed.scheduler - INFO - Receive client connection: Client-77648204-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:26,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40662
2023-12-18 06:41:27,421 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:41:27,422 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:41:27,423 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:27,426 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:41:27,427 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-18 06:41:30,025 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:30,030 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43689 instead
  warnings.warn(
2023-12-18 06:41:30,034 - distributed.scheduler - INFO - State start
2023-12-18 06:41:30,057 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:30,058 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:41:30,058 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:30,059 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:41:30,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45429'
2023-12-18 06:41:32,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:32,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:33,242 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:38,097 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42521
2023-12-18 06:41:38,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42521
2023-12-18 06:41:38,098 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-18 06:41:38,098 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:38,098 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:38,098 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:38,099 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:41:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aftiaz8h
2023-12-18 06:41:38,099 - distributed.worker - INFO - Starting Worker plugin PreImport-49cb5581-9c36-4b24-b55d-b9e96cf2bf03
2023-12-18 06:41:38,099 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9354d3c7-6dc3-4a26-9665-8ba9e7472084
2023-12-18 06:41:38,100 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f05cece-e449-448e-a548-cb7bcdc75ff0
2023-12-18 06:41:38,101 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:41,563 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45429'. Reason: nanny-close
2023-12-18 06:41:44,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:44,154 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:44,154 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:44,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:44,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:44,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42521. Reason: nanny-close
2023-12-18 06:41:44,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:44,195 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-18 06:41:49,480 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:49,484 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32929 instead
  warnings.warn(
2023-12-18 06:41:49,488 - distributed.scheduler - INFO - State start
2023-12-18 06:41:49,512 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:49,513 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:41:49,513 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:49,514 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:41:49,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45363'
2023-12-18 06:41:51,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:51,360 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:51,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:52,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45363'. Reason: nanny-close
2023-12-18 06:41:53,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35681
2023-12-18 06:41:53,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35681
2023-12-18 06:41:53,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43303
2023-12-18 06:41:53,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:53,079 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:53,079 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:53,079 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:41:53,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sx1hzfcx
2023-12-18 06:41:53,080 - distributed.worker - INFO - Starting Worker plugin PreImport-961d30d8-ba3c-4180-89ed-09712205b7d8
2023-12-18 06:41:53,082 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ef781e7-c764-4830-aeef-db56a106424d
2023-12-18 06:41:53,082 - distributed.worker - INFO - Starting Worker plugin RMMSetup-665ce050-7343-4b9f-91d4-04c2e0b226b6
2023-12-18 06:41:53,084 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:53,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:53,116 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:53,116 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:53,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:53,119 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:53,121 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35681. Reason: nanny-close
2023-12-18 06:41:53,123 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:53,125 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-18 06:41:56,248 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:56,252 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37437 instead
  warnings.warn(
2023-12-18 06:41:56,256 - distributed.scheduler - INFO - State start
2023-12-18 06:41:56,323 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:56,324 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:41:56,325 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37437/status
2023-12-18 06:41:56,325 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:42:00,610 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:33230'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33230>: Stream is closed
2023-12-18 06:42:00,849 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:42:00,849 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:42:00,850 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:00,851 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:42:00,851 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-18 06:42:03,053 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:03,058 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37045 instead
  warnings.warn(
2023-12-18 06:42:03,061 - distributed.scheduler - INFO - State start
2023-12-18 06:42:03,084 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:03,085 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-18 06:42:03,085 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37045/status
2023-12-18 06:42:03,086 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:42:03,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38433'
2023-12-18 06:42:03,451 - distributed.scheduler - INFO - Receive client connection: Client-8c9235e1-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:03,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57150
2023-12-18 06:42:03,852 - distributed.scheduler - INFO - Receive client connection: Client-8c5b4631-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:03,853 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57184
2023-12-18 06:42:04,901 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:04,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:04,905 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:05,913 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37097
2023-12-18 06:42:05,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37097
2023-12-18 06:42:05,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40745
2023-12-18 06:42:05,914 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:42:05,914 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:05,914 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:05,914 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:42:05,914 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-3tre4l13
2023-12-18 06:42:05,915 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38ad7801-c36d-464d-889c-f7b735cc7431
2023-12-18 06:42:05,915 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efd66092-6232-4aa9-9c1c-1a047a937460
2023-12-18 06:42:05,915 - distributed.worker - INFO - Starting Worker plugin PreImport-32295575-1520-44e8-a686-59f80cbccc49
2023-12-18 06:42:05,915 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:05,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37097', status: init, memory: 0, processing: 0>
2023-12-18 06:42:05,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37097
2023-12-18 06:42:05,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57190
2023-12-18 06:42:05,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:05,957 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:42:05,957 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:05,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:42:05,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:05,995 - distributed.scheduler - INFO - Remove client Client-8c5b4631-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:05,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57184; closing.
2023-12-18 06:42:05,996 - distributed.scheduler - INFO - Remove client Client-8c5b4631-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:05,996 - distributed.scheduler - INFO - Close client connection: Client-8c5b4631-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:05,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38433'. Reason: nanny-close
2023-12-18 06:42:05,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:05,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37097. Reason: nanny-close
2023-12-18 06:42:06,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57190; closing.
2023-12-18 06:42:06,000 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:42:06,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881726.001008')
2023-12-18 06:42:06,001 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:06,002 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:06,150 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39617', status: init, memory: 0, processing: 0>
2023-12-18 06:42:06,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39617
2023-12-18 06:42:06,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57202
2023-12-18 06:42:06,240 - distributed.scheduler - INFO - Remove client Client-8c9235e1-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:06,240 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57150; closing.
2023-12-18 06:42:06,241 - distributed.scheduler - INFO - Remove client Client-8c9235e1-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:06,241 - distributed.scheduler - INFO - Close client connection: Client-8c9235e1-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:06,245 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57202; closing.
2023-12-18 06:42:06,245 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39617', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881726.2457285')
2023-12-18 06:42:06,245 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:07,113 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:42:07,114 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:42:07,114 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:07,115 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-18 06:42:07,116 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-18 06:42:09,538 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:09,543 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41413 instead
  warnings.warn(
2023-12-18 06:42:09,547 - distributed.scheduler - INFO - State start
2023-12-18 06:42:09,655 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:09,656 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:42:09,658 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:09,659 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:42:09,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40241'
2023-12-18 06:42:09,929 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39787'
2023-12-18 06:42:09,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35859'
2023-12-18 06:42:09,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42323'
2023-12-18 06:42:09,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39943'
2023-12-18 06:42:09,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33065'
2023-12-18 06:42:09,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35047'
2023-12-18 06:42:09,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43731'
2023-12-18 06:42:11,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,870 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,870 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,874 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,903 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,951 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,951 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,956 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,962 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:17,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34173
2023-12-18 06:42:17,701 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34173
2023-12-18 06:42:17,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32829
2023-12-18 06:42:17,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,702 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,702 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,702 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5acfbrfo
2023-12-18 06:42:17,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9295ae25-22e5-4010-992f-0c32da0842d3
2023-12-18 06:42:17,703 - distributed.worker - INFO - Starting Worker plugin PreImport-94ed7bfd-5b4a-41bd-a4d5-c7c6b1b86b26
2023-12-18 06:42:17,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-983988d5-bc51-4794-80a9-4e16cc409d0f
2023-12-18 06:42:17,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42851
2023-12-18 06:42:17,797 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42851
2023-12-18 06:42:17,797 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45889
2023-12-18 06:42:17,797 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,798 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,798 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t0x1bhej
2023-12-18 06:42:17,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c3cd87f-4dcd-49d9-95ad-570b186171df
2023-12-18 06:42:17,798 - distributed.worker - INFO - Starting Worker plugin PreImport-3e3c8cc7-e65e-4da4-b9be-b36b8c8cdc9c
2023-12-18 06:42:17,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67d47906-a630-43e2-8534-c71caf498585
2023-12-18 06:42:17,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37841
2023-12-18 06:42:17,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37841
2023-12-18 06:42:17,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39857
2023-12-18 06:42:17,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,811 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,811 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f6xnt62i
2023-12-18 06:42:17,812 - distributed.worker - INFO - Starting Worker plugin PreImport-422ec575-cef9-4a83-9e94-b48589a265ad
2023-12-18 06:42:17,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d89979b4-befa-4e2d-b186-1938279e137f
2023-12-18 06:42:17,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41027
2023-12-18 06:42:17,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41027
2023-12-18 06:42:17,831 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43073
2023-12-18 06:42:17,831 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,831 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,831 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,831 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-655lm811
2023-12-18 06:42:17,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d77b88e-3734-415d-8975-2aa97777535d
2023-12-18 06:42:17,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45651
2023-12-18 06:42:17,842 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45651
2023-12-18 06:42:17,842 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44165
2023-12-18 06:42:17,842 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,842 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,842 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,842 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jc9706i6
2023-12-18 06:42:17,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df447505-fc6c-4312-b11a-4ca8ef7894bf
2023-12-18 06:42:17,843 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40333
2023-12-18 06:42:17,843 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40333
2023-12-18 06:42:17,843 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35151
2023-12-18 06:42:17,843 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,844 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,844 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,844 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,844 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w1sydw7_
2023-12-18 06:42:17,844 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-378f3449-cfca-4972-a956-e42227f1fd53
2023-12-18 06:42:17,847 - distributed.worker - INFO - Starting Worker plugin PreImport-5518f21f-7932-401e-ba0c-956f1bd1a02f
2023-12-18 06:42:17,848 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ad1e7bd-2930-4dd5-a9bf-5eb436baac6f
2023-12-18 06:42:17,847 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38867
2023-12-18 06:42:17,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38867
2023-12-18 06:42:17,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45083
2023-12-18 06:42:17,848 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,848 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,848 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,848 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mtx8rc9o
2023-12-18 06:42:17,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-feef4ce8-23de-491c-a5f6-f435714ce724
2023-12-18 06:42:17,849 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33729
2023-12-18 06:42:17,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33729
2023-12-18 06:42:17,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46475
2023-12-18 06:42:17,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,850 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,850 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,850 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vshhtl7b
2023-12-18 06:42:17,851 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdd7461f-26fe-46d6-a0b6-42780acfce1e
2023-12-18 06:42:17,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6882a1e3-97e8-4b9b-8a48-c95646360e8c
2023-12-18 06:42:17,857 - distributed.worker - INFO - Starting Worker plugin PreImport-cbd5c531-5002-4fa9-b042-46d8e5368109
2023-12-18 06:42:17,858 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2d23071-a69e-417c-a42d-bc07c27f5df4
2023-12-18 06:42:17,966 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,977 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c130f5a-2e10-4efa-bd1c-a119573b3dd9
2023-12-18 06:42:17,980 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,982 - distributed.worker - INFO - Starting Worker plugin PreImport-38ed086f-1f18-4242-94a4-b4841fafc623
2023-12-18 06:42:17,982 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd11153a-f02e-4dbb-a13c-fbe2fbd0ec5a
2023-12-18 06:42:17,982 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,988 - distributed.worker - INFO - Starting Worker plugin PreImport-f3c4cb8d-44d8-4031-9b0d-0f9ff27eac0e
2023-12-18 06:42:17,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-340640c9-871c-44e2-b35a-9fd9b0c1a039
2023-12-18 06:42:17,989 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,989 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,997 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,998 - distributed.worker - INFO - Starting Worker plugin PreImport-90aaac87-6e0d-469a-b616-74dab10df27f
2023-12-18 06:42:17,998 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,002 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,007 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,007 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,012 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,013 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,013 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,019 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,022 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,028 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,028 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,038 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,038 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,040 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,040 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,040 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,044 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,044 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:25,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40241'. Reason: nanny-close
2023-12-18 06:42:25,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39787'. Reason: nanny-close
2023-12-18 06:42:25,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35859'. Reason: nanny-close
2023-12-18 06:42:25,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37841. Reason: nanny-close
2023-12-18 06:42:25,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42323'. Reason: nanny-close
2023-12-18 06:42:25,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40333. Reason: nanny-close
2023-12-18 06:42:25,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45651. Reason: nanny-close
2023-12-18 06:42:25,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39943'. Reason: nanny-close
2023-12-18 06:42:25,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41027. Reason: nanny-close
2023-12-18 06:42:25,740 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33065'. Reason: nanny-close
2023-12-18 06:42:25,740 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34173. Reason: nanny-close
2023-12-18 06:42:25,740 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35047'. Reason: nanny-close
2023-12-18 06:42:25,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43731'. Reason: nanny-close
2023-12-18 06:42:25,741 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38867. Reason: nanny-close
2023-12-18 06:42:25,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,741 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42851. Reason: nanny-close
2023-12-18 06:42:25,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,743 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,743 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,743 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,743 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,743 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33729. Reason: nanny-close
2023-12-18 06:42:25,744 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,744 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,744 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,746 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,746 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,752 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-18 06:42:29,723 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:29,728 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43171 instead
  warnings.warn(
2023-12-18 06:42:29,732 - distributed.scheduler - INFO - State start
2023-12-18 06:42:29,753 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:29,754 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:42:29,755 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43171/status
2023-12-18 06:42:29,755 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:42:29,815 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37027'
2023-12-18 06:42:30,228 - distributed.scheduler - INFO - Receive client connection: Client-9c8dabfd-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:30,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35478
2023-12-18 06:42:30,764 - distributed.scheduler - INFO - Receive client connection: Client-9c3e40ed-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:30,764 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35494
2023-12-18 06:42:31,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:31,534 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:31,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:32,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45951
2023-12-18 06:42:32,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45951
2023-12-18 06:42:32,428 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46839
2023-12-18 06:42:32,428 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:32,428 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:32,428 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:32,428 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:42:32,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0jgi2__i
2023-12-18 06:42:32,429 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a6bbbea-b9bc-4671-90b3-6ed46e9971d7
2023-12-18 06:42:32,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c91d38be-5309-43ee-b702-89d8f77ed258
2023-12-18 06:42:32,436 - distributed.worker - INFO - Starting Worker plugin PreImport-4a48418a-f778-4cd9-bc42-7654ed03e19b
2023-12-18 06:42:32,437 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:32,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45951', status: init, memory: 0, processing: 0>
2023-12-18 06:42:32,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45951
2023-12-18 06:42:32,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35516
2023-12-18 06:42:32,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:32,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:32,468 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:32,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:32,520 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:42:32,524 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:42:32,525 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:32,526 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:32,528 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:32,529 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:32,530 - distributed.scheduler - INFO - Remove client Client-9c3e40ed-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:32,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35494; closing.
2023-12-18 06:42:32,530 - distributed.scheduler - INFO - Remove client Client-9c3e40ed-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:32,531 - distributed.scheduler - INFO - Close client connection: Client-9c3e40ed-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:32,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37027'. Reason: nanny-close
2023-12-18 06:42:32,532 - distributed.scheduler - INFO - Remove client Client-9c8dabfd-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:32,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:32,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35478; closing.
2023-12-18 06:42:32,532 - distributed.scheduler - INFO - Remove client Client-9c8dabfd-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:32,533 - distributed.scheduler - INFO - Close client connection: Client-9c8dabfd-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:32,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45951. Reason: nanny-close
2023-12-18 06:42:32,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:32,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35516; closing.
2023-12-18 06:42:32,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881752.535775')
2023-12-18 06:42:32,536 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:32,537 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:33,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35091', status: init, memory: 0, processing: 0>
2023-12-18 06:42:33,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35091
2023-12-18 06:42:33,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35532
2023-12-18 06:42:33,169 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35532; closing.
2023-12-18 06:42:33,170 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35091', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881753.1701612')
2023-12-18 06:42:33,170 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:33,598 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:42:33,599 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:42:33,599 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:33,600 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:42:33,601 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-18 06:42:36,034 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:36,039 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35629 instead
  warnings.warn(
2023-12-18 06:42:36,043 - distributed.scheduler - INFO - State start
2023-12-18 06:42:36,067 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:36,068 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:42:36,069 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35629/status
2023-12-18 06:42:36,069 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:42:36,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35095'
2023-12-18 06:42:36,932 - distributed.scheduler - INFO - Receive client connection: Client-a0423707-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:36,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35740
2023-12-18 06:42:37,088 - distributed.scheduler - INFO - Receive client connection: Client-9ffcaaaf-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:37,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35766
2023-12-18 06:42:37,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:37,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:37,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:38,581 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33825
2023-12-18 06:42:38,582 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33825
2023-12-18 06:42:38,582 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40033
2023-12-18 06:42:38,582 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:38,582 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:38,582 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:38,582 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:42:38,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-86m9bffz
2023-12-18 06:42:38,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81a0e8ce-c44b-4f13-835f-c66c0ab92c6b
2023-12-18 06:42:38,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63855acb-6615-4fb4-ac09-80ab83475dc1
2023-12-18 06:42:38,598 - distributed.worker - INFO - Starting Worker plugin PreImport-cf4aaa01-02b8-4186-bcc7-7015c4c4f427
2023-12-18 06:42:38,598 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:38,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33825', status: init, memory: 0, processing: 0>
2023-12-18 06:42:38,625 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33825
2023-12-18 06:42:38,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35792
2023-12-18 06:42:38,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:38,627 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:38,627 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:38,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:38,681 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-18 06:42:38,685 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:42:38,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:38,690 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:38,693 - distributed.scheduler - INFO - Remove client Client-a0423707-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:38,693 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35740; closing.
2023-12-18 06:42:38,693 - distributed.scheduler - INFO - Remove client Client-a0423707-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:38,693 - distributed.scheduler - INFO - Close client connection: Client-a0423707-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:38,719 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-18 06:42:38,723 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-18 06:42:38,726 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:38,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:38,730 - distributed.scheduler - INFO - Remove client Client-9ffcaaaf-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:38,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35766; closing.
2023-12-18 06:42:38,730 - distributed.scheduler - INFO - Remove client Client-9ffcaaaf-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:38,731 - distributed.scheduler - INFO - Close client connection: Client-9ffcaaaf-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:38,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35095'. Reason: nanny-close
2023-12-18 06:42:38,732 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:38,733 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33825. Reason: nanny-close
2023-12-18 06:42:38,735 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35792; closing.
2023-12-18 06:42:38,735 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:38,735 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881758.7357612')
2023-12-18 06:42:38,736 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:38,737 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:39,665 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41119', status: init, memory: 0, processing: 0>
2023-12-18 06:42:39,666 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41119
2023-12-18 06:42:39,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35800
2023-12-18 06:42:39,688 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35800; closing.
2023-12-18 06:42:39,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881759.6888993')
2023-12-18 06:42:39,689 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:39,697 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:42:39,697 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:42:39,698 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:39,699 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:42:39,699 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43821 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41911 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41141 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36007 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45443 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40537 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44279 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44229 instead
  warnings.warn(
[1702881875.572646] [dgx13:68624:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:34234) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37269 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35715 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40321 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43319 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46157 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42185 instead
  warnings.warn(
2023-12-18 06:46:36,690 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34022 remote=tcp://127.0.0.1:33143>: Stream is closed
2023-12-18 06:46:36,691 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34018 remote=tcp://127.0.0.1:33143>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] [1702882087.062710] [dgx13:72382:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:37190) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44483 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] [1702882290.438905] [dgx13:76116:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:33753) failed: Address already in use
[1702882293.507574] [dgx13:76123:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:34638) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45705 instead
  warnings.warn(
[1702882311.427899] [dgx13:76642:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:37262) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44527 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41727 instead
  warnings.warn(
[1702882360.376852] [dgx13:77346:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:37488) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34931 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43317 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35611 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46663 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43061 instead
  warnings.warn(
2023-12-18 06:54:15,746 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-8f91e446-3c67-4f53-990f-57bfe9c4cbfc
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7f3684614a60>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-18 06:54:15,757 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-be2dad96-a556-4d95-beb1-ae7e7de89d14
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7ff65f534790>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-18 06:54:15,779 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-2dd40de4-6d37-4387-a55a-6886d9b20e63
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7f325d261f70>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-18 06:54:15,793 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-68a0b946-e1ae-421e-8967-121c9cabe288
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7f53681569d0>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-18 06:54:16,056 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-0700141f-d149-4b52-b5ba-001f6830ed52
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7f325d261f70>, ('explicit-comms-shuffle-b298f9d350dc3bbbc5b838a7130ac54d', {0: set(), 1: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 0)}, 2: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 2)}, 3: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-18 06:54:16,056 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-89d2f12f-2e71-4024-977d-e34099a6d1c8
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7ff65f534790>, ('explicit-comms-shuffle-b298f9d350dc3bbbc5b838a7130ac54d', {0: set(), 1: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 0)}, 2: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 2)}, 3: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-18 06:54:16,063 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-9150be59-afe0-450a-b844-48d44fc0f75f
Function:  _run_coroutine_on_worker
args:      (202031003025341179650417042117383350723, <function shuffle_task at 0x7f53681569d0>, ('explicit-comms-shuffle-b298f9d350dc3bbbc5b838a7130ac54d', {0: set(), 1: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 0)}, 2: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 2)}, 3: {('from_pandas-f78d7bc4797a236520070f3ed4b241a4', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
