============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-07-26 05:29:05,836 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:05,840 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44779 instead
  warnings.warn(
2023-07-26 05:29:05,843 - distributed.scheduler - INFO - State start
2023-07-26 05:29:05,863 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:05,863 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-26 05:29:05,864 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44779/status
2023-07-26 05:29:06,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44085'
2023-07-26 05:29:06,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43443'
2023-07-26 05:29:06,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37827'
2023-07-26 05:29:06,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41153'
2023-07-26 05:29:07,507 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lt9jsevc', purging
2023-07-26 05:29:07,508 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-751bw148', purging
2023-07-26 05:29:07,508 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o_aigh8h', purging
2023-07-26 05:29:07,509 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ncs67ndl', purging
2023-07-26 05:29:07,509 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ijwq87vh', purging
2023-07-26 05:29:07,509 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-571_dytj', purging
2023-07-26 05:29:07,510 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pi201vd5', purging
2023-07-26 05:29:07,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:07,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:07,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-07-26 05:29:07,528 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40431
2023-07-26 05:29:07,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40431
2023-07-26 05:29:07,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45553
2023-07-26 05:29:07,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:29:07,528 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:07,528 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:29:07,528 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:29:07,528 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cx19y8v5
2023-07-26 05:29:07,529 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9bdd23ca-19a7-45cc-9f92-52e46dcf52d0
2023-07-26 05:29:07,529 - distributed.worker - INFO - Starting Worker plugin PreImport-37fec865-99ee-4de8-ac24-afefcef51f6f
2023-07-26 05:29:07,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c20f8964-784f-4620-87a1-cbcb6c00abc2
2023-07-26 05:29:07,529 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:07,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40431', status: init, memory: 0, processing: 0>
2023-07-26 05:29:07,556 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40431
2023-07-26 05:29:07,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49118
2023-07-26 05:29:07,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:29:07,557 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:07,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:29:07,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:07,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:07,600 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:07,600 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:07,601 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:07,601 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:07,605 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:07,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:07,608 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:07,691 - distributed.scheduler - INFO - Receive client connection: Client-55a98de1-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:07,691 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49134
2023-07-26 05:29:08,831 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33917
2023-07-26 05:29:08,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33917
2023-07-26 05:29:08,831 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41541
2023-07-26 05:29:08,831 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:29:08,831 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,831 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:29:08,831 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:29:08,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7425oppi
2023-07-26 05:29:08,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24502a36-8039-4c59-a76a-77e95679c063
2023-07-26 05:29:08,832 - distributed.worker - INFO - Starting Worker plugin PreImport-263b67ae-2198-4c1d-b337-ec8d4e7b0b6a
2023-07-26 05:29:08,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44859
2023-07-26 05:29:08,832 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fc14b4d-91e3-4df4-913d-182225f5fba3
2023-07-26 05:29:08,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44859
2023-07-26 05:29:08,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40119
2023-07-26 05:29:08,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:29:08,832 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,832 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:29:08,832 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:29:08,832 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ob7eaqom
2023-07-26 05:29:08,832 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33709569-a9c6-49a0-beaf-7fedebcab0e1
2023-07-26 05:29:08,832 - distributed.worker - INFO - Starting Worker plugin PreImport-f7a87f4b-cc9a-491b-b3eb-e33fbed8d6a0
2023-07-26 05:29:08,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-657dc9e6-ef13-45aa-9296-f714373b5711
2023-07-26 05:29:08,833 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38409
2023-07-26 05:29:08,837 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38409
2023-07-26 05:29:08,837 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34745
2023-07-26 05:29:08,837 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:29:08,837 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,837 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:29:08,837 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:29:08,837 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m8tpeam2
2023-07-26 05:29:08,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66ed7526-456d-41d9-82d5-8931ad3329d4
2023-07-26 05:29:08,838 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-706c57df-61c2-4358-93b9-2a10883d9bfc
2023-07-26 05:29:08,838 - distributed.worker - INFO - Starting Worker plugin PreImport-dd7329d9-1737-489f-aa55-71a4c9290f5f
2023-07-26 05:29:08,838 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44859', status: init, memory: 0, processing: 0>
2023-07-26 05:29:08,851 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44859
2023-07-26 05:29:08,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49164
2023-07-26 05:29:08,852 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:29:08,852 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,853 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33917', status: init, memory: 0, processing: 0>
2023-07-26 05:29:08,853 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33917
2023-07-26 05:29:08,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49162
2023-07-26 05:29:08,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:29:08,854 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:29:08,854 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:29:08,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38409', status: init, memory: 0, processing: 0>
2023-07-26 05:29:08,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38409
2023-07-26 05:29:08,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49174
2023-07-26 05:29:08,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:29:08,859 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:08,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:29:08,935 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:29:08,936 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:29:08,937 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:29:08,937 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:29:08,940 - distributed.scheduler - INFO - Remove client Client-55a98de1-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:08,941 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49134; closing.
2023-07-26 05:29:08,941 - distributed.scheduler - INFO - Remove client Client-55a98de1-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:08,941 - distributed.scheduler - INFO - Close client connection: Client-55a98de1-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:08,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44085'. Reason: nanny-close
2023-07-26 05:29:08,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:08,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43443'. Reason: nanny-close
2023-07-26 05:29:08,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:08,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37827'. Reason: nanny-close
2023-07-26 05:29:08,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38409. Reason: nanny-close
2023-07-26 05:29:08,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:08,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44859. Reason: nanny-close
2023-07-26 05:29:08,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41153'. Reason: nanny-close
2023-07-26 05:29:08,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:08,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33917. Reason: nanny-close
2023-07-26 05:29:08,946 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:29:08,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49174; closing.
2023-07-26 05:29:08,946 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40431. Reason: nanny-close
2023-07-26 05:29:08,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38409', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:08,946 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:29:08,946 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38409
2023-07-26 05:29:08,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49164; closing.
2023-07-26 05:29:08,947 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:08,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:29:08,947 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:08,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44859', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:08,947 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38409
2023-07-26 05:29:08,947 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44859
2023-07-26 05:29:08,948 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:29:08,948 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49162; closing.
2023-07-26 05:29:08,948 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:08,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33917', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:08,948 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33917
2023-07-26 05:29:08,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49118; closing.
2023-07-26 05:29:08,949 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:08,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40431', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:08,949 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40431
2023-07-26 05:29:08,949 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:09,958 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:09,959 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:09,959 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:09,960 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-26 05:29:09,960 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-07-26 05:29:11,714 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:11,718 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40557 instead
  warnings.warn(
2023-07-26 05:29:11,721 - distributed.scheduler - INFO - State start
2023-07-26 05:29:11,740 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:11,740 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:11,741 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:11,741 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-26 05:29:11,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42339'
2023-07-26 05:29:11,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39451'
2023-07-26 05:29:11,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34061'
2023-07-26 05:29:11,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43379'
2023-07-26 05:29:11,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33683'
2023-07-26 05:29:11,980 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40849'
2023-07-26 05:29:11,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42771'
2023-07-26 05:29:11,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36707'
2023-07-26 05:29:13,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,566 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,595 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:13,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:13,613 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:13,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:15,228 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34933
2023-07-26 05:29:15,229 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34933
2023-07-26 05:29:15,229 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39257
2023-07-26 05:29:15,229 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:15,229 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:15,229 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:15,229 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:15,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x_ur1nwt
2023-07-26 05:29:15,229 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f62551b-1d04-46bd-84d8-ffa7e477f639
2023-07-26 05:29:15,230 - distributed.worker - INFO - Starting Worker plugin RMMSetup-585b6669-87ab-4570-ba01-adae0b58862b
2023-07-26 05:29:15,413 - distributed.worker - INFO - Starting Worker plugin PreImport-a5b1288a-18f9-4e54-af30-d5c1218c030b
2023-07-26 05:29:15,414 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:15,636 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:15,636 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:15,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34977
2023-07-26 05:29:16,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34977
2023-07-26 05:29:16,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33827
2023-07-26 05:29:16,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,094 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,094 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qbzdk1cm
2023-07-26 05:29:16,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43303
2023-07-26 05:29:16,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43303
2023-07-26 05:29:16,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88a6bf61-ab9a-4979-9eee-6b0d87290732
2023-07-26 05:29:16,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35887
2023-07-26 05:29:16,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,094 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,095 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9ihnehag
2023-07-26 05:29:16,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29682d64-77a8-41ed-86aa-bbf3adf62147
2023-07-26 05:29:16,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b497b124-5519-477b-81f7-dd68cc144b6e
2023-07-26 05:29:16,123 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39115
2023-07-26 05:29:16,123 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39115
2023-07-26 05:29:16,123 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39131
2023-07-26 05:29:16,123 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,123 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,123 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,123 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,123 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dgocmygr
2023-07-26 05:29:16,124 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b24506e-e069-4108-917d-3edf6309be5a
2023-07-26 05:29:16,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58a07eac-820f-44ea-aeb0-fab2af0d7fda
2023-07-26 05:29:16,127 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36993
2023-07-26 05:29:16,127 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36993
2023-07-26 05:29:16,127 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40733
2023-07-26 05:29:16,128 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,128 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,128 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,128 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nk9nc2ri
2023-07-26 05:29:16,128 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94025900-809a-4c71-9d21-def689e49882
2023-07-26 05:29:16,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41131
2023-07-26 05:29:16,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41131
2023-07-26 05:29:16,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42287
2023-07-26 05:29:16,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,181 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,181 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-puif61l_
2023-07-26 05:29:16,182 - distributed.worker - INFO - Starting Worker plugin PreImport-81fdc009-0ed7-4b35-86ae-8d22dd0b2401
2023-07-26 05:29:16,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8616e83-a5e9-41bc-b732-aeb94acdb319
2023-07-26 05:29:16,183 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35677
2023-07-26 05:29:16,183 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35677
2023-07-26 05:29:16,183 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43347
2023-07-26 05:29:16,183 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,183 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,183 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,183 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7pykijed
2023-07-26 05:29:16,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d44be7e-42af-4a7b-ab67-805f74581bf3
2023-07-26 05:29:16,186 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35893
2023-07-26 05:29:16,186 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35893
2023-07-26 05:29:16,186 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37867
2023-07-26 05:29:16,187 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,187 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,187 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:16,187 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:16,187 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1lm04wfx
2023-07-26 05:29:16,187 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84d5f329-6fb2-42e7-9789-c6695be86171
2023-07-26 05:29:16,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-543e5681-6269-4348-85c5-7d76b7cc4452
2023-07-26 05:29:16,240 - distributed.worker - INFO - Starting Worker plugin PreImport-8ca6c389-056d-41fb-9860-b636bb3fe7b5
2023-07-26 05:29:16,240 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,240 - distributed.worker - INFO - Starting Worker plugin PreImport-83f7803c-5d2b-4868-8199-77a69e15a26e
2023-07-26 05:29:16,241 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,244 - distributed.worker - INFO - Starting Worker plugin PreImport-4144e65e-7d8a-4bff-a947-27bf1b2ff0ca
2023-07-26 05:29:16,245 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1db2ec2a-888a-401b-b91b-19f4f2083638
2023-07-26 05:29:16,258 - distributed.worker - INFO - Starting Worker plugin PreImport-c5ea1c2c-fe8a-4104-949f-85d95f27e2ef
2023-07-26 05:29:16,258 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,272 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,272 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,274 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,274 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,275 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,284 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,284 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bbccda7e-4aac-48dc-8042-cf00ab3b64e9
2023-07-26 05:29:16,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50589f61-b3aa-4ae9-986f-3561c489d669
2023-07-26 05:29:16,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25326cc4-0a76-4a9c-9242-32ab01db181e
2023-07-26 05:29:16,315 - distributed.worker - INFO - Starting Worker plugin PreImport-328fc00c-d371-4d9e-a6a8-4c962218a347
2023-07-26 05:29:16,316 - distributed.worker - INFO - Starting Worker plugin PreImport-1aa711d8-27f4-466f-af03-901195ca1324
2023-07-26 05:29:16,316 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,316 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,316 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,338 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,338 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,340 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,340 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,344 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:16,344 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:16,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:16,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,407 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,407 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,407 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,407 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,407 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,407 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:16,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43379'. Reason: nanny-close
2023-07-26 05:29:16,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42339'. Reason: nanny-close
2023-07-26 05:29:16,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39115. Reason: nanny-close
2023-07-26 05:29:16,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39451'. Reason: nanny-close
2023-07-26 05:29:16,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34061'. Reason: nanny-close
2023-07-26 05:29:16,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41131. Reason: nanny-close
2023-07-26 05:29:16,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36993. Reason: nanny-close
2023-07-26 05:29:16,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33683'. Reason: nanny-close
2023-07-26 05:29:16,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40849'. Reason: nanny-close
2023-07-26 05:29:16,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43303. Reason: nanny-close
2023-07-26 05:29:16,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42771'. Reason: nanny-close
2023-07-26 05:29:16,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34933. Reason: nanny-close
2023-07-26 05:29:16,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34977. Reason: nanny-close
2023-07-26 05:29:16,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36707'. Reason: nanny-close
2023-07-26 05:29:16,418 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:16,418 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39115
2023-07-26 05:29:16,418 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39115
2023-07-26 05:29:16,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35677. Reason: nanny-close
2023-07-26 05:29:16,419 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,419 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35893. Reason: nanny-close
2023-07-26 05:29:16,420 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,420 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39115
2023-07-26 05:29:16,420 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39115
2023-07-26 05:29:16,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:16,422 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,422 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,422 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:16,423 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-07-26 05:29:19,348 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:19,352 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46039 instead
  warnings.warn(
2023-07-26 05:29:19,356 - distributed.scheduler - INFO - State start
2023-07-26 05:29:19,375 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:19,376 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:19,376 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:19,376 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-26 05:29:19,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35141'
2023-07-26 05:29:19,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37085'
2023-07-26 05:29:19,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40161'
2023-07-26 05:29:19,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44975'
2023-07-26 05:29:19,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46007'
2023-07-26 05:29:19,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45403'
2023-07-26 05:29:19,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45087'
2023-07-26 05:29:19,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34321'
2023-07-26 05:29:21,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,215 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,217 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,221 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:21,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:21,223 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,243 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,248 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:21,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:23,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34867
2023-07-26 05:29:23,371 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34867
2023-07-26 05:29:23,371 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40865
2023-07-26 05:29:23,371 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,371 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,371 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,371 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,371 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dl2rgdau
2023-07-26 05:29:23,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c75eb21-1094-4f3a-8bae-d4b7b2801c70
2023-07-26 05:29:23,376 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-937e0e23-4eb1-473b-a5a4-2321a8935ca6
2023-07-26 05:29:23,377 - distributed.worker - INFO - Starting Worker plugin PreImport-c20e88b5-2f07-42bc-9fc2-bb23a918f6c8
2023-07-26 05:29:23,377 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,406 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37673
2023-07-26 05:29:23,568 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37673
2023-07-26 05:29:23,568 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44905
2023-07-26 05:29:23,568 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,568 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,569 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,569 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y0h83l64
2023-07-26 05:29:23,569 - distributed.worker - INFO - Starting Worker plugin PreImport-2e0f4479-4d2b-4357-9822-924c2807b2d3
2023-07-26 05:29:23,569 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7abff464-6712-48a6-868a-3fd9cb12a325
2023-07-26 05:29:23,569 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a01002de-8d18-457c-b073-329c39b1270a
2023-07-26 05:29:23,573 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,599 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,599 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,609 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40111
2023-07-26 05:29:23,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40111
2023-07-26 05:29:23,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42171
2023-07-26 05:29:23,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,610 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,610 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,610 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,610 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_z6ra85f
2023-07-26 05:29:23,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e6b64b2-17ef-4a19-b991-a1242191197d
2023-07-26 05:29:23,624 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40635
2023-07-26 05:29:23,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40635
2023-07-26 05:29:23,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38499
2023-07-26 05:29:23,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,625 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,625 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,625 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,625 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f8p0aiuu
2023-07-26 05:29:23,625 - distributed.worker - INFO - Starting Worker plugin PreImport-5279edce-c5d0-42bd-a37a-894f4f666f79
2023-07-26 05:29:23,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f385b0ea-7ace-4698-9c36-eebaaca197b2
2023-07-26 05:29:23,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc7b2146-8d5c-4a5a-886b-d5266cf715ac
2023-07-26 05:29:23,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-031acbac-78cf-43ff-bf7c-16c90b158aa0
2023-07-26 05:29:23,627 - distributed.worker - INFO - Starting Worker plugin PreImport-a99510a3-0dde-47ad-a1d0-be69bbc41833
2023-07-26 05:29:23,627 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,630 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,652 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,652 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,655 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,655 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36457
2023-07-26 05:29:23,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36457
2023-07-26 05:29:23,744 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41775
2023-07-26 05:29:23,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,745 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,745 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-apba_9ut
2023-07-26 05:29:23,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e6613fc-1272-4e3d-956e-514ca2846abd
2023-07-26 05:29:23,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38841
2023-07-26 05:29:23,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38841
2023-07-26 05:29:23,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37807
2023-07-26 05:29:23,749 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,749 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,749 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,749 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gewknf5t
2023-07-26 05:29:23,749 - distributed.worker - INFO - Starting Worker plugin PreImport-9a06308f-b685-4410-8310-d5471dad7ea7
2023-07-26 05:29:23,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ece84b9-7835-4e1b-a049-7b3099e64d66
2023-07-26 05:29:23,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da03872f-e73b-4265-8a27-30edf1a63a28
2023-07-26 05:29:23,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38131
2023-07-26 05:29:23,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38131
2023-07-26 05:29:23,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38817
2023-07-26 05:29:23,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,752 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,752 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-drvwf04t
2023-07-26 05:29:23,752 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43109
2023-07-26 05:29:23,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43109
2023-07-26 05:29:23,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33581
2023-07-26 05:29:23,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,753 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,753 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:23,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:23,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-piu6dwcn
2023-07-26 05:29:23,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbf6d785-7d03-405c-9905-f417182d5cf0
2023-07-26 05:29:23,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf355423-0f20-4c91-a687-8061905f38f5
2023-07-26 05:29:23,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-060acc30-3206-4524-9740-3a77c56db85d
2023-07-26 05:29:23,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e78ee18-d059-47b1-8f42-498eec165149
2023-07-26 05:29:23,765 - distributed.worker - INFO - Starting Worker plugin PreImport-0603d9bc-cb96-4e4b-bced-21a21e0934f3
2023-07-26 05:29:23,765 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,766 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,768 - distributed.worker - INFO - Starting Worker plugin PreImport-42935498-72d6-400f-8764-720a18814d17
2023-07-26 05:29:23,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69270abf-7319-45e6-b32c-891f98e3ae8d
2023-07-26 05:29:23,768 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,768 - distributed.worker - INFO - Starting Worker plugin PreImport-e45ad8dd-fbd1-43a6-9154-20ab4b89f7b6
2023-07-26 05:29:23,769 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,784 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,795 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,795 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,796 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,798 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:23,798 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:23,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:23,846 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,846 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,846 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,846 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,847 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,847 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,847 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,847 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:23,854 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37085'. Reason: nanny-close
2023-07-26 05:29:23,855 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,855 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46007'. Reason: nanny-close
2023-07-26 05:29:23,856 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35141'. Reason: nanny-close
2023-07-26 05:29:23,856 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38131. Reason: nanny-close
2023-07-26 05:29:23,856 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,857 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43109. Reason: nanny-close
2023-07-26 05:29:23,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40161'. Reason: nanny-close
2023-07-26 05:29:23,857 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,858 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37673. Reason: nanny-close
2023-07-26 05:29:23,858 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44975'. Reason: nanny-close
2023-07-26 05:29:23,858 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,858 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,858 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34867. Reason: nanny-close
2023-07-26 05:29:23,858 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45403'. Reason: nanny-close
2023-07-26 05:29:23,859 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45087'. Reason: nanny-close
2023-07-26 05:29:23,859 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,859 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40111. Reason: nanny-close
2023-07-26 05:29:23,859 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,859 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,859 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34321'. Reason: nanny-close
2023-07-26 05:29:23,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38841. Reason: nanny-close
2023-07-26 05:29:23,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:23,860 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38131
2023-07-26 05:29:23,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40635. Reason: nanny-close
2023-07-26 05:29:23,860 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38131
2023-07-26 05:29:23,860 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,861 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36457. Reason: nanny-close
2023-07-26 05:29:23,862 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,862 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38131
2023-07-26 05:29:23,862 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38131
2023-07-26 05:29:23,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,862 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38131
2023-07-26 05:29:23,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:23,864 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,864 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,864 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,864 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:23,866 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-07-26 05:29:26,769 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:26,773 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:26,775 - distributed.scheduler - INFO - State start
2023-07-26 05:29:26,794 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:26,795 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:26,795 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:27,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34441'
2023-07-26 05:29:27,030 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37113'
2023-07-26 05:29:27,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38981'
2023-07-26 05:29:27,041 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35549'
2023-07-26 05:29:27,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38663'
2023-07-26 05:29:27,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45307'
2023-07-26 05:29:27,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37577'
2023-07-26 05:29:27,075 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38889'
2023-07-26 05:29:28,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,640 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,698 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,711 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:28,724 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:28,731 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,736 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:28,964 - distributed.scheduler - INFO - Receive client connection: Client-62260afb-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:28,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51508
2023-07-26 05:29:30,295 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35675
2023-07-26 05:29:30,295 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35675
2023-07-26 05:29:30,295 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34119
2023-07-26 05:29:30,295 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:30,295 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:30,295 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:30,295 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:30,296 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n7uliybj
2023-07-26 05:29:30,296 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19cbabb4-8eeb-47a6-879f-905dba7ff8b7
2023-07-26 05:29:30,662 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ebfa9a4-c8c6-4ceb-a462-cfcb8f436185
2023-07-26 05:29:30,662 - distributed.worker - INFO - Starting Worker plugin PreImport-9c48e895-fa3e-4530-9e76-826a53d7fdb4
2023-07-26 05:29:30,662 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:30,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35675', status: init, memory: 0, processing: 0>
2023-07-26 05:29:30,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35675
2023-07-26 05:29:30,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51528
2023-07-26 05:29:30,697 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:30,697 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:30,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46479
2023-07-26 05:29:31,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46479
2023-07-26 05:29:31,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36655
2023-07-26 05:29:31,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,349 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46667
2023-07-26 05:29:31,349 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46667
2023-07-26 05:29:31,349 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43577
2023-07-26 05:29:31,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ji0eckl0
2023-07-26 05:29:31,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,349 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,349 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33799
2023-07-26 05:29:31,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u2zo9wxz
2023-07-26 05:29:31,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33799
2023-07-26 05:29:31,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66d6f297-e5c9-42a4-8001-d2069be22b05
2023-07-26 05:29:31,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33993
2023-07-26 05:29:31,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,350 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,350 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,350 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b147745-e053-4fd6-944c-c42953809443
2023-07-26 05:29:31,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2qomxn77
2023-07-26 05:29:31,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbefacbe-014b-4778-833b-ce96f66a394c
2023-07-26 05:29:31,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3e2ac87-be37-4804-af63-07aa9369bbdf
2023-07-26 05:29:31,510 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35903
2023-07-26 05:29:31,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35903
2023-07-26 05:29:31,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40309
2023-07-26 05:29:31,510 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,510 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,510 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wsxup8oi
2023-07-26 05:29:31,511 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45391
2023-07-26 05:29:31,511 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45391
2023-07-26 05:29:31,511 - distributed.worker - INFO - Starting Worker plugin PreImport-95a344ea-ed30-4f6b-9f3b-10e6fd382cb2
2023-07-26 05:29:31,511 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38703
2023-07-26 05:29:31,511 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,511 - distributed.worker - INFO - Starting Worker plugin RMMSetup-176a41ff-113f-47a6-9168-205b37b9c766
2023-07-26 05:29:31,511 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,512 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,512 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,512 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fsg6n5e_
2023-07-26 05:29:31,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04c8d9dd-823f-4c15-8808-e32ff5d1be9d
2023-07-26 05:29:31,512 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55fc204a-82f9-4e29-a274-19d4109b55e7
2023-07-26 05:29:31,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41061
2023-07-26 05:29:31,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41061
2023-07-26 05:29:31,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33353
2023-07-26 05:29:31,513 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,513 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,513 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-baws2o0y
2023-07-26 05:29:31,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f2606bc-6b05-454f-ae42-bae22469f242
2023-07-26 05:29:31,516 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40267
2023-07-26 05:29:31,516 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40267
2023-07-26 05:29:31,516 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43087
2023-07-26 05:29:31,516 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,516 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,516 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,516 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:31,516 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a9nthqn9
2023-07-26 05:29:31,517 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a51f640-f15a-44a1-b8a7-b500ce33f8ee
2023-07-26 05:29:31,550 - distributed.worker - INFO - Starting Worker plugin PreImport-3a37ddc7-db6e-4967-9e1f-ce0478f2e418
2023-07-26 05:29:31,550 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31aafa13-b28c-4ef1-b518-b02db4b8740a
2023-07-26 05:29:31,552 - distributed.worker - INFO - Starting Worker plugin PreImport-4e2ed269-70dc-4124-84c5-d7dec7ab1128
2023-07-26 05:29:31,552 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f34843a0-fd95-4c16-8471-a3c537a9c329
2023-07-26 05:29:31,560 - distributed.worker - INFO - Starting Worker plugin PreImport-b6d51234-de66-4fa7-a0e6-f432f988fed3
2023-07-26 05:29:31,561 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,576 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46479', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,576 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46479
2023-07-26 05:29:31,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58106
2023-07-26 05:29:31,577 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,577 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46667', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46667
2023-07-26 05:29:31,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58094
2023-07-26 05:29:31,585 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,585 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,591 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33799', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33799
2023-07-26 05:29:31,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58110
2023-07-26 05:29:31,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,593 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,664 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dc6b58c-81c1-49cb-82c5-12ba6c88c68d
2023-07-26 05:29:31,664 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,682 - distributed.worker - INFO - Starting Worker plugin PreImport-0b5c5d7c-1c84-42bc-9143-bc7e8211516c
2023-07-26 05:29:31,683 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1657deb-c760-4534-a9ea-8080ea387c2c
2023-07-26 05:29:31,683 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,684 - distributed.worker - INFO - Starting Worker plugin PreImport-1937b5d0-0052-442c-918d-018c66e119a2
2023-07-26 05:29:31,685 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,685 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f79583c0-589a-48ab-9528-4c889a3e4745
2023-07-26 05:29:31,685 - distributed.worker - INFO - Starting Worker plugin PreImport-865b3e04-177a-4a5f-a91d-a5871879842c
2023-07-26 05:29:31,685 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,689 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35903', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,690 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35903
2023-07-26 05:29:31,690 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58126
2023-07-26 05:29:31,690 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,690 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,707 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40267', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,707 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40267
2023-07-26 05:29:31,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58152
2023-07-26 05:29:31,708 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,708 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,708 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45391', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,709 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45391
2023-07-26 05:29:31,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58142
2023-07-26 05:29:31,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,709 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,715 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41061', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,715 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41061
2023-07-26 05:29:31,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58130
2023-07-26 05:29:31,716 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:31,716 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:31,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,785 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,785 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:31,795 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,795 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,795 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:31,803 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:31,804 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:31,806 - distributed.scheduler - INFO - Remove client Client-62260afb-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:31,807 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51508; closing.
2023-07-26 05:29:31,807 - distributed.scheduler - INFO - Remove client Client-62260afb-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:31,807 - distributed.scheduler - INFO - Close client connection: Client-62260afb-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:31,808 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35549'. Reason: nanny-close
2023-07-26 05:29:31,809 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,809 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38663'. Reason: nanny-close
2023-07-26 05:29:31,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,810 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41061. Reason: nanny-close
2023-07-26 05:29:31,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34441'. Reason: nanny-close
2023-07-26 05:29:31,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37113'. Reason: nanny-close
2023-07-26 05:29:31,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35903. Reason: nanny-close
2023-07-26 05:29:31,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46479. Reason: nanny-close
2023-07-26 05:29:31,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38981'. Reason: nanny-close
2023-07-26 05:29:31,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,813 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58130; closing.
2023-07-26 05:29:31,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45391. Reason: nanny-close
2023-07-26 05:29:31,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45307'. Reason: nanny-close
2023-07-26 05:29:31,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41061', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,813 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41061
2023-07-26 05:29:31,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46667. Reason: nanny-close
2023-07-26 05:29:31,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37577'. Reason: nanny-close
2023-07-26 05:29:31,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38889'. Reason: nanny-close
2023-07-26 05:29:31,814 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41061
2023-07-26 05:29:31,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33799. Reason: nanny-close
2023-07-26 05:29:31,814 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,814 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41061
2023-07-26 05:29:31,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58126; closing.
2023-07-26 05:29:31,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40267. Reason: nanny-close
2023-07-26 05:29:31,815 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,815 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,815 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35903', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,815 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35903
2023-07-26 05:29:31,815 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41061
2023-07-26 05:29:31,815 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,816 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35675. Reason: nanny-close
2023-07-26 05:29:31,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58106; closing.
2023-07-26 05:29:31,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,816 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46479', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,816 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46479
2023-07-26 05:29:31,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58142; closing.
2023-07-26 05:29:31,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45391', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,817 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41061
2023-07-26 05:29:31,817 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45391
2023-07-26 05:29:31,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,817 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58094; closing.
2023-07-26 05:29:31,818 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46667', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,818 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46667
2023-07-26 05:29:31,818 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:31,819 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58152; closing.
2023-07-26 05:29:31,819 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,819 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40267', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,819 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40267
2023-07-26 05:29:31,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51528; closing.
2023-07-26 05:29:31,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58110; closing.
2023-07-26 05:29:31,820 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:31,820 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35675', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,821 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35675
2023-07-26 05:29:31,821 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33799', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,821 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33799
2023-07-26 05:29:31,821 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:33,190 - distributed.scheduler - INFO - Receive client connection: Client-66dad571-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:33,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58168
2023-07-26 05:29:33,276 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:33,276 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:33,277 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:33,278 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:33,278 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-07-26 05:29:35,106 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:35,110 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44289 instead
  warnings.warn(
2023-07-26 05:29:35,115 - distributed.scheduler - INFO - State start
2023-07-26 05:29:35,136 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:35,136 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:35,137 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:35,137 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-26 05:29:35,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39501'
2023-07-26 05:29:35,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43431'
2023-07-26 05:29:35,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40395'
2023-07-26 05:29:35,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45615'
2023-07-26 05:29:35,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41659'
2023-07-26 05:29:35,345 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45923'
2023-07-26 05:29:35,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42769'
2023-07-26 05:29:35,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37535'
2023-07-26 05:29:36,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,953 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,998 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:37,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:37,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:37,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:37,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:37,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:37,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:37,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:37,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:37,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:37,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:37,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:37,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,254 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,258 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,263 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:40,409 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33931
2023-07-26 05:29:40,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33931
2023-07-26 05:29:40,410 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39209
2023-07-26 05:29:40,410 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,410 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,410 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,410 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,410 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zr2s4xef
2023-07-26 05:29:40,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dff18ea9-3e4a-45ae-9adf-69180cb6d40c
2023-07-26 05:29:40,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67f73e0f-206d-4dc9-8f5a-abbd6d447102
2023-07-26 05:29:40,687 - distributed.worker - INFO - Starting Worker plugin PreImport-59da8538-991c-422c-bcf2-c92fdea2ab43
2023-07-26 05:29:40,687 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,724 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45339
2023-07-26 05:29:41,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45339
2023-07-26 05:29:41,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41937
2023-07-26 05:29:41,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,406 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,406 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zo73sn12
2023-07-26 05:29:41,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-918bcdba-ef16-437a-bfaf-10851b4ef81e
2023-07-26 05:29:41,409 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36665
2023-07-26 05:29:41,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36665
2023-07-26 05:29:41,409 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35435
2023-07-26 05:29:41,409 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,409 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,409 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,409 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gh_5j4ak
2023-07-26 05:29:41,410 - distributed.worker - INFO - Starting Worker plugin PreImport-354de685-a3bc-45b4-ac7e-564d9ea2cfa9
2023-07-26 05:29:41,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0865ce94-5623-44cb-a457-7bca9ebfb58e
2023-07-26 05:29:41,446 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41913
2023-07-26 05:29:41,446 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41913
2023-07-26 05:29:41,446 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45535
2023-07-26 05:29:41,446 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,447 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,447 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,447 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,447 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r9nwd6c4
2023-07-26 05:29:41,447 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46655
2023-07-26 05:29:41,447 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46655
2023-07-26 05:29:41,447 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37969
2023-07-26 05:29:41,448 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8db36f6-424e-4763-9bd1-683a153da21a
2023-07-26 05:29:41,448 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,448 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,448 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,448 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fe_cj8_x
2023-07-26 05:29:41,449 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2718311f-07ef-4e7c-9e31-09f8420d6b86
2023-07-26 05:29:41,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38171
2023-07-26 05:29:41,512 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38171
2023-07-26 05:29:41,512 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38709
2023-07-26 05:29:41,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40115
2023-07-26 05:29:41,512 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,512 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40115
2023-07-26 05:29:41,512 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36283
2023-07-26 05:29:41,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40173
2023-07-26 05:29:41,513 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,513 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36283
2023-07-26 05:29:41,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,513 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46539
2023-07-26 05:29:41,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9vzikm_2
2023-07-26 05:29:41,513 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,513 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,513 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uz770oka
2023-07-26 05:29:41,513 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:41,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:41,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fnrdzp4h
2023-07-26 05:29:41,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54edc9e6-b6d0-49e6-bd24-68d4d3764417
2023-07-26 05:29:41,513 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63f7e945-80a4-42d2-af03-a7e554c36d52
2023-07-26 05:29:41,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5b15cd4-ff83-4885-8534-ef744051e6b0
2023-07-26 05:29:41,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d2f3140-aeb5-4db1-8ffc-3b4fc6caadcd
2023-07-26 05:29:41,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-090ec176-9bc5-4bab-b2d4-5dc2d9d27536
2023-07-26 05:29:41,584 - distributed.worker - INFO - Starting Worker plugin PreImport-83926aa8-adb1-48db-9d5b-175cbc12a513
2023-07-26 05:29:41,584 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aeed65f2-5991-4a2e-9b2e-442a747908b0
2023-07-26 05:29:41,592 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,609 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,609 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,623 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,624 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,631 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-547b1d52-79da-427d-ace7-d105c105507f
2023-07-26 05:29:41,631 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-791d2c17-7a08-4530-92d6-64358aef1398
2023-07-26 05:29:41,631 - distributed.worker - INFO - Starting Worker plugin PreImport-791e0379-e657-40c1-938b-ea1a7ada7f47
2023-07-26 05:29:41,631 - distributed.worker - INFO - Starting Worker plugin PreImport-fea83324-b69e-41c8-92d9-ac4517de19f1
2023-07-26 05:29:41,631 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,631 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,650 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f166495e-bd55-431e-84c1-2c51e62195bf
2023-07-26 05:29:41,650 - distributed.worker - INFO - Starting Worker plugin PreImport-8f5426e7-4a79-4d42-b7d8-7a3fd899b507
2023-07-26 05:29:41,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eaeba898-3972-4802-9a79-13d13c7d7f58
2023-07-26 05:29:41,651 - distributed.worker - INFO - Starting Worker plugin PreImport-53b9e391-94c9-436e-860d-45b6e176bb0d
2023-07-26 05:29:41,651 - distributed.worker - INFO - Starting Worker plugin PreImport-25233fa4-0e2d-4e15-b198-584eabac81e2
2023-07-26 05:29:41,651 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,651 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,651 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,658 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,662 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,663 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,681 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,681 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,682 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,683 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,684 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:49,213 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,214 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,214 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,214 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,214 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,214 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,215 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,215 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,232 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,312 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,313 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:49,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41659'. Reason: nanny-close
2023-07-26 05:29:49,321 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,321 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39501'. Reason: nanny-close
2023-07-26 05:29:49,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,322 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36283. Reason: nanny-close
2023-07-26 05:29:49,322 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43431'. Reason: nanny-close
2023-07-26 05:29:49,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36665. Reason: nanny-close
2023-07-26 05:29:49,323 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40395'. Reason: nanny-close
2023-07-26 05:29:49,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45339. Reason: nanny-close
2023-07-26 05:29:49,324 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45615'. Reason: nanny-close
2023-07-26 05:29:49,324 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33931. Reason: nanny-close
2023-07-26 05:29:49,325 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42769'. Reason: nanny-close
2023-07-26 05:29:49,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40115. Reason: nanny-close
2023-07-26 05:29:49,325 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37535'. Reason: nanny-close
2023-07-26 05:29:49,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,326 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38171. Reason: nanny-close
2023-07-26 05:29:49,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45923'. Reason: nanny-close
2023-07-26 05:29:49,326 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,326 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,327 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,327 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,327 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,327 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46655. Reason: nanny-close
2023-07-26 05:29:49,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,327 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,327 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,328 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41913. Reason: nanny-close
2023-07-26 05:29:49,328 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,329 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,329 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,329 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,330 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,330 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,331 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,331 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-07-26 05:29:52,606 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:52,610 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:52,613 - distributed.scheduler - INFO - State start
2023-07-26 05:29:52,631 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:52,632 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:52,632 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:52,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44759'
2023-07-26 05:29:53,442 - distributed.scheduler - INFO - Receive client connection: Client-71acf186-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:53,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56094
2023-07-26 05:29:53,945 - distributed.scheduler - INFO - Receive client connection: Client-71888ac6-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:53,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56112
2023-07-26 05:29:54,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:54,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:29:54,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:55,250 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40661', status: init, memory: 0, processing: 0>
2023-07-26 05:29:55,251 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40661
2023-07-26 05:29:55,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56140
2023-07-26 05:29:55,274 - distributed.scheduler - INFO - Remove client Client-71888ac6-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:55,274 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56112; closing.
2023-07-26 05:29:55,275 - distributed.scheduler - INFO - Remove client Client-71888ac6-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:55,275 - distributed.scheduler - INFO - Close client connection: Client-71888ac6-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:55,276 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44759'. Reason: nanny-close
2023-07-26 05:29:55,284 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:55,285 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:55,287 - distributed.scheduler - INFO - Remove client Client-71acf186-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:55,287 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56094; closing.
2023-07-26 05:29:55,288 - distributed.scheduler - INFO - Remove client Client-71acf186-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:55,288 - distributed.scheduler - INFO - Close client connection: Client-71acf186-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:55,292 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56140; closing.
2023-07-26 05:29:55,292 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40661', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:55,292 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40661
2023-07-26 05:29:55,292 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:55,334 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37131
2023-07-26 05:29:55,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37131
2023-07-26 05:29:55,334 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-07-26 05:29:55,334 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:55,334 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:55,335 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:55,335 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:29:55,335 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w826mmea
2023-07-26 05:29:55,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc588dc0-9f0d-4ef2-bc93-a79307e1c670
2023-07-26 05:29:55,335 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-758ea94a-2e45-4a03-9f47-861f2eb12753
2023-07-26 05:29:55,335 - distributed.worker - INFO - Starting Worker plugin PreImport-1e87c52a-9c15-4e18-a897-2254ec67d257
2023-07-26 05:29:55,336 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:55,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37131', status: init, memory: 0, processing: 0>
2023-07-26 05:29:55,361 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37131
2023-07-26 05:29:55,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56154
2023-07-26 05:29:55,362 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:55,362 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:55,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:55,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:55,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37131. Reason: nanny-close
2023-07-26 05:29:55,402 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56154; closing.
2023-07-26 05:29:55,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:55,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37131', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:55,403 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37131
2023-07-26 05:29:55,403 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:55,404 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:56,282 - distributed.scheduler - INFO - Receive client connection: Client-749e77ad-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:56,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56156
2023-07-26 05:29:56,393 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:56,393 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:56,393 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:56,394 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:56,395 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-07-26 05:29:59,902 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:59,906 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44587 instead
  warnings.warn(
2023-07-26 05:29:59,909 - distributed.scheduler - INFO - State start
2023-07-26 05:29:59,928 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:59,928 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:59,929 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:59,929 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-26 05:30:00,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44095'
2023-07-26 05:30:00,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44095'. Reason: nanny-close
2023-07-26 05:30:01,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:01,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:30:01,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:02,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34793
2023-07-26 05:30:02,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34793
2023-07-26 05:30:02,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34239
2023-07-26 05:30:02,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:02,750 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:02,750 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:02,751 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:30:02,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ydltb3xr
2023-07-26 05:30:02,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2f95a2f-a43d-446e-a908-c7d57d172ca8
2023-07-26 05:30:02,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6313dae9-1911-407a-aba1-f67c315bc3a3
2023-07-26 05:30:02,751 - distributed.worker - INFO - Starting Worker plugin PreImport-2fc46700-67b6-4eab-aad3-43209763d1ed
2023-07-26 05:30:02,752 - distributed.worker - INFO - -------------------------------------------------
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-26 05:30:31,906 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:31,910 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:30:31,913 - distributed.scheduler - INFO - State start
2023-07-26 05:30:31,932 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:31,933 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:30:31,933 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:30:37,218 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:30:37,218 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:30:37,219 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:30:37,219 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:30:37,219 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-07-26 05:30:39,107 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:39,111 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:30:39,115 - distributed.scheduler - INFO - State start
2023-07-26 05:30:39,135 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:39,136 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-26 05:30:39,136 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:30:39,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44809'
2023-07-26 05:30:40,200 - distributed.scheduler - INFO - Receive client connection: Client-8d3177b7-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:40,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52444
2023-07-26 05:30:40,574 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:40,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:40,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:41,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46203
2023-07-26 05:30:41,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46203
2023-07-26 05:30:41,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37387
2023-07-26 05:30:41,513 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:30:41,513 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:41,513 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:41,513 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:30:41,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s1i3swzi
2023-07-26 05:30:41,513 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80391e2b-e62d-4018-b10a-f2dcafb1bd70
2023-07-26 05:30:41,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55719c95-0c22-4e51-ad8a-89b4376d4eff
2023-07-26 05:30:41,514 - distributed.worker - INFO - Starting Worker plugin PreImport-2311c438-1927-417a-95fd-3e880298d472
2023-07-26 05:30:41,514 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:41,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46203', status: init, memory: 0, processing: 0>
2023-07-26 05:30:41,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46203
2023-07-26 05:30:41,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41186
2023-07-26 05:30:41,538 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:30:41,538 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:41,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:30:41,577 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:41,579 - distributed.scheduler - INFO - Remove client Client-8d3177b7-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:41,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52444; closing.
2023-07-26 05:30:41,580 - distributed.scheduler - INFO - Remove client Client-8d3177b7-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:41,580 - distributed.scheduler - INFO - Close client connection: Client-8d3177b7-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:41,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44809'. Reason: nanny-close
2023-07-26 05:30:41,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:41,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46203. Reason: nanny-close
2023-07-26 05:30:41,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:30:41,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41186; closing.
2023-07-26 05:30:41,584 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46203', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:41,584 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46203
2023-07-26 05:30:41,584 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:30:41,585 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:42,497 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:30:42,497 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:30:42,497 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:30:42,498 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-26 05:30:42,498 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-07-26 05:30:44,378 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:44,383 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:30:44,386 - distributed.scheduler - INFO - State start
2023-07-26 05:30:44,425 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:44,426 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:30:44,427 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:30:45,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35913'
2023-07-26 05:30:45,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46859'
2023-07-26 05:30:45,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42539'
2023-07-26 05:30:45,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46429'
2023-07-26 05:30:45,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40545'
2023-07-26 05:30:45,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43409'
2023-07-26 05:30:45,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46629'
2023-07-26 05:30:45,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40697'
2023-07-26 05:30:46,205 - distributed.scheduler - INFO - Receive client connection: Client-90604b0b-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:46,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55782
2023-07-26 05:30:47,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,457 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:47,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:47,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,472 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:47,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:50,060 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38073
2023-07-26 05:30:50,061 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38073
2023-07-26 05:30:50,061 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44515
2023-07-26 05:30:50,061 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,061 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,061 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,061 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,061 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7acwudya
2023-07-26 05:30:50,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24a5fabc-5917-4ffd-bb35-34c38eff1f9f
2023-07-26 05:30:50,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40875
2023-07-26 05:30:50,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40875
2023-07-26 05:30:50,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33501
2023-07-26 05:30:50,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,062 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,063 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4klk14cr
2023-07-26 05:30:50,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-034dc7b3-736a-48ef-bfc1-1ee671a3f1a5
2023-07-26 05:30:50,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-399d101f-ba3e-43ea-b3a6-429bcfacb5ea
2023-07-26 05:30:50,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35307
2023-07-26 05:30:50,064 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35307
2023-07-26 05:30:50,064 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46151
2023-07-26 05:30:50,064 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,065 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,065 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,065 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,065 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jgiuly83
2023-07-26 05:30:50,066 - distributed.worker - INFO - Starting Worker plugin PreImport-f97e444e-76b0-4af6-affe-311af8c28ffd
2023-07-26 05:30:50,066 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2bf62740-7b3a-4f89-b09d-80b593cd7a3c
2023-07-26 05:30:50,066 - distributed.worker - INFO - Starting Worker plugin RMMSetup-828e0505-fb55-41b7-a5b5-c66a028aaaf2
2023-07-26 05:30:50,219 - distributed.worker - INFO - Starting Worker plugin PreImport-8dad1a26-aae8-4634-9797-60d4790f7374
2023-07-26 05:30:50,219 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,220 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6507d51-34ec-4271-8d05-64834d510b46
2023-07-26 05:30:50,220 - distributed.worker - INFO - Starting Worker plugin PreImport-1c2233cc-e8a6-4901-a3fb-d3687428b784
2023-07-26 05:30:50,220 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,221 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,250 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38073', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,252 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38073
2023-07-26 05:30:50,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55796
2023-07-26 05:30:50,252 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,252 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40875', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,253 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40875
2023-07-26 05:30:50,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55794
2023-07-26 05:30:50,254 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,254 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,254 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35307', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,255 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35307
2023-07-26 05:30:50,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55812
2023-07-26 05:30:50,256 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,256 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42257
2023-07-26 05:30:50,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42257
2023-07-26 05:30:50,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35591
2023-07-26 05:30:50,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,258 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,258 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,258 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sr9bal1r
2023-07-26 05:30:50,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-705104c1-6a5d-4850-9180-8b5c0113eae1
2023-07-26 05:30:50,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40683
2023-07-26 05:30:50,260 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40683
2023-07-26 05:30:50,260 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35653
2023-07-26 05:30:50,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,261 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41411
2023-07-26 05:30:50,261 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,261 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41411
2023-07-26 05:30:50,261 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yx9x95lg
2023-07-26 05:30:50,261 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41145
2023-07-26 05:30:50,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,261 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,261 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,261 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nymyt6x3
2023-07-26 05:30:50,261 - distributed.worker - INFO - Starting Worker plugin PreImport-5a8d969e-0ea7-46b3-9e23-396cca4af53c
2023-07-26 05:30:50,261 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9521f08-ba45-4e4c-8eb9-077598f671f1
2023-07-26 05:30:50,261 - distributed.worker - INFO - Starting Worker plugin RMMSetup-202faaed-805c-41f4-a6de-b703d4c89234
2023-07-26 05:30:50,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-becf6d8a-8107-4740-afba-06ddb660a7c8
2023-07-26 05:30:50,262 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38975
2023-07-26 05:30:50,262 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38975
2023-07-26 05:30:50,262 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46065
2023-07-26 05:30:50,262 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36257
2023-07-26 05:30:50,262 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,262 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36257
2023-07-26 05:30:50,262 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,263 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45203
2023-07-26 05:30:50,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,263 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,263 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,263 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-79yk5276
2023-07-26 05:30:50,263 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:50,263 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:30:50,263 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c9d2_o1c
2023-07-26 05:30:50,263 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc160fb4-3648-47cd-a66b-bd3add68714c
2023-07-26 05:30:50,264 - distributed.worker - INFO - Starting Worker plugin PreImport-05667d3a-3d69-4e27-bd2a-7e18f006aea7
2023-07-26 05:30:50,264 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-daf4ac36-dfbe-4b83-809c-99ee6f58b9af
2023-07-26 05:30:50,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-772fcd85-2ab1-4bb0-a6ab-4a180dbe8c86
2023-07-26 05:30:50,385 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,385 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69b33b38-8b54-40b2-bfc2-3d9adc63ee61
2023-07-26 05:30:50,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c1b9118-0c22-4673-942c-9d7a05558f13
2023-07-26 05:30:50,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7df6ff19-621b-4ca8-80e3-ebae3560abbe
2023-07-26 05:30:50,393 - distributed.worker - INFO - Starting Worker plugin PreImport-02c11d1c-522c-4ed4-9345-42f5af4e03fb
2023-07-26 05:30:50,393 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,396 - distributed.worker - INFO - Starting Worker plugin PreImport-5debbf4f-557d-41cd-ad5f-24136afaf4e4
2023-07-26 05:30:50,396 - distributed.worker - INFO - Starting Worker plugin PreImport-60a75e1d-5b5a-4256-9463-dbf94f829c45
2023-07-26 05:30:50,396 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,396 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,411 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38975', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,412 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38975
2023-07-26 05:30:50,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55816
2023-07-26 05:30:50,412 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,413 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36257', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,416 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36257
2023-07-26 05:30:50,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55828
2023-07-26 05:30:50,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,417 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,417 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40683', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,418 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40683
2023-07-26 05:30:50,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55824
2023-07-26 05:30:50,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,419 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,424 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41411', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,424 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41411
2023-07-26 05:30:50,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55836
2023-07-26 05:30:50,425 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,425 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,426 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42257', status: init, memory: 0, processing: 0>
2023-07-26 05:30:50,427 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42257
2023-07-26 05:30:50,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55834
2023-07-26 05:30:50,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,428 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:50,428 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:50,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:50,482 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,483 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,483 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,483 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,483 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,483 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,484 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,484 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:30:50,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,497 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,497 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,497 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:50,500 - distributed.scheduler - INFO - Remove client Client-90604b0b-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:50,501 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55782; closing.
2023-07-26 05:30:50,501 - distributed.scheduler - INFO - Remove client Client-90604b0b-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:50,501 - distributed.scheduler - INFO - Close client connection: Client-90604b0b-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:50,502 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46859'. Reason: nanny-close
2023-07-26 05:30:50,503 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40545'. Reason: nanny-close
2023-07-26 05:30:50,504 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,504 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40875. Reason: nanny-close
2023-07-26 05:30:50,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35913'. Reason: nanny-close
2023-07-26 05:30:50,504 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,505 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38073. Reason: nanny-close
2023-07-26 05:30:50,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42539'. Reason: nanny-close
2023-07-26 05:30:50,505 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,505 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40683. Reason: nanny-close
2023-07-26 05:30:50,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46429'. Reason: nanny-close
2023-07-26 05:30:50,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,506 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55794; closing.
2023-07-26 05:30:50,506 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,506 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41411. Reason: nanny-close
2023-07-26 05:30:50,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43409'. Reason: nanny-close
2023-07-26 05:30:50,506 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40875', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,506 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40875
2023-07-26 05:30:50,506 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36257. Reason: nanny-close
2023-07-26 05:30:50,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46629'. Reason: nanny-close
2023-07-26 05:30:50,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35307. Reason: nanny-close
2023-07-26 05:30:50,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40697'. Reason: nanny-close
2023-07-26 05:30:50,507 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,508 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:50,508 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38975. Reason: nanny-close
2023-07-26 05:30:50,508 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40875
2023-07-26 05:30:50,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,508 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55796; closing.
2023-07-26 05:30:50,509 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40875
2023-07-26 05:30:50,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,509 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,509 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40875
2023-07-26 05:30:50,510 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42257. Reason: nanny-close
2023-07-26 05:30:50,510 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38073', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,510 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38073
2023-07-26 05:30:50,510 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,510 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40875
2023-07-26 05:30:50,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,510 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55824; closing.
2023-07-26 05:30:50,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,511 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40683', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,511 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40683
2023-07-26 05:30:50,511 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,512 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55836; closing.
2023-07-26 05:30:50,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:50,512 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55828; closing.
2023-07-26 05:30:50,512 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,513 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41411', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,513 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41411
2023-07-26 05:30:50,513 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:50,513 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36257', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,513 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36257
2023-07-26 05:30:50,514 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55812; closing.
2023-07-26 05:30:50,514 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55816; closing.
2023-07-26 05:30:50,515 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35307', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,515 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35307
2023-07-26 05:30:50,516 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38975', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,516 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38975
2023-07-26 05:30:50,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55834; closing.
2023-07-26 05:30:50,517 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42257', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:50,517 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:30:50,517 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:30:50,517 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55834>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-26 05:30:51,819 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:30:51,820 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:30:51,820 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:30:51,821 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:30:51,821 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-07-26 05:30:53,587 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:53,592 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:30:53,595 - distributed.scheduler - INFO - State start
2023-07-26 05:30:53,614 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:53,615 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:30:53,615 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:30:53,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35075'
2023-07-26 05:30:55,112 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:55,112 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:55,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:30:55,962 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34603
2023-07-26 05:30:55,962 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34603
2023-07-26 05:30:55,962 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44931
2023-07-26 05:30:55,962 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:30:55,962 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:55,962 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:30:55,962 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:30:55,962 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kgwbxs4r
2023-07-26 05:30:55,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d90f008f-d8e0-4e1d-b369-6ee4cf1c0888
2023-07-26 05:30:56,051 - distributed.worker - INFO - Starting Worker plugin PreImport-0bae1cb4-7461-4d8f-93e6-951293408704
2023-07-26 05:30:56,051 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2c3acca-5d09-49a6-94eb-0209fd28655c
2023-07-26 05:30:56,051 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:56,070 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34603', status: init, memory: 0, processing: 0>
2023-07-26 05:30:56,082 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34603
2023-07-26 05:30:56,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37702
2023-07-26 05:30:56,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:56,083 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:56,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:56,196 - distributed.scheduler - INFO - Receive client connection: Client-95e3ec77-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:56,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37714
2023-07-26 05:30:56,202 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:30:56,205 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:56,206 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:56,208 - distributed.scheduler - INFO - Remove client Client-95e3ec77-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:56,209 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37714; closing.
2023-07-26 05:30:56,209 - distributed.scheduler - INFO - Remove client Client-95e3ec77-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:56,209 - distributed.scheduler - INFO - Close client connection: Client-95e3ec77-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:56,210 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35075'. Reason: nanny-close
2023-07-26 05:30:56,210 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:56,211 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34603. Reason: nanny-close
2023-07-26 05:30:56,213 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:56,213 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37702; closing.
2023-07-26 05:30:56,213 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34603', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:56,213 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34603
2023-07-26 05:30:56,213 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:30:56,214 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:57,076 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:30:57,076 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:30:57,077 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:30:57,077 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:30:57,078 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-07-26 05:30:58,831 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:58,835 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:30:58,838 - distributed.scheduler - INFO - State start
2023-07-26 05:30:58,857 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:30:58,857 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:30:58,858 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:30:58,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34523'
2023-07-26 05:30:59,228 - distributed.scheduler - INFO - Receive client connection: Client-99058090-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:59,241 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37824
2023-07-26 05:31:00,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:00,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:00,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:31:01,250 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39647
2023-07-26 05:31:01,250 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39647
2023-07-26 05:31:01,250 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37845
2023-07-26 05:31:01,250 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:31:01,250 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:31:01,250 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:31:01,250 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:31:01,250 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jh71rel4
2023-07-26 05:31:01,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc6cb7c9-c721-476d-8114-8705e676fce9
2023-07-26 05:31:01,345 - distributed.worker - INFO - Starting Worker plugin PreImport-dc1a3fc5-bda1-453f-a2b3-9b98111210d3
2023-07-26 05:31:01,345 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f8438b7-ccbb-4ba9-aabd-e9cce51bfecf
2023-07-26 05:31:01,346 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:31:01,370 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39647', status: init, memory: 0, processing: 0>
2023-07-26 05:31:01,371 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39647
2023-07-26 05:31:01,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41846
2023-07-26 05:31:01,371 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:31:01,372 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:31:01,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:31:01,380 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-07-26 05:31:01,385 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:31:01,388 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:31:01,389 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:31:01,391 - distributed.scheduler - INFO - Remove client Client-99058090-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:31:01,391 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37824; closing.
2023-07-26 05:31:01,391 - distributed.scheduler - INFO - Remove client Client-99058090-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:31:01,392 - distributed.scheduler - INFO - Close client connection: Client-99058090-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:31:01,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34523'. Reason: nanny-close
2023-07-26 05:31:01,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:31:01,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39647. Reason: nanny-close
2023-07-26 05:31:01,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41846; closing.
2023-07-26 05:31:01,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:31:01,396 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39647', status: closing, memory: 0, processing: 0>
2023-07-26 05:31:01,396 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39647
2023-07-26 05:31:01,396 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:31:01,397 - distributed.nanny - INFO - Worker closed
2023-07-26 05:31:02,359 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:31:02,361 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:31:02,361 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:31:02,362 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:31:02,362 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:31:10,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,936 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,936 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:10,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:10,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:31:19,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:19,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:19,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45449 instead
  warnings.warn(
2023-07-26 05:31:28,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:28,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:28,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38735 instead
  warnings.warn(
2023-07-26 05:31:39,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,363 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:39,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:39,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:43,274 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1262, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1403' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:31:50,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:50,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:50,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:50,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:51,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:51,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:51,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:51,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:51,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:51,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:51,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:51,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:51,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:51,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:31:51,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:31:51,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
