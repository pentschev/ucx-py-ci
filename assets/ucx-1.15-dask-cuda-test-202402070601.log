============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.4
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-07 06:39:11,902 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:11,907 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44597 instead
  warnings.warn(
2024-02-07 06:39:11,910 - distributed.scheduler - INFO - State start
2024-02-07 06:39:11,931 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:11,932 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-07 06:39:11,932 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44597/status
2024-02-07 06:39:11,933 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:12,207 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37743'
2024-02-07 06:39:12,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38845'
2024-02-07 06:39:12,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43421'
2024-02-07 06:39:12,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45831'
2024-02-07 06:39:13,427 - distributed.scheduler - INFO - Receive client connection: Client-9996cf7d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:13,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43478
2024-02-07 06:39:13,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:13,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:13,854 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:13,855 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33285
2024-02-07 06:39:13,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33285
2024-02-07 06:39:13,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43841
2024-02-07 06:39:13,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-07 06:39:13,856 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,856 - distributed.worker - INFO -               Threads:                          4
2024-02-07 06:39:13,856 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-07 06:39:13,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-b2dreyp9
2024-02-07 06:39:13,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-813444af-9f9d-48c2-ad56-135bf03665df
2024-02-07 06:39:13,856 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-573d7bdf-ea4d-4886-a796-62899ed31e26
2024-02-07 06:39:13,856 - distributed.worker - INFO - Starting Worker plugin PreImport-c0256289-b6f1-400b-9c9e-54c226ec1a0a
2024-02-07 06:39:13,857 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:13,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:13,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:13,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:13,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:13,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:13,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:13,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:13,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:13,893 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43041
2024-02-07 06:39:13,893 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40991
2024-02-07 06:39:13,893 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43335
2024-02-07 06:39:13,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43335
2024-02-07 06:39:13,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40991
2024-02-07 06:39:13,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43041
2024-02-07 06:39:13,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43969
2024-02-07 06:39:13,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38733
2024-02-07 06:39:13,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34365
2024-02-07 06:39:13,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-07 06:39:13,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-07 06:39:13,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-07 06:39:13,893 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,893 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,893 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,893 - distributed.worker - INFO -               Threads:                          4
2024-02-07 06:39:13,893 - distributed.worker - INFO -               Threads:                          4
2024-02-07 06:39:13,893 - distributed.worker - INFO -               Threads:                          4
2024-02-07 06:39:13,893 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-07 06:39:13,893 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-07 06:39:13,893 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-07 06:39:13,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-jhw0cteg
2024-02-07 06:39:13,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-cj6ly7kn
2024-02-07 06:39:13,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-2xai97nt
2024-02-07 06:39:13,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d62a7eda-2ef6-4010-93b5-7c9b8b47f942
2024-02-07 06:39:13,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f39da5cc-318c-4e5f-8b54-6cfea23b6970
2024-02-07 06:39:13,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94e79481-3255-48bc-babf-99d4dbf9bc67
2024-02-07 06:39:13,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b692a6eb-4765-4683-afcb-d2f22df141a7
2024-02-07 06:39:13,894 - distributed.worker - INFO - Starting Worker plugin PreImport-9158c055-e246-4f5d-a849-eb4cbfa4eb49
2024-02-07 06:39:13,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54f684e9-6ce4-41be-9710-44aabeca8b55
2024-02-07 06:39:13,894 - distributed.worker - INFO - Starting Worker plugin PreImport-772f6b6e-fd02-412e-b498-3061185b45b1
2024-02-07 06:39:13,894 - distributed.worker - INFO - Starting Worker plugin PreImport-bf05c84b-eca1-4940-b90b-06050e4aeb10
2024-02-07 06:39:13,894 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,894 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99e3ea3d-f421-4f4b-8690-a0d058656faf
2024-02-07 06:39:13,894 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,895 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33285', status: init, memory: 0, processing: 0>
2024-02-07 06:39:13,927 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33285
2024-02-07 06:39:13,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43508
2024-02-07 06:39:13,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:13,929 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-07 06:39:13,929 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:13,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-07 06:39:14,006 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40991', status: init, memory: 0, processing: 0>
2024-02-07 06:39:14,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40991
2024-02-07 06:39:14,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43522
2024-02-07 06:39:14,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43335', status: init, memory: 0, processing: 0>
2024-02-07 06:39:14,007 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:14,008 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43335
2024-02-07 06:39:14,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43538
2024-02-07 06:39:14,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-07 06:39:14,008 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:14,009 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43041', status: init, memory: 0, processing: 0>
2024-02-07 06:39:14,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:14,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-07 06:39:14,009 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43041
2024-02-07 06:39:14,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43554
2024-02-07 06:39:14,010 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-07 06:39:14,010 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:14,011 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:14,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-07 06:39:14,011 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-07 06:39:14,011 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:14,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-07 06:39:14,057 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-07 06:39:14,057 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-07 06:39:14,057 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-07 06:39:14,057 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-07 06:39:14,062 - distributed.scheduler - INFO - Remove client Client-9996cf7d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:14,062 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43478; closing.
2024-02-07 06:39:14,063 - distributed.scheduler - INFO - Remove client Client-9996cf7d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:14,063 - distributed.scheduler - INFO - Close client connection: Client-9996cf7d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:14,064 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37743'. Reason: nanny-close
2024-02-07 06:39:14,064 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:14,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38845'. Reason: nanny-close
2024-02-07 06:39:14,065 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:14,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43421'. Reason: nanny-close
2024-02-07 06:39:14,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33285. Reason: nanny-close
2024-02-07 06:39:14,066 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:14,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45831'. Reason: nanny-close
2024-02-07 06:39:14,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43041. Reason: nanny-close
2024-02-07 06:39:14,066 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:14,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40991. Reason: nanny-close
2024-02-07 06:39:14,067 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43335. Reason: nanny-close
2024-02-07 06:39:14,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-07 06:39:14,068 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43554; closing.
2024-02-07 06:39:14,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-07 06:39:14,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-07 06:39:14,068 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43041', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287954.0686378')
2024-02-07 06:39:14,069 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43508; closing.
2024-02-07 06:39:14,069 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-07 06:39:14,069 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43538; closing.
2024-02-07 06:39:14,069 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:14,069 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:14,069 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287954.0698607')
2024-02-07 06:39:14,070 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:14,070 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43522; closing.
2024-02-07 06:39:14,070 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287954.0706887')
2024-02-07 06:39:14,070 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:14,071 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287954.0710745')
2024-02-07 06:39:14,071 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:14,071 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:43508>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-07 06:39:14,072 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:43538>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:43538>: Stream is closed
2024-02-07 06:39:14,729 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:14,729 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:14,730 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:14,731 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-07 06:39:14,731 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-07 06:39:16,752 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:16,757 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39809 instead
  warnings.warn(
2024-02-07 06:39:16,760 - distributed.scheduler - INFO - State start
2024-02-07 06:39:16,781 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:16,782 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:16,783 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39809/status
2024-02-07 06:39:16,783 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:16,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40269'
2024-02-07 06:39:16,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37897'
2024-02-07 06:39:16,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44585'
2024-02-07 06:39:16,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38979'
2024-02-07 06:39:16,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42501'
2024-02-07 06:39:16,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43135'
2024-02-07 06:39:16,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41555'
2024-02-07 06:39:16,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43539'
2024-02-07 06:39:17,117 - distributed.scheduler - INFO - Receive client connection: Client-9c6569d2-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:17,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33218
2024-02-07 06:39:18,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42831
2024-02-07 06:39:18,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42831
2024-02-07 06:39:18,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37075
2024-02-07 06:39:18,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,830 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,830 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,830 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,830 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8o4kxjw1
2024-02-07 06:39:18,830 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19756188-8854-4891-b041-388d5f387487
2024-02-07 06:39:18,830 - distributed.worker - INFO - Starting Worker plugin PreImport-0f75dfba-3375-4281-b870-7c6ed064d12e
2024-02-07 06:39:18,830 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2196ef51-3c3d-44fa-9b5f-184a24cb44dc
2024-02-07 06:39:18,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,835 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,836 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42245
2024-02-07 06:39:18,836 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42245
2024-02-07 06:39:18,836 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35725
2024-02-07 06:39:18,836 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,836 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,836 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,836 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,836 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t23lx13w
2024-02-07 06:39:18,837 - distributed.worker - INFO - Starting Worker plugin RMMSetup-336bdac1-a172-44f1-95c1-17405b8a5fff
2024-02-07 06:39:18,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41007
2024-02-07 06:39:18,840 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41007
2024-02-07 06:39:18,840 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35241
2024-02-07 06:39:18,840 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,840 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,840 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,840 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,840 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0jevdl6r
2024-02-07 06:39:18,840 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b599e207-7dcd-4471-b32f-ebcb8a6cbb88
2024-02-07 06:39:18,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,842 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,842 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,845 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,846 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42379
2024-02-07 06:39:18,846 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42379
2024-02-07 06:39:18,846 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,846 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43163
2024-02-07 06:39:18,846 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,846 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,846 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,846 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,846 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqjgxg4m
2024-02-07 06:39:18,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a031e917-2ec9-4dc8-af0f-c07c6ea28a0d
2024-02-07 06:39:18,847 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34451
2024-02-07 06:39:18,847 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34451
2024-02-07 06:39:18,847 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41355
2024-02-07 06:39:18,847 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,847 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,847 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,847 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,847 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1evpxouf
2024-02-07 06:39:18,847 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e329cd3-a70c-4fb8-b90c-93640b4dcba6
2024-02-07 06:39:18,847 - distributed.worker - INFO - Starting Worker plugin PreImport-74bb08c3-c23f-44b7-a25c-b311a9859d89
2024-02-07 06:39:18,848 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df1b1414-ce19-44c1-adf3-20a5a2ea929d
2024-02-07 06:39:18,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44761
2024-02-07 06:39:18,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44761
2024-02-07 06:39:18,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42151
2024-02-07 06:39:18,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,850 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,850 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,850 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,851 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cc5zcdij
2024-02-07 06:39:18,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1468c86-2476-43cf-8f1f-ba9f28634b51
2024-02-07 06:39:18,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,921 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33145
2024-02-07 06:39:18,921 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33145
2024-02-07 06:39:18,921 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41585
2024-02-07 06:39:18,921 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,921 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,921 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,921 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6_e8uba0
2024-02-07 06:39:18,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bcb82789-9b0b-4d69-be82-76d31a83c8ec
2024-02-07 06:39:18,925 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:18,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:18,929 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:18,930 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37363
2024-02-07 06:39:18,930 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37363
2024-02-07 06:39:18,930 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32915
2024-02-07 06:39:18,930 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:18,930 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:18,930 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:18,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:18,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mlfr90t4
2024-02-07 06:39:18,930 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34bad99c-e436-40b8-9da0-442d54d786eb
2024-02-07 06:39:18,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab373a31-8b92-4379-b6cf-03d4896fad3e
2024-02-07 06:39:20,803 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,828 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42831', status: init, memory: 0, processing: 0>
2024-02-07 06:39:20,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42831
2024-02-07 06:39:20,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43462
2024-02-07 06:39:20,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:20,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:20,833 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:20,948 - distributed.worker - INFO - Starting Worker plugin PreImport-09271cc0-0d88-485b-8e42-1335450d8739
2024-02-07 06:39:20,949 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aabfb7fd-272a-4d64-a998-59f07695edd7
2024-02-07 06:39:20,950 - distributed.worker - INFO - Starting Worker plugin PreImport-bc6e38ae-b960-4b63-bc56-14060909453b
2024-02-07 06:39:20,951 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9982eefa-49da-4c13-91d5-5db07071a976
2024-02-07 06:39:20,952 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,952 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,956 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,962 - distributed.worker - INFO - Starting Worker plugin PreImport-b4c6b7df-11a7-4dd7-977d-b50a71af4c2a
2024-02-07 06:39:20,962 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2c6f4f3-0e26-4fbf-b62a-6e1c7eda2bd8
2024-02-07 06:39:20,963 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,976 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6af49cd2-9994-4520-8559-120f286dd216
2024-02-07 06:39:20,977 - distributed.worker - INFO - Starting Worker plugin PreImport-cc07f5d8-7c35-4cc7-a637-083da2ffebe8
2024-02-07 06:39:20,978 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,979 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34451', status: init, memory: 0, processing: 0>
2024-02-07 06:39:20,980 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34451
2024-02-07 06:39:20,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43488
2024-02-07 06:39:20,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:20,981 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:20,981 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:20,985 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42245', status: init, memory: 0, processing: 0>
2024-02-07 06:39:20,985 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42245
2024-02-07 06:39:20,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43496
2024-02-07 06:39:20,986 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41007', status: init, memory: 0, processing: 0>
2024-02-07 06:39:20,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:20,987 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41007
2024-02-07 06:39:20,987 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43466
2024-02-07 06:39:20,987 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:20,987 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,988 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42379', status: init, memory: 0, processing: 0>
2024-02-07 06:39:20,988 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42379
2024-02-07 06:39:20,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43474
2024-02-07 06:39:20,989 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:20,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:20,990 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:20,990 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:20,991 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:20,991 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,992 - distributed.worker - INFO - Starting Worker plugin PreImport-e5dc32df-68c0-4f42-bb49-1fb0f23a36fc
2024-02-07 06:39:20,992 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9de3c5e-cca0-4df1-b425-c18f25734866
2024-02-07 06:39:20,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:20,993 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:20,993 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:20,999 - distributed.worker - INFO - Starting Worker plugin PreImport-4e0dd5a1-6c72-4a69-bf0f-7fc137a85eda
2024-02-07 06:39:21,001 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:21,011 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44761', status: init, memory: 0, processing: 0>
2024-02-07 06:39:21,012 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44761
2024-02-07 06:39:21,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43520
2024-02-07 06:39:21,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:21,013 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:21,013 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:21,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:21,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33145', status: init, memory: 0, processing: 0>
2024-02-07 06:39:21,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33145
2024-02-07 06:39:21,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43504
2024-02-07 06:39:21,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:21,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:21,019 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:21,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:21,029 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37363', status: init, memory: 0, processing: 0>
2024-02-07 06:39:21,030 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37363
2024-02-07 06:39:21,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43526
2024-02-07 06:39:21,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:21,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:21,032 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:21,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:21,055 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,056 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:21,061 - distributed.scheduler - INFO - Remove client Client-9c6569d2-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:21,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33218; closing.
2024-02-07 06:39:21,061 - distributed.scheduler - INFO - Remove client Client-9c6569d2-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:21,061 - distributed.scheduler - INFO - Close client connection: Client-9c6569d2-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:21,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40269'. Reason: nanny-close
2024-02-07 06:39:21,063 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,063 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37897'. Reason: nanny-close
2024-02-07 06:39:21,064 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44585'. Reason: nanny-close
2024-02-07 06:39:21,064 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42245. Reason: nanny-close
2024-02-07 06:39:21,064 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38979'. Reason: nanny-close
2024-02-07 06:39:21,064 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42501'. Reason: nanny-close
2024-02-07 06:39:21,065 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43135'. Reason: nanny-close
2024-02-07 06:39:21,065 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42379. Reason: nanny-close
2024-02-07 06:39:21,065 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41555'. Reason: nanny-close
2024-02-07 06:39:21,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42831. Reason: nanny-close
2024-02-07 06:39:21,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43539'. Reason: nanny-close
2024-02-07 06:39:21,066 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43496; closing.
2024-02-07 06:39:21,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34451. Reason: nanny-close
2024-02-07 06:39:21,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,067 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.067005')
2024-02-07 06:39:21,067 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41007. Reason: nanny-close
2024-02-07 06:39:21,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,068 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:21,069 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,069 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44761. Reason: nanny-close
2024-02-07 06:39:21,069 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,069 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43462; closing.
2024-02-07 06:39:21,070 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43474; closing.
2024-02-07 06:39:21,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37363. Reason: nanny-close
2024-02-07 06:39:21,070 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,070 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43488; closing.
2024-02-07 06:39:21,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33145. Reason: nanny-close
2024-02-07 06:39:21,070 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,070 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,071 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,071 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.0709507')
2024-02-07 06:39:21,071 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42379', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.0713093')
2024-02-07 06:39:21,071 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.0715792')
2024-02-07 06:39:21,072 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,072 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,072 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:21,072 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43466; closing.
2024-02-07 06:39:21,073 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,073 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.0735037')
2024-02-07 06:39:21,073 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43520; closing.
2024-02-07 06:39:21,074 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43526; closing.
2024-02-07 06:39:21,074 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,074 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.074679')
2024-02-07 06:39:21,074 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:21,075 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.075055')
2024-02-07 06:39:21,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43504; closing.
2024-02-07 06:39:21,076 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33145', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287961.0759475')
2024-02-07 06:39:21,076 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:21,928 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:21,929 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:21,929 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:21,930 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:39:21,931 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-07 06:39:23,849 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:23,853 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43305 instead
  warnings.warn(
2024-02-07 06:39:23,857 - distributed.scheduler - INFO - State start
2024-02-07 06:39:23,877 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:23,878 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:23,879 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43305/status
2024-02-07 06:39:23,879 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:23,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34135'
2024-02-07 06:39:23,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35371'
2024-02-07 06:39:24,001 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41117'
2024-02-07 06:39:24,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41257'
2024-02-07 06:39:24,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35421'
2024-02-07 06:39:24,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42717'
2024-02-07 06:39:24,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38329'
2024-02-07 06:39:24,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40675'
2024-02-07 06:39:25,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:25,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:25,724 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:25,725 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34459
2024-02-07 06:39:25,725 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34459
2024-02-07 06:39:25,725 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45237
2024-02-07 06:39:25,725 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:25,725 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:25,725 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:25,725 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:25,725 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0l0b675t
2024-02-07 06:39:25,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6bb8ca5-8f05-4482-8404-243f365e1990
2024-02-07 06:39:25,726 - distributed.worker - INFO - Starting Worker plugin PreImport-1a232200-85be-4177-aacb-accef3f19060
2024-02-07 06:39:25,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aade137e-043b-4aea-b9c9-a28ba771eeaa
2024-02-07 06:39:25,728 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:25,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:25,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:25,733 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32973
2024-02-07 06:39:25,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32973
2024-02-07 06:39:25,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46405
2024-02-07 06:39:25,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:25,734 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:25,734 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:25,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:25,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ix3cd49
2024-02-07 06:39:25,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e172e852-bb56-4a1a-bc85-91c77e46a1c7
2024-02-07 06:39:25,734 - distributed.worker - INFO - Starting Worker plugin PreImport-7fffda37-f2c8-43e5-a302-d22a6ce2a202
2024-02-07 06:39:25,734 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09c259c6-152e-4fb0-a2c4-7a65fd951b4c
2024-02-07 06:39:25,771 - distributed.scheduler - INFO - Receive client connection: Client-a0b060a6-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:25,775 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:25,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:25,780 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:25,781 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43609
2024-02-07 06:39:25,781 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43609
2024-02-07 06:39:25,781 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44149
2024-02-07 06:39:25,781 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:25,781 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:25,781 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:25,781 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:25,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_u5hmwxy
2024-02-07 06:39:25,782 - distributed.worker - INFO - Starting Worker plugin PreImport-132b4347-9702-4161-bab9-8a0433cef88f
2024-02-07 06:39:25,782 - distributed.worker - INFO - Starting Worker plugin RMMSetup-980a41cb-dda7-4a08-a2c4-f1493758e452
2024-02-07 06:39:25,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43646
2024-02-07 06:39:25,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:25,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:25,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:25,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39739
2024-02-07 06:39:25,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39739
2024-02-07 06:39:25,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37653
2024-02-07 06:39:25,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:25,967 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:25,967 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:25,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:25,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fkwxrp1x
2024-02-07 06:39:25,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55f8ddbb-c5f7-427c-80af-2ee9bfedd58a
2024-02-07 06:39:25,968 - distributed.worker - INFO - Starting Worker plugin PreImport-63447670-0a9f-46e8-b012-c4c8f3317048
2024-02-07 06:39:25,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5bfa871-b107-4110-8d26-7e33a8973f3c
2024-02-07 06:39:25,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:25,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:25,979 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:25,980 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37793
2024-02-07 06:39:25,980 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37793
2024-02-07 06:39:25,980 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37259
2024-02-07 06:39:25,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:25,980 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:25,980 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:25,980 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:25,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ipysvajz
2024-02-07 06:39:25,980 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19dc591c-341d-4881-966b-66c3b589b9fd
2024-02-07 06:39:26,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:26,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:26,143 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:26,144 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41533
2024-02-07 06:39:26,144 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41533
2024-02-07 06:39:26,144 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44307
2024-02-07 06:39:26,144 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:26,144 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:26,144 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:26,144 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:26,144 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6dix9skq
2024-02-07 06:39:26,144 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09ba1948-af27-4157-b9de-f2cf4954f93a
2024-02-07 06:39:26,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:26,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:26,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:26,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:26,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:26,164 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40759
2024-02-07 06:39:26,164 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40759
2024-02-07 06:39:26,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34377
2024-02-07 06:39:26,164 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:26,164 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:26,164 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:26,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:26,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tuud1e3e
2024-02-07 06:39:26,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75ba4795-9b66-4ea3-b1a5-4bee47a4a88b
2024-02-07 06:39:26,165 - distributed.worker - INFO - Starting Worker plugin PreImport-90aa7c03-e075-499d-a90e-02dfb36193fe
2024-02-07 06:39:26,165 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61cade63-36c6-4d09-ab36-08c790854508
2024-02-07 06:39:26,169 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:26,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45983
2024-02-07 06:39:26,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45983
2024-02-07 06:39:26,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34695
2024-02-07 06:39:26,171 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:26,171 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:26,171 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:26,171 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:26,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pka8ko6s
2024-02-07 06:39:26,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fd51011-ca8e-4e18-ad6d-5fe9247db582
2024-02-07 06:39:26,172 - distributed.worker - INFO - Starting Worker plugin PreImport-e77fcf52-f1e3-44f5-8f77-ed3f1e69e4ad
2024-02-07 06:39:26,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86c34266-21d1-4842-a6a1-a2d9a3a182e3
2024-02-07 06:39:26,509 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:26,533 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34459', status: init, memory: 0, processing: 0>
2024-02-07 06:39:26,535 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34459
2024-02-07 06:39:26,535 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43658
2024-02-07 06:39:26,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:26,537 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:26,537 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:26,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,329 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,357 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32973', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32973
2024-02-07 06:39:27,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43670
2024-02-07 06:39:27,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,360 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,361 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,723 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3c4e1f8-4ff7-4636-a67f-0eec47e418e4
2024-02-07 06:39:27,729 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,748 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39739', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39739
2024-02-07 06:39:27,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43692
2024-02-07 06:39:27,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,751 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,751 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,756 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43609', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,757 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43609
2024-02-07 06:39:27,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43706
2024-02-07 06:39:27,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,759 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,759 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,792 - distributed.worker - INFO - Starting Worker plugin PreImport-5700147d-691e-4a2d-8a98-1532ffe95ec2
2024-02-07 06:39:27,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-541a4cb8-6a59-4c93-98b8-62b207e7e7dd
2024-02-07 06:39:27,793 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37793', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37793
2024-02-07 06:39:27,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43712
2024-02-07 06:39:27,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,821 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,828 - distributed.worker - INFO - Starting Worker plugin PreImport-ea74c3e0-42f7-4999-88bd-38ee6d886612
2024-02-07 06:39:27,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d63ee7a-8844-4275-b98b-e713e34aa876
2024-02-07 06:39:27,832 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,845 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,849 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,866 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40759', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,867 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40759
2024-02-07 06:39:27,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43728
2024-02-07 06:39:27,868 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,869 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,869 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,869 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45983', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,869 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45983
2024-02-07 06:39:27,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43736
2024-02-07 06:39:27,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41533', status: init, memory: 0, processing: 0>
2024-02-07 06:39:27,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,871 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,872 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41533
2024-02-07 06:39:27,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43716
2024-02-07 06:39:27,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:27,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:27,875 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:27,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:27,932 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,932 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,932 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,932 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,932 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,932 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,933 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,933 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:27,937 - distributed.scheduler - INFO - Remove client Client-a0b060a6-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:27,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43646; closing.
2024-02-07 06:39:27,938 - distributed.scheduler - INFO - Remove client Client-a0b060a6-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:27,938 - distributed.scheduler - INFO - Close client connection: Client-a0b060a6-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:27,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34135'. Reason: nanny-close
2024-02-07 06:39:27,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,940 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35371'. Reason: nanny-close
2024-02-07 06:39:27,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41117'. Reason: nanny-close
2024-02-07 06:39:27,941 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43609. Reason: nanny-close
2024-02-07 06:39:27,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41257'. Reason: nanny-close
2024-02-07 06:39:27,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32973. Reason: nanny-close
2024-02-07 06:39:27,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37793. Reason: nanny-close
2024-02-07 06:39:27,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35421'. Reason: nanny-close
2024-02-07 06:39:27,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42717'. Reason: nanny-close
2024-02-07 06:39:27,943 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40759. Reason: nanny-close
2024-02-07 06:39:27,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38329'. Reason: nanny-close
2024-02-07 06:39:27,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40675'. Reason: nanny-close
2024-02-07 06:39:27,944 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34459. Reason: nanny-close
2024-02-07 06:39:27,944 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43706; closing.
2024-02-07 06:39:27,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:27,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41533. Reason: nanny-close
2024-02-07 06:39:27,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45983. Reason: nanny-close
2024-02-07 06:39:27,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.9452188')
2024-02-07 06:39:27,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39739. Reason: nanny-close
2024-02-07 06:39:27,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43712; closing.
2024-02-07 06:39:27,946 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,946 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.9466918')
2024-02-07 06:39:27,946 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,946 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43728; closing.
2024-02-07 06:39:27,947 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.9482338')
2024-02-07 06:39:27,948 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,948 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43670; closing.
2024-02-07 06:39:27,948 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,949 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,949 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:27,952 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:27,949 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43712>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-07 06:39:27,953 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.953162')
2024-02-07 06:39:27,953 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43658; closing.
2024-02-07 06:39:27,953 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43736; closing.
2024-02-07 06:39:27,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34459', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.954376')
2024-02-07 06:39:27,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.9547844')
2024-02-07 06:39:27,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43692; closing.
2024-02-07 06:39:27,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43716; closing.
2024-02-07 06:39:27,955 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39739', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.9557803')
2024-02-07 06:39:27,956 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41533', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287967.9562168')
2024-02-07 06:39:27,956 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:28,905 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:28,906 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:28,906 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:28,908 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:39:28,908 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-07 06:39:31,061 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:31,065 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40973 instead
  warnings.warn(
2024-02-07 06:39:31,069 - distributed.scheduler - INFO - State start
2024-02-07 06:39:31,089 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:31,090 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:31,091 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40973/status
2024-02-07 06:39:31,091 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:31,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39107'
2024-02-07 06:39:31,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35543'
2024-02-07 06:39:31,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42049'
2024-02-07 06:39:31,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35695'
2024-02-07 06:39:31,240 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46053'
2024-02-07 06:39:31,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33385'
2024-02-07 06:39:31,258 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37763'
2024-02-07 06:39:31,267 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37627'
2024-02-07 06:39:32,950 - distributed.scheduler - INFO - Receive client connection: Client-a4ee626d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:32,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53708
2024-02-07 06:39:33,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,022 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37673
2024-02-07 06:39:33,023 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37673
2024-02-07 06:39:33,023 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46199
2024-02-07 06:39:33,023 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,023 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,023 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,023 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-csc90w06
2024-02-07 06:39:33,023 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4d80d6e-ab68-4f53-9db6-4eff85a115bd
2024-02-07 06:39:33,024 - distributed.worker - INFO - Starting Worker plugin PreImport-aa9b13a7-fc05-4419-8426-26a2adbd3de1
2024-02-07 06:39:33,024 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af8b6bf7-a0e1-4a35-93f5-e90e769edb3e
2024-02-07 06:39:33,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,052 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33753
2024-02-07 06:39:33,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33753
2024-02-07 06:39:33,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44659
2024-02-07 06:39:33,054 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,054 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,054 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,054 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9rcbmfwb
2024-02-07 06:39:33,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca534755-869f-4e48-b5e1-e196d892f40e
2024-02-07 06:39:33,054 - distributed.worker - INFO - Starting Worker plugin PreImport-911118aa-0bd8-488b-bfc2-b26f0644f4aa
2024-02-07 06:39:33,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c7a58a0-f084-42b4-a257-67917123b729
2024-02-07 06:39:33,055 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44471
2024-02-07 06:39:33,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44471
2024-02-07 06:39:33,056 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32797
2024-02-07 06:39:33,056 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,056 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,057 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-734j9672
2024-02-07 06:39:33,057 - distributed.worker - INFO - Starting Worker plugin PreImport-8888d4e9-c247-49ce-864f-6c84e0565c58
2024-02-07 06:39:33,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58e4df78-3e3a-4f3d-b622-5c81a736f4ca
2024-02-07 06:39:33,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5aeb47fa-f6be-4ffd-9d06-82f458c8d32e
2024-02-07 06:39:33,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,078 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,079 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40391
2024-02-07 06:39:33,080 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40391
2024-02-07 06:39:33,080 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41101
2024-02-07 06:39:33,080 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,080 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,080 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,080 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1nlhp8o2
2024-02-07 06:39:33,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3c320e6-5768-4bb2-964c-4729b778e81f
2024-02-07 06:39:33,276 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,278 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,281 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,282 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43755
2024-02-07 06:39:33,282 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43755
2024-02-07 06:39:33,282 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42271
2024-02-07 06:39:33,282 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,282 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,282 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,282 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-okyr_zhx
2024-02-07 06:39:33,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,282 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14822ea2-6804-43fb-b0a8-129f68dde176
2024-02-07 06:39:33,283 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37781
2024-02-07 06:39:33,283 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37781
2024-02-07 06:39:33,283 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41029
2024-02-07 06:39:33,283 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,283 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,283 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,284 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r9_n2anq
2024-02-07 06:39:33,284 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2a5bf24-4786-4b15-be31-d1f13377ac4e
2024-02-07 06:39:33,284 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0df09fe2-be94-4dad-87e6-6f1d238ecffe
2024-02-07 06:39:33,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,289 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,290 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34087
2024-02-07 06:39:33,290 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34087
2024-02-07 06:39:33,290 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32819
2024-02-07 06:39:33,290 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,290 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,290 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,290 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,290 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_vyn7jut
2024-02-07 06:39:33,291 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac4be3b8-75be-450a-9118-9387d3940668
2024-02-07 06:39:33,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:33,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:33,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:33,299 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41821
2024-02-07 06:39:33,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41821
2024-02-07 06:39:33,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46877
2024-02-07 06:39:33,299 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:33,299 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:33,299 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:33,299 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:33,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pwj7mj2m
2024-02-07 06:39:33,299 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ffb2246-85fb-4e1c-9fa0-4e96f632ec88
2024-02-07 06:39:33,300 - distributed.worker - INFO - Starting Worker plugin PreImport-4d2bfb4f-117b-4b38-b756-d2a8f9d7b466
2024-02-07 06:39:33,300 - distributed.worker - INFO - Starting Worker plugin RMMSetup-babb2d61-3b0a-431f-bb49-caeceaa830aa
2024-02-07 06:39:34,272 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:34,299 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37673', status: init, memory: 0, processing: 0>
2024-02-07 06:39:34,301 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37673
2024-02-07 06:39:34,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53734
2024-02-07 06:39:34,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:34,303 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:34,303 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:34,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,229 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,244 - distributed.worker - INFO - Starting Worker plugin PreImport-d603bca6-c977-41da-8568-a0d75b729493
2024-02-07 06:39:35,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91f18aae-f84e-40fc-94ce-ba0389e672d2
2024-02-07 06:39:35,247 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,253 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,254 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33753', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,255 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33753
2024-02-07 06:39:35,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53748
2024-02-07 06:39:35,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,257 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,257 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,276 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40391', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,276 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40391
2024-02-07 06:39:35,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53760
2024-02-07 06:39:35,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,279 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,279 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,286 - distributed.worker - INFO - Starting Worker plugin PreImport-37a78a6d-ad2e-4532-81a6-d3fc777b0f99
2024-02-07 06:39:35,286 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6722c4bc-d0cd-4160-b70c-b578af92d2e2
2024-02-07 06:39:35,286 - distributed.worker - INFO - Starting Worker plugin PreImport-f83652ac-12a6-4a22-a860-1dafe6e13247
2024-02-07 06:39:35,287 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c32841f7-1758-4305-bf28-658a0223e99f
2024-02-07 06:39:35,287 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,288 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44471', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,289 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44471
2024-02-07 06:39:35,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53764
2024-02-07 06:39:35,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,290 - distributed.worker - INFO - Starting Worker plugin PreImport-5f0876f7-ced4-486f-b937-9e1ae7cce1b2
2024-02-07 06:39:35,291 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,291 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,292 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,301 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,313 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34087', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,313 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34087
2024-02-07 06:39:35,313 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53778
2024-02-07 06:39:35,314 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,314 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43755', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,315 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43755
2024-02-07 06:39:35,315 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53784
2024-02-07 06:39:35,315 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37781', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,316 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37781
2024-02-07 06:39:35,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53788
2024-02-07 06:39:35,317 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,317 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,318 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41821', status: init, memory: 0, processing: 0>
2024-02-07 06:39:35,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41821
2024-02-07 06:39:35,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53798
2024-02-07 06:39:35,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:35,334 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:35,334 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:35,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:35,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,422 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,423 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:35,430 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:35,432 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:35,434 - distributed.scheduler - INFO - Remove client Client-a4ee626d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:35,434 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53708; closing.
2024-02-07 06:39:35,435 - distributed.scheduler - INFO - Remove client Client-a4ee626d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:35,435 - distributed.scheduler - INFO - Close client connection: Client-a4ee626d-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:35,436 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39107'. Reason: nanny-close
2024-02-07 06:39:35,436 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35543'. Reason: nanny-close
2024-02-07 06:39:35,437 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42049'. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44471. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35695'. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40391. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46053'. Reason: nanny-close
2024-02-07 06:39:35,438 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33753. Reason: nanny-close
2024-02-07 06:39:35,439 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33385'. Reason: nanny-close
2024-02-07 06:39:35,439 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43755. Reason: nanny-close
2024-02-07 06:39:35,439 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37763'. Reason: nanny-close
2024-02-07 06:39:35,439 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41821. Reason: nanny-close
2024-02-07 06:39:35,440 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37627'. Reason: nanny-close
2024-02-07 06:39:35,440 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37673. Reason: nanny-close
2024-02-07 06:39:35,440 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:35,440 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,440 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37781. Reason: nanny-close
2024-02-07 06:39:35,440 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53764; closing.
2024-02-07 06:39:35,441 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,441 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,441 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44471', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4411511')
2024-02-07 06:39:35,441 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34087. Reason: nanny-close
2024-02-07 06:39:35,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53748; closing.
2024-02-07 06:39:35,442 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53760; closing.
2024-02-07 06:39:35,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53784; closing.
2024-02-07 06:39:35,442 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,442 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,442 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,442 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,442 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,443 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:35,443 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4437268')
2024-02-07 06:39:35,444 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4440897')
2024-02-07 06:39:35,444 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,444 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43755', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4444537')
2024-02-07 06:39:35,444 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:35,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53798; closing.
2024-02-07 06:39:35,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4459124')
2024-02-07 06:39:35,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53734; closing.
2024-02-07 06:39:35,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53788; closing.
2024-02-07 06:39:35,447 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4470367')
2024-02-07 06:39:35,447 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.447428')
2024-02-07 06:39:35,447 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53778; closing.
2024-02-07 06:39:35,448 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287975.4482148')
2024-02-07 06:39:35,448 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:36,402 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:36,402 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:36,402 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:36,404 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:39:36,404 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-07 06:39:38,352 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:38,356 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43925 instead
  warnings.warn(
2024-02-07 06:39:38,360 - distributed.scheduler - INFO - State start
2024-02-07 06:39:38,381 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:38,382 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:38,383 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43925/status
2024-02-07 06:39:38,383 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:38,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32887'
2024-02-07 06:39:38,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43279'
2024-02-07 06:39:38,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44847'
2024-02-07 06:39:38,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33459'
2024-02-07 06:39:38,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34113'
2024-02-07 06:39:38,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46225'
2024-02-07 06:39:38,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37003'
2024-02-07 06:39:38,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36233'
2024-02-07 06:39:38,912 - distributed.scheduler - INFO - Receive client connection: Client-a9510b19-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:38,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53952
2024-02-07 06:39:40,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,428 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,429 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40271
2024-02-07 06:39:40,429 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40271
2024-02-07 06:39:40,429 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38845
2024-02-07 06:39:40,429 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,429 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,429 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,429 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gyxvov9m
2024-02-07 06:39:40,429 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f51a8cd-9ffc-4b23-8a46-4e12f9c6eceb
2024-02-07 06:39:40,430 - distributed.worker - INFO - Starting Worker plugin PreImport-bcdd1478-29c5-4f5f-bd83-133032b053c4
2024-02-07 06:39:40,430 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4873de5f-851e-4aa5-8870-9f9aabd40863
2024-02-07 06:39:40,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41411
2024-02-07 06:39:40,664 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41411
2024-02-07 06:39:40,664 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36241
2024-02-07 06:39:40,664 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,664 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,664 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,664 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,665 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k5jbpk8v
2024-02-07 06:39:40,665 - distributed.worker - INFO - Starting Worker plugin PreImport-0f3ee6ed-8fcc-4056-8361-5b000dc008b8
2024-02-07 06:39:40,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0fa2279-4ac5-465d-aa1d-f43a1185f34a
2024-02-07 06:39:40,665 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f30d3a4f-b941-4f28-85e5-b7180d081572
2024-02-07 06:39:40,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,668 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,673 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,673 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,674 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37605
2024-02-07 06:39:40,674 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37605
2024-02-07 06:39:40,674 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36677
2024-02-07 06:39:40,674 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,674 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,674 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,674 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,674 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3uuqd1ld
2024-02-07 06:39:40,674 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36021
2024-02-07 06:39:40,674 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36021
2024-02-07 06:39:40,674 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54f318d3-0df0-433a-abe6-dadca395ca59
2024-02-07 06:39:40,674 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46639
2024-02-07 06:39:40,674 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,674 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,674 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,674 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,674 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h4sfj5wv
2024-02-07 06:39:40,674 - distributed.worker - INFO - Starting Worker plugin PreImport-a5a36404-051c-4e69-84a2-edace46f15c8
2024-02-07 06:39:40,675 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86729d14-1f9d-4059-b798-28933e465325
2024-02-07 06:39:40,675 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d75c1848-80e7-41cd-b93f-39907b14b2c7
2024-02-07 06:39:40,675 - distributed.worker - INFO - Starting Worker plugin PreImport-0a733e85-a5f3-45b8-8de2-b3807c73d395
2024-02-07 06:39:40,675 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f1f2ae8-a9f4-4c66-9337-0144c5fa931a
2024-02-07 06:39:40,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:40,677 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:40,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,679 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,679 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45049
2024-02-07 06:39:40,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45049
2024-02-07 06:39:40,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39113
2024-02-07 06:39:40,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,679 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,679 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,679 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-94or5px5
2024-02-07 06:39:40,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91942f31-7d2b-4cf1-831c-c72bd0a42d6d
2024-02-07 06:39:40,680 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45901
2024-02-07 06:39:40,680 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45901
2024-02-07 06:39:40,680 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40739
2024-02-07 06:39:40,680 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,680 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,680 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,681 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_3ibvhrs
2024-02-07 06:39:40,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-895891af-fa44-40fd-a9e0-1854a0b4fa90
2024-02-07 06:39:40,682 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40011
2024-02-07 06:39:40,682 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40011
2024-02-07 06:39:40,682 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32837
2024-02-07 06:39:40,682 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,682 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,682 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,682 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,682 - distributed.worker - INFO - Starting Worker plugin PreImport-3b02df4d-9168-4bf2-93a6-63bd33d0a70a
2024-02-07 06:39:40,682 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tzkkkqbs
2024-02-07 06:39:40,682 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09e7130b-6808-4e31-97fc-d4d374473a53
2024-02-07 06:39:40,682 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98b9f1bf-07f0-4388-adb9-a96cc1d93e88
2024-02-07 06:39:40,682 - distributed.worker - INFO - Starting Worker plugin PreImport-ea365a68-110c-4b5a-b99e-d78927e09921
2024-02-07 06:39:40,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddff3303-991f-437d-8d87-0adc2d2d606f
2024-02-07 06:39:40,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:40,685 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39667
2024-02-07 06:39:40,685 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39667
2024-02-07 06:39:40,685 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37119
2024-02-07 06:39:40,685 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,685 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,685 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:40,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:40,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-72av9r53
2024-02-07 06:39:40,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-748e8643-3ceb-4bea-8d68-9cd0efe0d4a4
2024-02-07 06:39:40,950 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,972 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40271', status: init, memory: 0, processing: 0>
2024-02-07 06:39:40,974 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40271
2024-02-07 06:39:40,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33576
2024-02-07 06:39:40,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:40,975 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:40,975 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:40,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,603 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,607 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41411', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41411
2024-02-07 06:39:42,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33606
2024-02-07 06:39:42,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,628 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,628 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,634 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45901', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45901
2024-02-07 06:39:42,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33618
2024-02-07 06:39:42,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,637 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,638 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,645 - distributed.worker - INFO - Starting Worker plugin PreImport-f90daa12-2ed7-45c5-b6bb-004374a48cfb
2024-02-07 06:39:42,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9c6643a-46f6-4688-96fa-a8adbf0c2322
2024-02-07 06:39:42,646 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,646 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,651 - distributed.worker - INFO - Starting Worker plugin PreImport-bbaeecac-83c9-46c6-bfd6-203ef58a433a
2024-02-07 06:39:42,652 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4415fe1-1358-423d-9f8d-c83d3f783176
2024-02-07 06:39:42,652 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,660 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,671 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37605', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,672 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37605
2024-02-07 06:39:42,672 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33626
2024-02-07 06:39:42,672 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39667', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,673 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39667
2024-02-07 06:39:42,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33640
2024-02-07 06:39:42,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,675 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,675 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,675 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,675 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,681 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36021', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,681 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36021
2024-02-07 06:39:42,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33668
2024-02-07 06:39:42,682 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40011', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,682 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,682 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40011
2024-02-07 06:39:42,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33652
2024-02-07 06:39:42,682 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,682 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45049', status: init, memory: 0, processing: 0>
2024-02-07 06:39:42,684 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45049
2024-02-07 06:39:42,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33660
2024-02-07 06:39:42,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,685 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,685 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:42,686 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:42,686 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:42,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:42,697 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,697 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,709 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,709 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,709 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,710 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,710 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,710 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,710 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,710 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:39:42,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,719 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:42,721 - distributed.scheduler - INFO - Remove client Client-a9510b19-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:42,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53952; closing.
2024-02-07 06:39:42,721 - distributed.scheduler - INFO - Remove client Client-a9510b19-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:42,722 - distributed.scheduler - INFO - Close client connection: Client-a9510b19-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:42,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32887'. Reason: nanny-close
2024-02-07 06:39:42,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43279'. Reason: nanny-close
2024-02-07 06:39:42,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41411. Reason: nanny-close
2024-02-07 06:39:42,724 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44847'. Reason: nanny-close
2024-02-07 06:39:42,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33459'. Reason: nanny-close
2024-02-07 06:39:42,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36021. Reason: nanny-close
2024-02-07 06:39:42,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34113'. Reason: nanny-close
2024-02-07 06:39:42,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45049. Reason: nanny-close
2024-02-07 06:39:42,726 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46225'. Reason: nanny-close
2024-02-07 06:39:42,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,726 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33606; closing.
2024-02-07 06:39:42,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45901. Reason: nanny-close
2024-02-07 06:39:42,726 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41411', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7270393')
2024-02-07 06:39:42,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39667. Reason: nanny-close
2024-02-07 06:39:42,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37003'. Reason: nanny-close
2024-02-07 06:39:42,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40271. Reason: nanny-close
2024-02-07 06:39:42,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36233'. Reason: nanny-close
2024-02-07 06:39:42,728 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,728 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:42,728 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33668; closing.
2024-02-07 06:39:42,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,728 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37605. Reason: nanny-close
2024-02-07 06:39:42,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,729 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36021', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7295332')
2024-02-07 06:39:42,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40011. Reason: nanny-close
2024-02-07 06:39:42,730 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,730 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33660; closing.
2024-02-07 06:39:42,731 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7316003')
2024-02-07 06:39:42,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,731 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,732 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33618; closing.
2024-02-07 06:39:42,732 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33640; closing.
2024-02-07 06:39:42,732 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33576; closing.
2024-02-07 06:39:42,732 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:42,733 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45901', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7329743')
2024-02-07 06:39:42,733 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7333486')
2024-02-07 06:39:42,733 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,733 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7337313')
2024-02-07 06:39:42,734 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33626; closing.
2024-02-07 06:39:42,734 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:42,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37605', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.734706')
2024-02-07 06:39:42,735 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33652; closing.
2024-02-07 06:39:42,735 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287982.7355373')
2024-02-07 06:39:42,735 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:43,638 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:43,638 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:43,639 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:43,640 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:39:43,640 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-07 06:39:45,789 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:45,793 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43643 instead
  warnings.warn(
2024-02-07 06:39:45,796 - distributed.scheduler - INFO - State start
2024-02-07 06:39:45,817 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:45,818 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:45,819 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43643/status
2024-02-07 06:39:45,819 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:45,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39453'
2024-02-07 06:39:45,929 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32957'
2024-02-07 06:39:45,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36209'
2024-02-07 06:39:45,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44187'
2024-02-07 06:39:45,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45465'
2024-02-07 06:39:45,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39201'
2024-02-07 06:39:45,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35205'
2024-02-07 06:39:45,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44223'
2024-02-07 06:39:47,315 - distributed.scheduler - INFO - Receive client connection: Client-adb488c0-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:47,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33808
2024-02-07 06:39:47,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,786 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,786 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,788 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45075
2024-02-07 06:39:47,789 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45075
2024-02-07 06:39:47,789 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43247
2024-02-07 06:39:47,789 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,789 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,789 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,789 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cvpxia4b
2024-02-07 06:39:47,789 - distributed.worker - INFO - Starting Worker plugin PreImport-410bde5a-3940-445d-a0ed-2e0cf29c8de5
2024-02-07 06:39:47,789 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9e14a56-c8f5-4165-90f7-f350548a88ae
2024-02-07 06:39:47,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50fa0bc4-f797-4202-b1ed-98d561e443a6
2024-02-07 06:39:47,790 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45391
2024-02-07 06:39:47,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45391
2024-02-07 06:39:47,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33669
2024-02-07 06:39:47,791 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,791 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,791 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,791 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nldcg4vd
2024-02-07 06:39:47,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4df4c6e4-683c-4f7c-8c8f-a90771a29c61
2024-02-07 06:39:47,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,817 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,818 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45829
2024-02-07 06:39:47,818 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45829
2024-02-07 06:39:47,818 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35477
2024-02-07 06:39:47,818 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,818 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,818 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,818 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,818 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n9ygbmz9
2024-02-07 06:39:47,819 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ccd7e18-bb15-48f5-ac96-cc3e83db6b92
2024-02-07 06:39:47,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37747
2024-02-07 06:39:47,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37747
2024-02-07 06:39:47,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35663
2024-02-07 06:39:47,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,850 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,850 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,850 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,850 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l7eraq39
2024-02-07 06:39:47,850 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0c17dfa-a747-442f-ae07-5518f7696f91
2024-02-07 06:39:47,851 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40853
2024-02-07 06:39:47,851 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40853
2024-02-07 06:39:47,851 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44241
2024-02-07 06:39:47,851 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,851 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,851 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,851 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,851 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7usq7puh
2024-02-07 06:39:47,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4bb0d6ec-855a-4a4d-ade9-84962b4b5e11
2024-02-07 06:39:47,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,853 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34455
2024-02-07 06:39:47,853 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34455
2024-02-07 06:39:47,853 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34023
2024-02-07 06:39:47,853 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,853 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,853 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,853 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xjxpojwn
2024-02-07 06:39:47,854 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a664e7cb-f95f-45b5-973c-bae19e83975a
2024-02-07 06:39:47,854 - distributed.worker - INFO - Starting Worker plugin PreImport-8cdf1825-2a9b-44b0-b1d7-1c2ba9de2c45
2024-02-07 06:39:47,854 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a97d4fd5-1769-42ce-8928-3ce00af70352
2024-02-07 06:39:47,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,856 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:47,856 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:47,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37815
2024-02-07 06:39:47,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37815
2024-02-07 06:39:47,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38983
2024-02-07 06:39:47,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,860 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,860 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:47,860 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,860 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,860 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g3bkj5yr
2024-02-07 06:39:47,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-823fd67a-0b33-43de-b811-5d88ecd54cfa
2024-02-07 06:39:47,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0018edf9-6bfd-4b58-8bf9-f192c3eac27c
2024-02-07 06:39:47,861 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39339
2024-02-07 06:39:47,861 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39339
2024-02-07 06:39:47,861 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37325
2024-02-07 06:39:47,861 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:47,861 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:47,861 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:47,861 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:39:47,861 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2peozsp7
2024-02-07 06:39:47,862 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-100598e8-4ccb-4bb1-ad9b-bc521533ac40
2024-02-07 06:39:47,862 - distributed.worker - INFO - Starting Worker plugin PreImport-a3dfdbb9-419f-4fc2-bd34-643fd486f7e1
2024-02-07 06:39:47,862 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb5d0b8e-959b-4848-a698-0ad834596cdb
2024-02-07 06:39:49,650 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61d00930-ee8a-4d69-87d2-1d252fd5ea6d
2024-02-07 06:39:49,651 - distributed.worker - INFO - Starting Worker plugin PreImport-6c16f39c-fcb6-4354-9fc9-001e0f87dfcb
2024-02-07 06:39:49,653 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,684 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45391', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,686 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45391
2024-02-07 06:39:49,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33844
2024-02-07 06:39:49,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,688 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,688 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,690 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,708 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,728 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45075', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,729 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45075
2024-02-07 06:39:49,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33852
2024-02-07 06:39:49,730 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,730 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,830 - distributed.worker - INFO - Starting Worker plugin PreImport-23610557-1c49-453f-b1ac-7176f4d181dc
2024-02-07 06:39:49,830 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6b60dfd-6a5b-450c-8d0f-100a0cf8becd
2024-02-07 06:39:49,831 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,848 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,853 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45829', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,853 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45829
2024-02-07 06:39:49,853 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33858
2024-02-07 06:39:49,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,855 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,855 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34455', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,869 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34455
2024-02-07 06:39:49,869 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33866
2024-02-07 06:39:49,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,871 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,878 - distributed.worker - INFO - Starting Worker plugin PreImport-feb45ce0-60d7-470c-8e8d-2a7dab8ce5d9
2024-02-07 06:39:49,880 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,881 - distributed.worker - INFO - Starting Worker plugin PreImport-d29cc005-7ed8-48b5-acc2-c520bfacead6
2024-02-07 06:39:49,882 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a566ecc-47d6-4997-873f-35ac072a292d
2024-02-07 06:39:49,883 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,886 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,890 - distributed.worker - INFO - Starting Worker plugin PreImport-f199e8d5-9915-45af-b65d-b5112c9bfb94
2024-02-07 06:39:49,891 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b4bb5ca-2811-4c91-a045-f2383156fc96
2024-02-07 06:39:49,892 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,908 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39339', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,908 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39339
2024-02-07 06:39:49,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33888
2024-02-07 06:39:49,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,910 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37815', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,914 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37815
2024-02-07 06:39:49,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33868
2024-02-07 06:39:49,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,917 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,917 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40853', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,918 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40853
2024-02-07 06:39:49,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33874
2024-02-07 06:39:49,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,921 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,921 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37747', status: init, memory: 0, processing: 0>
2024-02-07 06:39:49,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37747
2024-02-07 06:39:49,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33896
2024-02-07 06:39:49,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:49,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:49,927 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:49,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:49,967 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,967 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,968 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,968 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,968 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,968 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,968 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,968 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:39:49,972 - distributed.scheduler - INFO - Remove client Client-adb488c0-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:49,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33808; closing.
2024-02-07 06:39:49,973 - distributed.scheduler - INFO - Remove client Client-adb488c0-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:49,973 - distributed.scheduler - INFO - Close client connection: Client-adb488c0-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:49,974 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39453'. Reason: nanny-close
2024-02-07 06:39:49,974 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,974 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32957'. Reason: nanny-close
2024-02-07 06:39:49,975 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,975 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36209'. Reason: nanny-close
2024-02-07 06:39:49,975 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45075. Reason: nanny-close
2024-02-07 06:39:49,976 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44187'. Reason: nanny-close
2024-02-07 06:39:49,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45829. Reason: nanny-close
2024-02-07 06:39:49,976 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45465'. Reason: nanny-close
2024-02-07 06:39:49,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45391. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39201'. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37747. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35205'. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34455. Reason: nanny-close
2024-02-07 06:39:49,977 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,977 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33852; closing.
2024-02-07 06:39:49,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,978 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45075', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.9780169')
2024-02-07 06:39:49,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44223'. Reason: nanny-close
2024-02-07 06:39:49,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39339. Reason: nanny-close
2024-02-07 06:39:49,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:49,978 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37815. Reason: nanny-close
2024-02-07 06:39:49,979 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40853. Reason: nanny-close
2024-02-07 06:39:49,979 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,979 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33858; closing.
2024-02-07 06:39:49,979 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,979 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,980 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,980 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33844; closing.
2024-02-07 06:39:49,980 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,980 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45829', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.98071')
2024-02-07 06:39:49,981 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.9815507')
2024-02-07 06:39:49,981 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,981 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33866; closing.
2024-02-07 06:39:49,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:49,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33896; closing.
2024-02-07 06:39:49,982 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34455', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.9829175')
2024-02-07 06:39:49,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37747', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.983229')
2024-02-07 06:39:49,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33888; closing.
2024-02-07 06:39:49,983 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,983 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:49,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.9842167')
2024-02-07 06:39:49,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33868; closing.
2024-02-07 06:39:49,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33874; closing.
2024-02-07 06:39:49,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.9851992')
2024-02-07 06:39:49,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40853', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287989.9855866')
2024-02-07 06:39:49,985 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:50,940 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:50,940 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:50,941 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:50,942 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:39:50,943 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-07 06:39:53,009 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:53,013 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40653 instead
  warnings.warn(
2024-02-07 06:39:53,016 - distributed.scheduler - INFO - State start
2024-02-07 06:39:53,036 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:53,037 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:53,038 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40653/status
2024-02-07 06:39:53,038 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:53,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46737'
2024-02-07 06:39:53,162 - distributed.scheduler - INFO - Receive client connection: Client-b2071ca1-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:53,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33268
2024-02-07 06:39:54,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:39:54,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:39:55,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:39:55,205 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42521
2024-02-07 06:39:55,205 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42521
2024-02-07 06:39:55,205 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-07 06:39:55,205 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:39:55,205 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:55,205 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:39:55,205 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-07 06:39:55,205 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0tgecba_
2024-02-07 06:39:55,206 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d5ab56b-0c46-46ec-9a7b-afd0404188ea
2024-02-07 06:39:55,206 - distributed.worker - INFO - Starting Worker plugin PreImport-c85a6629-4d74-4499-be7f-b00e56447b8e
2024-02-07 06:39:55,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97ba4cab-a341-47da-a00f-ec32e7c89e45
2024-02-07 06:39:55,206 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:55,256 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42521', status: init, memory: 0, processing: 0>
2024-02-07 06:39:55,257 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42521
2024-02-07 06:39:55,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33292
2024-02-07 06:39:55,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:39:55,259 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:39:55,259 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:39:55,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:39:55,310 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:39:55,313 - distributed.scheduler - INFO - Remove client Client-b2071ca1-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:55,313 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33268; closing.
2024-02-07 06:39:55,313 - distributed.scheduler - INFO - Remove client Client-b2071ca1-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:55,314 - distributed.scheduler - INFO - Close client connection: Client-b2071ca1-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:39:55,314 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46737'. Reason: nanny-close
2024-02-07 06:39:55,315 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:39:55,315 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42521. Reason: nanny-close
2024-02-07 06:39:55,317 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33292; closing.
2024-02-07 06:39:55,317 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:39:55,317 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42521', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707287995.3177533')
2024-02-07 06:39:55,318 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:39:55,318 - distributed.nanny - INFO - Worker closed
2024-02-07 06:39:55,829 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:39:55,829 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:39:55,830 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:39:55,831 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:39:55,831 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-07 06:39:59,751 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:59,756 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36185 instead
  warnings.warn(
2024-02-07 06:39:59,759 - distributed.scheduler - INFO - State start
2024-02-07 06:39:59,782 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:39:59,782 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:39:59,783 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36185/status
2024-02-07 06:39:59,784 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:39:59,847 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46421'
2024-02-07 06:40:00,436 - distributed.scheduler - INFO - Receive client connection: Client-b5fe3717-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:00,446 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56394
2024-02-07 06:40:01,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:01,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:02,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:02,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46861
2024-02-07 06:40:02,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46861
2024-02-07 06:40:02,049 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33115
2024-02-07 06:40:02,049 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:02,049 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:02,049 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:02,049 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-07 06:40:02,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7t2av90w
2024-02-07 06:40:02,051 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e2953f9-ebf3-4e2a-a690-4b1c2b0e08f1
2024-02-07 06:40:02,051 - distributed.worker - INFO - Starting Worker plugin PreImport-3bf26415-85d9-4691-b594-e5fafd995d74
2024-02-07 06:40:02,052 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3261681-0220-4c73-b325-78a5df5a2184
2024-02-07 06:40:02,052 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:02,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46861', status: init, memory: 0, processing: 0>
2024-02-07 06:40:02,108 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46861
2024-02-07 06:40:02,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56422
2024-02-07 06:40:02,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:02,111 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:02,111 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:02,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:02,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:02,177 - distributed.scheduler - INFO - Remove client Client-b5fe3717-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:02,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56394; closing.
2024-02-07 06:40:02,177 - distributed.scheduler - INFO - Remove client Client-b5fe3717-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:02,177 - distributed.scheduler - INFO - Close client connection: Client-b5fe3717-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:02,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46421'. Reason: nanny-close
2024-02-07 06:40:02,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:02,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46861. Reason: nanny-close
2024-02-07 06:40:02,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56422; closing.
2024-02-07 06:40:02,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:02,182 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288002.1825154')
2024-02-07 06:40:02,182 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:40:02,183 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:02,843 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:40:02,844 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:40:02,844 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:40:02,845 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:40:02,845 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-07 06:40:04,855 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:04,859 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-07 06:40:04,862 - distributed.scheduler - INFO - State start
2024-02-07 06:40:04,883 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:04,884 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:40:04,884 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-07 06:40:04,885 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:40:07,185 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:56436'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56436>: Stream is closed
2024-02-07 06:40:07,518 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:40:07,518 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:40:07,519 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:40:07,520 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:40:07,520 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-07 06:40:09,575 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:09,579 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-07 06:40:09,583 - distributed.scheduler - INFO - State start
2024-02-07 06:40:09,605 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:09,606 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-07 06:40:09,606 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-07 06:40:09,607 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:40:09,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35869'
2024-02-07 06:40:10,410 - distributed.scheduler - INFO - Receive client connection: Client-bbdfab05-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:10,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52206
2024-02-07 06:40:11,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:11,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:11,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:11,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41241
2024-02-07 06:40:11,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41241
2024-02-07 06:40:11,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37377
2024-02-07 06:40:11,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-07 06:40:11,289 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:11,289 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:11,289 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-07 06:40:11,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fnfapohu
2024-02-07 06:40:11,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd7e3ef9-55d1-4274-a15d-fcf69ab02a77
2024-02-07 06:40:11,290 - distributed.worker - INFO - Starting Worker plugin PreImport-a5856b5b-991d-4881-895e-04bff4b99da6
2024-02-07 06:40:11,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3c13b68-24b6-4a55-b832-4e282dd0b5de
2024-02-07 06:40:11,290 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:11,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41241', status: init, memory: 0, processing: 0>
2024-02-07 06:40:11,414 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41241
2024-02-07 06:40:11,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52226
2024-02-07 06:40:11,415 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:11,416 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-07 06:40:11,416 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:11,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-07 06:40:11,442 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:11,445 - distributed.scheduler - INFO - Remove client Client-bbdfab05-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:11,445 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52206; closing.
2024-02-07 06:40:11,445 - distributed.scheduler - INFO - Remove client Client-bbdfab05-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:11,445 - distributed.scheduler - INFO - Close client connection: Client-bbdfab05-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:11,446 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35869'. Reason: nanny-close
2024-02-07 06:40:11,447 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:11,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41241. Reason: nanny-close
2024-02-07 06:40:11,449 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-07 06:40:11,449 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52226; closing.
2024-02-07 06:40:11,450 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288011.4500973')
2024-02-07 06:40:11,450 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:40:11,451 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:12,012 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:40:12,012 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:40:12,012 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:40:12,013 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-07 06:40:12,013 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-07 06:40:14,035 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:14,040 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-07 06:40:14,043 - distributed.scheduler - INFO - State start
2024-02-07 06:40:14,224 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:14,225 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:40:14,226 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-07 06:40:14,226 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:40:14,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33003'
2024-02-07 06:40:14,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45551'
2024-02-07 06:40:14,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43843'
2024-02-07 06:40:14,413 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36369'
2024-02-07 06:40:14,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44201'
2024-02-07 06:40:14,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40385'
2024-02-07 06:40:14,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41195'
2024-02-07 06:40:14,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33539'
2024-02-07 06:40:16,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,262 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33407
2024-02-07 06:40:16,262 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33407
2024-02-07 06:40:16,262 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40331
2024-02-07 06:40:16,262 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,262 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,262 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,262 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45237
2024-02-07 06:40:16,262 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,262 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41289
2024-02-07 06:40:16,262 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45237
2024-02-07 06:40:16,262 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lsc8lx3c
2024-02-07 06:40:16,262 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41289
2024-02-07 06:40:16,262 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45287
2024-02-07 06:40:16,262 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45839
2024-02-07 06:40:16,262 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,262 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,262 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,262 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,262 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d66b559-2599-4ccb-86e2-9280c17f710a
2024-02-07 06:40:16,262 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,262 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,262 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,262 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4p2q1o8h
2024-02-07 06:40:16,262 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-azrw4x4q
2024-02-07 06:40:16,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af4cff64-a5ff-49e5-9937-7fb7899fe90a
2024-02-07 06:40:16,263 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b89a793-db19-42f0-8fad-c990357a3d29
2024-02-07 06:40:16,263 - distributed.worker - INFO - Starting Worker plugin PreImport-a5d73fb8-d1d9-4ee3-97e3-b245c7894e87
2024-02-07 06:40:16,263 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c7380e5-7c6a-4301-aabe-2f3220e864dc
2024-02-07 06:40:16,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,264 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42883
2024-02-07 06:40:16,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42883
2024-02-07 06:40:16,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36637
2024-02-07 06:40:16,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,269 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,269 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,269 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zkriquzy
2024-02-07 06:40:16,269 - distributed.worker - INFO - Starting Worker plugin PreImport-449d2c6e-b625-46ae-aab9-e6a4d6267fab
2024-02-07 06:40:16,269 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-394aa32b-8e9f-4439-862e-f0a9763957e1
2024-02-07 06:40:16,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5076b8d-efc1-445a-90a3-3e092f5208a7
2024-02-07 06:40:16,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,275 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46223
2024-02-07 06:40:16,275 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46223
2024-02-07 06:40:16,275 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45269
2024-02-07 06:40:16,275 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,275 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,275 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,275 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,275 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j2dvtvw9
2024-02-07 06:40:16,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,275 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb98ddf3-eafc-4475-b970-629b1d0a98ff
2024-02-07 06:40:16,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1e421fe-8e0b-418b-a61e-d3c005d578a6
2024-02-07 06:40:16,276 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39045
2024-02-07 06:40:16,276 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39045
2024-02-07 06:40:16,276 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44701
2024-02-07 06:40:16,276 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,276 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,276 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,276 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kwj9tzcl
2024-02-07 06:40:16,277 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8aeb96d1-698a-48d1-898f-c94dc3a81a47
2024-02-07 06:40:16,277 - distributed.worker - INFO - Starting Worker plugin PreImport-9c9af23a-d2db-4193-8456-82c07d6a6c35
2024-02-07 06:40:16,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,278 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7adf767a-987b-42b7-ab71-b85ada5f0ddd
2024-02-07 06:40:16,278 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35477
2024-02-07 06:40:16,278 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35477
2024-02-07 06:40:16,279 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42515
2024-02-07 06:40:16,279 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,279 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,279 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,279 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ldp2eb9
2024-02-07 06:40:16,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a866a56-1fca-461a-8d0d-7a37f4955612
2024-02-07 06:40:16,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:16,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:16,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:16,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46309
2024-02-07 06:40:16,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46309
2024-02-07 06:40:16,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42769
2024-02-07 06:40:16,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:16,373 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:16,373 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:16,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-07 06:40:16,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wwwek148
2024-02-07 06:40:16,373 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3716996e-01e7-497c-8420-cd73c57d1cfe
2024-02-07 06:40:16,374 - distributed.worker - INFO - Starting Worker plugin PreImport-d2c369a4-0d03-46d5-acf4-0ecec9da68c2
2024-02-07 06:40:16,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4396e9ca-92ed-416c-b45d-74ec93e84ba0
2024-02-07 06:40:17,386 - distributed.scheduler - INFO - Receive client connection: Client-be947b2e-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:17,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34914
2024-02-07 06:40:18,340 - distributed.worker - INFO - Starting Worker plugin PreImport-2cbafbd6-50e6-4438-98fd-58e72f638caa
2024-02-07 06:40:18,341 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6693c3d8-2e34-4256-a68d-5096fa9d0939
2024-02-07 06:40:18,343 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33407', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,380 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33407
2024-02-07 06:40:18,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34926
2024-02-07 06:40:18,382 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,383 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,383 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,404 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,414 - distributed.worker - INFO - Starting Worker plugin PreImport-ada75449-52e2-4dcd-8ebc-f5174e60935f
2024-02-07 06:40:18,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7394be02-3d18-4a88-9365-34ee4a27b3a3
2024-02-07 06:40:18,416 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,425 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41289', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,426 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41289
2024-02-07 06:40:18,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34930
2024-02-07 06:40:18,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,428 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,428 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,435 - distributed.worker - INFO - Starting Worker plugin PreImport-63849722-490c-4438-be83-eeda6e870b9f
2024-02-07 06:40:18,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8489c8e0-4cfc-40c6-84d9-443e44472f93
2024-02-07 06:40:18,436 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45237', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,436 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,436 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45237
2024-02-07 06:40:18,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34932
2024-02-07 06:40:18,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,438 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,438 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,441 - distributed.worker - INFO - Starting Worker plugin PreImport-681c9d12-0d3c-47f4-9469-9d41b64425c8
2024-02-07 06:40:18,442 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,455 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35477', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,456 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35477
2024-02-07 06:40:18,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34934
2024-02-07 06:40:18,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,458 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,461 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46223', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,464 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46223
2024-02-07 06:40:18,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34942
2024-02-07 06:40:18,464 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,465 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,465 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,466 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,475 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,492 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46309', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,493 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46309
2024-02-07 06:40:18,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34958
2024-02-07 06:40:18,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,496 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42883', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,496 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,496 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,496 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42883
2024-02-07 06:40:18,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34970
2024-02-07 06:40:18,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,499 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,499 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,505 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39045', status: init, memory: 0, processing: 0>
2024-02-07 06:40:18,505 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39045
2024-02-07 06:40:18,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34978
2024-02-07 06:40:18,507 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:18,508 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:18,508 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:18,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:18,532 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,534 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,534 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-07 06:40:18,546 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,548 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:18,552 - distributed.scheduler - INFO - Remove client Client-be947b2e-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:18,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34914; closing.
2024-02-07 06:40:18,552 - distributed.scheduler - INFO - Remove client Client-be947b2e-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:18,552 - distributed.scheduler - INFO - Close client connection: Client-be947b2e-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:18,553 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33003'. Reason: nanny-close
2024-02-07 06:40:18,554 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,554 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45551'. Reason: nanny-close
2024-02-07 06:40:18,555 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,555 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43843'. Reason: nanny-close
2024-02-07 06:40:18,555 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42883. Reason: nanny-close
2024-02-07 06:40:18,556 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,556 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36369'. Reason: nanny-close
2024-02-07 06:40:18,556 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33407. Reason: nanny-close
2024-02-07 06:40:18,556 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,556 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44201'. Reason: nanny-close
2024-02-07 06:40:18,556 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41289. Reason: nanny-close
2024-02-07 06:40:18,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40385'. Reason: nanny-close
2024-02-07 06:40:18,557 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45237. Reason: nanny-close
2024-02-07 06:40:18,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41195'. Reason: nanny-close
2024-02-07 06:40:18,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39045. Reason: nanny-close
2024-02-07 06:40:18,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,558 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34970; closing.
2024-02-07 06:40:18,558 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33539'. Reason: nanny-close
2024-02-07 06:40:18,558 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42883', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.5586147')
2024-02-07 06:40:18,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:18,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46309. Reason: nanny-close
2024-02-07 06:40:18,558 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,558 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46223. Reason: nanny-close
2024-02-07 06:40:18,559 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35477. Reason: nanny-close
2024-02-07 06:40:18,559 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34932; closing.
2024-02-07 06:40:18,560 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,560 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,560 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45237', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.560849')
2024-02-07 06:40:18,560 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,561 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34926; closing.
2024-02-07 06:40:18,561 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34930; closing.
2024-02-07 06:40:18,561 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,562 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:18,562 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,562 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,563 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,562 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34932>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34932>: Stream is closed
2024-02-07 06:40:18,564 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:18,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.564354')
2024-02-07 06:40:18,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.5647268')
2024-02-07 06:40:18,565 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34978; closing.
2024-02-07 06:40:18,566 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39045', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.565988')
2024-02-07 06:40:18,566 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34942; closing.
2024-02-07 06:40:18,566 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34958; closing.
2024-02-07 06:40:18,567 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46223', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.5670843')
2024-02-07 06:40:18,567 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.5675027')
2024-02-07 06:40:18,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34934; closing.
2024-02-07 06:40:18,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288018.56838')
2024-02-07 06:40:18,568 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:40:18,568 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34934>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-07 06:40:19,469 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:40:19,470 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:40:19,470 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:40:19,472 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:40:19,472 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-07 06:40:21,672 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:21,676 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-07 06:40:21,680 - distributed.scheduler - INFO - State start
2024-02-07 06:40:21,703 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:21,704 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:40:21,705 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-07 06:40:21,705 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:40:21,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39293'
2024-02-07 06:40:21,922 - distributed.scheduler - INFO - Receive client connection: Client-c3078275-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:21,933 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38450
2024-02-07 06:40:23,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:23,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:23,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:23,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36803
2024-02-07 06:40:23,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36803
2024-02-07 06:40:23,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37561
2024-02-07 06:40:23,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:23,369 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:23,369 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:23,369 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-07 06:40:23,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8uosqres
2024-02-07 06:40:23,369 - distributed.worker - INFO - Starting Worker plugin PreImport-5cbc6d1d-50db-46e9-9c8e-ef1518da1167
2024-02-07 06:40:23,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef46366b-339a-4a27-8c8f-c707f2d35c92
2024-02-07 06:40:23,683 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab0ef568-d633-4118-b146-11e4a120b379
2024-02-07 06:40:23,684 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:23,743 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36803', status: init, memory: 0, processing: 0>
2024-02-07 06:40:23,744 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36803
2024-02-07 06:40:23,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38456
2024-02-07 06:40:23,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:23,746 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:23,746 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:23,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:23,769 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:40:23,773 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:23,774 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:23,777 - distributed.scheduler - INFO - Remove client Client-c3078275-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:23,777 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38450; closing.
2024-02-07 06:40:23,777 - distributed.scheduler - INFO - Remove client Client-c3078275-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:23,778 - distributed.scheduler - INFO - Close client connection: Client-c3078275-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:23,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39293'. Reason: nanny-close
2024-02-07 06:40:23,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:23,780 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36803. Reason: nanny-close
2024-02-07 06:40:23,782 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38456; closing.
2024-02-07 06:40:23,782 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:23,782 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288023.7828918')
2024-02-07 06:40:23,783 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:40:23,783 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:24,544 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:40:24,545 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:40:24,545 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:40:24,546 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:40:24,546 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-07 06:40:26,670 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:26,675 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-07 06:40:26,678 - distributed.scheduler - INFO - State start
2024-02-07 06:40:26,700 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-07 06:40:26,701 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-07 06:40:26,702 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-07 06:40:26,702 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-07 06:40:26,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35041'
2024-02-07 06:40:27,298 - distributed.scheduler - INFO - Receive client connection: Client-c611cbec-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:27,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38542
2024-02-07 06:40:28,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-07 06:40:28,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-07 06:40:28,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-07 06:40:28,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39727
2024-02-07 06:40:28,394 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39727
2024-02-07 06:40:28,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46661
2024-02-07 06:40:28,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-07 06:40:28,394 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:28,394 - distributed.worker - INFO -               Threads:                          1
2024-02-07 06:40:28,394 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-07 06:40:28,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6o5sg668
2024-02-07 06:40:28,395 - distributed.worker - INFO - Starting Worker plugin PreImport-7ddd5810-11dd-49fe-ae62-bfcb3d28a937
2024-02-07 06:40:28,395 - distributed.worker - INFO - Starting Worker plugin RMMSetup-407eb730-07e2-45ee-9c7b-b032310a5511
2024-02-07 06:40:28,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-883b748a-a0c9-4fc8-89ff-d7be197b5457
2024-02-07 06:40:28,751 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:28,798 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39727', status: init, memory: 0, processing: 0>
2024-02-07 06:40:28,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39727
2024-02-07 06:40:28,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38566
2024-02-07 06:40:28,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-07 06:40:28,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-07 06:40:28,801 - distributed.worker - INFO - -------------------------------------------------
2024-02-07 06:40:28,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-07 06:40:28,838 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-07 06:40:28,842 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-07 06:40:28,851 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:28,853 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-07 06:40:28,855 - distributed.scheduler - INFO - Remove client Client-c611cbec-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:28,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38542; closing.
2024-02-07 06:40:28,855 - distributed.scheduler - INFO - Remove client Client-c611cbec-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:28,856 - distributed.scheduler - INFO - Close client connection: Client-c611cbec-c583-11ee-b0b0-d8c49764f6bb
2024-02-07 06:40:28,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35041'. Reason: nanny-close
2024-02-07 06:40:28,857 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-07 06:40:28,859 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39727. Reason: nanny-close
2024-02-07 06:40:28,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38566; closing.
2024-02-07 06:40:28,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-07 06:40:28,863 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707288028.8638165')
2024-02-07 06:40:28,864 - distributed.scheduler - INFO - Lost all workers
2024-02-07 06:40:28,866 - distributed.nanny - INFO - Worker closed
2024-02-07 06:40:29,472 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-07 06:40:29,472 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-07 06:40:29,472 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-07 06:40:29,473 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-07 06:40:29,474 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46307 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33795 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41933 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35503 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42167 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34377 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46683 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40595 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46663 instead
  warnings.warn(
[1707288213.785712] [dgx13:68756:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:49181) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35739 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46251 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34659 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44703 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46089 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44027 instead
  warnings.warn(
[1707288468.499334] [dgx13:71554:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:49309) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44005 instead
  warnings.warn(
[1707288502.065250] [dgx13:72395:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:55832) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40725 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34059 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33693 instead
  warnings.warn(
[1707288612.077835] [dgx13:73748:0]            sock.c:470  UCX  ERROR bind(fd=121 addr=0.0.0.0:42017) failed: Address already in use
[1707288614.802055] [dgx13:73748:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:51403) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39971 instead
  warnings.warn(
[1707288633.609043] [dgx13:73988:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:51375) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36221 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39077 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41593 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39195 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45729 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46649 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33907 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41483 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35173 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44061 instead
  warnings.warn(
Process SpawnProcess-52:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5278, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5279, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2034, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1914, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 532, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 296, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 419, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 109, in cudf._lib.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41239 instead
  warnings.warn(
Process SpawnProcess-53:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5278, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5279, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2034, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1914, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 532, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 296, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 419, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 109, in cudf._lib.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46505 instead
  warnings.warn(
2024-02-07 06:59:04,835 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-9b226b86-0664-4aff-b799-4f52843f9bdd
Function:  _run_coroutine_on_worker
args:      (196655607828031725379207250374022628748, <function shuffle_task at 0x7f20d27c9a60>, ('explicit-comms-shuffle-73594999631dcad0d65c630e2f6b18af', {0: set(), 1: {('from_pandas-042f56ad2fd1cc4d095a550ce07a5908', 0)}, 2: set()}, {0: {0}, 1: {1}, 2: set()}, ['key'], 2, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
