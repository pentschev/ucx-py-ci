2023-02-01 22:46:16,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,491 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,502 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,502 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,545 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,599 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:16,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:16,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:19136:0:19136] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19136) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f6ab18566b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f6ab185688f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f6ab1856bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f6b3915e980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f6ab1ad21b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f6ab1afb638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f6ac80270ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f6ac8027674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f6ac8029a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f6ab18608a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f6ac8029b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f6ab1acef5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f6ab1d7ac79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x55f0c7e57b08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55f0c7e48112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55f0c7e4127a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55f0c7e52c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x55f0c7e6770e]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f6ad37762fe]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55f0c7e4b2bc]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x55f0c7dfe817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x55f0c7e49f83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55f0c7e47d36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55f0c7e4127a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55f0c7e52c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55f0c7e46fa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55f0c7e4127a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x55f0c7e60935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55f0c7e61104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x55f0c7f27fc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55f0c7e4b2bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55f0c7e461bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x55f0c7e60c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55f0c7e461bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55f0c7e4127a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55f0c7e52c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55f0c7e4281b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55f0c7e52ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55f0c7e42568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55f0c7e4127a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55f0c7e52c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55f0c7e433cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55f0c7e4127a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55f0c7e40f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55f0c7e40eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55f0c7ef18bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x55f0c7f1fadc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x55f0c7f1bc24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55f0c7f137ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55f0c7f136bd]
=================================
2023-02-01 22:46:24,617 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:33141 -> ucx://127.0.0.1:34085
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fbb14105100, tag: 0x51bf597212cedcf7, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:24,617 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42573 -> ucx://127.0.0.1:34085
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f32cc0e5100, tag: 0xfc4dce79f28164ca, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:24,717 - distributed.nanny - WARNING - Restarting worker
[dgx13:19138:0:19138] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19138) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7ff81ce236b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7ff81ce2388f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7ff81ce23bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7ff8a04ea980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7ff81d09f1b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7ff81d0c8638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7ff81cbdb0ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7ff81cbdb674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7ff81cbdda78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7ff81ce2d8a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7ff81cbddb1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7ff81d09bf5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7ff81d347c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x563fe315cb08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x563fe314d112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x563fe314627a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563fe3157c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x563fe316c70e]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7ff83aaff2fe]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x563fe31502bc]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x563fe3103817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x563fe314ef83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x563fe314cd36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x563fe314627a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563fe3157c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x563fe314bfa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x563fe314627a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x563fe3165935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x563fe3166104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x563fe322cfc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x563fe31502bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x563fe314b1bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x563fe3165c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x563fe314b1bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x563fe314627a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563fe3157c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563fe314781b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x563fe3157ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x563fe3147568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x563fe314627a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563fe3157c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x563fe31483cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x563fe314627a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x563fe3145f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x563fe3145eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x563fe31f68bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x563fe3224adc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x563fe3220c24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x563fe32187ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x563fe32186bd]
=================================
2023-02-01 22:46:25,002 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42573 -> ucx://127.0.0.1:46887
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f32cc0e5100, tag: 0xb57eb9a56a0c9703, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:25,198 - distributed.nanny - WARNING - Restarting worker
[dgx13:19141:0:19141] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19141) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f428ddcd6b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f428ddcd88f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f428ddcdbb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f43115cd980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f42a01a81b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f42a01d1638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f428db850ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f428db85674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f428db87a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f428ddd78a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f428db87b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f42a01a4f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f42a0450c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x55c09d3c9b08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55c09d3ba112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55c09d3b327a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c09d3c4c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x55c09d3d970e]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f42abbe32fe]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c09d3bd2bc]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x55c09d370817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x55c09d3bbf83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55c09d3b9d36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55c09d3b327a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c09d3c4c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55c09d3b8fa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55c09d3b327a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x55c09d3d2935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55c09d3d3104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x55c09d499fc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c09d3bd2bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c09d3b81bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x55c09d3d2c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c09d3b81bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55c09d3b327a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c09d3c4c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c09d3b481b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55c09d3c4ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55c09d3b4568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55c09d3b327a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c09d3c4c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55c09d3b53cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55c09d3b327a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55c09d3b2f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55c09d3b2eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55c09d4638bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x55c09d491adc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x55c09d48dc24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55c09d4857ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55c09d4856bd]
=================================
Task exception was never retrieved
future: <Task finished name='Task-1029' coro=<_listener_handler_coroutine() done, defined at /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1024' coro=<_listener_handler_coroutine() done, defined at /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-02-01 22:46:25,563 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35947 -> ucx://127.0.0.1:33245
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f2334803240, tag: 0x28fb362703f0d0d3, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-1013' coro=<_listener_handler_coroutine() done, defined at /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-02-01 22:46:25,565 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33245
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7fbb14105180, tag: 0xc261347eaafc3968, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 328, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7fbb14105180, tag: 0xc261347eaafc3968, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in _get_data
    comm = await rpc.connect(worker)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1439, in connect
    return await connect_attempt
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1375, in _connect
    comm = await connect(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 333, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:33245 after 30 s
2023-02-01 22:46:25,566 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33245
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f67fc085200, tag: 0x468574540ee87243, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 328, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f67fc085200, tag: 0x468574540ee87243, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in _get_data
    comm = await rpc.connect(worker)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1439, in connect
    return await connect_attempt
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1375, in _connect
    comm = await connect(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 333, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:33245 after 30 s
2023-02-01 22:46:25,568 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33245
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f32cc0e51c0, tag: 0xb651a3a9626e8882, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 328, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f32cc0e51c0, tag: 0xb651a3a9626e8882, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in _get_data
    comm = await rpc.connect(worker)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1439, in connect
    return await connect_attempt
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1375, in _connect
    comm = await connect(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 333, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:33245 after 30 s
Task exception was never retrieved
future: <Task finished name='Task-1089' coro=<_listener_handler_coroutine() done, defined at /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Endpoint timeout')>
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Endpoint timeout
2023-02-01 22:46:25,666 - distributed.nanny - WARNING - Restarting worker
[dgx13:19117:0:19117] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19117) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f232ce236b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f232ce2388f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f232ce23bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f23b0532980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f232d09f1b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f232d0c8638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f232cbdb0ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f232cbdb674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f232cbdda78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f232ce2d8a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f232cbddb1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f232d09bf5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f232d347c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x557fd930cb08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x557fd92fd112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x557fd92f627a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x557fd9307c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x557fd931c70e]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f234ab432fe]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x557fd93002bc]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x557fd92b3817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x557fd92fef83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x557fd92fcd36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x557fd92f627a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x557fd9307c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x557fd92fbfa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x557fd92f627a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x557fd9315935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x557fd9316104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x557fd93dcfc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x557fd93002bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x557fd92fb1bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x557fd9315c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x557fd92fb1bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x557fd92f627a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x557fd9307c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x557fd92f781b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x557fd9307ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x557fd92f7568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x557fd92f627a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x557fd9307c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x557fd92f83cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x557fd92f627a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x557fd92f5f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x557fd92f5eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x557fd93a68bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x557fd93d4adc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x557fd93d0c24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x557fd93c87ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x557fd93c86bd]
=================================
2023-02-01 22:46:25,984 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44601 -> ucx://127.0.0.1:38225
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f67fc085280, tag: 0x50a706d14d21ff14, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:25,984 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38225
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fbb14105200, tag: 0xbf90f5e82ec3d8a8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fbb14105200, tag: 0xbf90f5e82ec3d8a8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-02-01 22:46:25,985 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38225
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f67fc085180, tag: 0x5380808576a0c677, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f67fc085180, tag: 0x5380808576a0c677, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-02-01 22:46:25,985 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:33141 -> ucx://127.0.0.1:38225
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fbb14105240, tag: 0x2bfe233d6095d006, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:25,986 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35947 -> ucx://127.0.0.1:38225
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f2334803140, tag: 0xee7913f308a2d1c2, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:26,048 - distributed.nanny - WARNING - Restarting worker
2023-02-01 22:46:26,057 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38225
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f32cc0e5140, tag: 0x546ceac7f2c93a1c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f32cc0e5140, tag: 0x546ceac7f2c93a1c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-02-01 22:46:26,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:26,631 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:27,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:27,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:27,296 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-0259a3f282df492a8f862c71ab98153c', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7f2e18
args:      (               key   payload
shuffle                     
7           113793  39780415
7           113798  75684970
7            22209  22922262
7           113807  31917689
7            22210  57634316
...            ...       ...
7        799984870  58479941
7        799984880  32769867
7        799984883  78970665
7        799984887  49757335
7        799984890  56435545

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-02-01 22:46:27,416 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7f2d75
args:      ([                key   payload
38116     844619102  30826908
41152     866115437   7937146
38137     603404562  86262275
41153     851270661  15466363
38139     848100724  90038457
...             ...       ...
99985309  800545058  29567365
99985392  814077156  26263905
99985393  825580476  14974775
99985394  306086206  71853255
99985399  608156078  77538069

[12498632 rows x 2 columns],                 key   payload
31909     112665559  35415564
31914     908039404  39369705
31929     941152421  27338905
31930     114545948  90125017
31933     918854898  56834124
...             ...       ...
99974423  618051730  51942309
99986769  957623416  51447451
99974429  901364364  14836711
99986774  323169964  83847272
99986778  922176227  40978169

[12502237 rows x 2 columns],                  key   payload
134243    1041217846  10923273
134249    1066568094  34001185
134264     131003443  10516942
134269    1048011858  48132184
134271    1056917798  19289612
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-02-01 22:46:27,424 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7f1dd9
args:      ([                key   payload
38115     843299737  35402599
41157     848983421  39156851
38127     843946052  76608098
41168       6684723  28702622
38129     817896391  48315311
...             ...       ...
99985300  505885115    521166
99985377  844105707  26850108
99985380  838806491   8423208
99985391  840440003  72296630
99985406  841426005  48134370

[12500589 rows x 2 columns],                 key   payload
31904     950310834  49355160
31915     913045907  71912211
31919     935589403  24998810
31935     215925114  22377575
31971     713284658  25320338
...             ...       ...
99956216  925967035  94543908
99956222  909487854  74878692
99986776  518582432  88500234
99974413  959698615  73963790
99974420  220430563  35299814

[12501715 rows x 2 columns],                  key   payload
134245    1023842756  28107052
134258    1044838415  41454832
52450     1034541170  52024175
52460     1038495711  58632235
52470     1067300861  56630403
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-02-01 22:46:27,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:27,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:27,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:27,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:19130:0:19130] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19130) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7fbb1440c6b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7fbb1440c88f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7fbb1440cbb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7fbb97afc980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fbb146881b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fbb146b1638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7fbb141c40ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7fbb141c4674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7fbb141c6a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fbb144168a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fbb141c6b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fbb14684f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7fbb14930c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x559de8d67b08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x559de8d58112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x559de8d5127a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x559de8d62c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147a16) [0x559de8d70a16]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x2579b1) [0x559de8e809b1]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x559de8d0e817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x559de8d59f83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x559de8d57d36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x559de8d5127a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x559de8d62c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x559de8d56fa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x559de8d5127a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x559de8d70935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x559de8d71104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x559de8e37fc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x559de8d5b2bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x559de8d561bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x559de8d70c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x559de8d561bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x559de8d5127a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x559de8d62c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x559de8d5281b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x559de8d62ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x559de8d52568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x559de8d5127a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x559de8d62c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x559de8d533cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x559de8d5127a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x559de8d50f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x559de8d50eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x559de8e018bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x559de8e2fadc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x559de8e2bc24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x559de8e237ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x559de8e236bd]
=================================
[1675320390.877349] [dgx13:19122:0]    cuda_copy_md.c:172  UCX  ERROR   cuMemAlloc_v2((CUdeviceptr*)address_p, *length_p)() failed: out of memory
[1675320390.877365] [dgx13:19122:0]         uct_mem.c:155  UCX  ERROR   failed to allocate 536870912 bytes using md cuda_cpy for ucp_rndv_frags: Input/output error
[1675320390.877370] [dgx13:19122:0]           mpool.c:226  UCX  ERROR Failed to allocate memory pool (name=ucp_rndv_frags) chunk: Out of memory
[dgx13:19122:0:19122]        rndv.c:1332 Fatal: failed to allocate fragment memory buffer
==== backtrace (tid:  19122) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f2321b026b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f2321aff668]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2a749) [0x7f2321aff749]
 3  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(+0x6d584) [0x7f2321da2584]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_receive+0x45c) [0x7f2321da30cc]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_tag_rndv_process_rts+0x20a) [0x7f2321db87aa]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f23218ba0ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f23218ba674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f23218bca78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f2321b0c8a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f23218bcb1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f2321d7af5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f2334173c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x55ab0cd2bb08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55ab0cd1c112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55ab0cd1527a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ab0cd26c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x55ab0cd3b70e]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f233fb5f2fe]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55ab0cd1f2bc]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x55ab0ccd2817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x55ab0cd1df83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55ab0cd1bd36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55ab0cd1527a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ab0cd26c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55ab0cd1afa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55ab0cd1527a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x55ab0cd34935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55ab0cd35104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x55ab0cdfbfc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55ab0cd1f2bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55ab0cd1a1bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x55ab0cd34c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55ab0cd1a1bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55ab0cd1527a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ab0cd26c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ab0cd1681b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55ab0cd26ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55ab0cd16568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55ab0cd1527a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ab0cd26c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55ab0cd173cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55ab0cd1527a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55ab0cd14f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55ab0cd14eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55ab0cdc58bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x55ab0cdf3adc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x55ab0cdefc24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55ab0cde77ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55ab0cde76bd]
=================================
[1675320390.890096] [dgx13:19132:0]    cuda_copy_md.c:172  UCX  ERROR   cuMemAlloc_v2((CUdeviceptr*)address_p, *length_p)() failed: out of memory
[1675320390.890110] [dgx13:19132:0]         uct_mem.c:155  UCX  ERROR   failed to allocate 536870912 bytes using md cuda_cpy for ucp_rndv_frags: Input/output error
[1675320390.890116] [dgx13:19132:0]           mpool.c:226  UCX  ERROR Failed to allocate memory pool (name=ucp_rndv_frags) chunk: Out of memory
[dgx13:19132:0:19132]        rndv.c:1332 Fatal: failed to allocate fragment memory buffer
==== backtrace (tid:  19132) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f32cccf06b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f32ccced668]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2a749) [0x7f32ccced749]
 3  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(+0x6d584) [0x7f32ccf90584]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_receive+0x45c) [0x7f32ccf910cc]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_tag_recv_nbx+0xc0a) [0x7f32ccfa9afa]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_tag_recv_nb+0x56) [0x7f32ccfa8ed6]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x501e5) [0x7f32cd23e1e5]
 8  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x149dc7) [0x561b7814fdc7]
 9  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0x158) [0x561b7814e1a8]
10  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x561b78134d36]
11  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x561b7812e27a]
12  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561b7813fc05]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x561b781303cb]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x561b7812e27a]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561b7813fc05]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x561b781303cb]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561b78135923]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x561b7815470e]
36  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f32dc9c72fe]
37  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f32dc9c7b4e]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x561b781382bc]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x561b780eb817]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x561b78136f83]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x561b78134d36]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x561b7813fef3]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561b7812f81b]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x561b7813fef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561b7812f81b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x561b7813fef3]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561b7812f81b]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x561b7813fef3]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561b7812f81b]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x561b7812e27a]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561b7813fc05]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x561b78133fa7]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x561b7812e27a]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x561b7814d935]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x561b7814e104]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x561b78214fc8]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x561b781382bc]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x561b781331bb]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x561b7813fef3]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x561b7814dc72]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x561b781331bb]
=================================
[dgx13:19708:0:19708] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19708) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f4820d596b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f4820d5988f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f4820d59bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f48962e4980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f4820fd51b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f4820ffe638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f4820b110ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f4820b11674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f4820b13a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f4820d638a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f4820b13b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f4820fd1f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f482127dc79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x55adeac54b08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55adeac45112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55adeac3e27a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55adeac4fc05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147a16) [0x55adeac5da16]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x2579b1) [0x55adead6d9b1]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x55adeabfb817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x55adeac46f83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55adeac44d36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55adeac3e27a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55adeac4fc05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55adeac43fa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55adeac3e27a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x55adeac5d935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55adeac5e104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x55adead24fc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55adeac482bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55adeac431bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x55adeac5dc72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55adeac431bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55adeac3e27a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55adeac4fc05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55adeac3f81b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55adeac4fef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55adeac3f568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55adeac3e27a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55adeac4fc05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55adeac403cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55adeac3e27a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55adeac3df07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55adeac3deb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55adeacee8bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x55adead1cadc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x55adead18c24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55adead107ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55adead106bd]
=================================
[dgx13:19762:0:19762] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  19762) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f76c543e6b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f76c543e88f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f76c543ebb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f773a99f980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f76c56ba1b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f76c56e3638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f76c51f60ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f76c51f6674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f76c51f8a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f76c54488a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f76c51f8b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f76c56b6f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f76c5962c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13eb08) [0x55e2ddbe8b08]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55e2ddbd9112]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55e2ddbd227a]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e2ddbe3c05]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14e70e) [0x55e2ddbf870e]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f76d4fb22fe]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55e2ddbdc2bc]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xe5817) [0x55e2ddb8f817]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x130f83) [0x55e2ddbdaf83]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55e2ddbd8d36]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55e2ddbd227a]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e2ddbe3c05]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55e2ddbd7fa7]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55e2ddbd227a]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147935) [0x55e2ddbf1935]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55e2ddbf2104]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20efc8) [0x55e2ddcb8fc8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55e2ddbdc2bc]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55e2ddbd71bb]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x147c72) [0x55e2ddbf1c72]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55e2ddbd71bb]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55e2ddbd227a]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e2ddbe3c05]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e2ddbd381b]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x139ef3) [0x55e2ddbe3ef3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55e2ddbd3568]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55e2ddbd227a]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e2ddbe3c05]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55e2ddbd43cb]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12827a) [0x55e2ddbd227a]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55e2ddbd1f07]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55e2ddbd1eb9]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55e2ddc828bb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x206adc) [0x55e2ddcb0adc]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x202c24) [0x55e2ddcacc24]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55e2ddca47ed]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55e2ddca46bd]
=================================
2023-02-01 22:46:31,071 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35947
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #107] ep: 0x7f67fc0851c0, tag: 0x66b33e6e8076aabd, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #107] ep: 0x7f67fc0851c0, tag: 0x66b33e6e8076aabd, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-02-01 22:46:31,071 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35947
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd690125100, tag: 0xe6720be8f7b3226b, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd690125100, tag: 0xe6720be8f7b3226b, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-02-01 22:46:31,085 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33141
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd690125140, tag: 0x1024510485fd29aa, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd690125140, tag: 0x1024510485fd29aa, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-02-01 22:46:31,085 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44601 -> ucx://127.0.0.1:33141
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #102] ep: 0x7f67fc085300, tag: 0x1b19370f5f12e16b, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1767, in get_data
    response = await comm.read(deserializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #102] ep: 0x7f67fc085300, tag: 0x1b19370f5f12e16b, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-02-01 22:46:31,086 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:54669 -> ucx://127.0.0.1:33141
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fd6901252c0, tag: 0xcbfbf7297ceecf94, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:31,105 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42573
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd6901251c0, tag: 0xf6807f1424f661ba, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd6901251c0, tag: 0xf6807f1424f661ba, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
Task exception was never retrieved
future: <Task finished name='Task-516' coro=<_listener_handler_coroutine() done, defined at /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Endpoint timeout')>
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout
2023-02-01 22:46:31,153 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41723
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd690125200, tag: 0xda06d164099185ae, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd690125200, tag: 0xda06d164099185ae, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-02-01 22:46:31,158 - distributed.nanny - WARNING - Restarting worker
2023-02-01 22:46:31,217 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44601 -> ucx://127.0.0.1:41723
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f67fc085200, tag: 0xf436ea9b91523e4, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:31,217 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44601 -> ucx://127.0.0.1:35741
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f67fc0853c0, tag: 0x61bbcc03cbf6ed8, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-02-01 22:46:31,218 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41723
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f67fc085280, tag: 0x73d051a09e1989c1, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f67fc085280, tag: 0x73d051a09e1989c1, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-02-01 22:46:31,239 - distributed.nanny - WARNING - Restarting worker
2023-02-01 22:46:31,289 - distributed.nanny - WARNING - Restarting worker
2023-02-01 22:46:31,353 - distributed.nanny - WARNING - Restarting worker
2023-02-01 22:46:31,432 - distributed.nanny - WARNING - Restarting worker
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 230, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Exception ignored in: 'cupy.cuda.thrust.cupy_malloc'
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 230, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-02-01 22:46:32,536 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 3)
Function:  generate_chunk
args:      (3, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "RuntimeError('transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-02-01 22:46:33,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:33,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:33,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:33,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:33,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:33,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:33,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:33,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:33,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-02-01 22:46:33,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-02-01 22:46:34,698 - distributed.core - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-02-01 22:46:34,699 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-02-01 22:46:34,709 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-02-01 22:46:34,709 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-02-01 22:46:34,723 - distributed.core - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-02-01 22:46:34,723 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
