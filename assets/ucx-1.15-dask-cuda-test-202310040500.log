============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-04 05:37:27,464 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:27,468 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34785 instead
  warnings.warn(
2023-10-04 05:37:27,472 - distributed.scheduler - INFO - State start
2023-10-04 05:37:27,493 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:27,494 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-04 05:37:27,494 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34785/status
2023-10-04 05:37:27,494 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:37:27,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33625'
2023-10-04 05:37:27,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46273'
2023-10-04 05:37:27,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38389'
2023-10-04 05:37:27,809 - distributed.scheduler - INFO - Receive client connection: Client-197c8748-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:27,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35687'
2023-10-04 05:37:27,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49444
2023-10-04 05:37:29,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:29,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:29,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:29,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:29,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:29,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:29,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:29,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:29,504 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:29,504 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:29,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:29,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-04 05:37:29,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44463
2023-10-04 05:37:29,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44463
2023-10-04 05:37:29,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35677
2023-10-04 05:37:29,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-04 05:37:29,525 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:29,525 - distributed.worker - INFO -               Threads:                          4
2023-10-04 05:37:29,525 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-04 05:37:29,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-k4f8w0m7
2023-10-04 05:37:29,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8081988a-599d-4fc1-adf6-ff634fe5dc32
2023-10-04 05:37:29,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b31ca0dd-a8af-4c58-8e0a-fc3f0fa00b78
2023-10-04 05:37:29,525 - distributed.worker - INFO - Starting Worker plugin PreImport-a703adbd-e6fb-4191-8a16-4f0bfb9bb581
2023-10-04 05:37:29,525 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,396 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44463', status: init, memory: 0, processing: 0>
2023-10-04 05:37:30,398 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44463
2023-10-04 05:37:30,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49506
2023-10-04 05:37:30,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:30,400 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-04 05:37:30,400 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-04 05:37:30,952 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38643
2023-10-04 05:37:30,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38643
2023-10-04 05:37:30,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38467
2023-10-04 05:37:30,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-04 05:37:30,953 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,953 - distributed.worker - INFO -               Threads:                          4
2023-10-04 05:37:30,953 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-04 05:37:30,953 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9zf3ruoi
2023-10-04 05:37:30,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3ee5b34-7c64-4e28-a2f5-ae888f3a33a1
2023-10-04 05:37:30,954 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bd964c4-ba99-4fb6-92fe-3314a2d46f84
2023-10-04 05:37:30,954 - distributed.worker - INFO - Starting Worker plugin PreImport-3fa582eb-91bb-4a49-a3dc-34ae4bf8dd97
2023-10-04 05:37:30,954 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,974 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37765
2023-10-04 05:37:30,974 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37765
2023-10-04 05:37:30,974 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37249
2023-10-04 05:37:30,974 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-04 05:37:30,974 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,974 - distributed.worker - INFO -               Threads:                          4
2023-10-04 05:37:30,975 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-04 05:37:30,975 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-wnmemcld
2023-10-04 05:37:30,975 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2ca88b4-5930-4b9b-ba4c-258e5898913f
2023-10-04 05:37:30,975 - distributed.worker - INFO - Starting Worker plugin PreImport-e00002cf-122f-4686-a561-be393b281021
2023-10-04 05:37:30,975 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c470cabe-35da-47a3-ba26-63d36884040b
2023-10-04 05:37:30,976 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,991 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38643', status: init, memory: 0, processing: 0>
2023-10-04 05:37:30,991 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38643
2023-10-04 05:37:30,991 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33082
2023-10-04 05:37:30,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:30,995 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-04 05:37:30,995 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:30,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-04 05:37:31,003 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37765', status: init, memory: 0, processing: 0>
2023-10-04 05:37:31,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37765
2023-10-04 05:37:31,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33092
2023-10-04 05:37:31,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:31,006 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-04 05:37:31,006 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:31,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-04 05:37:31,016 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40749
2023-10-04 05:37:31,016 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40749
2023-10-04 05:37:31,016 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38213
2023-10-04 05:37:31,017 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-04 05:37:31,017 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:31,017 - distributed.worker - INFO -               Threads:                          4
2023-10-04 05:37:31,017 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-04 05:37:31,017 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mutg10v7
2023-10-04 05:37:31,017 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b58a637-e21b-48a9-8757-a655ea291d89
2023-10-04 05:37:31,017 - distributed.worker - INFO - Starting Worker plugin PreImport-42493a72-b682-44a6-90ff-5fc2b45edacb
2023-10-04 05:37:31,018 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32ae2749-45a4-4141-abf2-0c174e444e6a
2023-10-04 05:37:31,018 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:31,050 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40749', status: init, memory: 0, processing: 0>
2023-10-04 05:37:31,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40749
2023-10-04 05:37:31,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33098
2023-10-04 05:37:31,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:31,054 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-04 05:37:31,054 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:31,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-04 05:37:31,096 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-04 05:37:31,096 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-04 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-04 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-04 05:37:31,101 - distributed.scheduler - INFO - Remove client Client-197c8748-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:31,102 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49444; closing.
2023-10-04 05:37:31,102 - distributed.scheduler - INFO - Remove client Client-197c8748-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:31,102 - distributed.scheduler - INFO - Close client connection: Client-197c8748-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:31,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33625'. Reason: nanny-close
2023-10-04 05:37:31,103 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:31,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46273'. Reason: nanny-close
2023-10-04 05:37:31,104 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:31,105 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37765. Reason: nanny-close
2023-10-04 05:37:31,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38389'. Reason: nanny-close
2023-10-04 05:37:31,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:31,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35687'. Reason: nanny-close
2023-10-04 05:37:31,105 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38643. Reason: nanny-close
2023-10-04 05:37:31,106 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:31,106 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40749. Reason: nanny-close
2023-10-04 05:37:31,106 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44463. Reason: nanny-close
2023-10-04 05:37:31,107 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-04 05:37:31,107 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33092; closing.
2023-10-04 05:37:31,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397851.1073499')
2023-10-04 05:37:31,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-04 05:37:31,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-04 05:37:31,108 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:31,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33082; closing.
2023-10-04 05:37:31,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397851.1095533')
2023-10-04 05:37:31,109 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-04 05:37:31,109 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:31,109 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49506; closing.
2023-10-04 05:37:31,110 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33098; closing.
2023-10-04 05:37:31,110 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:31,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397851.1104417')
2023-10-04 05:37:31,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397851.1107194')
2023-10-04 05:37:31,110 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:37:31,111 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:32,119 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:37:32,120 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:37:32,120 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:37:32,121 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-04 05:37:32,122 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-04 05:37:34,271 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:34,275 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42313 instead
  warnings.warn(
2023-10-04 05:37:34,279 - distributed.scheduler - INFO - State start
2023-10-04 05:37:34,342 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:34,343 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:37:34,344 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42313/status
2023-10-04 05:37:34,344 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:37:34,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33397'
2023-10-04 05:37:34,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42001'
2023-10-04 05:37:34,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44661'
2023-10-04 05:37:34,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39601'
2023-10-04 05:37:34,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38433'
2023-10-04 05:37:34,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45275'
2023-10-04 05:37:34,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33513'
2023-10-04 05:37:34,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35239'
2023-10-04 05:37:35,935 - distributed.scheduler - INFO - Receive client connection: Client-1d7a6d3e-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:35,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48716
2023-10-04 05:37:36,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,417 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,417 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,422 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:36,460 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:36,460 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:36,464 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:38,800 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34911
2023-10-04 05:37:38,802 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34911
2023-10-04 05:37:38,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41051
2023-10-04 05:37:38,803 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:38,803 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:38,803 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:38,803 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:38,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oarr3_im
2023-10-04 05:37:38,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d9b7f2c-690a-4015-8b4d-840ff984613f
2023-10-04 05:37:38,915 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2105eda8-f8bf-4d1b-845a-8f2b249bf810
2023-10-04 05:37:38,915 - distributed.worker - INFO - Starting Worker plugin PreImport-21ab28db-9f9d-445f-9b0e-703383c8cd05
2023-10-04 05:37:38,916 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:38,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34911', status: init, memory: 0, processing: 0>
2023-10-04 05:37:38,954 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34911
2023-10-04 05:37:38,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48732
2023-10-04 05:37:38,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:38,957 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:38,957 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:38,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,003 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33811
2023-10-04 05:37:39,004 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33811
2023-10-04 05:37:39,004 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37503
2023-10-04 05:37:39,004 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,004 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,004 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,004 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,004 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3glti0et
2023-10-04 05:37:39,005 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ef0834e-c260-466e-9ffe-cef03057a31b
2023-10-04 05:37:39,004 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36937
2023-10-04 05:37:39,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36937
2023-10-04 05:37:39,005 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74379f75-a757-402b-a9ab-e0d060d84626
2023-10-04 05:37:39,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38139
2023-10-04 05:37:39,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,006 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,006 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,006 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rghtj51y
2023-10-04 05:37:39,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2dc72738-a549-4b1a-85e1-b901faffbc3d
2023-10-04 05:37:39,131 - distributed.worker - INFO - Starting Worker plugin PreImport-3fd5ceb2-a8ba-4d41-9c7d-a01f79b7efda
2023-10-04 05:37:39,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5d29e29-cefd-4256-a900-992e0af4f2a2
2023-10-04 05:37:39,132 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,133 - distributed.worker - INFO - Starting Worker plugin PreImport-d78b84f9-0705-4c08-a09e-48be5bb33c24
2023-10-04 05:37:39,133 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,175 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36937', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,176 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36937
2023-10-04 05:37:39,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48748
2023-10-04 05:37:39,177 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33811', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,177 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33811
2023-10-04 05:37:39,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48744
2023-10-04 05:37:39,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,178 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,178 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,180 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,239 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42633
2023-10-04 05:37:39,239 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42333
2023-10-04 05:37:39,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42633
2023-10-04 05:37:39,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42333
2023-10-04 05:37:39,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44677
2023-10-04 05:37:39,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37979
2023-10-04 05:37:39,240 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,240 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,240 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,240 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-omiylwpn
2023-10-04 05:37:39,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-su85fc39
2023-10-04 05:37:39,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aab35bf4-1f60-4ff2-936e-98b3e78fb87e
2023-10-04 05:37:39,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7d12ba1-d870-4d33-9dca-d401c9cffbbf
2023-10-04 05:37:39,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33961
2023-10-04 05:37:39,245 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33961
2023-10-04 05:37:39,245 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36061
2023-10-04 05:37:39,245 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,246 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,246 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,246 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,246 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wuj02scb
2023-10-04 05:37:39,246 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f68462ba-eda0-468d-a361-6d58ba264a9b
2023-10-04 05:37:39,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45373
2023-10-04 05:37:39,247 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45373
2023-10-04 05:37:39,247 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42733
2023-10-04 05:37:39,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,248 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,248 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-712ef2dc
2023-10-04 05:37:39,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f6825e2-33e3-4092-944b-897a895c24d0
2023-10-04 05:37:39,249 - distributed.worker - INFO - Starting Worker plugin PreImport-b2c4835a-95db-47d2-964c-49189a029541
2023-10-04 05:37:39,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-087a100a-416d-4dfa-91b4-7c392aadcb13
2023-10-04 05:37:39,251 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46879
2023-10-04 05:37:39,252 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46879
2023-10-04 05:37:39,252 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34989
2023-10-04 05:37:39,252 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,252 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,252 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:39,252 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:39,252 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qt0p4m8o
2023-10-04 05:37:39,253 - distributed.worker - INFO - Starting Worker plugin PreImport-a07a81a7-01a8-43d1-a975-957b0dcee2ca
2023-10-04 05:37:39,253 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-648f3bb4-fdb0-4b95-9524-50ad9b5c86d0
2023-10-04 05:37:39,253 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b605f79e-2c1c-43ac-b967-229113e31388
2023-10-04 05:37:39,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9c0db88-ed37-4369-b8d2-0ed3081b7104
2023-10-04 05:37:39,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4fafc60-83b5-4f6d-bcf5-7355c74df381
2023-10-04 05:37:39,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2106946e-576a-498a-8717-c8148226f98f
2023-10-04 05:37:39,383 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,383 - distributed.worker - INFO - Starting Worker plugin PreImport-b642980b-36e5-46a7-8c6d-76853b66fc59
2023-10-04 05:37:39,383 - distributed.worker - INFO - Starting Worker plugin PreImport-3a276482-90d1-42be-a0ed-4f52ae3b6e08
2023-10-04 05:37:39,383 - distributed.worker - INFO - Starting Worker plugin PreImport-492a019e-7f40-403f-ae86-6a9a66bcc246
2023-10-04 05:37:39,383 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,384 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,384 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,384 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,410 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42633', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,411 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42633
2023-10-04 05:37:39,411 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48790
2023-10-04 05:37:39,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33961', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,412 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33961
2023-10-04 05:37:39,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48800
2023-10-04 05:37:39,413 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,413 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45373', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,414 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45373
2023-10-04 05:37:39,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48774
2023-10-04 05:37:39,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42333', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,415 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,415 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,415 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42333
2023-10-04 05:37:39,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48780
2023-10-04 05:37:39,416 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,416 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46879', status: init, memory: 0, processing: 0>
2023-10-04 05:37:39,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46879
2023-10-04 05:37:39,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48764
2023-10-04 05:37:39,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,417 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:39,418 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:39,419 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:39,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,522 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:39,527 - distributed.scheduler - INFO - Remove client Client-1d7a6d3e-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:39,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48716; closing.
2023-10-04 05:37:39,527 - distributed.scheduler - INFO - Remove client Client-1d7a6d3e-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:39,528 - distributed.scheduler - INFO - Close client connection: Client-1d7a6d3e-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:39,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33397'. Reason: nanny-close
2023-10-04 05:37:39,529 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42001'. Reason: nanny-close
2023-10-04 05:37:39,530 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45373. Reason: nanny-close
2023-10-04 05:37:39,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44661'. Reason: nanny-close
2023-10-04 05:37:39,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,531 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42633. Reason: nanny-close
2023-10-04 05:37:39,532 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48774; closing.
2023-10-04 05:37:39,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.5331511')
2023-10-04 05:37:39,533 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,534 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48790; closing.
2023-10-04 05:37:39,534 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39601'. Reason: nanny-close
2023-10-04 05:37:39,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.5353885')
2023-10-04 05:37:39,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33811. Reason: nanny-close
2023-10-04 05:37:39,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38433'. Reason: nanny-close
2023-10-04 05:37:39,536 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34911. Reason: nanny-close
2023-10-04 05:37:39,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45275'. Reason: nanny-close
2023-10-04 05:37:39,536 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46879. Reason: nanny-close
2023-10-04 05:37:39,537 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33513'. Reason: nanny-close
2023-10-04 05:37:39,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42333. Reason: nanny-close
2023-10-04 05:37:39,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48744; closing.
2023-10-04 05:37:39,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35239'. Reason: nanny-close
2023-10-04 05:37:39,538 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:39,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.538493')
2023-10-04 05:37:39,538 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33961. Reason: nanny-close
2023-10-04 05:37:39,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48764; closing.
2023-10-04 05:37:39,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46879', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.539137')
2023-10-04 05:37:39,539 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36937. Reason: nanny-close
2023-10-04 05:37:39,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48732; closing.
2023-10-04 05:37:39,539 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,540 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,540 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34911', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.540418')
2023-10-04 05:37:39,540 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,540 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48780; closing.
2023-10-04 05:37:39,541 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:39,541 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.5416243')
2023-10-04 05:37:39,542 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,543 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:39,541 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48732>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:37:39,543 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48800; closing.
2023-10-04 05:37:39,544 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.5442863')
2023-10-04 05:37:39,544 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48748; closing.
2023-10-04 05:37:39,545 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36937', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397859.5450873')
2023-10-04 05:37:39,545 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:37:41,247 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:37:41,247 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:37:41,247 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:37:41,249 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:37:41,249 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-04 05:37:43,354 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:43,358 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38031 instead
  warnings.warn(
2023-10-04 05:37:43,361 - distributed.scheduler - INFO - State start
2023-10-04 05:37:43,382 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:43,383 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:37:43,384 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38031/status
2023-10-04 05:37:43,384 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:37:43,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45657'
2023-10-04 05:37:43,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33023'
2023-10-04 05:37:43,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40541'
2023-10-04 05:37:43,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42279'
2023-10-04 05:37:43,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45845'
2023-10-04 05:37:43,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33193'
2023-10-04 05:37:43,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36683'
2023-10-04 05:37:43,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42967'
2023-10-04 05:37:45,116 - distributed.scheduler - INFO - Receive client connection: Client-22e2fcdb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:45,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48996
2023-10-04 05:37:45,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,554 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,574 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,579 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:45,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:45,616 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,620 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:45,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:47,951 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39961
2023-10-04 05:37:47,952 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39961
2023-10-04 05:37:47,952 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40641
2023-10-04 05:37:47,952 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:47,952 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:47,952 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:47,952 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:47,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-966yb5rp
2023-10-04 05:37:47,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c55af370-595d-44a1-8b6d-18a4f4516225
2023-10-04 05:37:47,953 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dae018fd-51ef-40fc-bb1b-72e79a8fdd33
2023-10-04 05:37:48,061 - distributed.worker - INFO - Starting Worker plugin PreImport-ab6b5c33-b17f-46c4-a624-89fd4b9037ac
2023-10-04 05:37:48,061 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,085 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39961', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,087 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39961
2023-10-04 05:37:48,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49022
2023-10-04 05:37:48,088 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,089 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,797 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36139
2023-10-04 05:37:48,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36139
2023-10-04 05:37:48,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33925
2023-10-04 05:37:48,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,798 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,798 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qczjifjx
2023-10-04 05:37:48,798 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34099
2023-10-04 05:37:48,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34099
2023-10-04 05:37:48,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37743
2023-10-04 05:37:48,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,798 - distributed.worker - INFO - Starting Worker plugin PreImport-8bd50081-70c8-4574-8490-e258eed7386f
2023-10-04 05:37:48,799 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,799 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-850fe465-fe5c-43a7-aad6-7a64398f4e9e
2023-10-04 05:37:48,799 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c438c7d-cbfb-4bde-869a-5be8396bc838
2023-10-04 05:37:48,799 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,799 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xys8qly0
2023-10-04 05:37:48,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01ff8494-aad6-4350-9803-98009ca4947f
2023-10-04 05:37:48,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43439
2023-10-04 05:37:48,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43439
2023-10-04 05:37:48,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41021
2023-10-04 05:37:48,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,811 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,811 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,812 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_6hjw8a1
2023-10-04 05:37:48,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e95c50c-1e89-40ef-bc88-2e286f3f95dc
2023-10-04 05:37:48,819 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34801
2023-10-04 05:37:48,820 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34801
2023-10-04 05:37:48,820 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37611
2023-10-04 05:37:48,820 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,820 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,820 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,820 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,820 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rdhdous8
2023-10-04 05:37:48,821 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44986e09-5295-4000-8056-fef6b99246ec
2023-10-04 05:37:48,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41853
2023-10-04 05:37:48,827 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41853
2023-10-04 05:37:48,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40911
2023-10-04 05:37:48,827 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,827 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,827 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,827 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sws47pt9
2023-10-04 05:37:48,827 - distributed.worker - INFO - Starting Worker plugin PreImport-0eb9dd44-a8c6-4065-9e29-93ba900ea22c
2023-10-04 05:37:48,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b51bbc6b-ea06-4b55-9b50-1ae8f494a364
2023-10-04 05:37:48,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76db4f39-b349-412b-864a-2dff6596d945
2023-10-04 05:37:48,845 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38695
2023-10-04 05:37:48,846 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38695
2023-10-04 05:37:48,846 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35181
2023-10-04 05:37:48,846 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,846 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,846 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,846 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,847 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_nej5ljf
2023-10-04 05:37:48,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c64abe7-79be-4d72-bc1c-dd033d6a61d9
2023-10-04 05:37:48,846 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43621
2023-10-04 05:37:48,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43621
2023-10-04 05:37:48,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33937
2023-10-04 05:37:48,849 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,849 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,849 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:48,849 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:48,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5repb24d
2023-10-04 05:37:48,850 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea6237ff-e28d-433f-833a-1b00a0965203
2023-10-04 05:37:48,850 - distributed.worker - INFO - Starting Worker plugin PreImport-fe8ca13d-dd99-4e81-b3f8-210197576776
2023-10-04 05:37:48,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f93ab62-2a3a-452c-8895-5ad9db182618
2023-10-04 05:37:48,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be46ecfc-5ea4-4412-8185-cbb550b9b9e6
2023-10-04 05:37:48,860 - distributed.worker - INFO - Starting Worker plugin PreImport-dc97ce82-73b5-490c-ac2f-1081428c2a9b
2023-10-04 05:37:48,860 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec67efc0-a245-4d91-8dd5-a60232b592fc
2023-10-04 05:37:48,863 - distributed.worker - INFO - Starting Worker plugin PreImport-66f535cc-bedf-4d85-a057-0c21b6161d1e
2023-10-04 05:37:48,864 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,871 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9d42397-0372-41a0-be89-e6645f6512d1
2023-10-04 05:37:48,872 - distributed.worker - INFO - Starting Worker plugin PreImport-5ecfe0fa-fdc3-41b9-b8f1-3181382fd816
2023-10-04 05:37:48,872 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,880 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2fb3afe-24a5-4629-95a8-39928d4b4e66
2023-10-04 05:37:48,881 - distributed.worker - INFO - Starting Worker plugin PreImport-54d5d46c-4e7c-45b3-9a52-b9a25121979b
2023-10-04 05:37:48,881 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34099', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34099
2023-10-04 05:37:48,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49038
2023-10-04 05:37:48,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,887 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,887 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,890 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43439', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43439
2023-10-04 05:37:48,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49054
2023-10-04 05:37:48,892 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,893 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34801', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34801
2023-10-04 05:37:48,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49066
2023-10-04 05:37:48,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,894 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,896 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38695', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38695
2023-10-04 05:37:48,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49068
2023-10-04 05:37:48,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36139', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,910 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36139
2023-10-04 05:37:48,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49080
2023-10-04 05:37:48,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,912 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41853', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,919 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41853
2023-10-04 05:37:48,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49090
2023-10-04 05:37:48,920 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43621', status: init, memory: 0, processing: 0>
2023-10-04 05:37:48,920 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43621
2023-10-04 05:37:48,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49104
2023-10-04 05:37:48,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,921 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,921 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:48,923 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:48,923 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:48,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:48,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,977 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,977 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,977 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:48,981 - distributed.scheduler - INFO - Remove client Client-22e2fcdb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:48,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48996; closing.
2023-10-04 05:37:48,981 - distributed.scheduler - INFO - Remove client Client-22e2fcdb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:48,982 - distributed.scheduler - INFO - Close client connection: Client-22e2fcdb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:48,983 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45657'. Reason: nanny-close
2023-10-04 05:37:48,983 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33023'. Reason: nanny-close
2023-10-04 05:37:48,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,984 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43621. Reason: nanny-close
2023-10-04 05:37:48,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40541'. Reason: nanny-close
2023-10-04 05:37:48,985 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,985 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41853. Reason: nanny-close
2023-10-04 05:37:48,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42279'. Reason: nanny-close
2023-10-04 05:37:48,985 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,986 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34099. Reason: nanny-close
2023-10-04 05:37:48,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45845'. Reason: nanny-close
2023-10-04 05:37:48,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,986 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39961. Reason: nanny-close
2023-10-04 05:37:48,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33193'. Reason: nanny-close
2023-10-04 05:37:48,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,987 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,987 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49104; closing.
2023-10-04 05:37:48,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38695. Reason: nanny-close
2023-10-04 05:37:48,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36683'. Reason: nanny-close
2023-10-04 05:37:48,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9878974')
2023-10-04 05:37:48,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43439. Reason: nanny-close
2023-10-04 05:37:48,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42967'. Reason: nanny-close
2023-10-04 05:37:48,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:48,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34801. Reason: nanny-close
2023-10-04 05:37:48,989 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49022; closing.
2023-10-04 05:37:48,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36139. Reason: nanny-close
2023-10-04 05:37:48,990 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,990 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,990 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9901457')
2023-10-04 05:37:48,990 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49038; closing.
2023-10-04 05:37:48,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49090; closing.
2023-10-04 05:37:48,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,992 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:48,993 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,991 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49022>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49022>: Stream is closed
2023-10-04 05:37:48,993 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,993 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.993602')
2023-10-04 05:37:48,994 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41853', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9940438')
2023-10-04 05:37:48,994 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49068; closing.
2023-10-04 05:37:48,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9956272')
2023-10-04 05:37:48,995 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:48,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49054; closing.
2023-10-04 05:37:48,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49066; closing.
2023-10-04 05:37:48,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49080; closing.
2023-10-04 05:37:48,996 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43439', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9968297')
2023-10-04 05:37:48,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9971573')
2023-10-04 05:37:48,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397868.9975073')
2023-10-04 05:37:48,997 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:37:48,998 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49066>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:37:48,998 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49080>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:37:48,998 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49054>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:37:50,450 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:37:50,450 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:37:50,451 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:37:50,452 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:37:50,452 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-04 05:37:52,658 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:52,663 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38857 instead
  warnings.warn(
2023-10-04 05:37:52,666 - distributed.scheduler - INFO - State start
2023-10-04 05:37:52,687 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:37:52,688 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:37:52,689 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38857/status
2023-10-04 05:37:52,689 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:37:52,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40507'
2023-10-04 05:37:52,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43757'
2023-10-04 05:37:52,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42959'
2023-10-04 05:37:52,834 - distributed.scheduler - INFO - Receive client connection: Client-2860757c-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:52,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46091'
2023-10-04 05:37:52,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45579'
2023-10-04 05:37:52,849 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33504
2023-10-04 05:37:52,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38651'
2023-10-04 05:37:52,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46133'
2023-10-04 05:37:52,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45807'
2023-10-04 05:37:54,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,722 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,723 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,727 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,777 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,782 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:54,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:37:54,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:37:54,857 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:37:58,996 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40851
2023-10-04 05:37:58,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40851
2023-10-04 05:37:58,997 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39185
2023-10-04 05:37:58,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:58,997 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:58,997 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:58,997 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:58,997 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r6jqw0oq
2023-10-04 05:37:58,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1816761b-cb21-40b4-814d-84c72b3ea129
2023-10-04 05:37:59,162 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35457
2023-10-04 05:37:59,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35457
2023-10-04 05:37:59,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37067
2023-10-04 05:37:59,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,163 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,163 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nstndo10
2023-10-04 05:37:59,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-041ac930-f925-4363-9642-1020d76d75bf
2023-10-04 05:37:59,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af6e97b9-7de5-4083-9718-6a247eda41b2
2023-10-04 05:37:59,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34631
2023-10-04 05:37:59,172 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34631
2023-10-04 05:37:59,172 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44251
2023-10-04 05:37:59,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,172 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,172 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,172 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mtbrcly1
2023-10-04 05:37:59,173 - distributed.worker - INFO - Starting Worker plugin PreImport-d0ac6092-5192-4de1-9540-b73c2b976e01
2023-10-04 05:37:59,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcc2776c-d41b-47fc-81bd-6b87c2b10727
2023-10-04 05:37:59,173 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe8c07ec-977a-4cb7-b005-81aa9cbad774
2023-10-04 05:37:59,178 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38197
2023-10-04 05:37:59,179 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38197
2023-10-04 05:37:59,179 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41051
2023-10-04 05:37:59,180 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,180 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,180 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,180 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zlx7gk77
2023-10-04 05:37:59,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c97fbbb6-77ec-4c45-ba0b-32a8abf7de1f
2023-10-04 05:37:59,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35271
2023-10-04 05:37:59,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35271
2023-10-04 05:37:59,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43319
2023-10-04 05:37:59,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,192 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,192 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mzh_9w62
2023-10-04 05:37:59,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e45427de-1da6-4b9b-aa20-7f879ba0931e
2023-10-04 05:37:59,197 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34893
2023-10-04 05:37:59,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34893
2023-10-04 05:37:59,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34047
2023-10-04 05:37:59,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,198 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,198 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,198 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-519n1nb0
2023-10-04 05:37:59,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b89cccbf-2611-4d42-95f7-21b2269e40c9
2023-10-04 05:37:59,212 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3665ea73-ca18-4bbb-ae18-5fa3c685100e
2023-10-04 05:37:59,212 - distributed.worker - INFO - Starting Worker plugin PreImport-5979c011-1d05-439d-a736-09b12a4a9431
2023-10-04 05:37:59,212 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40851', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,249 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40851
2023-10-04 05:37:59,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33510
2023-10-04 05:37:59,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,251 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,251 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,267 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35419
2023-10-04 05:37:59,268 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35419
2023-10-04 05:37:59,268 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34159
2023-10-04 05:37:59,268 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,268 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,268 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,268 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,268 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-is2syzfc
2023-10-04 05:37:59,269 - distributed.worker - INFO - Starting Worker plugin RMMSetup-598e3a9c-e06e-4c28-9c8c-d8a5508a75f0
2023-10-04 05:37:59,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43259
2023-10-04 05:37:59,287 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43259
2023-10-04 05:37:59,287 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35517
2023-10-04 05:37:59,287 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,287 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,287 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:37:59,287 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:37:59,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-745hosem
2023-10-04 05:37:59,288 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4992cda0-dee6-4017-bbe1-4074fc06e16b
2023-10-04 05:37:59,339 - distributed.worker - INFO - Starting Worker plugin PreImport-07c4e13a-ebd6-489a-b347-ba7f357f9974
2023-10-04 05:37:59,340 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,369 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35457', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,370 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35457
2023-10-04 05:37:59,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33512
2023-10-04 05:37:59,371 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,372 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,372 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,382 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,384 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc30c47b-9a52-434d-8621-fa497d351d78
2023-10-04 05:37:59,384 - distributed.worker - INFO - Starting Worker plugin PreImport-3f03aae0-a9cd-4ed4-a755-97cbf622ac7e
2023-10-04 05:37:59,385 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c902fcd-7663-4ceb-a98b-04445c6d4541
2023-10-04 05:37:59,386 - distributed.worker - INFO - Starting Worker plugin PreImport-1eac4157-1738-40ea-9fac-4be1b886c256
2023-10-04 05:37:59,386 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,390 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7ab40aa-c0b0-4851-b3d5-79cc5410ed5d
2023-10-04 05:37:59,391 - distributed.worker - INFO - Starting Worker plugin PreImport-81f983ad-01b3-4613-8ae4-227a97debcef
2023-10-04 05:37:59,392 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,414 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34631', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34631
2023-10-04 05:37:59,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33514
2023-10-04 05:37:59,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38197', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38197
2023-10-04 05:37:59,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33520
2023-10-04 05:37:59,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,417 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,419 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28733f2d-a8d1-46e7-8bb1-f4b06846db5b
2023-10-04 05:37:59,425 - distributed.worker - INFO - Starting Worker plugin PreImport-86a93458-1d9b-4473-a0a6-e19f6907428f
2023-10-04 05:37:59,426 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,428 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35271', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,428 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35271
2023-10-04 05:37:59,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33532
2023-10-04 05:37:59,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,431 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34893', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,433 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34893
2023-10-04 05:37:59,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33528
2023-10-04 05:37:59,434 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-327ec8f8-74c5-4834-bab8-5b66e6a7c54c
2023-10-04 05:37:59,434 - distributed.worker - INFO - Starting Worker plugin PreImport-3e2f7293-c2ab-4848-9a25-17889bf8968b
2023-10-04 05:37:59,434 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,435 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,435 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,453 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35419', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,454 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35419
2023-10-04 05:37:59,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33538
2023-10-04 05:37:59,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,456 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,456 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,458 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43259', status: init, memory: 0, processing: 0>
2023-10-04 05:37:59,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43259
2023-10-04 05:37:59,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33540
2023-10-04 05:37:59,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:37:59,460 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:37:59,461 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:37:59,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:37:59,503 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:37:59,516 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,516 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,517 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,517 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,517 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,517 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,517 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:37:59,524 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:37:59,526 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:37:59,528 - distributed.scheduler - INFO - Remove client Client-2860757c-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:59,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33504; closing.
2023-10-04 05:37:59,529 - distributed.scheduler - INFO - Remove client Client-2860757c-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:59,529 - distributed.scheduler - INFO - Close client connection: Client-2860757c-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:37:59,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40507'. Reason: nanny-close
2023-10-04 05:37:59,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43757'. Reason: nanny-close
2023-10-04 05:37:59,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40851. Reason: nanny-close
2023-10-04 05:37:59,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42959'. Reason: nanny-close
2023-10-04 05:37:59,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38197. Reason: nanny-close
2023-10-04 05:37:59,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46091'. Reason: nanny-close
2023-10-04 05:37:59,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35457. Reason: nanny-close
2023-10-04 05:37:59,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45579'. Reason: nanny-close
2023-10-04 05:37:59,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33510; closing.
2023-10-04 05:37:59,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34893. Reason: nanny-close
2023-10-04 05:37:59,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38651'. Reason: nanny-close
2023-10-04 05:37:59,534 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.5348713')
2023-10-04 05:37:59,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34631. Reason: nanny-close
2023-10-04 05:37:59,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46133'. Reason: nanny-close
2023-10-04 05:37:59,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43259. Reason: nanny-close
2023-10-04 05:37:59,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45807'. Reason: nanny-close
2023-10-04 05:37:59,536 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:37:59,536 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35271. Reason: nanny-close
2023-10-04 05:37:59,536 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,536 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35419. Reason: nanny-close
2023-10-04 05:37:59,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33520; closing.
2023-10-04 05:37:59,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.5389972')
2023-10-04 05:37:59,539 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,539 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33512; closing.
2023-10-04 05:37:59,539 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,540 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:37:59,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35457', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.5406575')
2023-10-04 05:37:59,540 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33528; closing.
2023-10-04 05:37:59,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33514; closing.
2023-10-04 05:37:59,541 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,542 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.5427256')
2023-10-04 05:37:59,543 - distributed.nanny - INFO - Worker closed
2023-10-04 05:37:59,543 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34631', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.5432174')
2023-10-04 05:37:59,543 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33540; closing.
2023-10-04 05:37:59,543 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33532; closing.
2023-10-04 05:37:59,544 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.544522')
2023-10-04 05:37:59,545 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.5449872')
2023-10-04 05:37:59,545 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33538; closing.
2023-10-04 05:37:59,545 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35419', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397879.545866')
2023-10-04 05:37:59,546 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:37:59,546 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33538>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:38:01,248 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:01,249 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:01,250 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:01,251 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:38:01,252 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-04 05:38:03,398 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:03,403 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46053 instead
  warnings.warn(
2023-10-04 05:38:03,407 - distributed.scheduler - INFO - State start
2023-10-04 05:38:03,463 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:03,464 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:38:03,464 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46053/status
2023-10-04 05:38:03,465 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:04,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35671'
2023-10-04 05:38:04,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38933'
2023-10-04 05:38:04,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33291'
2023-10-04 05:38:04,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38637'
2023-10-04 05:38:04,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36497'
2023-10-04 05:38:04,220 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36981'
2023-10-04 05:38:04,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43437'
2023-10-04 05:38:04,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43323'
2023-10-04 05:38:04,330 - distributed.scheduler - INFO - Receive client connection: Client-2ee781da-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:04,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35048
2023-10-04 05:38:06,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:06,202 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:06,202 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:06,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:09,411 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36671
2023-10-04 05:38:09,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36671
2023-10-04 05:38:09,412 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46589
2023-10-04 05:38:09,412 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,412 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,412 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2lgur2qe
2023-10-04 05:38:09,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04f43e75-cee9-4839-8b45-6c3c00bdd4ac
2023-10-04 05:38:09,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33973
2023-10-04 05:38:09,417 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33973
2023-10-04 05:38:09,417 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44335
2023-10-04 05:38:09,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,417 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,417 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xrn56nc8
2023-10-04 05:38:09,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8f7582c-acae-4bc7-972e-41d5d8b4742f
2023-10-04 05:38:09,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-484e26dd-b567-44bf-b9ad-bd59927f5825
2023-10-04 05:38:09,422 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39495
2023-10-04 05:38:09,423 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39495
2023-10-04 05:38:09,423 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34875
2023-10-04 05:38:09,423 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,423 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,423 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,423 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,423 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-38_36lzw
2023-10-04 05:38:09,424 - distributed.worker - INFO - Starting Worker plugin PreImport-41c74878-94e8-45ca-8b46-2173f5be5c6f
2023-10-04 05:38:09,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5417936d-e8b6-4b31-bbed-7400ef0fc06d
2023-10-04 05:38:09,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-536c015f-ad0d-41ac-8edd-7325bd49906e
2023-10-04 05:38:09,494 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42909
2023-10-04 05:38:09,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42909
2023-10-04 05:38:09,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35407
2023-10-04 05:38:09,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,495 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,495 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9y_hgbd3
2023-10-04 05:38:09,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c2f4291-5056-4208-90e8-7beeff311486
2023-10-04 05:38:09,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43559
2023-10-04 05:38:09,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43559
2023-10-04 05:38:09,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35745
2023-10-04 05:38:09,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,501 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,501 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,501 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vky6nvs_
2023-10-04 05:38:09,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a4bb768-f9a2-4a3e-9856-069e55315a3b
2023-10-04 05:38:09,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37365
2023-10-04 05:38:09,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37365
2023-10-04 05:38:09,526 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35309
2023-10-04 05:38:09,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,526 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,526 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,526 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7x6imu13
2023-10-04 05:38:09,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfa2d596-3072-432b-8b3a-e291a121209d
2023-10-04 05:38:09,528 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44947
2023-10-04 05:38:09,529 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44947
2023-10-04 05:38:09,529 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46517
2023-10-04 05:38:09,529 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,529 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,529 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3cu6znwd
2023-10-04 05:38:09,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-561b6601-b0b5-44aa-89a6-1a4e8373171a
2023-10-04 05:38:09,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46147
2023-10-04 05:38:09,531 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46147
2023-10-04 05:38:09,531 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44543
2023-10-04 05:38:09,531 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,531 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,531 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:09,531 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:09,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3f50dlek
2023-10-04 05:38:09,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-548e04b4-84bb-4cd0-b985-681575bfbd46
2023-10-04 05:38:09,627 - distributed.worker - INFO - Starting Worker plugin PreImport-5f27711e-5d51-4fd7-8aaa-5c6964d975b5
2023-10-04 05:38:09,627 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a0a51a9-0507-48ec-80c7-556d78326001
2023-10-04 05:38:09,629 - distributed.worker - INFO - Starting Worker plugin PreImport-a4cc1abf-ee9f-4dea-9099-a8f0d58c4421
2023-10-04 05:38:09,629 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,652 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33973', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33973
2023-10-04 05:38:09,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35066
2023-10-04 05:38:09,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,659 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,659 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,669 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36671', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,669 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36671
2023-10-04 05:38:09,670 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35070
2023-10-04 05:38:09,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,672 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,672 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,690 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39495', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,690 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39495
2023-10-04 05:38:09,690 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35080
2023-10-04 05:38:09,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,693 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,693 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f40c96eb-4498-47e9-a0f9-ce83f9ede50a
2023-10-04 05:38:09,701 - distributed.worker - INFO - Starting Worker plugin PreImport-a10ef8ef-c8de-41a3-9479-41144329d2f7
2023-10-04 05:38:09,701 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,718 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd496249-9090-4a5e-bd95-03b56bb0914f
2023-10-04 05:38:09,719 - distributed.worker - INFO - Starting Worker plugin PreImport-efaed1ea-5fa6-4c8a-a8f6-013aef38c717
2023-10-04 05:38:09,719 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03fbc15d-f86c-4a90-919f-05a064d2144b
2023-10-04 05:38:09,723 - distributed.worker - INFO - Starting Worker plugin PreImport-f065c516-f4ab-443b-bdac-3a3a4d1a5672
2023-10-04 05:38:09,724 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,724 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43559', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43559
2023-10-04 05:38:09,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35086
2023-10-04 05:38:09,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,726 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,726 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d226e42-a869-43a0-a8e7-527b1883bac8
2023-10-04 05:38:09,727 - distributed.worker - INFO - Starting Worker plugin PreImport-54b52082-6661-4bc0-83a1-10da91b01b6e
2023-10-04 05:38:09,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6a4fca2-be36-44b8-bb29-5c0d2a75eda0
2023-10-04 05:38:09,727 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,727 - distributed.worker - INFO - Starting Worker plugin PreImport-87a0c6f1-93ed-4f0e-a318-724860400f89
2023-10-04 05:38:09,728 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44947', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44947
2023-10-04 05:38:09,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35108
2023-10-04 05:38:09,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,752 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,752 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46147', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46147
2023-10-04 05:38:09,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35112
2023-10-04 05:38:09,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,756 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,757 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37365', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,758 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37365
2023-10-04 05:38:09,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35102
2023-10-04 05:38:09,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,760 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,760 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,760 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42909', status: init, memory: 0, processing: 0>
2023-10-04 05:38:09,761 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42909
2023-10-04 05:38:09,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35104
2023-10-04 05:38:09,762 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:09,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,763 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:09,763 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:09,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:09,788 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,789 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,802 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,803 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,803 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,803 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,803 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,803 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,803 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,804 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:38:09,810 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,812 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:09,814 - distributed.scheduler - INFO - Remove client Client-2ee781da-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:09,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35048; closing.
2023-10-04 05:38:09,815 - distributed.scheduler - INFO - Remove client Client-2ee781da-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:09,815 - distributed.scheduler - INFO - Close client connection: Client-2ee781da-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:09,816 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35671'. Reason: nanny-close
2023-10-04 05:38:09,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38933'. Reason: nanny-close
2023-10-04 05:38:09,818 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36671. Reason: nanny-close
2023-10-04 05:38:09,818 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33291'. Reason: nanny-close
2023-10-04 05:38:09,818 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,819 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37365. Reason: nanny-close
2023-10-04 05:38:09,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38637'. Reason: nanny-close
2023-10-04 05:38:09,819 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,819 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33973. Reason: nanny-close
2023-10-04 05:38:09,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36497'. Reason: nanny-close
2023-10-04 05:38:09,820 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35070; closing.
2023-10-04 05:38:09,820 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44947. Reason: nanny-close
2023-10-04 05:38:09,821 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36671', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8210175')
2023-10-04 05:38:09,821 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36981'. Reason: nanny-close
2023-10-04 05:38:09,821 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,821 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39495. Reason: nanny-close
2023-10-04 05:38:09,821 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43437'. Reason: nanny-close
2023-10-04 05:38:09,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42909. Reason: nanny-close
2023-10-04 05:38:09,822 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,822 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43323'. Reason: nanny-close
2023-10-04 05:38:09,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35066; closing.
2023-10-04 05:38:09,823 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:09,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43559. Reason: nanny-close
2023-10-04 05:38:09,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35102; closing.
2023-10-04 05:38:09,823 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,823 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,824 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8239765')
2023-10-04 05:38:09,824 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46147. Reason: nanny-close
2023-10-04 05:38:09,824 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,824 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,824 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8243585')
2023-10-04 05:38:09,824 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35108; closing.
2023-10-04 05:38:09,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44947', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8251147')
2023-10-04 05:38:09,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,825 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35080; closing.
2023-10-04 05:38:09,826 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35104; closing.
2023-10-04 05:38:09,826 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:09,826 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,826 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8268158')
2023-10-04 05:38:09,826 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,827 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42909', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8272016')
2023-10-04 05:38:09,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35086; closing.
2023-10-04 05:38:09,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35112; closing.
2023-10-04 05:38:09,827 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:09,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.8281991')
2023-10-04 05:38:09,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397889.828562')
2023-10-04 05:38:09,828 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:11,684 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:11,684 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:11,685 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:11,686 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:38:11,686 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-04 05:38:14,069 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:14,074 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45417 instead
  warnings.warn(
2023-10-04 05:38:14,080 - distributed.scheduler - INFO - State start
2023-10-04 05:38:14,110 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:14,111 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:38:14,112 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45417/status
2023-10-04 05:38:14,113 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:14,127 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34029'
2023-10-04 05:38:14,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36557'
2023-10-04 05:38:14,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42165'
2023-10-04 05:38:14,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38543'
2023-10-04 05:38:14,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41943'
2023-10-04 05:38:14,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46749'
2023-10-04 05:38:14,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40563'
2023-10-04 05:38:14,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45105'
2023-10-04 05:38:14,301 - distributed.scheduler - INFO - Receive client connection: Client-35173775-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:14,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47742
2023-10-04 05:38:15,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:15,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:15,997 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,059 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,059 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,100 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,106 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:16,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:16,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:16,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:17,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45083
2023-10-04 05:38:17,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45083
2023-10-04 05:38:17,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41267
2023-10-04 05:38:17,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:17,874 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:17,874 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:17,875 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:17,875 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o0b6_xyt
2023-10-04 05:38:17,876 - distributed.worker - INFO - Starting Worker plugin RMMSetup-550db9ad-ea26-4aae-94ce-01ac508773e2
2023-10-04 05:38:18,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ffb2e70-4f19-4843-8aa4-9ae7573fe7d6
2023-10-04 05:38:18,097 - distributed.worker - INFO - Starting Worker plugin PreImport-7898816f-d751-4fe2-b24d-785ff42a1da1
2023-10-04 05:38:18,097 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:18,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45083', status: init, memory: 0, processing: 0>
2023-10-04 05:38:18,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45083
2023-10-04 05:38:18,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47748
2023-10-04 05:38:18,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:18,131 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:18,131 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:18,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:18,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36073
2023-10-04 05:38:18,960 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36073
2023-10-04 05:38:18,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40679
2023-10-04 05:38:18,960 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40679
2023-10-04 05:38:18,960 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35181
2023-10-04 05:38:18,960 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:18,960 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42651
2023-10-04 05:38:18,960 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:18,960 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:18,961 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:18,961 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:18,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:18,961 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:18,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8xh2jy_1
2023-10-04 05:38:18,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:18,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9l7d2r89
2023-10-04 05:38:18,961 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e74b8e00-d799-4c17-a4e0-e2902157819b
2023-10-04 05:38:18,961 - distributed.worker - INFO - Starting Worker plugin PreImport-c62d365c-da1d-40a7-acb2-9e6fbda97955
2023-10-04 05:38:18,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3377dc5a-4b84-4368-b554-ff5d96e04cda
2023-10-04 05:38:18,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-016c7b05-bf4e-4c90-84fe-73fecac6ecb5
2023-10-04 05:38:18,974 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41957
2023-10-04 05:38:18,975 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41957
2023-10-04 05:38:18,975 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34255
2023-10-04 05:38:18,975 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:18,975 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:18,975 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:18,975 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:18,975 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vzcxmqyi
2023-10-04 05:38:18,976 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90b92131-8773-4833-9a25-dbae01134c53
2023-10-04 05:38:18,976 - distributed.worker - INFO - Starting Worker plugin RMMSetup-982727d2-0763-4e2c-b237-763b81ccb27e
2023-10-04 05:38:19,025 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36879
2023-10-04 05:38:19,025 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41307
2023-10-04 05:38:19,026 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36879
2023-10-04 05:38:19,026 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41307
2023-10-04 05:38:19,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36673
2023-10-04 05:38:19,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44839
2023-10-04 05:38:19,026 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,026 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,026 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:19,026 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:19,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pmjnms7
2023-10-04 05:38:19,026 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:19,026 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:19,027 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b_9fq0pl
2023-10-04 05:38:19,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f001dfb9-416d-4920-b7c0-0ea5573718bf
2023-10-04 05:38:19,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e5a32c4c-240e-4230-8d7c-9c88bb2b7d81
2023-10-04 05:38:19,039 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45267
2023-10-04 05:38:19,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45267
2023-10-04 05:38:19,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45541
2023-10-04 05:38:19,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,040 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,040 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:19,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:19,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e_9n6bx7
2023-10-04 05:38:19,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6add7c40-8ef9-4ddd-8138-5dc08a69ffff
2023-10-04 05:38:19,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33635
2023-10-04 05:38:19,041 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33635
2023-10-04 05:38:19,041 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36733
2023-10-04 05:38:19,041 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,041 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,041 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:19,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:19,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o376j9sc
2023-10-04 05:38:19,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69123d61-d1a0-4394-bff6-e45db8b93d0f
2023-10-04 05:38:19,146 - distributed.worker - INFO - Starting Worker plugin PreImport-052107b4-3f12-4f9f-adb4-0f681e244bb3
2023-10-04 05:38:19,147 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b0f9aac-0081-4f1e-a000-ffcbbfd23a01
2023-10-04 05:38:19,157 - distributed.worker - INFO - Starting Worker plugin PreImport-6cd4af5a-e4fc-4622-aac0-90043f23b16a
2023-10-04 05:38:19,157 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,168 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-109ac705-89c5-4bcb-810e-a9eefdcfda42
2023-10-04 05:38:19,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8086df88-2b36-41fa-a91a-73255cb8bc79
2023-10-04 05:38:19,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36fa689e-a190-4685-964e-8e3e2e10457a
2023-10-04 05:38:19,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84e543c7-f35d-4889-9bec-addac533d212
2023-10-04 05:38:19,179 - distributed.worker - INFO - Starting Worker plugin PreImport-d5988d62-7172-47e1-8ebe-614396e34e56
2023-10-04 05:38:19,179 - distributed.worker - INFO - Starting Worker plugin PreImport-56e45a3a-583c-4ed9-883e-dc63c50a5345
2023-10-04 05:38:19,179 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,179 - distributed.worker - INFO - Starting Worker plugin PreImport-e58492b3-7c62-4718-ab59-56574f79c617
2023-10-04 05:38:19,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,180 - distributed.worker - INFO - Starting Worker plugin PreImport-91c54c65-c2e3-4f98-9f7b-c572ea6ef1ef
2023-10-04 05:38:19,180 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41957', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41957
2023-10-04 05:38:19,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47764
2023-10-04 05:38:19,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,183 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,189 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33635', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33635
2023-10-04 05:38:19,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47770
2023-10-04 05:38:19,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,192 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,196 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40679', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,196 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40679
2023-10-04 05:38:19,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47784
2023-10-04 05:38:19,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,198 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,198 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41307', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,208 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41307
2023-10-04 05:38:19,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47794
2023-10-04 05:38:19,208 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45267', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,209 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45267
2023-10-04 05:38:19,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47790
2023-10-04 05:38:19,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,210 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,210 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,211 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,211 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36879', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36879
2023-10-04 05:38:19,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47808
2023-10-04 05:38:19,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36073', status: init, memory: 0, processing: 0>
2023-10-04 05:38:19,217 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36073
2023-10-04 05:38:19,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47810
2023-10-04 05:38:19,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:19,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:19,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:19,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:19,324 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,324 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,324 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,324 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,324 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,324 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,325 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,325 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:38:19,329 - distributed.scheduler - INFO - Remove client Client-35173775-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:19,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47742; closing.
2023-10-04 05:38:19,329 - distributed.scheduler - INFO - Remove client Client-35173775-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:19,330 - distributed.scheduler - INFO - Close client connection: Client-35173775-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:19,330 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34029'. Reason: nanny-close
2023-10-04 05:38:19,331 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36557'. Reason: nanny-close
2023-10-04 05:38:19,332 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,332 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45267. Reason: nanny-close
2023-10-04 05:38:19,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42165'. Reason: nanny-close
2023-10-04 05:38:19,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,333 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41307. Reason: nanny-close
2023-10-04 05:38:19,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38543'. Reason: nanny-close
2023-10-04 05:38:19,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,334 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41957. Reason: nanny-close
2023-10-04 05:38:19,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41943'. Reason: nanny-close
2023-10-04 05:38:19,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47790; closing.
2023-10-04 05:38:19,334 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36073. Reason: nanny-close
2023-10-04 05:38:19,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46749'. Reason: nanny-close
2023-10-04 05:38:19,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.3349562')
2023-10-04 05:38:19,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40679. Reason: nanny-close
2023-10-04 05:38:19,335 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40563'. Reason: nanny-close
2023-10-04 05:38:19,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33635. Reason: nanny-close
2023-10-04 05:38:19,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45105'. Reason: nanny-close
2023-10-04 05:38:19,336 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,336 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:19,336 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,336 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45083. Reason: nanny-close
2023-10-04 05:38:19,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47794; closing.
2023-10-04 05:38:19,336 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,337 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36879. Reason: nanny-close
2023-10-04 05:38:19,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.33738')
2023-10-04 05:38:19,337 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47764; closing.
2023-10-04 05:38:19,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,338 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41957', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.3382792')
2023-10-04 05:38:19,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47810; closing.
2023-10-04 05:38:19,338 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,339 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,339 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:19,339 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.3393128')
2023-10-04 05:38:19,339 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47784; closing.
2023-10-04 05:38:19,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47770; closing.
2023-10-04 05:38:19,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.3404486')
2023-10-04 05:38:19,340 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.3408215')
2023-10-04 05:38:19,340 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:19,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47748; closing.
2023-10-04 05:38:19,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47808; closing.
2023-10-04 05:38:19,341 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45083', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.341694')
2023-10-04 05:38:19,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36879', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397899.34208')
2023-10-04 05:38:19,342 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:20,948 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:20,948 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:20,949 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:20,950 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:38:20,950 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-04 05:38:23,263 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:23,268 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42159 instead
  warnings.warn(
2023-10-04 05:38:23,271 - distributed.scheduler - INFO - State start
2023-10-04 05:38:23,390 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:23,391 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:38:23,392 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42159/status
2023-10-04 05:38:23,392 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:23,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39901'
2023-10-04 05:38:25,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:25,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:25,554 - distributed.scheduler - INFO - Receive client connection: Client-3aaca4b4-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:25,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35932
2023-10-04 05:38:25,608 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:25,692 - distributed.scheduler - INFO - Receive client connection: Client-3d2ab03f-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:25,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35948
2023-10-04 05:38:26,569 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33625
2023-10-04 05:38:26,571 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33625
2023-10-04 05:38:26,572 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-04 05:38:26,572 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:26,572 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:26,572 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:26,572 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-04 05:38:26,572 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ouaaphwj
2023-10-04 05:38:26,574 - distributed.worker - INFO - Starting Worker plugin PreImport-58549bbf-ba82-45e8-9f65-2e76683b9415
2023-10-04 05:38:26,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0f74067-600b-491b-8be8-348b03d60626
2023-10-04 05:38:26,574 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef381c9d-828b-4eec-84d0-db19d58c5646
2023-10-04 05:38:26,575 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:26,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33625', status: init, memory: 0, processing: 0>
2023-10-04 05:38:26,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33625
2023-10-04 05:38:26,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35962
2023-10-04 05:38:26,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:26,610 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:26,610 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:26,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:26,691 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:26,694 - distributed.scheduler - INFO - Remove client Client-3aaca4b4-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:26,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35932; closing.
2023-10-04 05:38:26,695 - distributed.scheduler - INFO - Remove client Client-3aaca4b4-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:26,695 - distributed.scheduler - INFO - Close client connection: Client-3aaca4b4-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:26,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39901'. Reason: nanny-close
2023-10-04 05:38:26,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:26,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33625. Reason: nanny-close
2023-10-04 05:38:26,700 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35962; closing.
2023-10-04 05:38:26,700 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:26,700 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397906.7004738')
2023-10-04 05:38:26,700 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:26,702 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:27,862 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:27,863 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:27,864 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:27,868 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:38:27,869 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-04 05:38:32,252 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:32,256 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36419 instead
  warnings.warn(
2023-10-04 05:38:32,260 - distributed.scheduler - INFO - State start
2023-10-04 05:38:32,456 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:32,457 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:38:32,458 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36419/status
2023-10-04 05:38:32,458 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:32,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37007'
2023-10-04 05:38:32,627 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33999', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,638 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33999
2023-10-04 05:38:32,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52770
2023-10-04 05:38:32,640 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36081', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,640 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36081
2023-10-04 05:38:32,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52764
2023-10-04 05:38:32,641 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34283', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,641 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34283
2023-10-04 05:38:32,641 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52796
2023-10-04 05:38:32,642 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40879', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,642 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40879
2023-10-04 05:38:32,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52748
2023-10-04 05:38:32,643 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39197', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,643 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39197
2023-10-04 05:38:32,643 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52786
2023-10-04 05:38:32,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45773', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,644 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45773
2023-10-04 05:38:32,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52800
2023-10-04 05:38:32,645 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34367', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,646 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34367
2023-10-04 05:38:32,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52802
2023-10-04 05:38:32,669 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52764; closing.
2023-10-04 05:38:32,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36081', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.6695833')
2023-10-04 05:38:32,670 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52770; closing.
2023-10-04 05:38:32,671 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.6718428')
2023-10-04 05:38:32,672 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52786; closing.
2023-10-04 05:38:32,672 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52770>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52770>: Stream is closed
2023-10-04 05:38:32,674 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.67484')
2023-10-04 05:38:32,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52802; closing.
2023-10-04 05:38:32,675 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.675745')
2023-10-04 05:38:32,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52800; closing.
2023-10-04 05:38:32,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52796; closing.
2023-10-04 05:38:32,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45773', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.6774259')
2023-10-04 05:38:32,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34283', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.6777678')
2023-10-04 05:38:32,678 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52748; closing.
2023-10-04 05:38:32,678 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40879', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.6784904')
2023-10-04 05:38:32,678 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:32,678 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52748>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:38:32,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33701', status: init, memory: 0, processing: 0>
2023-10-04 05:38:32,709 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33701
2023-10-04 05:38:32,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52810
2023-10-04 05:38:32,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52810; closing.
2023-10-04 05:38:32,725 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397912.7256346')
2023-10-04 05:38:32,725 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:33,328 - distributed.scheduler - INFO - Receive client connection: Client-3ff995e2-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:33,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52826
2023-10-04 05:38:34,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:34,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:34,520 - distributed.scheduler - INFO - Receive client connection: Client-3d2ab03f-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:34,521 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52852
2023-10-04 05:38:34,804 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:35,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41595
2023-10-04 05:38:35,646 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41595
2023-10-04 05:38:35,646 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33051
2023-10-04 05:38:35,646 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:35,646 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:35,646 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:35,646 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-04 05:38:35,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-atapsamt
2023-10-04 05:38:35,647 - distributed.worker - INFO - Starting Worker plugin PreImport-3eca3ca7-2afc-4e88-9ee3-42a76a04e915
2023-10-04 05:38:35,648 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04d9c8a2-7150-4599-8168-2a822524224c
2023-10-04 05:38:35,649 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f6772c1-5a28-4e3d-b67b-9d20eae57518
2023-10-04 05:38:35,649 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:35,682 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41595', status: init, memory: 0, processing: 0>
2023-10-04 05:38:35,682 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41595
2023-10-04 05:38:35,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52866
2023-10-04 05:38:35,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:35,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:38:35,684 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:35,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:38:35,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:35,690 - distributed.scheduler - INFO - Remove client Client-3ff995e2-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:35,691 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52826; closing.
2023-10-04 05:38:35,691 - distributed.scheduler - INFO - Remove client Client-3ff995e2-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:35,691 - distributed.scheduler - INFO - Close client connection: Client-3ff995e2-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:35,692 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37007'. Reason: nanny-close
2023-10-04 05:38:35,702 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:35,703 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41595. Reason: nanny-close
2023-10-04 05:38:35,704 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52866; closing.
2023-10-04 05:38:35,704 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:38:35,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397915.7051375')
2023-10-04 05:38:35,705 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:35,706 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:36,708 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:36,708 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:36,709 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:36,710 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:38:36,710 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-04 05:38:38,613 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:38,618 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37393 instead
  warnings.warn(
2023-10-04 05:38:38,623 - distributed.scheduler - INFO - State start
2023-10-04 05:38:38,646 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:38,647 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:38:38,648 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37393/status
2023-10-04 05:38:38,648 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:40,294 - distributed.scheduler - INFO - Receive client connection: Client-3d2ab03f-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:40,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34608
2023-10-04 05:38:41,719 - distributed.scheduler - INFO - Remove client Client-3d2ab03f-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:41,719 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34608; closing.
2023-10-04 05:38:41,719 - distributed.scheduler - INFO - Remove client Client-3d2ab03f-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:41,720 - distributed.scheduler - INFO - Close client connection: Client-3d2ab03f-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:42,050 - distributed.scheduler - INFO - Receive client connection: Client-46eab9be-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:42,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34626
2023-10-04 05:38:42,323 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53108'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53108>: Stream is closed
2023-10-04 05:38:42,665 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:42,665 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:42,666 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:42,668 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:38:42,669 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-04 05:38:44,743 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:44,748 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36807 instead
  warnings.warn(
2023-10-04 05:38:44,752 - distributed.scheduler - INFO - State start
2023-10-04 05:38:44,780 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:44,781 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-04 05:38:44,782 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36807/status
2023-10-04 05:38:44,782 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:44,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35197'
2023-10-04 05:38:46,419 - distributed.scheduler - INFO - Receive client connection: Client-4778b635-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:46,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44426
2023-10-04 05:38:46,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:46,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:46,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:50,030 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44361
2023-10-04 05:38:50,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44361
2023-10-04 05:38:50,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42377
2023-10-04 05:38:50,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-04 05:38:50,032 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:50,032 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:50,032 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-04 05:38:50,032 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-1fly4o1x
2023-10-04 05:38:50,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c619b7c-bbc1-4f73-84f9-269ddfb46779
2023-10-04 05:38:50,033 - distributed.worker - INFO - Starting Worker plugin PreImport-35521815-da30-43ec-ad11-24081f0d9a20
2023-10-04 05:38:50,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-adad0176-19a9-4ee0-862b-0fc5f2de7469
2023-10-04 05:38:50,034 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:50,062 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44361', status: init, memory: 0, processing: 0>
2023-10-04 05:38:50,063 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44361
2023-10-04 05:38:50,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39762
2023-10-04 05:38:50,065 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:38:50,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-04 05:38:50,066 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:50,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-04 05:38:50,162 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:38:50,165 - distributed.scheduler - INFO - Remove client Client-4778b635-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:50,166 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44426; closing.
2023-10-04 05:38:50,166 - distributed.scheduler - INFO - Remove client Client-4778b635-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:50,166 - distributed.scheduler - INFO - Close client connection: Client-4778b635-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:50,167 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35197'. Reason: nanny-close
2023-10-04 05:38:50,168 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:38:50,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44361. Reason: nanny-close
2023-10-04 05:38:50,171 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-04 05:38:50,171 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39762; closing.
2023-10-04 05:38:50,171 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397930.17151')
2023-10-04 05:38:50,171 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:38:50,172 - distributed.nanny - INFO - Worker closed
2023-10-04 05:38:51,434 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:38:51,435 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:38:51,436 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:38:51,437 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-04 05:38:51,438 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-04 05:38:53,767 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:53,771 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45107 instead
  warnings.warn(
2023-10-04 05:38:53,774 - distributed.scheduler - INFO - State start
2023-10-04 05:38:54,062 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:38:54,062 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:38:54,063 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45107/status
2023-10-04 05:38:54,063 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:38:54,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32895'
2023-10-04 05:38:54,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38635'
2023-10-04 05:38:54,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43439'
2023-10-04 05:38:54,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41913'
2023-10-04 05:38:54,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36575'
2023-10-04 05:38:54,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44177'
2023-10-04 05:38:54,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43201'
2023-10-04 05:38:54,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40451'
2023-10-04 05:38:54,489 - distributed.scheduler - INFO - Receive client connection: Client-4d395be8-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:38:54,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41402
2023-10-04 05:38:55,270 - distributed.scheduler - INFO - Receive client connection: Client-4cdd38eb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:38:55,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41472
2023-10-04 05:38:55,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:55,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:55,975 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:55,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:55,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:55,988 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:56,024 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:56,024 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:56,025 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:56,025 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:56,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:56,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:56,059 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:56,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:56,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:56,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:56,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:56,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:56,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:56,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:56,074 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:56,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:38:56,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:38:56,125 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:38:59,480 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46197
2023-10-04 05:38:59,481 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46197
2023-10-04 05:38:59,481 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38441
2023-10-04 05:38:59,482 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:59,482 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:59,482 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:59,482 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:59,482 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztdsqpwf
2023-10-04 05:38:59,483 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d555f733-dd32-4b68-b45b-cf72001f263f
2023-10-04 05:38:59,485 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41429
2023-10-04 05:38:59,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41429
2023-10-04 05:38:59,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39405
2023-10-04 05:38:59,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:59,486 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:59,486 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:59,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:59,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8h34wrzy
2023-10-04 05:38:59,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24bfcee5-40ac-409d-aa19-9ee620531688
2023-10-04 05:38:59,701 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42977
2023-10-04 05:38:59,702 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42977
2023-10-04 05:38:59,702 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35607
2023-10-04 05:38:59,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:59,702 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:59,702 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:59,702 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:59,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2r9wcoe7
2023-10-04 05:38:59,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd362791-be4c-42d8-82a8-c1d67f9b4b89
2023-10-04 05:38:59,721 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36335
2023-10-04 05:38:59,722 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36335
2023-10-04 05:38:59,722 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35405
2023-10-04 05:38:59,722 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:38:59,722 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:38:59,722 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:38:59,722 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:38:59,722 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zzylhot_
2023-10-04 05:38:59,723 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0b2aeb5-a928-45df-82c0-3258811262ad
2023-10-04 05:38:59,723 - distributed.worker - INFO - Starting Worker plugin RMMSetup-986d9d03-c29a-405c-b149-05fa787debd7
2023-10-04 05:39:00,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83ebe8d5-36bd-4221-83d5-689252bcfd33
2023-10-04 05:39:00,272 - distributed.worker - INFO - Starting Worker plugin PreImport-a6daae94-c698-4e9f-959b-266a656a957f
2023-10-04 05:39:00,272 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-061ed419-ad6e-4005-b53d-0a4c443ca713
2023-10-04 05:39:00,273 - distributed.worker - INFO - Starting Worker plugin PreImport-cf1aa8d1-06af-4069-ae99-2167d6fc93f7
2023-10-04 05:39:00,273 - distributed.worker - INFO - Starting Worker plugin PreImport-5043ecb3-f70c-4c54-b4ef-5ef6f4f883c5
2023-10-04 05:39:00,274 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,274 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e66fdb6-0cb4-4362-978e-4b1da7c798ff
2023-10-04 05:39:00,293 - distributed.worker - INFO - Starting Worker plugin PreImport-3c158aed-ea90-46c3-8793-f569e602b780
2023-10-04 05:39:00,294 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42977', status: init, memory: 0, processing: 0>
2023-10-04 05:39:00,300 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42977
2023-10-04 05:39:00,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59314
2023-10-04 05:39:00,301 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36335', status: init, memory: 0, processing: 0>
2023-10-04 05:39:00,301 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:00,301 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36335
2023-10-04 05:39:00,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59316
2023-10-04 05:39:00,302 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:00,302 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:00,303 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:00,303 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:00,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:00,306 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46197', status: init, memory: 0, processing: 0>
2023-10-04 05:39:00,306 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46197
2023-10-04 05:39:00,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59330
2023-10-04 05:39:00,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:00,308 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:00,309 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:00,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41429', status: init, memory: 0, processing: 0>
2023-10-04 05:39:00,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41429
2023-10-04 05:39:00,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59338
2023-10-04 05:39:00,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:00,334 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:00,334 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:00,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:01,083 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33243
2023-10-04 05:39:01,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33243
2023-10-04 05:39:01,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43425
2023-10-04 05:39:01,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:39:01,084 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:01,085 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:39:01,085 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:39:01,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hbv1pvmm
2023-10-04 05:39:01,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-943c3b7b-475c-47c4-a473-687a64725773
2023-10-04 05:39:02,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70b87e0c-b2d7-499c-a06a-5835414f8a34
2023-10-04 05:39:02,055 - distributed.worker - INFO - Starting Worker plugin PreImport-097972b1-b58b-4c77-b0a8-2f2969704a99
2023-10-04 05:39:02,055 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,100 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33243', status: init, memory: 0, processing: 0>
2023-10-04 05:39:02,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33243
2023-10-04 05:39:02,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59378
2023-10-04 05:39:02,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:02,103 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:02,104 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:02,207 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37009
2023-10-04 05:39:02,208 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37009
2023-10-04 05:39:02,208 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33619
2023-10-04 05:39:02,208 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:39:02,208 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,208 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:39:02,208 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:39:02,208 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8l0f4gpp
2023-10-04 05:39:02,209 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d054bd8-823f-454c-b775-3cec9f60fe03
2023-10-04 05:39:02,357 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46393
2023-10-04 05:39:02,358 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46393
2023-10-04 05:39:02,358 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44065
2023-10-04 05:39:02,358 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:39:02,358 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,358 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:39:02,358 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:39:02,358 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-39ocnv9s
2023-10-04 05:39:02,359 - distributed.worker - INFO - Starting Worker plugin PreImport-a98e2a19-275f-411f-8065-840edaba75f4
2023-10-04 05:39:02,359 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ecb7ec71-7f9a-417e-b10d-52fa9530b8b0
2023-10-04 05:39:02,365 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2292312-36c7-4b43-88b2-5adff7336c15
2023-10-04 05:39:02,623 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a2e30d7-0d24-4658-9fe9-9b995b862f5b
2023-10-04 05:39:02,623 - distributed.worker - INFO - Starting Worker plugin PreImport-9915230f-052f-402c-8ffb-159d687f580b
2023-10-04 05:39:02,623 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37009', status: init, memory: 0, processing: 0>
2023-10-04 05:39:02,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37009
2023-10-04 05:39:02,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59388
2023-10-04 05:39:02,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:02,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:02,658 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:02,773 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46393', status: init, memory: 0, processing: 0>
2023-10-04 05:39:02,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46393
2023-10-04 05:39:02,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59396
2023-10-04 05:39:02,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:02,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:02,821 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:02,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:03,187 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45553
2023-10-04 05:39:03,187 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45553
2023-10-04 05:39:03,187 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41215
2023-10-04 05:39:03,188 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:39:03,188 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:03,188 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:39:03,188 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-04 05:39:03,188 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wb34h4vq
2023-10-04 05:39:03,188 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ed88f8e-d4b6-4427-a1c1-fa1700a15507
2023-10-04 05:39:03,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8dc55a1-a0c8-4481-932d-7692892042b5
2023-10-04 05:39:03,414 - distributed.worker - INFO - Starting Worker plugin PreImport-74172bb9-acc0-468e-9d56-716dd07895e9
2023-10-04 05:39:03,414 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:03,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45553', status: init, memory: 0, processing: 0>
2023-10-04 05:39:03,441 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45553
2023-10-04 05:39:03,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59414
2023-10-04 05:39:03,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:03,443 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:03,443 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:03,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:03,503 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,503 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,503 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,503 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,503 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,508 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,508 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,508 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-04 05:39:03,517 - distributed.scheduler - INFO - Remove client Client-4d395be8-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:39:03,517 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41402; closing.
2023-10-04 05:39:03,518 - distributed.scheduler - INFO - Remove client Client-4d395be8-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:39:03,519 - distributed.scheduler - INFO - Close client connection: Client-4d395be8-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:03,528 - distributed.scheduler - INFO - Remove client Client-4cdd38eb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:03,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41472; closing.
2023-10-04 05:39:03,528 - distributed.scheduler - INFO - Remove client Client-4cdd38eb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:03,528 - distributed.scheduler - INFO - Close client connection: Client-4cdd38eb-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:03,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32895'. Reason: nanny-close
2023-10-04 05:39:03,530 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38635'. Reason: nanny-close
2023-10-04 05:39:03,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41429. Reason: nanny-close
2023-10-04 05:39:03,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43439'. Reason: nanny-close
2023-10-04 05:39:03,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46197. Reason: nanny-close
2023-10-04 05:39:03,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41913'. Reason: nanny-close
2023-10-04 05:39:03,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36335. Reason: nanny-close
2023-10-04 05:39:03,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36575'. Reason: nanny-close
2023-10-04 05:39:03,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42977. Reason: nanny-close
2023-10-04 05:39:03,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44177'. Reason: nanny-close
2023-10-04 05:39:03,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43201'. Reason: nanny-close
2023-10-04 05:39:03,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46393. Reason: nanny-close
2023-10-04 05:39:03,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59330; closing.
2023-10-04 05:39:03,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59338; closing.
2023-10-04 05:39:03,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33243. Reason: nanny-close
2023-10-04 05:39:03,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40451'. Reason: nanny-close
2023-10-04 05:39:03,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:03,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.535487')
2023-10-04 05:39:03,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37009. Reason: nanny-close
2023-10-04 05:39:03,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,536 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.5361133')
2023-10-04 05:39:03,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45553. Reason: nanny-close
2023-10-04 05:39:03,536 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59314; closing.
2023-10-04 05:39:03,536 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,536 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,537 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42977', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.537119')
2023-10-04 05:39:03,537 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,537 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59316; closing.
2023-10-04 05:39:03,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:03,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.5384574')
2023-10-04 05:39:03,538 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,538 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,539 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,539 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:03,539 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59314>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-04 05:39:03,540 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59396; closing.
2023-10-04 05:39:03,540 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59378; closing.
2023-10-04 05:39:03,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59388; closing.
2023-10-04 05:39:03,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.5414095')
2023-10-04 05:39:03,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33243', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.541786')
2023-10-04 05:39:03,542 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37009', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.5421221')
2023-10-04 05:39:03,542 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59414; closing.
2023-10-04 05:39:03,542 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45553', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397943.54279')
2023-10-04 05:39:03,542 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:05,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32807', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,355 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32807
2023-10-04 05:39:05,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59466
2023-10-04 05:39:05,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35801', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,357 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35801
2023-10-04 05:39:05,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59424
2023-10-04 05:39:05,357 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42077', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42077
2023-10-04 05:39:05,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59454
2023-10-04 05:39:05,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34561', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,359 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34561
2023-10-04 05:39:05,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59464
2023-10-04 05:39:05,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38943', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,361 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38943
2023-10-04 05:39:05,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59438
2023-10-04 05:39:05,365 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59424; closing.
2023-10-04 05:39:05,365 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.3656056')
2023-10-04 05:39:05,367 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41299', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,367 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41299
2023-10-04 05:39:05,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59488
2023-10-04 05:39:05,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59466; closing.
2023-10-04 05:39:05,371 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.3712707')
2023-10-04 05:39:05,373 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35573', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,373 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35573
2023-10-04 05:39:05,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59472
2023-10-04 05:39:05,375 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45099', status: init, memory: 0, processing: 0>
2023-10-04 05:39:05,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45099
2023-10-04 05:39:05,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59490
2023-10-04 05:39:05,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59464; closing.
2023-10-04 05:39:05,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59488; closing.
2023-10-04 05:39:05,418 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34561', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.4180186')
2023-10-04 05:39:05,418 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41299', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.418423')
2023-10-04 05:39:05,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59454; closing.
2023-10-04 05:39:05,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.4200232')
2023-10-04 05:39:05,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59490; closing.
2023-10-04 05:39:05,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59472; closing.
2023-10-04 05:39:05,421 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.4210875')
2023-10-04 05:39:05,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59438; closing.
2023-10-04 05:39:05,421 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.421702')
2023-10-04 05:39:05,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397945.4221227')
2023-10-04 05:39:05,422 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:05,548 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:39:05,548 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:39:05,549 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:39:05,550 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:39:05,551 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-04 05:39:07,727 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:39:07,734 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33891 instead
  warnings.warn(
2023-10-04 05:39:07,741 - distributed.scheduler - INFO - State start
2023-10-04 05:39:07,772 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:39:07,773 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:39:07,774 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33891/status
2023-10-04 05:39:07,775 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:39:07,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39287'
2023-10-04 05:39:08,019 - distributed.scheduler - INFO - Receive client connection: Client-560ae8ba-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:39:08,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59642
2023-10-04 05:39:09,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:39:09,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:39:09,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:39:10,595 - distributed.scheduler - INFO - Receive client connection: Client-552f5785-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:10,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47822
2023-10-04 05:39:12,501 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39051
2023-10-04 05:39:12,502 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39051
2023-10-04 05:39:12,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45957
2023-10-04 05:39:12,503 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:39:12,503 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:12,503 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:39:12,503 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-04 05:39:12,503 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1qyr8q43
2023-10-04 05:39:12,504 - distributed.worker - INFO - Starting Worker plugin PreImport-96c630ce-734e-4010-95f3-b5b6e22b184b
2023-10-04 05:39:12,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c55c7465-abe0-41ad-b987-ea7dd4b8f9c2
2023-10-04 05:39:12,504 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b83138df-4b9d-40f5-a4c0-89b9a6b0087e
2023-10-04 05:39:12,768 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:12,939 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39051', status: init, memory: 0, processing: 0>
2023-10-04 05:39:12,940 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39051
2023-10-04 05:39:12,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47836
2023-10-04 05:39:12,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:12,942 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:12,942 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:12,944 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:13,042 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:39:13,046 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:13,048 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:13,050 - distributed.scheduler - INFO - Remove client Client-552f5785-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:13,050 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47822; closing.
2023-10-04 05:39:13,050 - distributed.scheduler - INFO - Remove client Client-552f5785-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:13,051 - distributed.scheduler - INFO - Close client connection: Client-552f5785-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:13,051 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39287'. Reason: nanny-close
2023-10-04 05:39:13,052 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:13,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39051. Reason: nanny-close
2023-10-04 05:39:13,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:13,055 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47836; closing.
2023-10-04 05:39:13,055 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397953.0558736')
2023-10-04 05:39:13,056 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:13,057 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:14,418 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:39:14,419 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:39:14,419 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:39:14,422 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:39:14,422 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-04 05:39:16,496 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:39:16,500 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36727 instead
  warnings.warn(
2023-10-04 05:39:16,503 - distributed.scheduler - INFO - State start
2023-10-04 05:39:16,525 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-04 05:39:16,525 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-04 05:39:16,526 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36727/status
2023-10-04 05:39:16,526 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-04 05:39:16,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46063'
2023-10-04 05:39:16,833 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44931', status: init, memory: 0, processing: 0>
2023-10-04 05:39:16,846 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44931
2023-10-04 05:39:16,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48844
2023-10-04 05:39:16,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45477', status: init, memory: 0, processing: 0>
2023-10-04 05:39:16,852 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45477
2023-10-04 05:39:16,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48852
2023-10-04 05:39:16,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48852; closing.
2023-10-04 05:39:16,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397956.868124')
2023-10-04 05:39:16,900 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48844; closing.
2023-10-04 05:39:16,900 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44931', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397956.9007506')
2023-10-04 05:39:16,900 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:16,974 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42155', status: init, memory: 0, processing: 0>
2023-10-04 05:39:16,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42155
2023-10-04 05:39:16,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48864
2023-10-04 05:39:17,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48864; closing.
2023-10-04 05:39:17,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397957.0016205')
2023-10-04 05:39:17,001 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:17,038 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34243', status: init, memory: 0, processing: 0>
2023-10-04 05:39:17,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34243
2023-10-04 05:39:17,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48874
2023-10-04 05:39:17,051 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48874; closing.
2023-10-04 05:39:17,052 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34243', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397957.0521617')
2023-10-04 05:39:17,052 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:17,084 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43027', status: init, memory: 0, processing: 0>
2023-10-04 05:39:17,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43027
2023-10-04 05:39:17,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48878
2023-10-04 05:39:17,102 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48878; closing.
2023-10-04 05:39:17,103 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43027', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397957.10314')
2023-10-04 05:39:17,103 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:17,431 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41759', status: init, memory: 0, processing: 0>
2023-10-04 05:39:17,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41759
2023-10-04 05:39:17,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48894
2023-10-04 05:39:17,458 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48894; closing.
2023-10-04 05:39:17,459 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397957.4590113')
2023-10-04 05:39:17,459 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:17,467 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34601', status: init, memory: 0, processing: 0>
2023-10-04 05:39:17,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34601
2023-10-04 05:39:17,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48896
2023-10-04 05:39:17,510 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48896; closing.
2023-10-04 05:39:17,511 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397957.511368')
2023-10-04 05:39:17,511 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:18,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-04 05:39:18,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-04 05:39:18,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-04 05:39:19,201 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44513
2023-10-04 05:39:19,202 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44513
2023-10-04 05:39:19,202 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36967
2023-10-04 05:39:19,202 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-04 05:39:19,202 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:19,202 - distributed.worker - INFO -               Threads:                          1
2023-10-04 05:39:19,202 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-04 05:39:19,202 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qsdwiue0
2023-10-04 05:39:19,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d1a5b7e-841c-444e-bf60-cb156b556bb8
2023-10-04 05:39:19,203 - distributed.worker - INFO - Starting Worker plugin PreImport-3b2af1d3-e590-4ec2-965b-39aaa96fde29
2023-10-04 05:39:19,203 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3976496d-33d2-416c-ab68-8cb7bb46ead5
2023-10-04 05:39:19,355 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:19,384 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44513', status: init, memory: 0, processing: 0>
2023-10-04 05:39:19,385 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44513
2023-10-04 05:39:19,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48908
2023-10-04 05:39:19,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-04 05:39:19,387 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-04 05:39:19,387 - distributed.worker - INFO - -------------------------------------------------
2023-10-04 05:39:19,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-04 05:39:20,326 - distributed.scheduler - INFO - Receive client connection: Client-560ae8ba-6278-11ee-b5b3-d8c49764f6bb
2023-10-04 05:39:20,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53428
2023-10-04 05:39:20,471 - distributed.scheduler - INFO - Receive client connection: Client-5a7e6293-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:20,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53436
2023-10-04 05:39:20,477 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-04 05:39:20,482 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-04 05:39:20,485 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:20,487 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-04 05:39:20,490 - distributed.scheduler - INFO - Remove client Client-5a7e6293-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:20,490 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53436; closing.
2023-10-04 05:39:20,490 - distributed.scheduler - INFO - Remove client Client-5a7e6293-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:20,490 - distributed.scheduler - INFO - Close client connection: Client-5a7e6293-6278-11ee-b643-d8c49764f6bb
2023-10-04 05:39:20,491 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46063'. Reason: nanny-close
2023-10-04 05:39:20,492 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-04 05:39:20,493 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44513. Reason: nanny-close
2023-10-04 05:39:20,495 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-04 05:39:20,495 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48908; closing.
2023-10-04 05:39:20,495 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696397960.495406')
2023-10-04 05:39:20,495 - distributed.scheduler - INFO - Lost all workers
2023-10-04 05:39:20,496 - distributed.nanny - INFO - Worker closed
2023-10-04 05:39:21,908 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-04 05:39:21,908 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-04 05:39:21,909 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-04 05:39:21,911 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-04 05:39:21,911 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45857 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37619 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46315 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36111 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37315 instead
  warnings.warn(
2023-10-04 05:40:25,039 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-04 05:40:25,048 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:50255', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41385 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37003 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34517 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40425 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43429 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39127 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44261 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38917 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42051 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37709 instead
  warnings.warn(
[1696398198.936757] [dgx13:71144:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:38140) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41115 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36233 instead
  warnings.warn(
[1696398228.600492] [dgx13:71681:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:58163) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35493 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40233 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46107 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36839 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41643 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41729 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43625 instead
  warnings.warn(
2023-10-04 05:46:11,273 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-04 05:46:11,286 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46043'.
2023-10-04 05:46:11,287 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:46043'. Shutting down.
2023-10-04 05:46:11,290 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc5055b57c0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-04 05:46:13,293 - distributed.nanny - ERROR - Worker process died unexpectedly
