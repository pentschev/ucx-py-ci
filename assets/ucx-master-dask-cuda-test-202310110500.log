============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-11 05:31:35,362 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:35,367 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42801 instead
  warnings.warn(
2023-10-11 05:31:35,370 - distributed.scheduler - INFO - State start
2023-10-11 05:31:35,392 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:35,393 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-11 05:31:35,393 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42801/status
2023-10-11 05:31:35,394 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:31:35,565 - distributed.scheduler - INFO - Receive client connection: Client-7076ab6f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:35,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39066
2023-10-11 05:31:35,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45965'
2023-10-11 05:31:35,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40665'
2023-10-11 05:31:35,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33903'
2023-10-11 05:31:35,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36705'
2023-10-11 05:31:37,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:37,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:37,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:37,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:37,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-11 05:31:37,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38087
2023-10-11 05:31:37,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38087
2023-10-11 05:31:37,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41237
2023-10-11 05:31:37,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:31:37,312 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:37,312 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:31:37,313 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:31:37,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-q2s5cazz
2023-10-11 05:31:37,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86289e30-0589-4c94-ad4e-b8be049b03dd
2023-10-11 05:31:37,313 - distributed.worker - INFO - Starting Worker plugin PreImport-16a527d9-789a-4777-b842-587465cd8b83
2023-10-11 05:31:37,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0c0c60a-88be-4301-9a78-ea0d3797684c
2023-10-11 05:31:37,314 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:37,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:37,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:37,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:37,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:37,352 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:37,352 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:37,357 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:37,423 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38087', status: init, memory: 0, processing: 0>
2023-10-11 05:31:37,424 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38087
2023-10-11 05:31:37,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39112
2023-10-11 05:31:37,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:37,426 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:31:37,427 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:37,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:31:38,642 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44063
2023-10-11 05:31:38,643 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44063
2023-10-11 05:31:38,643 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43775
2023-10-11 05:31:38,643 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:31:38,643 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,643 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:31:38,644 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:31:38,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fo98efgx
2023-10-11 05:31:38,644 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04265efa-8369-47e7-9bb3-3d5d81a071a3
2023-10-11 05:31:38,644 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4cabfee3-a8fc-44fe-a59c-48ffcd1fac88
2023-10-11 05:31:38,645 - distributed.worker - INFO - Starting Worker plugin PreImport-bec003be-fa4a-4c18-987a-d7e3bcf5e73f
2023-10-11 05:31:38,645 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,668 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36373
2023-10-11 05:31:38,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36373
2023-10-11 05:31:38,669 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46541
2023-10-11 05:31:38,669 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:31:38,669 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,669 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:31:38,669 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:31:38,669 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-li6vbpm3
2023-10-11 05:31:38,669 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42469
2023-10-11 05:31:38,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42469
2023-10-11 05:31:38,670 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bd74e23-c1ec-4c6c-9de3-bfd26cdf9ef4
2023-10-11 05:31:38,670 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39635
2023-10-11 05:31:38,670 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:31:38,670 - distributed.worker - INFO - Starting Worker plugin PreImport-a176e029-d1cd-45df-9be3-d9f5f75ff610
2023-10-11 05:31:38,670 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,670 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:31:38,670 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5635f279-f80f-40d0-8404-a922ef60c379
2023-10-11 05:31:38,670 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:31:38,670 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ix4wbwcs
2023-10-11 05:31:38,670 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a1bb04d-7158-47fc-9a35-04a11d771f87
2023-10-11 05:31:38,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f42aa25c-85fd-458d-aab1-d1ddd2494b85
2023-10-11 05:31:38,671 - distributed.worker - INFO - Starting Worker plugin PreImport-429eda53-3c56-4f8e-a489-cfd5731cb594
2023-10-11 05:31:38,671 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,677 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44063', status: init, memory: 0, processing: 0>
2023-10-11 05:31:38,678 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44063
2023-10-11 05:31:38,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39120
2023-10-11 05:31:38,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:38,681 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:31:38,681 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:31:38,691 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36373', status: init, memory: 0, processing: 0>
2023-10-11 05:31:38,691 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36373
2023-10-11 05:31:38,691 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39124
2023-10-11 05:31:38,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:38,693 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:31:38,693 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:31:38,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42469', status: init, memory: 0, processing: 0>
2023-10-11 05:31:38,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42469
2023-10-11 05:31:38,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39138
2023-10-11 05:31:38,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:38,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:31:38,698 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:38,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:31:38,712 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:31:38,712 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:31:38,712 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:31:38,713 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:31:38,718 - distributed.scheduler - INFO - Remove client Client-7076ab6f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:38,719 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39066; closing.
2023-10-11 05:31:38,719 - distributed.scheduler - INFO - Remove client Client-7076ab6f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:38,719 - distributed.scheduler - INFO - Close client connection: Client-7076ab6f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:38,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40665'. Reason: nanny-close
2023-10-11 05:31:38,721 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:38,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33903'. Reason: nanny-close
2023-10-11 05:31:38,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:38,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44063. Reason: nanny-close
2023-10-11 05:31:38,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45965'. Reason: nanny-close
2023-10-11 05:31:38,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:38,723 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36373. Reason: nanny-close
2023-10-11 05:31:38,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36705'. Reason: nanny-close
2023-10-11 05:31:38,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:38,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42469. Reason: nanny-close
2023-10-11 05:31:38,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38087. Reason: nanny-close
2023-10-11 05:31:38,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:31:38,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:31:38,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39120; closing.
2023-10-11 05:31:38,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39124; closing.
2023-10-11 05:31:38,725 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44063', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002298.7258072')
2023-10-11 05:31:38,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:31:38,726 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002298.726231')
2023-10-11 05:31:38,726 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:38,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:31:38,726 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:38,727 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:38,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39112; closing.
2023-10-11 05:31:38,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39138; closing.
2023-10-11 05:31:38,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002298.7281218')
2023-10-11 05:31:38,728 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:38,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002298.7285767')
2023-10-11 05:31:38,728 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:31:39,736 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:31:39,737 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:31:39,737 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:31:39,738 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-11 05:31:39,738 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-11 05:31:41,864 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:41,868 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40571 instead
  warnings.warn(
2023-10-11 05:31:41,872 - distributed.scheduler - INFO - State start
2023-10-11 05:31:41,892 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:41,893 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:31:41,894 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40571/status
2023-10-11 05:31:41,894 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:31:42,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42401'
2023-10-11 05:31:42,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36395'
2023-10-11 05:31:42,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44575'
2023-10-11 05:31:42,075 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40153'
2023-10-11 05:31:42,077 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34331'
2023-10-11 05:31:42,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42383'
2023-10-11 05:31:42,093 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37539'
2023-10-11 05:31:42,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44093'
2023-10-11 05:31:43,650 - distributed.scheduler - INFO - Receive client connection: Client-745cf6c1-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:43,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41946
2023-10-11 05:31:43,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:43,885 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:43,885 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:43,885 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:43,889 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:43,889 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:43,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:43,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:43,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:43,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:43,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:43,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:43,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:43,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:43,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:44,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:44,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:44,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:44,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:44,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:44,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:44,115 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:44,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:44,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:46,683 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36801
2023-10-11 05:31:46,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36801
2023-10-11 05:31:46,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34367
2023-10-11 05:31:46,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,684 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,684 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,684 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ot32umnu
2023-10-11 05:31:46,685 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8512a0df-b763-4a7f-9a99-502f13e2bfd6
2023-10-11 05:31:46,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0bdaecb-6e09-49c8-954d-0a0602ef818f
2023-10-11 05:31:46,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43449
2023-10-11 05:31:46,685 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43449
2023-10-11 05:31:46,685 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33785
2023-10-11 05:31:46,685 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,685 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,685 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,686 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g1sml9ka
2023-10-11 05:31:46,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c61a8949-206e-4035-825a-aaa71492a8fb
2023-10-11 05:31:46,686 - distributed.worker - INFO - Starting Worker plugin PreImport-02c1af09-e486-4438-beb0-cf5fbe1b7fcc
2023-10-11 05:31:46,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-446e6e0c-de84-4b68-9805-0ca7aa6415a9
2023-10-11 05:31:46,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43117
2023-10-11 05:31:46,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43117
2023-10-11 05:31:46,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46409
2023-10-11 05:31:46,733 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,733 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,733 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_sxzalkv
2023-10-11 05:31:46,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33717
2023-10-11 05:31:46,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33717
2023-10-11 05:31:46,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40635
2023-10-11 05:31:46,734 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38b4dde1-6867-43c9-9750-02b08ce67bfe
2023-10-11 05:31:46,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,734 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,734 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mumha9yg
2023-10-11 05:31:46,736 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e8838ab-413f-4510-8d14-91d51230a589
2023-10-11 05:31:46,736 - distributed.worker - INFO - Starting Worker plugin PreImport-0d0c4d85-d9b1-4d6c-8b20-bc72bf69ad4b
2023-10-11 05:31:46,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4ac7ca3-6081-42bb-a5e3-4f0479bc346d
2023-10-11 05:31:46,847 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41023
2023-10-11 05:31:46,847 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41023
2023-10-11 05:31:46,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34739
2023-10-11 05:31:46,848 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,848 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,848 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,848 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,848 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-daigvpmb
2023-10-11 05:31:46,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37cb1576-4af0-46dd-b22f-c28decd5b930
2023-10-11 05:31:46,849 - distributed.worker - INFO - Starting Worker plugin PreImport-18503875-4dd3-4b55-874f-9447218501fa
2023-10-11 05:31:46,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-383832bb-f629-4d12-b8d5-581e6c30d593
2023-10-11 05:31:46,855 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,856 - distributed.worker - INFO - Starting Worker plugin PreImport-c897a87a-382c-414e-b316-3b3f0db3f7a2
2023-10-11 05:31:46,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,857 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40313
2023-10-11 05:31:46,858 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40313
2023-10-11 05:31:46,858 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37363
2023-10-11 05:31:46,858 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,858 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,858 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,858 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,858 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-os4zq86r
2023-10-11 05:31:46,859 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca61c8d5-b8ef-48b3-acdd-da8892169238
2023-10-11 05:31:46,859 - distributed.worker - INFO - Starting Worker plugin PreImport-5809afd3-c0c8-4f28-8e89-b4ad6c268860
2023-10-11 05:31:46,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2926dc0-0083-4322-9460-3f0542165c7c
2023-10-11 05:31:46,862 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33453
2023-10-11 05:31:46,863 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33453
2023-10-11 05:31:46,863 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39653
2023-10-11 05:31:46,863 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,862 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43617
2023-10-11 05:31:46,863 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,863 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43617
2023-10-11 05:31:46,863 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32909
2023-10-11 05:31:46,863 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,863 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,863 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,863 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2pto2xad
2023-10-11 05:31:46,863 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:46,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:46,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jv0x4m2r
2023-10-11 05:31:46,864 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3c80391-ec6b-4ee1-bc91-b8ae10e3ddf6
2023-10-11 05:31:46,864 - distributed.worker - INFO - Starting Worker plugin PreImport-49f8e549-929a-47ba-a812-a152f2216c79
2023-10-11 05:31:46,864 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-304aa051-f838-4fa0-8162-e3fdd2902a13
2023-10-11 05:31:46,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-854eb806-66b5-47fc-b325-214048dfc123
2023-10-11 05:31:46,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65b3e894-cdeb-49df-bfe0-55cba9f50d71
2023-10-11 05:31:46,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36801', status: init, memory: 0, processing: 0>
2023-10-11 05:31:46,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36801
2023-10-11 05:31:46,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41990
2023-10-11 05:31:46,883 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43449', status: init, memory: 0, processing: 0>
2023-10-11 05:31:46,883 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:46,884 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43449
2023-10-11 05:31:46,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41978
2023-10-11 05:31:46,884 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,884 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:46,886 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,886 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:46,887 - distributed.worker - INFO - Starting Worker plugin PreImport-c986597d-e665-42f5-a131-8baba7a05064
2023-10-11 05:31:46,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:46,888 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-743daae5-704e-4582-a017-3244e189bc6c
2023-10-11 05:31:46,888 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,888 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43117', status: init, memory: 0, processing: 0>
2023-10-11 05:31:46,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43117
2023-10-11 05:31:46,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41992
2023-10-11 05:31:46,910 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:46,911 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,911 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:46,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33717', status: init, memory: 0, processing: 0>
2023-10-11 05:31:46,919 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33717
2023-10-11 05:31:46,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41998
2023-10-11 05:31:46,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:46,920 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:46,921 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:46,984 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,984 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,991 - distributed.worker - INFO - Starting Worker plugin PreImport-bbb309f3-b89a-4cf0-b4cb-3d1f5017336c
2023-10-11 05:31:46,991 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:46,994 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:47,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40313', status: init, memory: 0, processing: 0>
2023-10-11 05:31:47,017 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40313
2023-10-11 05:31:47,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42006
2023-10-11 05:31:47,018 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41023', status: init, memory: 0, processing: 0>
2023-10-11 05:31:47,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41023
2023-10-11 05:31:47,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:47,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42004
2023-10-11 05:31:47,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:47,019 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:47,020 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:47,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:47,021 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:47,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:47,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:47,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33453', status: init, memory: 0, processing: 0>
2023-10-11 05:31:47,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33453
2023-10-11 05:31:47,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42020
2023-10-11 05:31:47,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43617', status: init, memory: 0, processing: 0>
2023-10-11 05:31:47,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43617
2023-10-11 05:31:47,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42030
2023-10-11 05:31:47,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:47,027 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:47,027 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:47,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:47,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:47,029 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:47,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:47,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:47,070 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,070 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,071 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,071 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,071 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,071 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,071 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,072 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:47,075 - distributed.scheduler - INFO - Remove client Client-745cf6c1-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:47,076 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41946; closing.
2023-10-11 05:31:47,076 - distributed.scheduler - INFO - Remove client Client-745cf6c1-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:47,076 - distributed.scheduler - INFO - Close client connection: Client-745cf6c1-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:47,077 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42401'. Reason: nanny-close
2023-10-11 05:31:47,078 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,078 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36395'. Reason: nanny-close
2023-10-11 05:31:47,079 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,079 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43617. Reason: nanny-close
2023-10-11 05:31:47,079 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44575'. Reason: nanny-close
2023-10-11 05:31:47,079 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,080 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33453. Reason: nanny-close
2023-10-11 05:31:47,080 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40153'. Reason: nanny-close
2023-10-11 05:31:47,080 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,080 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43449. Reason: nanny-close
2023-10-11 05:31:47,080 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34331'. Reason: nanny-close
2023-10-11 05:31:47,081 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,081 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43117. Reason: nanny-close
2023-10-11 05:31:47,081 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42383'. Reason: nanny-close
2023-10-11 05:31:47,081 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,081 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42030; closing.
2023-10-11 05:31:47,082 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40313. Reason: nanny-close
2023-10-11 05:31:47,082 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37539'. Reason: nanny-close
2023-10-11 05:31:47,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43617', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.082204')
2023-10-11 05:31:47,082 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,082 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,082 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,082 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41023. Reason: nanny-close
2023-10-11 05:31:47,082 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44093'. Reason: nanny-close
2023-10-11 05:31:47,083 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:47,083 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,083 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33717. Reason: nanny-close
2023-10-11 05:31:47,083 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36801. Reason: nanny-close
2023-10-11 05:31:47,083 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,083 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41992; closing.
2023-10-11 05:31:47,083 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,084 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42020; closing.
2023-10-11 05:31:47,084 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,084 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41978; closing.
2023-10-11 05:31:47,084 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,084 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.084918')
2023-10-11 05:31:47,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.0852828')
2023-10-11 05:31:47,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.0856297')
2023-10-11 05:31:47,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:47,086 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42006; closing.
2023-10-11 05:31:47,086 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,087 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.0869749')
2023-10-11 05:31:47,087 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,087 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42004; closing.
2023-10-11 05:31:47,087 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,087 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41998; closing.
2023-10-11 05:31:47,087 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.0879326')
2023-10-11 05:31:47,088 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:47,088 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.0883222')
2023-10-11 05:31:47,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41990; closing.
2023-10-11 05:31:47,089 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002307.089076')
2023-10-11 05:31:47,089 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:31:48,645 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:31:48,645 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:31:48,646 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:31:48,647 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:31:48,647 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-11 05:31:50,746 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:50,750 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46297 instead
  warnings.warn(
2023-10-11 05:31:50,755 - distributed.scheduler - INFO - State start
2023-10-11 05:31:50,783 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:50,784 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:31:50,785 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46297/status
2023-10-11 05:31:50,785 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:31:50,842 - distributed.scheduler - INFO - Receive client connection: Client-79a43f00-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:50,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39376
2023-10-11 05:31:50,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37681'
2023-10-11 05:31:50,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42933'
2023-10-11 05:31:50,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32861'
2023-10-11 05:31:50,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39093'
2023-10-11 05:31:50,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34447'
2023-10-11 05:31:50,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41795'
2023-10-11 05:31:50,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42309'
2023-10-11 05:31:50,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46811'
2023-10-11 05:31:52,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,662 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:31:52,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:31:52,935 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:52,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:31:54,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37303
2023-10-11 05:31:54,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37303
2023-10-11 05:31:54,507 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38979
2023-10-11 05:31:54,507 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:54,507 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:54,507 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:54,507 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:54,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pkogscja
2023-10-11 05:31:54,507 - distributed.worker - INFO - Starting Worker plugin PreImport-3840e8fb-b8c1-4c94-9407-a70ef0085da7
2023-10-11 05:31:54,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-518f39d7-fb59-467d-84ad-4c829d3eb4b5
2023-10-11 05:31:54,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d23d286e-e9d0-4d87-8b1b-d3f4134c9134
2023-10-11 05:31:54,578 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:54,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37303', status: init, memory: 0, processing: 0>
2023-10-11 05:31:54,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37303
2023-10-11 05:31:54,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39472
2023-10-11 05:31:54,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:54,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:54,607 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:54,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,423 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38111
2023-10-11 05:31:55,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38111
2023-10-11 05:31:55,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33997
2023-10-11 05:31:55,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,425 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,425 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,425 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-roap966b
2023-10-11 05:31:55,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d07cbf53-b379-473c-a8ca-914828076e93
2023-10-11 05:31:55,436 - distributed.worker - INFO - Starting Worker plugin PreImport-607036ee-1e66-4cdd-acbd-a8d8d82dbf17
2023-10-11 05:31:55,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-577ba4ae-89b2-4469-8a1c-45ef8cbb1dbd
2023-10-11 05:31:55,436 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,458 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38111', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38111
2023-10-11 05:31:55,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39476
2023-10-11 05:31:55,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,461 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,591 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36835
2023-10-11 05:31:55,591 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36835
2023-10-11 05:31:55,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45279
2023-10-11 05:31:55,592 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,592 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,592 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,592 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-umr_rpla
2023-10-11 05:31:55,592 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec50a0ba-bc04-47a0-812a-f31c5a29f057
2023-10-11 05:31:55,593 - distributed.worker - INFO - Starting Worker plugin PreImport-b84463d1-7800-4586-990b-50f049a7048f
2023-10-11 05:31:55,593 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19564417-28ef-483b-8b9c-7f95fc09e2e2
2023-10-11 05:31:55,593 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42391
2023-10-11 05:31:55,594 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42391
2023-10-11 05:31:55,594 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46779
2023-10-11 05:31:55,594 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,594 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,594 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,594 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,594 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6hkggdx8
2023-10-11 05:31:55,593 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37723
2023-10-11 05:31:55,594 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37723
2023-10-11 05:31:55,594 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35493
2023-10-11 05:31:55,595 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,595 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,595 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8536f2db-870a-4e44-a781-eaa16999b321
2023-10-11 05:31:55,595 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,595 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a3y9_4xh
2023-10-11 05:31:55,595 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a01b294-035e-45b0-8c24-308e62b6c536
2023-10-11 05:31:55,595 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42859
2023-10-11 05:31:55,596 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42859
2023-10-11 05:31:55,596 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45155
2023-10-11 05:31:55,596 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,596 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,596 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k43r_7j4
2023-10-11 05:31:55,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b43b154-7ac3-4459-a5cf-1947e2d9a9a4
2023-10-11 05:31:55,598 - distributed.worker - INFO - Starting Worker plugin PreImport-f65bbe99-9e42-48d5-a9f5-8c29089e7dd7
2023-10-11 05:31:55,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2d50545-d31d-4402-8dd4-db369ba08133
2023-10-11 05:31:55,599 - distributed.worker - INFO - Starting Worker plugin PreImport-3c186bf0-02e0-4ca9-b76b-b3a3a573b904
2023-10-11 05:31:55,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-903fbb73-401a-400c-8aaf-719d5ffb0af8
2023-10-11 05:31:55,600 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44463
2023-10-11 05:31:55,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44463
2023-10-11 05:31:55,601 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43019
2023-10-11 05:31:55,601 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,601 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,601 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_silwrcv
2023-10-11 05:31:55,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-472c8517-2f79-42e8-a951-28b50fa65256
2023-10-11 05:31:55,602 - distributed.worker - INFO - Starting Worker plugin PreImport-689fe7d7-5f5b-4fbb-a8e9-ac787cdce434
2023-10-11 05:31:55,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf21dded-c1b9-4e77-8388-6a944d8a0b96
2023-10-11 05:31:55,604 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41315
2023-10-11 05:31:55,605 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41315
2023-10-11 05:31:55,605 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35205
2023-10-11 05:31:55,605 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,605 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,605 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:31:55,606 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:31:55,606 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zjv2pril
2023-10-11 05:31:55,606 - distributed.worker - INFO - Starting Worker plugin PreImport-450c2e0c-419b-43f6-a63b-8302ef11e49d
2023-10-11 05:31:55,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60bbbca1-5b1e-4cee-8bfd-c661c54bced3
2023-10-11 05:31:55,606 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d29de457-8387-4738-a38b-b742b6ce42ca
2023-10-11 05:31:55,623 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,626 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,629 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,632 - distributed.worker - INFO - Starting Worker plugin PreImport-8c789bd1-1f3a-4a70-8e2f-ca8ef7e46a96
2023-10-11 05:31:55,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b2658eb-a5f7-43e6-81dd-e1b9154461d6
2023-10-11 05:31:55,633 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,634 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,635 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,650 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36835', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,650 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36835
2023-10-11 05:31:55,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39492
2023-10-11 05:31:55,651 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44463', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44463
2023-10-11 05:31:55,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39506
2023-10-11 05:31:55,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37723', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,653 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,653 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,653 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37723
2023-10-11 05:31:55,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39504
2023-10-11 05:31:55,654 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,654 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42391', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,656 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,656 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42391
2023-10-11 05:31:55,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39520
2023-10-11 05:31:55,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,657 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41315', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,658 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,658 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41315
2023-10-11 05:31:55,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39530
2023-10-11 05:31:55,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,660 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,660 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42859', status: init, memory: 0, processing: 0>
2023-10-11 05:31:55,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,661 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42859
2023-10-11 05:31:55,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39544
2023-10-11 05:31:55,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:31:55,664 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:31:55,664 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:31:55,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:31:55,681 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,682 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:31:55,687 - distributed.scheduler - INFO - Remove client Client-79a43f00-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:55,687 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39376; closing.
2023-10-11 05:31:55,687 - distributed.scheduler - INFO - Remove client Client-79a43f00-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:55,688 - distributed.scheduler - INFO - Close client connection: Client-79a43f00-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:55,689 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37681'. Reason: nanny-close
2023-10-11 05:31:55,689 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42933'. Reason: nanny-close
2023-10-11 05:31:55,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32861'. Reason: nanny-close
2023-10-11 05:31:55,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39093'. Reason: nanny-close
2023-10-11 05:31:55,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34447'. Reason: nanny-close
2023-10-11 05:31:55,690 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,691 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41795'. Reason: nanny-close
2023-10-11 05:31:55,691 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42309'. Reason: nanny-close
2023-10-11 05:31:55,691 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38111. Reason: nanny-close
2023-10-11 05:31:55,691 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,692 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46811'. Reason: nanny-close
2023-10-11 05:31:55,692 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37303. Reason: nanny-close
2023-10-11 05:31:55,693 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,693 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39476; closing.
2023-10-11 05:31:55,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.694121')
2023-10-11 05:31:55,694 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,695 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39472; closing.
2023-10-11 05:31:55,696 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,696 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37303', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.6967194')
2023-10-11 05:31:55,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,706 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44463. Reason: nanny-close
2023-10-11 05:31:55,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42859. Reason: nanny-close
2023-10-11 05:31:55,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36835. Reason: nanny-close
2023-10-11 05:31:55,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42391. Reason: nanny-close
2023-10-11 05:31:55,708 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37723. Reason: nanny-close
2023-10-11 05:31:55,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:31:55,708 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39506; closing.
2023-10-11 05:31:55,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,709 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.70906')
2023-10-11 05:31:55,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41315. Reason: nanny-close
2023-10-11 05:31:55,709 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,709 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,709 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39544; closing.
2023-10-11 05:31:55,709 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,710 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,710 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,710 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39520; closing.
2023-10-11 05:31:55,710 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.7107987')
2023-10-11 05:31:55,711 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,711 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,711 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39492; closing.
2023-10-11 05:31:55,711 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:31:55,711 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.7117858')
2023-10-11 05:31:55,712 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39504; closing.
2023-10-11 05:31:55,712 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.712345')
2023-10-11 05:31:55,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37723', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.7128763')
2023-10-11 05:31:55,712 - distributed.nanny - INFO - Worker closed
2023-10-11 05:31:55,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39530; closing.
2023-10-11 05:31:55,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41315', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002315.7135775')
2023-10-11 05:31:55,713 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:31:57,207 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:31:57,207 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:31:57,208 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:31:57,209 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:31:57,209 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-11 05:31:59,270 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:59,274 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44819 instead
  warnings.warn(
2023-10-11 05:31:59,278 - distributed.scheduler - INFO - State start
2023-10-11 05:31:59,298 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:31:59,299 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:31:59,300 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44819/status
2023-10-11 05:31:59,300 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:31:59,455 - distributed.scheduler - INFO - Receive client connection: Client-7ebceb0b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:31:59,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39612
2023-10-11 05:31:59,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42393'
2023-10-11 05:31:59,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35589'
2023-10-11 05:31:59,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43585'
2023-10-11 05:31:59,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38825'
2023-10-11 05:31:59,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39033'
2023-10-11 05:31:59,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36807'
2023-10-11 05:31:59,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46523'
2023-10-11 05:31:59,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42581'
2023-10-11 05:32:01,430 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,443 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,443 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,448 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,449 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,449 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,454 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,492 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,501 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:01,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:01,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:01,650 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:04,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44147
2023-10-11 05:32:04,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44147
2023-10-11 05:32:04,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36677
2023-10-11 05:32:04,190 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,190 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,190 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,190 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cax01lrl
2023-10-11 05:32:04,191 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b205a1da-deaf-4c48-b83f-f6afcdac3093
2023-10-11 05:32:04,199 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43479
2023-10-11 05:32:04,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43479
2023-10-11 05:32:04,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39093
2023-10-11 05:32:04,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,200 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,200 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,201 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,201 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bbugh4u9
2023-10-11 05:32:04,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96e33e51-1951-4371-9b5d-d719ba4e2daf
2023-10-11 05:32:04,201 - distributed.worker - INFO - Starting Worker plugin PreImport-155d148f-fd63-4e4e-b1ba-282b3c8bb287
2023-10-11 05:32:04,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-18411c4e-90fe-4859-be13-2c76b29c2c21
2023-10-11 05:32:04,231 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46531
2023-10-11 05:32:04,232 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46531
2023-10-11 05:32:04,232 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46249
2023-10-11 05:32:04,232 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,232 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,232 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,233 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,233 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8fc2to8j
2023-10-11 05:32:04,233 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c09ac46-13d1-4a8f-8265-7a3108d96853
2023-10-11 05:32:04,234 - distributed.worker - INFO - Starting Worker plugin PreImport-1185a14a-d9b2-4589-af2b-a1ab0c7443ce
2023-10-11 05:32:04,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5fc494a-0cbe-46a9-bb9c-6a9c85c2efba
2023-10-11 05:32:04,234 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34757
2023-10-11 05:32:04,235 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34757
2023-10-11 05:32:04,235 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34447
2023-10-11 05:32:04,234 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46613
2023-10-11 05:32:04,235 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,235 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46613
2023-10-11 05:32:04,235 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,235 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33577
2023-10-11 05:32:04,235 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,235 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,235 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,235 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,236 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-srlj2xav
2023-10-11 05:32:04,236 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,236 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,236 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vc6whd_t
2023-10-11 05:32:04,236 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d2a3f13-e324-4ff5-a8a7-016ef8a13dc5
2023-10-11 05:32:04,236 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb78b757-47de-4514-bfa9-eda82a569c93
2023-10-11 05:32:04,236 - distributed.worker - INFO - Starting Worker plugin PreImport-bda3d5a0-8a92-4fb6-b03f-310f4b3a4edf
2023-10-11 05:32:04,236 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c86c36a3-3606-419c-9475-85f976b81e73
2023-10-11 05:32:04,238 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37769
2023-10-11 05:32:04,239 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37769
2023-10-11 05:32:04,239 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46357
2023-10-11 05:32:04,239 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,239 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,239 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,239 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zxectnd7
2023-10-11 05:32:04,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-627eaa76-6093-41d0-8bda-e4eae03e8324
2023-10-11 05:32:04,241 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34655
2023-10-11 05:32:04,242 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34655
2023-10-11 05:32:04,242 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40473
2023-10-11 05:32:04,242 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,242 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,242 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,242 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,242 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-razdr3fp
2023-10-11 05:32:04,243 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9fb3d4d-2166-4ac6-98a5-cff71e4bfd70
2023-10-11 05:32:04,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43587
2023-10-11 05:32:04,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43587
2023-10-11 05:32:04,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40341
2023-10-11 05:32:04,344 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,344 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,344 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:04,345 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:04,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o_s4kyjo
2023-10-11 05:32:04,347 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2dccc98f-d503-42e2-af61-8cf7476992c8
2023-10-11 05:32:04,347 - distributed.worker - INFO - Starting Worker plugin PreImport-abd2fcc6-f455-40a8-95be-c37e17906ae4
2023-10-11 05:32:04,354 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a04bbce6-1802-4795-8f42-857a91e30451
2023-10-11 05:32:04,419 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,425 - distributed.worker - INFO - Starting Worker plugin PreImport-eff66b5f-41af-47e0-a768-903e7472dadc
2023-10-11 05:32:04,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce8b6f50-2cdc-4d3d-a8d4-22fda17d52f5
2023-10-11 05:32:04,425 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f5258ae-0637-417a-b3c6-7426c74dd753
2023-10-11 05:32:04,441 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43479', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,443 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43479
2023-10-11 05:32:04,443 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58052
2023-10-11 05:32:04,443 - distributed.worker - INFO - Starting Worker plugin PreImport-f661ea57-3f06-41e6-92cd-177ae2e23c45
2023-10-11 05:32:04,444 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,444 - distributed.worker - INFO - Starting Worker plugin PreImport-c970cd9e-42af-4ea3-b246-91a7bf0a812c
2023-10-11 05:32:04,444 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d29af4a3-da6d-427b-817a-fa9b5e5f0e4c
2023-10-11 05:32:04,444 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,444 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,445 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,446 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,446 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,454 - distributed.worker - INFO - Starting Worker plugin PreImport-a2136498-e298-47f0-bdeb-18dfb6a6eb18
2023-10-11 05:32:04,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e403f572-1fbb-4f7c-8b5c-448018f48dcf
2023-10-11 05:32:04,455 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,456 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44147', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,456 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44147
2023-10-11 05:32:04,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58068
2023-10-11 05:32:04,458 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,459 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,459 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,467 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34655', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34655
2023-10-11 05:32:04,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58076
2023-10-11 05:32:04,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,470 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,471 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34757', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34757
2023-10-11 05:32:04,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58088
2023-10-11 05:32:04,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,481 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,481 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,483 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46613', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,484 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46613
2023-10-11 05:32:04,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58106
2023-10-11 05:32:04,485 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,486 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,486 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37769', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,487 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37769
2023-10-11 05:32:04,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58094
2023-10-11 05:32:04,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,489 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,490 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,502 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46531', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,502 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46531
2023-10-11 05:32:04,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58118
2023-10-11 05:32:04,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,505 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,505 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,507 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,507 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,533 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43587', status: init, memory: 0, processing: 0>
2023-10-11 05:32:04,534 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43587
2023-10-11 05:32:04,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58124
2023-10-11 05:32:04,535 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:04,536 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:04,536 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:04,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:04,549 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,550 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,550 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,550 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,550 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,550 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,550 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,551 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:04,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,561 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,562 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,562 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:04,568 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:04,569 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:04,572 - distributed.scheduler - INFO - Remove client Client-7ebceb0b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:04,572 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39612; closing.
2023-10-11 05:32:04,573 - distributed.scheduler - INFO - Remove client Client-7ebceb0b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:04,573 - distributed.scheduler - INFO - Close client connection: Client-7ebceb0b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:04,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42393'. Reason: nanny-close
2023-10-11 05:32:04,574 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35589'. Reason: nanny-close
2023-10-11 05:32:04,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,575 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34757. Reason: nanny-close
2023-10-11 05:32:04,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43585'. Reason: nanny-close
2023-10-11 05:32:04,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46531. Reason: nanny-close
2023-10-11 05:32:04,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38825'. Reason: nanny-close
2023-10-11 05:32:04,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43479. Reason: nanny-close
2023-10-11 05:32:04,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39033'. Reason: nanny-close
2023-10-11 05:32:04,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36807'. Reason: nanny-close
2023-10-11 05:32:04,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43587. Reason: nanny-close
2023-10-11 05:32:04,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37769. Reason: nanny-close
2023-10-11 05:32:04,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46523'. Reason: nanny-close
2023-10-11 05:32:04,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58088; closing.
2023-10-11 05:32:04,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5786135')
2023-10-11 05:32:04,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44147. Reason: nanny-close
2023-10-11 05:32:04,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42581'. Reason: nanny-close
2023-10-11 05:32:04,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:04,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34655. Reason: nanny-close
2023-10-11 05:32:04,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46613. Reason: nanny-close
2023-10-11 05:32:04,580 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,580 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,580 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58052; closing.
2023-10-11 05:32:04,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,581 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58118; closing.
2023-10-11 05:32:04,581 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58124; closing.
2023-10-11 05:32:04,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5823147')
2023-10-11 05:32:04,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:04,582 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,582 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5828142')
2023-10-11 05:32:04,583 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5837636')
2023-10-11 05:32:04,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58094; closing.
2023-10-11 05:32:04,584 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:04,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5855992')
2023-10-11 05:32:04,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58068; closing.
2023-10-11 05:32:04,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58076; closing.
2023-10-11 05:32:04,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58106; closing.
2023-10-11 05:32:04,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5873177')
2023-10-11 05:32:04,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5877645')
2023-10-11 05:32:04,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46613', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002324.5882452')
2023-10-11 05:32:04,588 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:32:06,142 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:06,142 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:06,142 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:06,144 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:32:06,144 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-11 05:32:08,233 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:08,238 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46875 instead
  warnings.warn(
2023-10-11 05:32:08,242 - distributed.scheduler - INFO - State start
2023-10-11 05:32:08,266 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:08,267 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:32:08,268 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46875/status
2023-10-11 05:32:08,268 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:08,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40343'
2023-10-11 05:32:08,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39201'
2023-10-11 05:32:08,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32861'
2023-10-11 05:32:08,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45005'
2023-10-11 05:32:08,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36331'
2023-10-11 05:32:08,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41375'
2023-10-11 05:32:08,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45401'
2023-10-11 05:32:08,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44905'
2023-10-11 05:32:10,218 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,218 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,222 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,249 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,251 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,284 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:10,325 - distributed.scheduler - INFO - Receive client connection: Client-840f082a-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:10,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41832
2023-10-11 05:32:10,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:10,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:10,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:12,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33155
2023-10-11 05:32:12,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33155
2023-10-11 05:32:12,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36007
2023-10-11 05:32:12,966 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:12,966 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:12,966 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:12,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:12,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1qa_uvjn
2023-10-11 05:32:12,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22506b1d-cec0-4fbd-b95f-107ae5a17510
2023-10-11 05:32:12,967 - distributed.worker - INFO - Starting Worker plugin PreImport-c1c525b5-ef76-414f-a1d1-3ca0b725f656
2023-10-11 05:32:12,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b3959e3-15b9-4c14-a6eb-e4bda1937eb9
2023-10-11 05:32:12,979 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38327
2023-10-11 05:32:12,979 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38327
2023-10-11 05:32:12,980 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45011
2023-10-11 05:32:12,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:12,980 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:12,980 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:12,980 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:12,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-llqv2cjn
2023-10-11 05:32:12,980 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bde320fe-13e2-462f-9765-a18e7bdf4a5c
2023-10-11 05:32:12,981 - distributed.worker - INFO - Starting Worker plugin PreImport-af13aded-1228-47e5-93c2-1e3bfd80d608
2023-10-11 05:32:12,981 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c5c41de-6ce8-4b0e-96b6-172d6c48da98
2023-10-11 05:32:13,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36799
2023-10-11 05:32:13,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36799
2023-10-11 05:32:13,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40241
2023-10-11 05:32:13,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,009 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,009 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:13,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:13,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_wys5a7q
2023-10-11 05:32:13,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95d984e4-9ce7-4ebd-b2af-0f4d76ece77b
2023-10-11 05:32:13,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a43fc6f6-c027-4a69-8344-8c49b2349bd7
2023-10-11 05:32:13,012 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46357
2023-10-11 05:32:13,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46357
2023-10-11 05:32:13,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39493
2023-10-11 05:32:13,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,013 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,013 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:13,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:13,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-igroayp5
2023-10-11 05:32:13,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9031fa75-025d-4311-a5c0-fdeef282e8c5
2023-10-11 05:32:13,014 - distributed.worker - INFO - Starting Worker plugin PreImport-75f75ad8-7534-4f3e-a1cc-1cf80376ad31
2023-10-11 05:32:13,014 - distributed.worker - INFO - Starting Worker plugin RMMSetup-426b9981-992b-4896-8a5a-28046803b149
2023-10-11 05:32:13,016 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34535
2023-10-11 05:32:13,018 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34535
2023-10-11 05:32:13,018 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36017
2023-10-11 05:32:13,018 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,018 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,018 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:13,019 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:13,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vf492v0x
2023-10-11 05:32:13,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-533d38aa-95f2-4e55-86d4-e99a1c47bb89
2023-10-11 05:32:13,057 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45905
2023-10-11 05:32:13,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45905
2023-10-11 05:32:13,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35351
2023-10-11 05:32:13,058 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,058 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,059 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:13,059 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:13,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vyz41x46
2023-10-11 05:32:13,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a822bdb-bca4-4655-9ef0-6a0430b73e6b
2023-10-11 05:32:13,060 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43095
2023-10-11 05:32:13,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43095
2023-10-11 05:32:13,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35767
2023-10-11 05:32:13,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,062 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,062 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:13,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:13,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yveg64nx
2023-10-11 05:32:13,063 - distributed.worker - INFO - Starting Worker plugin PreImport-4281b421-37b1-4e85-8773-814c554200ab
2023-10-11 05:32:13,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2057dbe-9c78-401b-a1c8-f929c6e5d9c1
2023-10-11 05:32:13,064 - distributed.worker - INFO - Starting Worker plugin PreImport-1a957cd9-1b31-49e0-9c8d-c24dbffa7c96
2023-10-11 05:32:13,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6bb2c16-13e7-4148-abe6-4086a784b17c
2023-10-11 05:32:13,065 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35653
2023-10-11 05:32:13,066 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35653
2023-10-11 05:32:13,066 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37445
2023-10-11 05:32:13,066 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,066 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,066 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:13,066 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:13,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ea95r4nf
2023-10-11 05:32:13,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86021994-810e-471a-a72c-1a866c30d955
2023-10-11 05:32:13,067 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6afdd1e-0a94-4c38-8b44-1c6bd2d6b64e
2023-10-11 05:32:13,191 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,192 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38327', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38327
2023-10-11 05:32:13,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41844
2023-10-11 05:32:13,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,223 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,223 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,225 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33155', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,226 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33155
2023-10-11 05:32:13,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41856
2023-10-11 05:32:13,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,228 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,247 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,259 - distributed.worker - INFO - Starting Worker plugin PreImport-4f180f7c-f8e2-45d8-aa6b-433ea3438387
2023-10-11 05:32:13,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed232ac3-de1c-4df2-bd78-62954222a291
2023-10-11 05:32:13,260 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,262 - distributed.worker - INFO - Starting Worker plugin PreImport-8211cbb5-14ca-4fb7-a9d6-5ac70c9edcb1
2023-10-11 05:32:13,263 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46357', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,272 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46357
2023-10-11 05:32:13,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41868
2023-10-11 05:32:13,273 - distributed.worker - INFO - Starting Worker plugin PreImport-e82c0ed7-7e80-4597-9255-e525b38b2957
2023-10-11 05:32:13,273 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,274 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,274 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,276 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b75abe45-1b06-4b7c-b5c7-50a1d6bfcaa8
2023-10-11 05:32:13,276 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,277 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,288 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34535', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,289 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34535
2023-10-11 05:32:13,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41870
2023-10-11 05:32:13,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,292 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,292 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,293 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36799', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36799
2023-10-11 05:32:13,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41882
2023-10-11 05:32:13,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,295 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35653', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,296 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35653
2023-10-11 05:32:13,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41884
2023-10-11 05:32:13,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,297 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,297 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,298 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,308 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45905', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,308 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45905
2023-10-11 05:32:13,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41886
2023-10-11 05:32:13,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43095', status: init, memory: 0, processing: 0>
2023-10-11 05:32:13,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43095
2023-10-11 05:32:13,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41896
2023-10-11 05:32:13,310 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,312 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,312 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:13,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:13,313 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:13,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:13,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,366 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,367 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,367 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,367 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,377 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,377 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,378 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,378 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,378 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,378 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,378 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,378 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:32:13,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:13,388 - distributed.scheduler - INFO - Remove client Client-840f082a-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:13,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41832; closing.
2023-10-11 05:32:13,388 - distributed.scheduler - INFO - Remove client Client-840f082a-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:13,389 - distributed.scheduler - INFO - Close client connection: Client-840f082a-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:13,389 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40343'. Reason: nanny-close
2023-10-11 05:32:13,390 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39201'. Reason: nanny-close
2023-10-11 05:32:13,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,391 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43095. Reason: nanny-close
2023-10-11 05:32:13,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32861'. Reason: nanny-close
2023-10-11 05:32:13,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36799. Reason: nanny-close
2023-10-11 05:32:13,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45005'. Reason: nanny-close
2023-10-11 05:32:13,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38327. Reason: nanny-close
2023-10-11 05:32:13,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36331'. Reason: nanny-close
2023-10-11 05:32:13,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34535. Reason: nanny-close
2023-10-11 05:32:13,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41375'. Reason: nanny-close
2023-10-11 05:32:13,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,394 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41896; closing.
2023-10-11 05:32:13,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33155. Reason: nanny-close
2023-10-11 05:32:13,394 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.3943224')
2023-10-11 05:32:13,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45401'. Reason: nanny-close
2023-10-11 05:32:13,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45905. Reason: nanny-close
2023-10-11 05:32:13,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44905'. Reason: nanny-close
2023-10-11 05:32:13,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:13,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46357. Reason: nanny-close
2023-10-11 05:32:13,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41882; closing.
2023-10-11 05:32:13,395 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,396 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41844; closing.
2023-10-11 05:32:13,396 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35653. Reason: nanny-close
2023-10-11 05:32:13,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,396 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.3967986')
2023-10-11 05:32:13,396 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41870; closing.
2023-10-11 05:32:13,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.3974316')
2023-10-11 05:32:13,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:13,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34535', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.3981154')
2023-10-11 05:32:13,398 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41856; closing.
2023-10-11 05:32:13,398 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,398 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,399 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.3990822')
2023-10-11 05:32:13,399 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,399 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41886; closing.
2023-10-11 05:32:13,399 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:13,399 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41868; closing.
2023-10-11 05:32:13,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.400105')
2023-10-11 05:32:13,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.4005601')
2023-10-11 05:32:13,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41884; closing.
2023-10-11 05:32:13,401 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002333.401342')
2023-10-11 05:32:13,401 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:32:14,907 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:14,907 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:14,908 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:14,909 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:32:14,909 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-11 05:32:17,042 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:17,046 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42901 instead
  warnings.warn(
2023-10-11 05:32:17,050 - distributed.scheduler - INFO - State start
2023-10-11 05:32:17,070 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:17,071 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:32:17,072 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42901/status
2023-10-11 05:32:17,072 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:17,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41629'
2023-10-11 05:32:17,227 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44027'
2023-10-11 05:32:17,234 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33549'
2023-10-11 05:32:17,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41957'
2023-10-11 05:32:17,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45665'
2023-10-11 05:32:17,258 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45853'
2023-10-11 05:32:17,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43457'
2023-10-11 05:32:17,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44949'
2023-10-11 05:32:19,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,021 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,082 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,117 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,117 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,122 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,125 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:19,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:19,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:19,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:21,277 - distributed.scheduler - INFO - Receive client connection: Client-893d843f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:21,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35378
2023-10-11 05:32:21,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40691
2023-10-11 05:32:21,968 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40691
2023-10-11 05:32:21,968 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43271
2023-10-11 05:32:21,968 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:21,968 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:21,968 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:21,969 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:21,969 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y1h96q1k
2023-10-11 05:32:21,969 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc49abee-7539-4f9b-9434-6db37a6bb470
2023-10-11 05:32:21,969 - distributed.worker - INFO - Starting Worker plugin PreImport-596a9ab9-394a-4813-8013-5188630680ca
2023-10-11 05:32:21,969 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46645
2023-10-11 05:32:21,970 - distributed.worker - INFO - Starting Worker plugin RMMSetup-edf9c0fc-045a-483e-aaa7-e17d5cafa1f2
2023-10-11 05:32:21,970 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46645
2023-10-11 05:32:21,970 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46821
2023-10-11 05:32:21,970 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:21,970 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:21,970 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:21,970 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:21,970 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9uhnk56r
2023-10-11 05:32:21,971 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c46766cd-2de2-4e7c-9b2b-ccd039818c20
2023-10-11 05:32:21,971 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41785
2023-10-11 05:32:21,972 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41785
2023-10-11 05:32:21,972 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36715
2023-10-11 05:32:21,972 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:21,972 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:21,972 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:21,973 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:21,973 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7cj6exl8
2023-10-11 05:32:21,973 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6484f7e-6cdc-4fd5-953d-fbadee6194d8
2023-10-11 05:32:21,973 - distributed.worker - INFO - Starting Worker plugin PreImport-a2403ccc-9393-44c7-b9d9-75990c373ccf
2023-10-11 05:32:21,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-514a8f30-3c8b-42d8-8a46-5b9b5e9c8ca1
2023-10-11 05:32:21,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35113
2023-10-11 05:32:21,994 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35113
2023-10-11 05:32:21,994 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35425
2023-10-11 05:32:21,995 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:21,995 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:21,995 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:21,995 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:21,995 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_l6uf3hc
2023-10-11 05:32:21,995 - distributed.worker - INFO - Starting Worker plugin PreImport-ae5d890a-6193-4d45-9346-f489ec436f40
2023-10-11 05:32:21,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71270a24-076c-438d-8894-aef77b6b31b8
2023-10-11 05:32:22,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42423
2023-10-11 05:32:22,014 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42423
2023-10-11 05:32:22,014 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46437
2023-10-11 05:32:22,014 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,014 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,014 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:22,014 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:22,014 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-se7tsm7g
2023-10-11 05:32:22,015 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c93c6c61-4bc6-481a-a4cd-c6c017316b44
2023-10-11 05:32:22,015 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44354860-7b9f-4060-8009-b950502e84ac
2023-10-11 05:32:22,017 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46257
2023-10-11 05:32:22,018 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46257
2023-10-11 05:32:22,018 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42629
2023-10-11 05:32:22,018 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,018 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,018 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:22,018 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:22,018 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3sudbu8b
2023-10-11 05:32:22,019 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9fd9f8f-88da-427d-9ea4-81e34eb574b0
2023-10-11 05:32:22,019 - distributed.worker - INFO - Starting Worker plugin PreImport-9eca72e2-c2c2-40b9-ac74-0e92111f3864
2023-10-11 05:32:22,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fffd4a5-86ee-4e0d-844d-13aad64d369c
2023-10-11 05:32:22,021 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44299
2023-10-11 05:32:22,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44299
2023-10-11 05:32:22,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44155
2023-10-11 05:32:22,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,022 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,022 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:22,022 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:22,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-abufh_dn
2023-10-11 05:32:22,023 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ff7f385-7fe6-402c-a07d-3ca2db67723f
2023-10-11 05:32:22,025 - distributed.worker - INFO - Starting Worker plugin PreImport-2599389c-f6c5-43ed-8299-ad16072b9539
2023-10-11 05:32:22,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8ed0d6e-8a92-4d36-a52d-fc3f3caf8b7e
2023-10-11 05:32:22,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45755
2023-10-11 05:32:22,034 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45755
2023-10-11 05:32:22,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40321
2023-10-11 05:32:22,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,035 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,035 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:22,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:32:22,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ibgljnsg
2023-10-11 05:32:22,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0584835c-7e92-4e68-bede-4facb521a8da
2023-10-11 05:32:22,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1006d3ae-543c-4065-bb4e-0eb80cfefe1a
2023-10-11 05:32:22,162 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,165 - distributed.worker - INFO - Starting Worker plugin PreImport-54acc8f1-e8af-479f-9e10-5c6244872bf9
2023-10-11 05:32:22,165 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7035454-2dc5-4b84-8cdc-1a70e8e11468
2023-10-11 05:32:22,166 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-385ca9da-0786-4c72-8b9e-bd1d5604fecb
2023-10-11 05:32:22,171 - distributed.worker - INFO - Starting Worker plugin PreImport-21337d4a-96cf-4e79-8dcc-97da80bc7056
2023-10-11 05:32:22,172 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,172 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,181 - distributed.worker - INFO - Starting Worker plugin PreImport-92cfb6f8-c7f2-4068-a6b1-82db2307bb30
2023-10-11 05:32:22,181 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,182 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,182 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,190 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40691', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40691
2023-10-11 05:32:22,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35392
2023-10-11 05:32:22,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46645', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46645
2023-10-11 05:32:22,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35404
2023-10-11 05:32:22,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41785', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,195 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41785
2023-10-11 05:32:22,195 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35394
2023-10-11 05:32:22,196 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,196 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,197 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,197 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,201 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45755', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,202 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45755
2023-10-11 05:32:22,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35416
2023-10-11 05:32:22,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,204 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,204 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,209 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35113', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,209 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35113
2023-10-11 05:32:22,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35430
2023-10-11 05:32:22,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,213 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,213 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42423', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42423
2023-10-11 05:32:22,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35446
2023-10-11 05:32:22,215 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44299', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44299
2023-10-11 05:32:22,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35434
2023-10-11 05:32:22,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46257', status: init, memory: 0, processing: 0>
2023-10-11 05:32:22,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,217 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46257
2023-10-11 05:32:22,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35458
2023-10-11 05:32:22,217 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,218 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,218 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:22,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:22,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:22,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,322 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,323 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:32:22,327 - distributed.scheduler - INFO - Remove client Client-893d843f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:22,327 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35378; closing.
2023-10-11 05:32:22,328 - distributed.scheduler - INFO - Remove client Client-893d843f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:22,328 - distributed.scheduler - INFO - Close client connection: Client-893d843f-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:22,329 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41629'. Reason: nanny-close
2023-10-11 05:32:22,330 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,331 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44027'. Reason: nanny-close
2023-10-11 05:32:22,331 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,331 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35113. Reason: nanny-close
2023-10-11 05:32:22,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33549'. Reason: nanny-close
2023-10-11 05:32:22,332 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,332 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42423. Reason: nanny-close
2023-10-11 05:32:22,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41957'. Reason: nanny-close
2023-10-11 05:32:22,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,333 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40691. Reason: nanny-close
2023-10-11 05:32:22,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45665'. Reason: nanny-close
2023-10-11 05:32:22,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,333 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46645. Reason: nanny-close
2023-10-11 05:32:22,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45853'. Reason: nanny-close
2023-10-11 05:32:22,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35430; closing.
2023-10-11 05:32:22,334 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44299. Reason: nanny-close
2023-10-11 05:32:22,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.33448')
2023-10-11 05:32:22,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43457'. Reason: nanny-close
2023-10-11 05:32:22,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46257. Reason: nanny-close
2023-10-11 05:32:22,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44949'. Reason: nanny-close
2023-10-11 05:32:22,335 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:22,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41785. Reason: nanny-close
2023-10-11 05:32:22,336 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45755. Reason: nanny-close
2023-10-11 05:32:22,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35404; closing.
2023-10-11 05:32:22,336 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,336 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35446; closing.
2023-10-11 05:32:22,336 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35392; closing.
2023-10-11 05:32:22,336 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.3373384')
2023-10-11 05:32:22,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.3377316')
2023-10-11 05:32:22,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.3381138')
2023-10-11 05:32:22,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:22,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35434; closing.
2023-10-11 05:32:22,338 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,339 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,339 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44299', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.3395436')
2023-10-11 05:32:22,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35394; closing.
2023-10-11 05:32:22,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35458; closing.
2023-10-11 05:32:22,340 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.340538')
2023-10-11 05:32:22,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46257', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.3409064')
2023-10-11 05:32:22,341 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:22,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35416; closing.
2023-10-11 05:32:22,341 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45755', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002342.3416088')
2023-10-11 05:32:22,341 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:32:23,896 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:23,897 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:23,897 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:23,899 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:32:23,899 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-11 05:32:26,047 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:26,052 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45801 instead
  warnings.warn(
2023-10-11 05:32:26,056 - distributed.scheduler - INFO - State start
2023-10-11 05:32:26,080 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:26,081 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:32:26,082 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45801/status
2023-10-11 05:32:26,082 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:26,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35307'
2023-10-11 05:32:28,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:28,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:29,030 - distributed.scheduler - INFO - Receive client connection: Client-8eac3f8e-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:29,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35574
2023-10-11 05:32:29,371 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:30,403 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34763
2023-10-11 05:32:30,403 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34763
2023-10-11 05:32:30,403 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-11 05:32:30,403 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:30,403 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:30,404 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:30,404 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:32:30,404 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g226xit4
2023-10-11 05:32:30,404 - distributed.worker - INFO - Starting Worker plugin PreImport-d90aa365-52e8-4ad2-930e-d71cbb6fc8f8
2023-10-11 05:32:30,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1283d8ae-4b79-4820-93d8-21259f038df6
2023-10-11 05:32:30,404 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1c26441-c082-499c-9e3b-5feb9f2c194a
2023-10-11 05:32:30,405 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:30,425 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34763', status: init, memory: 0, processing: 0>
2023-10-11 05:32:30,427 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34763
2023-10-11 05:32:30,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33952
2023-10-11 05:32:30,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:30,428 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:30,428 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:30,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:30,501 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:30,504 - distributed.scheduler - INFO - Remove client Client-8eac3f8e-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:30,504 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35574; closing.
2023-10-11 05:32:30,505 - distributed.scheduler - INFO - Remove client Client-8eac3f8e-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:30,505 - distributed.scheduler - INFO - Close client connection: Client-8eac3f8e-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:30,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35307'. Reason: nanny-close
2023-10-11 05:32:30,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:30,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34763. Reason: nanny-close
2023-10-11 05:32:30,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:30,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33952; closing.
2023-10-11 05:32:30,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34763', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002350.5100062')
2023-10-11 05:32:30,510 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:32:30,511 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:31,622 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:31,623 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:31,623 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:31,624 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:32:31,624 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-11 05:32:36,105 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:36,110 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45449 instead
  warnings.warn(
2023-10-11 05:32:36,114 - distributed.scheduler - INFO - State start
2023-10-11 05:32:36,137 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:36,137 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:32:36,138 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45449/status
2023-10-11 05:32:36,138 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:36,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41893'
2023-10-11 05:32:36,380 - distributed.scheduler - INFO - Receive client connection: Client-9490c07b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:36,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34046
2023-10-11 05:32:38,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:38,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:38,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:39,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42067
2023-10-11 05:32:39,652 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42067
2023-10-11 05:32:39,653 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38259
2023-10-11 05:32:39,653 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:32:39,653 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:39,653 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:39,653 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:32:39,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pwc4vees
2023-10-11 05:32:39,654 - distributed.worker - INFO - Starting Worker plugin PreImport-5600a5b9-345a-4bb3-83bd-d0b936fb9c4e
2023-10-11 05:32:39,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a956913d-f33c-4944-88d7-71da12f2a565
2023-10-11 05:32:39,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e2e98d8-a935-41d1-bb21-1affce7dd0bf
2023-10-11 05:32:39,657 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:39,681 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42067', status: init, memory: 0, processing: 0>
2023-10-11 05:32:39,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42067
2023-10-11 05:32:39,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34072
2023-10-11 05:32:39,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:39,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:32:39,685 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:39,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:32:39,763 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:39,766 - distributed.scheduler - INFO - Remove client Client-9490c07b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:39,766 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34046; closing.
2023-10-11 05:32:39,767 - distributed.scheduler - INFO - Remove client Client-9490c07b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:39,767 - distributed.scheduler - INFO - Close client connection: Client-9490c07b-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:39,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41893'. Reason: nanny-close
2023-10-11 05:32:39,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:39,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42067. Reason: nanny-close
2023-10-11 05:32:39,772 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34072; closing.
2023-10-11 05:32:39,772 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:32:39,772 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002359.772742')
2023-10-11 05:32:39,773 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:32:39,774 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:40,835 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:40,835 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:40,836 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:40,837 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:32:40,838 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-11 05:32:43,102 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:43,107 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42677 instead
  warnings.warn(
2023-10-11 05:32:43,111 - distributed.scheduler - INFO - State start
2023-10-11 05:32:43,133 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:43,134 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:32:43,135 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42677/status
2023-10-11 05:32:43,136 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:47,240 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:56440'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56440>: Stream is closed
2023-10-11 05:32:47,498 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:47,498 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:47,499 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:47,500 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:32:47,500 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-11 05:32:49,661 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:49,665 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43953 instead
  warnings.warn(
2023-10-11 05:32:49,669 - distributed.scheduler - INFO - State start
2023-10-11 05:32:49,691 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:49,692 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-11 05:32:49,693 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43953/status
2023-10-11 05:32:49,693 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:49,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34129'
2023-10-11 05:32:51,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:51,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:51,603 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:51,835 - distributed.scheduler - INFO - Receive client connection: Client-9cbceefe-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:51,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44322
2023-10-11 05:32:52,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41245
2023-10-11 05:32:52,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41245
2023-10-11 05:32:52,479 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39803
2023-10-11 05:32:52,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:32:52,479 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:52,479 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:32:52,479 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:32:52,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mt4npama
2023-10-11 05:32:52,479 - distributed.worker - INFO - Starting Worker plugin PreImport-9f6fd963-729a-4e8b-9ee4-bbe09cb48444
2023-10-11 05:32:52,479 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7ec0a6c-96f1-4fca-826e-94459b0e7057
2023-10-11 05:32:52,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e1d447d-054d-4719-9fb2-6fd671a92f24
2023-10-11 05:32:52,480 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:52,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41245', status: init, memory: 0, processing: 0>
2023-10-11 05:32:52,499 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41245
2023-10-11 05:32:52,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44342
2023-10-11 05:32:52,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:32:52,501 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:32:52,501 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:32:52,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:32:52,568 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:32:52,571 - distributed.scheduler - INFO - Remove client Client-9cbceefe-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:52,571 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44322; closing.
2023-10-11 05:32:52,572 - distributed.scheduler - INFO - Remove client Client-9cbceefe-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:52,572 - distributed.scheduler - INFO - Close client connection: Client-9cbceefe-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:52,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34129'. Reason: nanny-close
2023-10-11 05:32:52,573 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:32:52,574 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41245. Reason: nanny-close
2023-10-11 05:32:52,575 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44342; closing.
2023-10-11 05:32:52,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:32:52,576 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002372.576154')
2023-10-11 05:32:52,576 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:32:52,577 - distributed.nanny - INFO - Worker closed
2023-10-11 05:32:53,639 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:32:53,639 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:32:53,640 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:32:53,641 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-11 05:32:53,642 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-11 05:32:55,852 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:55,857 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46873 instead
  warnings.warn(
2023-10-11 05:32:55,861 - distributed.scheduler - INFO - State start
2023-10-11 05:32:55,883 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:32:55,884 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:32:55,885 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46873/status
2023-10-11 05:32:55,885 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:32:56,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46013'
2023-10-11 05:32:56,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35249'
2023-10-11 05:32:56,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44323'
2023-10-11 05:32:56,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36677'
2023-10-11 05:32:56,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35161'
2023-10-11 05:32:56,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34877'
2023-10-11 05:32:56,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43213'
2023-10-11 05:32:56,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33771'
2023-10-11 05:32:57,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:57,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:57,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,042 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,045 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:58,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:32:58,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:32:58,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:32:59,055 - distributed.scheduler - INFO - Receive client connection: Client-a06775d4-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:32:59,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56694
2023-10-11 05:33:01,037 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43343
2023-10-11 05:33:01,039 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43343
2023-10-11 05:33:01,039 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40991
2023-10-11 05:33:01,039 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,039 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,039 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,039 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,039 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zwd77_2h
2023-10-11 05:33:01,041 - distributed.worker - INFO - Starting Worker plugin PreImport-e138d57b-14b7-4a9f-850f-d203ac6a45ac
2023-10-11 05:33:01,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba452ac7-a5a7-46e4-af8f-bb03b6eb4f11
2023-10-11 05:33:01,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32951
2023-10-11 05:33:01,047 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32951
2023-10-11 05:33:01,047 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34327
2023-10-11 05:33:01,047 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,047 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,047 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,047 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,047 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e678tpxt
2023-10-11 05:33:01,048 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef9178f0-0b7e-4fe4-b1eb-b1907f2a3e67
2023-10-11 05:33:01,048 - distributed.worker - INFO - Starting Worker plugin PreImport-4ff901c1-924e-473d-8336-b8719c5afc3b
2023-10-11 05:33:01,048 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a73d75f-c812-4107-8f82-8e82a1597600
2023-10-11 05:33:01,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39753
2023-10-11 05:33:01,064 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39753
2023-10-11 05:33:01,064 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41315
2023-10-11 05:33:01,065 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,065 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,065 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,065 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,065 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-626iy8o3
2023-10-11 05:33:01,065 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28d24ff7-9a6d-40c1-9447-a8fd48268627
2023-10-11 05:33:01,066 - distributed.worker - INFO - Starting Worker plugin PreImport-57e9387f-a941-4add-95e4-605e3ce3b604
2023-10-11 05:33:01,066 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ed2a0e3-6bf3-4db5-bc6e-751402fc8939
2023-10-11 05:33:01,070 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45049
2023-10-11 05:33:01,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45049
2023-10-11 05:33:01,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39387
2023-10-11 05:33:01,071 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,071 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,071 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,071 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x6jwyfuu
2023-10-11 05:33:01,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8898c2d5-6f5a-431b-bfd3-fcd3553dcc46
2023-10-11 05:33:01,072 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba7683fc-a1cd-4004-8edc-7278a4c39e76
2023-10-11 05:33:01,097 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44163
2023-10-11 05:33:01,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44163
2023-10-11 05:33:01,098 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43223
2023-10-11 05:33:01,098 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,098 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,098 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,098 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,098 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0x52i1lq
2023-10-11 05:33:01,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0f41a27-3955-4641-8649-b49e1270c7a3
2023-10-11 05:33:01,099 - distributed.worker - INFO - Starting Worker plugin PreImport-1306475f-5995-409f-8627-12ca8137b2b2
2023-10-11 05:33:01,099 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87eec538-0042-4f5c-9c05-18aaaf053508
2023-10-11 05:33:01,099 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43925
2023-10-11 05:33:01,100 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43925
2023-10-11 05:33:01,100 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40631
2023-10-11 05:33:01,100 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,100 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,100 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,100 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,100 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-shb6h6j1
2023-10-11 05:33:01,101 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1a84957-02f5-42d5-a719-521846fd1aaf
2023-10-11 05:33:01,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46221
2023-10-11 05:33:01,108 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46221
2023-10-11 05:33:01,108 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41319
2023-10-11 05:33:01,108 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,108 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,108 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,109 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g7m_ng3v
2023-10-11 05:33:01,110 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-319a8784-e5e6-43d4-ba15-1a3b365b38c7
2023-10-11 05:33:01,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-043b8e28-b7c8-480f-8f9e-a956e95b29d5
2023-10-11 05:33:01,120 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36441
2023-10-11 05:33:01,121 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36441
2023-10-11 05:33:01,121 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43347
2023-10-11 05:33:01,121 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,121 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,121 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:01,121 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:33:01,121 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-114ikeab
2023-10-11 05:33:01,122 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-265bc020-f7fb-430d-aaa0-0306f5a98ea1
2023-10-11 05:33:01,123 - distributed.worker - INFO - Starting Worker plugin PreImport-b06e2ba9-e8eb-4920-a755-103104cd6e8e
2023-10-11 05:33:01,123 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7cf900b5-e9c0-47da-9dcb-93c542f2bfe2
2023-10-11 05:33:01,245 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,249 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,276 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32951', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,277 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32951
2023-10-11 05:33:01,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57752
2023-10-11 05:33:01,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,279 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,279 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,282 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39753', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,282 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39753
2023-10-11 05:33:01,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57736
2023-10-11 05:33:01,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,284 - distributed.worker - INFO - Starting Worker plugin PreImport-58945843-095b-43a0-bed5-f0172e75bbb4
2023-10-11 05:33:01,285 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,286 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,286 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,288 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7092670-b779-45a6-987c-12bdc082e120
2023-10-11 05:33:01,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,288 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,290 - distributed.worker - INFO - Starting Worker plugin PreImport-b21099bb-edd0-40b4-9732-9588c7d6dcfb
2023-10-11 05:33:01,290 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,293 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,296 - distributed.worker - INFO - Starting Worker plugin PreImport-0349d59f-08b8-4571-b20c-65f753be7bbd
2023-10-11 05:33:01,297 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d025959-2cb5-40e2-b4c2-d35dac90ae53
2023-10-11 05:33:01,297 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,300 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45049', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45049
2023-10-11 05:33:01,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57754
2023-10-11 05:33:01,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,312 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,312 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,317 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44163', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,317 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44163
2023-10-11 05:33:01,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57776
2023-10-11 05:33:01,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,319 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43343', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,319 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,319 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,319 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43343
2023-10-11 05:33:01,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57760
2023-10-11 05:33:01,320 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43925', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,321 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,321 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43925
2023-10-11 05:33:01,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57790
2023-10-11 05:33:01,322 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,322 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,322 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,323 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,323 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46221', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46221
2023-10-11 05:33:01,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57762
2023-10-11 05:33:01,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,326 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,326 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36441', status: init, memory: 0, processing: 0>
2023-10-11 05:33:01,335 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36441
2023-10-11 05:33:01,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57804
2023-10-11 05:33:01,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:01,338 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:01,338 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:01,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:01,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:33:01,430 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,431 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:01,435 - distributed.scheduler - INFO - Remove client Client-a06775d4-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:01,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56694; closing.
2023-10-11 05:33:01,436 - distributed.scheduler - INFO - Remove client Client-a06775d4-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:01,436 - distributed.scheduler - INFO - Close client connection: Client-a06775d4-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:01,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46013'. Reason: nanny-close
2023-10-11 05:33:01,438 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35249'. Reason: nanny-close
2023-10-11 05:33:01,439 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,440 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43343. Reason: nanny-close
2023-10-11 05:33:01,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44323'. Reason: nanny-close
2023-10-11 05:33:01,440 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,440 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46221. Reason: nanny-close
2023-10-11 05:33:01,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36677'. Reason: nanny-close
2023-10-11 05:33:01,441 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32951. Reason: nanny-close
2023-10-11 05:33:01,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35161'. Reason: nanny-close
2023-10-11 05:33:01,441 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43925. Reason: nanny-close
2023-10-11 05:33:01,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34877'. Reason: nanny-close
2023-10-11 05:33:01,442 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,442 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,442 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36441. Reason: nanny-close
2023-10-11 05:33:01,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57760; closing.
2023-10-11 05:33:01,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43213'. Reason: nanny-close
2023-10-11 05:33:01,442 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43343', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4427857')
2023-10-11 05:33:01,442 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39753. Reason: nanny-close
2023-10-11 05:33:01,443 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33771'. Reason: nanny-close
2023-10-11 05:33:01,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,443 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:01,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44163. Reason: nanny-close
2023-10-11 05:33:01,444 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,444 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45049. Reason: nanny-close
2023-10-11 05:33:01,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57790; closing.
2023-10-11 05:33:01,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57762; closing.
2023-10-11 05:33:01,444 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57752; closing.
2023-10-11 05:33:01,444 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,444 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.445481')
2023-10-11 05:33:01,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4458334')
2023-10-11 05:33:01,446 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4461985')
2023-10-11 05:33:01,446 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:01,446 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,447 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,447 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57804; closing.
2023-10-11 05:33:01,447 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,447 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:01,447 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4478467')
2023-10-11 05:33:01,448 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57736; closing.
2023-10-11 05:33:01,448 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57776; closing.
2023-10-11 05:33:01,448 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57754; closing.
2023-10-11 05:33:01,448 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4489186')
2023-10-11 05:33:01,449 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44163', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4493382')
2023-10-11 05:33:01,449 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002381.4497168')
2023-10-11 05:33:01,449 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:33:02,954 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:33:02,955 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:33:02,955 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:33:02,956 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:33:02,957 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-11 05:33:05,159 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:33:05,164 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41357 instead
  warnings.warn(
2023-10-11 05:33:05,168 - distributed.scheduler - INFO - State start
2023-10-11 05:33:05,190 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:33:05,191 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:33:05,192 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41357/status
2023-10-11 05:33:05,192 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:33:05,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46573'
2023-10-11 05:33:05,320 - distributed.scheduler - INFO - Receive client connection: Client-a5f5b311-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:05,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57906
2023-10-11 05:33:07,010 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:33:07,010 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:33:07,014 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:33:07,956 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33599
2023-10-11 05:33:07,957 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33599
2023-10-11 05:33:07,957 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45139
2023-10-11 05:33:07,957 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:07,957 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:07,957 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:07,957 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:33:07,957 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uq9oyrjj
2023-10-11 05:33:07,958 - distributed.worker - INFO - Starting Worker plugin PreImport-669fdb61-849d-4213-884d-bdec8ce3a66e
2023-10-11 05:33:07,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-441a74b6-30dd-4248-abc4-8a56f003181b
2023-10-11 05:33:08,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c666d4fd-bb92-4b48-bdfc-3bf92f3d5fb2
2023-10-11 05:33:08,060 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:08,085 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33599', status: init, memory: 0, processing: 0>
2023-10-11 05:33:08,086 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33599
2023-10-11 05:33:08,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57930
2023-10-11 05:33:08,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:08,088 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:08,088 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:08,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:08,133 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:33:08,137 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:08,139 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:33:08,141 - distributed.scheduler - INFO - Remove client Client-a5f5b311-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:08,141 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57906; closing.
2023-10-11 05:33:08,141 - distributed.scheduler - INFO - Remove client Client-a5f5b311-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:08,142 - distributed.scheduler - INFO - Close client connection: Client-a5f5b311-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:08,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46573'. Reason: nanny-close
2023-10-11 05:33:08,143 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:08,144 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33599. Reason: nanny-close
2023-10-11 05:33:08,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:08,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57930; closing.
2023-10-11 05:33:08,146 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33599', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002388.1466575')
2023-10-11 05:33:08,146 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:33:08,147 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:09,209 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:33:09,209 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:33:09,210 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:33:09,211 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:33:09,212 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-11 05:33:11,346 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:33:11,351 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38047 instead
  warnings.warn(
2023-10-11 05:33:11,355 - distributed.scheduler - INFO - State start
2023-10-11 05:33:11,377 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:33:11,378 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:33:11,378 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38047/status
2023-10-11 05:33:11,378 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:33:11,400 - distributed.scheduler - INFO - Receive client connection: Client-a9a855bf-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:11,411 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46330
2023-10-11 05:33:11,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42237'
2023-10-11 05:33:13,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:33:13,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:33:21,448 - distributed.scheduler - INFO - Remove client Client-a9a855bf-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:21,449 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46330; closing.
2023-10-11 05:33:21,449 - distributed.scheduler - INFO - Remove client Client-a9a855bf-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:21,449 - distributed.scheduler - INFO - Close client connection: Client-a9a855bf-67f7-11ee-b5fa-d8c49764f6bb
2023-10-11 05:33:21,450 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42237'. Reason: nanny-close
2023-10-11 05:33:26,416 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:33:27,361 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33625
2023-10-11 05:33:27,361 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33625
2023-10-11 05:33:27,361 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33603
2023-10-11 05:33:27,361 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:33:27,361 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:27,361 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:33:27,362 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:33:27,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qh3he3sw
2023-10-11 05:33:27,362 - distributed.worker - INFO - Starting Worker plugin PreImport-e6d26c14-733b-4be8-8fed-b69e5911d6b0
2023-10-11 05:33:27,362 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0a5a8a7-1bb4-4a44-9aed-5fa47a42ded9
2023-10-11 05:33:27,362 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a5f1d80-b56e-4005-a064-cf26f16540b7
2023-10-11 05:33:27,472 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:27,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33625', status: init, memory: 0, processing: 0>
2023-10-11 05:33:27,499 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33625
2023-10-11 05:33:27,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41808
2023-10-11 05:33:27,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:33:27,501 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:33:27,501 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:33:27,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:33:27,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:33:27,522 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33625. Reason: nanny-close
2023-10-11 05:33:27,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41808; closing.
2023-10-11 05:33:27,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:33:27,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002407.5242233')
2023-10-11 05:33:27,524 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:33:27,525 - distributed.nanny - INFO - Worker closed
2023-10-11 05:33:28,429 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:33:28,430 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:33:28,430 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:33:28,431 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:33:28,432 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41229 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39265 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45873 instead
  warnings.warn(
2023-10-11 05:34:33,823 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-11 05:34:33,831 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-11 05:34:33,832 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-11 05:34:33,835 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:32771', name: 5, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
Task exception was never retrieved
future: <Task finished name='Task-1340' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1339' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34209 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36317 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39475 instead
  warnings.warn(
[1697002507.339105] [dgx13:68437:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:50462) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35473 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33515 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43841 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34497 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45921 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42265 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44215 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46131 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42903 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45461 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32977 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44733 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41159 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36209 instead
  warnings.warn(
2023-10-11 05:42:42,685 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-11 05:42:42,688 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-11 05:42:42,728 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:38833', name: 2, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-11 05:42:42,730 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:53991', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42743 instead
  warnings.warn(
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f1ab0206140, tag: 0xed6cb6ccd4e46f0e>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f1ab0206140, tag: 0xed6cb6ccd4e46f0e>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-6909' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36191 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37385 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41731 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44029 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37301 instead
  warnings.warn(
2023-10-11 05:45:34,445 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-11 05:45:34,446 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41547 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44513 instead
  warnings.warn(
2023-10-11 05:46:35,331 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,335 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,346 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,346 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,557 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,564 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,573 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,579 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,613 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:56315'.
2023-10-11 05:46:35,614 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:56315'. Shutting down.
2023-10-11 05:46:35,615 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:50250'.
2023-10-11 05:46:35,616 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:56331'.
2023-10-11 05:46:35,616 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:50250'. Shutting down.
2023-10-11 05:46:35,616 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:53399'.
2023-10-11 05:46:35,616 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:56331'. Shutting down.
2023-10-11 05:46:35,617 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:53399'. Shutting down.
2023-10-11 05:46:35,744 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc7d1469370>>, <Task finished name='Task-22' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-22' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,759 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f452b0eb370>>, <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,765 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7c5f58f370>>, <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:35,779 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fcc0381b370>>, <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-11 05:46:37,748 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-11 05:46:37,762 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-11 05:46:37,768 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-11 05:46:37,783 - distributed.nanny - ERROR - Worker process died unexpectedly
