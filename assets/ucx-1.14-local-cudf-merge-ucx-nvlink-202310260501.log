[dgx13:83610:0:83610] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  83610) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f33f33b3f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f33f33b4114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f33f33b42da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f3494445420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f33f342d5d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f33f3452859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f33f336f42f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f33f3372798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f33f33bc989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f33f337162d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f33f342ac4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f33f34dc06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x557cdef7c6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x557cdef78094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557cdef89519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x557cdef96e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x557cdf0a1b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x557cdef33d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x557cdef807f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x557cdef7e929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x557cdef78094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557cdef89519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x557cdef7a128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x557cdef78094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x557cdef96ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x557cdef9744c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x557cdf05a10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x557cdef8177c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x557cdef7c6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x557cdef96dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x557cdef7c6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x557cdef78094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557cdef89519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x557cdef795c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x557cdef897c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x557cdef79312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x557cdef78094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557cdef89519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x557cdef7a128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x557cdef78094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x557cdef77d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x557cdef77d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x557cdf02507b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x557cdf051fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x557cdf04e353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x557cdf04616a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x557cdf04605c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x557cdf045297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x557cdf018f07]
=================================
[dgx13:83613:0:83613] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  83613) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f9a5cbb1f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f9a5cbb2114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f9a5cbb22da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9aefc3c420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f9a5cc2b5d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9a5cc50859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f9a5cb6d42f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f9a5cb70798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f9a5cbba989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f9a5cb6f62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f9a5cc28c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f9a5ccda06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55eab9bcd6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eab9bc9094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eab9bda519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x55eab9be7e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x55eab9cf2b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55eab9b84d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55eab9bd17f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55eab9bcf929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eab9bc9094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eab9bda519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55eab9bcb128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eab9bc9094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55eab9be7ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55eab9be844c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55eab9cab10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55eab9bd277c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55eab9bcd6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55eab9be7dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55eab9bcd6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eab9bc9094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eab9bda519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eab9bca5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eab9bda7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55eab9bca312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eab9bc9094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eab9bda519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55eab9bcb128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eab9bc9094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55eab9bc8d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55eab9bc8d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55eab9c7607b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55eab9ca2fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55eab9c9f353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55eab9c9716a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55eab9c9705c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55eab9c96297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55eab9c69f07]
=================================
[dgx13:83599:0:83599] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  83599) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fbe495e8f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fbe495e9114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fbe495e92da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fbeee660420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fbe496625d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fbe49687859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fbe495a442f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fbe495a7798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fbe495f1989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fbe495a662d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fbe4965fc4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fbe4971106a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55580b0826fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55580b07e094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55580b08f519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55580b132162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fbe6e8ee1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55580b08777c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55580b039d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55580b0867f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55580b084929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55580b07e094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55580b08f519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55580b080128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55580b07e094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55580b09cccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55580b09d44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55580b16010e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55580b08777c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55580b0826fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55580b09cdac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55580b0826fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55580b07e094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55580b08f519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55580b07f5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55580b08f7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55580b07f312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55580b07e094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55580b08f519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55580b080128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55580b07e094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55580b07dd68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55580b07dd19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55580b12b07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55580b157fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55580b154353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55580b14c16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55580b14c05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55580b14b297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55580b11ef07]
=================================
[dgx13:83608:0:83608] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  83608) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f4a3c049f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f4a3c04a114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f4a3c04a2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f4acef51420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f4a29ed75d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f4a29efc859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f4a29e7542f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f4a29e78798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f4a3c052989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f4a29e7762d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f4a29ed4c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f4a29f8606a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55936b18f6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55936b18b094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55936b19c519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x55936b1a9e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x55936b2b4b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55936b146d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55936b1937f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55936b191929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55936b18b094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55936b19c519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55936b18d128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55936b18b094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55936b1a9ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55936b1aa44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55936b26d10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55936b19477c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55936b18f6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55936b1a9dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55936b18f6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55936b18b094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55936b19c519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55936b18c5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55936b19c7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55936b18c312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55936b18b094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55936b19c519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55936b18d128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55936b18b094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55936b18ad68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55936b18ad19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55936b23807b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55936b264fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55936b261353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55936b25916a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55936b25905c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55936b258297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55936b22bf07]
=================================
[dgx13:83619:0:83619] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  83619) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f0d359f3f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f0d359f4114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f0d359f42da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f0ddca81420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f0d35a6d5d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f0d35a92859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f0d359af42f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f0d359b2798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f0d359fc989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f0d359b162d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f0d35a6ac4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f0d35b1c06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55b6020306fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b60202c094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b60203d519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55b6020e0162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f0dd00261e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55b60203577c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55b601fe7d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55b6020347f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55b602032929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b60202c094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b60203d519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55b60202e128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b60202c094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55b60204accb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55b60204b44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55b60210e10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55b60203577c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55b6020306fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55b60204adac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55b6020306fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b60202c094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b60203d519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b60202d5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b60203d7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55b60202d312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b60202c094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b60203d519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55b60202e128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b60202c094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55b60202bd68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55b60202bd19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55b6020d907b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55b602105fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55b602102353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55b6020fa16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55b6020fa05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55b6020f9297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55b6020ccf07]
=================================
[dgx13:83616:0:83616] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  83616) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7ff3e851bf1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7ff3e851c114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7ff3e851c2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7ff48958c420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7ff3e85955d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7ff3e85ba859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7ff3e84d742f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7ff3e84da798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7ff3e8524989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7ff3e84d962d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7ff3e8592c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7ff3e864406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55d23c28d6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55d23c289094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d23c29a519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55d23c33d162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7ff47c2521e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55d23c29277c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55d23c244d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55d23c2917f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55d23c28f929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55d23c289094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d23c29a519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55d23c28b128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55d23c289094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55d23c2a7ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55d23c2a844c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55d23c36b10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55d23c29277c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55d23c28d6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55d23c2a7dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55d23c28d6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55d23c289094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d23c29a519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d23c28a5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55d23c29a7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55d23c28a312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55d23c289094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d23c29a519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55d23c28b128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55d23c289094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55d23c288d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55d23c288d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55d23c33607b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55d23c362fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55d23c35f353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55d23c35716a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55d23c35705c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55d23c356297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55d23c329f07]
=================================
2023-10-26 06:08:47,916 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7fedfef7b180, tag: 0xb27769da85e40f53, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7fedfef7b180, tag: 0xb27769da85e40f53, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-10-26 06:08:47,916 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7faed4111200, tag: 0x2d7e988b8963a9b3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7faed4111200, tag: 0x2d7e988b8963a9b3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-10-26 06:08:47,927 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56619
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7faed41111c0, tag: 0xcaaf8c3056f1a578, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7faed41111c0, tag: 0xcaaf8c3056f1a578, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-26 06:08:47,928 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51415 -> ucx://127.0.0.1:56619
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7faed41112c0, tag: 0xacd6550f8816083a, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-26 06:08:47,929 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51415 -> ucx://127.0.0.1:60053
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7faed4111400, tag: 0xbdbde5e90dacd4e3, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-26 06:08:47,930 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54497
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7faed4111280, tag: 0xd0b85ef98336cf58, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7faed4111280, tag: 0xd0b85ef98336cf58, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-26 06:08:47,930 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:60053
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7faed4111140, tag: 0x5f683ae7008c13bb, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7faed4111140, tag: 0x5f683ae7008c13bb, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-26 06:08:47,998 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58793 -> ucx://127.0.0.1:56619
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fedfef7b2c0, tag: 0x4addff30ae99a98f, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-26 06:08:47,999 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58793 -> ucx://127.0.0.1:54497
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fedfef7b300, tag: 0xac3fe70dff8c5b1, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-26 06:08:48,000 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56619
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fedfef7b240, tag: 0x5bbfcbfacc9333cc, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fedfef7b240, tag: 0x5bbfcbfacc9333cc, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-26 06:08:48,000 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54497
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fedfef7b140, tag: 0x5fdb7cd9c7bfe5cf, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fedfef7b140, tag: 0x5fdb7cd9c7bfe5cf, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-26 06:08:48,002 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:60053
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7fedfef7b280, tag: 0x17c77160d7ce84c6, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7fedfef7b280, tag: 0x17c77160d7ce84c6, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-10-26 06:08:49,088 - distributed.nanny - WARNING - Restarting worker
2023-10-26 06:08:49,659 - distributed.nanny - WARNING - Restarting worker
2023-10-26 06:08:50,764 - distributed.nanny - ERROR - Failed to initialize worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-3m6l15a2'
2023-10-26 06:08:50,813 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-3m6l15a2'
2023-10-26 06:08:50,837 - distributed.nanny - ERROR - Failed to restart worker after its process exited
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 563, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-3m6l15a2'
2023-10-26 06:08:51,293 - distributed.nanny - ERROR - Failed to initialize worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-3dllnn4v'
2023-10-26 06:08:51,320 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-3dllnn4v'
2023-10-26 06:08:51,342 - distributed.nanny - ERROR - Failed to restart worker after its process exited
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 563, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-3dllnn4v'
2023-10-26 06:08:52,110 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49999
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7faed4111240, tag: 0x8ede131b2b51e1be, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7faed4111240, tag: 0x8ede131b2b51e1be, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-26 06:08:52,140 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58793 -> ucx://127.0.0.1:56805
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fedfef7b3c0, tag: 0x5a84f48a42fadf15, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-26 06:08:52,146 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56805
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fedfef7b200, tag: 0x707daff8f24d8c69, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fedfef7b200, tag: 0x707daff8f24d8c69, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-26 06:08:52,153 - distributed.nanny - WARNING - Restarting worker
2023-10-26 06:08:52,154 - distributed.nanny - WARNING - Restarting worker
2023-10-26 06:08:52,198 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51415 -> ucx://127.0.0.1:56805
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7faed4111340, tag: 0xc30bce0b8bca4463, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-26 06:08:52,200 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56805
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7faed4111180, tag: 0x442ed7c8eeb7c834, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7faed4111180, tag: 0x442ed7c8eeb7c834, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-26 06:08:52,233 - distributed.nanny - WARNING - Restarting worker
2023-10-26 06:08:53,094 - distributed.nanny - WARNING - Restarting worker
2023-10-26 06:08:54,023 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 0)
Function:  subgraph_callable-5f02fa75-9a09-4bad-8c7d-5bd80b83
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0             75905        0  47521077            0
1             75916        0  26188733            0
2             75928        0   6869943            0
3             14465        0  47469824            0
4             14466        0  86991107            0
...             ...      ...       ...          ...
99999995  799992862        0  72740784            0
99999996  799993062        0  26568378            0
99999997  799993064        0  46319310            0
99999998  799992960        0  32681246            0
99999999  799992977        0  40956016            0

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-26 06:08:54,158 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 2)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0            75906        2  68204573            2
1            75908        2  76178372            2
2            75920        2  92169559            2
3            75925        2   3669844            2
4            75929        2  83202656            2
...            ...      ...       ...          ...
12499995  99997547        2  50263005            2
12499996  99997551        2  71734470            2
12499997  99997553        2  34662211            2
12499998  99997559        2  94525404            2
12499999  99997560        2  22480286            2

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100009351        2  70633121            2
1         100009359        2   9793547            2
2         100009367        2  22933190            2
3         100009369        2  40077615            2
4         100009379        2  10626824            2
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-26 06:08:54,308 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 5)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0            75909        5  13610086            5
1            75917        5  85579871            5
2            75918        5  53639753            5
3            75919        5  80039153            5
4            75924        5  52045530            5
...            ...      ...       ...          ...
12499995  99997537        5  60097998            5
12499996  99997538        5  47797601            5
12499997  99997539        5  53342324            5
12499998  99997544        5  23317599            5
12499999  99997561        5  37388493            5

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100009344        5  50030251            5
1         100009345        5  68816473            5
2         100009347        5  98482456            5
3         100009349        5  62690571            5
4         100009353        5  18273024            5
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-26 06:08:54,341 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 4)
Function:  subgraph_callable-5f02fa75-9a09-4bad-8c7d-5bd80b83
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0             75910        4  59501992            4
1             75926        4  28244032            4
2             14471        4  28793692            4
3             14475        4  17473326            4
4            104106        4  37347033            4
...             ...      ...       ...          ...
99999995  799993086        4  16898484            4
99999996  799992967        4  49708930            4
99999997  799992971        4  81613678            4
99999998  799992981        4  54133257            4
99999999  799992988        4  23735536            4

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-26 06:08:54,471 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 6)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0            75911        6  68518226            6
1            75927        6  45957945            6
2            75934        6  16403384            6
3            14483        6  67172677            6
4            14485        6  72007242            6
...            ...      ...       ...          ...
12499995  99997554        6  28870868            6
12499996  99997558        6  65557792            6
12499997  99997563        6  43161386            6
12499998  99997564        6  71024048            6
12499999  99997566        6  84494140            6

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100009346        6  32671695            6
1         100009352        6  90896996            6
2         100009356        6  93400743            6
3         100009368        6  66857634            6
4         100009377        6  13522834            6
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-26 06:08:54,542 - distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
2023-10-26 06:08:54,543 - distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
2023-10-26 06:08:54,543 - distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
[1698300534.562664] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23270 was not returned to mpool ucp_rndv_frags
[1698300534.562672] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23288 was not returned to mpool ucp_rndv_frags
[1698300534.562674] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde232a0 was not returned to mpool ucp_rndv_frags
[1698300534.562676] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde232b8 was not returned to mpool ucp_rndv_frags
[1698300534.562677] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde232d0 was not returned to mpool ucp_rndv_frags
[1698300534.562679] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde232e8 was not returned to mpool ucp_rndv_frags
[1698300534.562681] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23300 was not returned to mpool ucp_rndv_frags
[1698300534.562682] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23318 was not returned to mpool ucp_rndv_frags
[1698300534.562684] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23330 was not returned to mpool ucp_rndv_frags
[1698300534.562686] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23348 was not returned to mpool ucp_rndv_frags
[1698300534.562687] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23360 was not returned to mpool ucp_rndv_frags
[1698300534.562689] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23378 was not returned to mpool ucp_rndv_frags
[1698300534.562691] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23390 was not returned to mpool ucp_rndv_frags
[1698300534.562693] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde233a8 was not returned to mpool ucp_rndv_frags
[1698300534.562695] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde233c0 was not returned to mpool ucp_rndv_frags
[1698300534.562696] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde233d8 was not returned to mpool ucp_rndv_frags
[1698300534.562698] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde233f0 was not returned to mpool ucp_rndv_frags
[1698300534.562700] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23408 was not returned to mpool ucp_rndv_frags
[1698300534.562701] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23420 was not returned to mpool ucp_rndv_frags
[1698300534.562703] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23438 was not returned to mpool ucp_rndv_frags
[1698300534.562704] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23450 was not returned to mpool ucp_rndv_frags
[1698300534.562706] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23468 was not returned to mpool ucp_rndv_frags
[1698300534.562708] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23480 was not returned to mpool ucp_rndv_frags
[1698300534.562709] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23498 was not returned to mpool ucp_rndv_frags
[1698300534.562711] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde234b0 was not returned to mpool ucp_rndv_frags
[1698300534.562713] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde234c8 was not returned to mpool ucp_rndv_frags
[1698300534.562714] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde234e0 was not returned to mpool ucp_rndv_frags
[1698300534.562716] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde234f8 was not returned to mpool ucp_rndv_frags
[1698300534.562718] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23510 was not returned to mpool ucp_rndv_frags
[1698300534.562719] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23528 was not returned to mpool ucp_rndv_frags
[1698300534.562721] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23540 was not returned to mpool ucp_rndv_frags
[1698300534.562729] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23558 was not returned to mpool ucp_rndv_frags
[1698300534.562730] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23570 was not returned to mpool ucp_rndv_frags
[1698300534.562732] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23588 was not returned to mpool ucp_rndv_frags
[1698300534.562734] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde235a0 was not returned to mpool ucp_rndv_frags
[1698300534.562735] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde235b8 was not returned to mpool ucp_rndv_frags
[1698300534.562737] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde235d0 was not returned to mpool ucp_rndv_frags
[1698300534.562739] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde235e8 was not returned to mpool ucp_rndv_frags
[1698300534.562740] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23600 was not returned to mpool ucp_rndv_frags
[1698300534.562742] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23618 was not returned to mpool ucp_rndv_frags
[1698300534.562743] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23630 was not returned to mpool ucp_rndv_frags
[1698300534.562763] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23648 was not returned to mpool ucp_rndv_frags
[1698300534.562764] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23660 was not returned to mpool ucp_rndv_frags
[1698300534.562766] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23678 was not returned to mpool ucp_rndv_frags
[1698300534.562768] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23690 was not returned to mpool ucp_rndv_frags
[1698300534.562770] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde236a8 was not returned to mpool ucp_rndv_frags
[1698300534.562771] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde236c0 was not returned to mpool ucp_rndv_frags
[1698300534.562773] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde236d8 was not returned to mpool ucp_rndv_frags
[1698300534.562775] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde236f0 was not returned to mpool ucp_rndv_frags
[1698300534.562776] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23708 was not returned to mpool ucp_rndv_frags
[1698300534.562778] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23720 was not returned to mpool ucp_rndv_frags
[1698300534.562780] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23738 was not returned to mpool ucp_rndv_frags
[1698300534.562797] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23750 was not returned to mpool ucp_rndv_frags
[1698300534.562799] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23768 was not returned to mpool ucp_rndv_frags
[1698300534.562800] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23780 was not returned to mpool ucp_rndv_frags
[1698300534.562802] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23798 was not returned to mpool ucp_rndv_frags
[1698300534.562804] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde237b0 was not returned to mpool ucp_rndv_frags
[1698300534.562825] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde237c8 was not returned to mpool ucp_rndv_frags
[1698300534.562827] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde237e0 was not returned to mpool ucp_rndv_frags
[1698300534.562828] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde237f8 was not returned to mpool ucp_rndv_frags
[1698300534.562830] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23810 was not returned to mpool ucp_rndv_frags
[1698300534.562832] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x2023-10-26 06:08:54,731 - distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
2023-10-26 06:08:54,732 - distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
2023-10-26 06:08:54,732 - distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
[1698300534.752733] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fb60 was not returned to mpool ucp_rndv_frags
[1698300534.752742] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fb78 was not returned to mpool ucp_rndv_frags
[1698300534.752744] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fb90 was not returned to mpool ucp_rndv_frags
[1698300534.752746] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fba8 was not returned to mpool ucp_rndv_frags
[1698300534.752748] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fbc0 was not returned to mpool ucp_rndv_frags
[1698300534.752750] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fbd8 was not returned to mpool ucp_rndv_frags
[1698300534.752752] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fbf0 was not returned to mpool ucp_rndv_frags
[1698300534.752754] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc08 was not returned to mpool ucp_rndv_frags
[1698300534.752755] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc20 was not returned to mpool ucp_rndv_frags
[1698300534.752757] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc38 was not returned to mpool ucp_rndv_frags
[1698300534.752758] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc50 was not returned to mpool ucp_rndv_frags
[1698300534.752760] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc68 was not returned to mpool ucp_rndv_frags
[1698300534.752762] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc80 was not returned to mpool ucp_rndv_frags
[1698300534.752763] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fc98 was not returned to mpool ucp_rndv_frags
[1698300534.752765] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fcb0 was not returned to mpool ucp_rndv_frags
[1698300534.752767] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fcc8 was not returned to mpool ucp_rndv_frags
[1698300534.752768] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fce0 was not returned to mpool ucp_rndv_frags
[1698300534.752770] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fcf8 was not returned to mpool ucp_rndv_frags
[1698300534.752772] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fd10 was not returned to mpool ucp_rndv_frags
[1698300534.752773] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fd28 was not returned to mpool ucp_rndv_frags
[1698300534.752775] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fd40 was not returned to mpool ucp_rndv_frags
[1698300534.752777] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fd58 was not returned to mpool ucp_rndv_frags
[1698300534.752778] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fd70 was not returned to mpool ucp_rndv_frags
[1698300534.752780] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fd88 was not returned to mpool ucp_rndv_frags
[1698300534.752781] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fda0 was not returned to mpool ucp_rndv_frags
[1698300534.752783] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fdb8 was not returned to mpool ucp_rndv_frags
[1698300534.752785] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fdd0 was not returned to mpool ucp_rndv_frags
[1698300534.752786] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fde8 was not returned to mpool ucp_rndv_frags
[1698300534.752788] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe00 was not returned to mpool ucp_rndv_frags
[1698300534.752790] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe18 was not returned to mpool ucp_rndv_frags
[1698300534.752791] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe30 was not return55b3cde23828 was not returned to mpool ucp_rndv_frags
[1698300534.562837] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23840 was not returned to mpool ucp_rndv_frags
[1698300534.562838] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23858 was not returned to mpool ucp_rndv_frags
[1698300534.562856] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23870 was not returned to mpool ucp_rndv_frags
[1698300534.562857] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23888 was not returned to mpool ucp_rndv_frags
[1698300534.562859] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde238a0 was not returned to mpool ucp_rndv_frags
[1698300534.562861] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde238b8 was not returned to mpool ucp_rndv_frags
[1698300534.562862] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde238d0 was not returned to mpool ucp_rndv_frags
[1698300534.562864] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde238e8 was not returned to mpool ucp_rndv_frags
[1698300534.562866] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23900 was not returned to mpool ucp_rndv_frags
[1698300534.562867] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cde23918 was not returned to mpool ucp_rndv_frags
[1698300534.564248] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cd967080 {flags:0x44040 send length 100000000 ucp_proto_progress_tag_rndv_rts() comp:???()cuda memory} was not returned to mpool ucp_requests
[1698300534.564253] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cd967a80 {flags:0x44040 send length 100000000 ucp_proto_progress_tag_rndv_rts() comp:???()cuda memory} was not returned to mpool ucp_requests
[1698300534.564255] [dgx13:83595:0]           mpool.c:54   UCX  WARN  object 0x55b3cd968840 {flags:0x44040 send length 100000000 ucp_proto_progress_tag_rndv_rts() comp:???()cuda memory} was not returned to mpool ucp_requests
ed to mpool ucp_rndv_frags
[1698300534.752800] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe48 was not returned to mpool ucp_rndv_frags
[1698300534.752801] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe60 was not returned to mpool ucp_rndv_frags
[1698300534.752803] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe78 was not returned to mpool ucp_rndv_frags
[1698300534.752805] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fe90 was not returned to mpool ucp_rndv_frags
[1698300534.752806] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fea8 was not returned to mpool ucp_rndv_frags
[1698300534.752808] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fec0 was not returned to mpool ucp_rndv_frags
[1698300534.752828] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fed8 was not returned to mpool ucp_rndv_frags
[1698300534.752829] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7fef0 was not returned to mpool ucp_rndv_frags
[1698300534.752831] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff08 was not returned to mpool ucp_rndv_frags
[1698300534.752833] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff20 was not returned to mpool ucp_rndv_frags
[1698300534.752834] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff38 was not returned to mpool ucp_rndv_frags
[1698300534.752836] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff50 was not returned to mpool ucp_rndv_frags
[1698300534.752838] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff68 was not returned to mpool ucp_rndv_frags
[1698300534.752839] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff80 was not returned to mpool ucp_rndv_frags
[1698300534.752841] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ff98 was not returned to mpool ucp_rndv_frags
[1698300534.752843] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ffb0 was not returned to mpool ucp_rndv_frags
[1698300534.752845] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x564354b7ffc8 was not returned to mpool ucp_rndv_frags
[1698300534.754361] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x5643539b8340 {flags:0x44040 send length 100000000 ucp_proto_progress_tag_rndv_rts() comp:???()cuda memory} was not returned to mpool ucp_requests
[1698300534.754365] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x5643539b8480 {flags:0x44040 send length 100000000 ucp_proto_progress_tag_rndv_rts() comp:???()cuda memory} was not returned to mpool ucp_requests
[1698300534.754368] [dgx13:83603:0]           mpool.c:54   UCX  WARN  object 0x5643539b8840 {flags:0x44040 send length 100000000 ucp_proto_progress_tag_rndv_rts() comp:???()cuda memory} was not returned to mpool ucp_requests
2023-10-26 06:08:55,449 - distributed.nanny - ERROR - Failed to initialize worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-5sw0v1q0'
2023-10-26 06:08:55,449 - distributed.nanny - ERROR - Failed to initialize worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-domqp9bp'
2023-10-26 06:08:55,450 - distributed.nanny - ERROR - Failed to initialize worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-ch63txdb'
2023-10-26 06:08:55,451 - distributed.nanny - ERROR - Failed to initialize worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-ipfj81wy'
2023-10-26 06:08:55,502 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-domqp9bp'
2023-10-26 06:08:55,503 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-5sw0v1q0'
2023-10-26 06:08:55,504 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-ch63txdb'
2023-10-26 06:08:55,504 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 940, in run
    worker = worker_factory()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 718, in __init__
    ServerNode.__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 363, in __init__
    self._workdir = self._workspace.new_work_dir(prefix=f"{name}-")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 274, in new_work_dir
    return WorkDir(self, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diskutils.py", line 49, in __init__
    self.dir_path = tempfile.mkdtemp(prefix=prefix, dir=workspace.base_dir)
  File "/opt/conda/envs/gdf/lib/python3.9/tempfile.py", line 363, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/dask-scratch-space/worker-ipfj81wy'
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
