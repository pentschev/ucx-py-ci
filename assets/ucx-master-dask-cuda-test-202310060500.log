============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-06 05:29:57,001 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:29:57,006 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34407 instead
  warnings.warn(
2023-10-06 05:29:57,010 - distributed.scheduler - INFO - State start
2023-10-06 05:29:57,033 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:29:57,034 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-06 05:29:57,034 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34407/status
2023-10-06 05:29:57,035 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:29:57,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44299'
2023-10-06 05:29:57,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43501'
2023-10-06 05:29:57,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38047'
2023-10-06 05:29:57,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36513'
2023-10-06 05:29:57,863 - distributed.scheduler - INFO - Receive client connection: Client-61be300a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:29:57,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58470
2023-10-06 05:29:58,807 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:29:58,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:29:58,812 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:29:58,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:29:58,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:29:58,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-06 05:29:58,848 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33199
2023-10-06 05:29:58,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33199
2023-10-06 05:29:58,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34177
2023-10-06 05:29:58,848 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-06 05:29:58,848 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:29:58,848 - distributed.worker - INFO -               Threads:                          4
2023-10-06 05:29:58,848 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-06 05:29:58,848 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-s9qn6td_
2023-10-06 05:29:58,849 - distributed.worker - INFO - Starting Worker plugin PreImport-c47c6b93-b25c-4085-bed4-a2a5122b8f7a
2023-10-06 05:29:58,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e455ec65-4116-4d85-84e4-387a473b4ee0
2023-10-06 05:29:58,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bfaea52-f5cc-454c-bf8d-dab632976a0f
2023-10-06 05:29:58,849 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:29:58,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:29:58,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:29:58,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:29:58,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:29:58,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:29:58,888 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:29:59,191 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33199', status: init, memory: 0, processing: 0>
2023-10-06 05:29:59,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33199
2023-10-06 05:29:59,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58488
2023-10-06 05:29:59,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:29:59,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-06 05:29:59,194 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:29:59,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-06 05:30:00,063 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36155
2023-10-06 05:30:00,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36155
2023-10-06 05:30:00,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42473
2023-10-06 05:30:00,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-06 05:30:00,064 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,064 - distributed.worker - INFO -               Threads:                          4
2023-10-06 05:30:00,064 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-06 05:30:00,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-34_i8uva
2023-10-06 05:30:00,064 - distributed.worker - INFO - Starting Worker plugin PreImport-fac0b709-01c0-4561-9df9-bfbefe47de77
2023-10-06 05:30:00,065 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfd7848c-d3fa-4e71-8544-7297b32244b4
2023-10-06 05:30:00,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-706a7d86-1cf5-4532-91f7-2648f4266958
2023-10-06 05:30:00,065 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36155', status: init, memory: 0, processing: 0>
2023-10-06 05:30:00,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36155
2023-10-06 05:30:00,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47498
2023-10-06 05:30:00,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:00,105 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-06 05:30:00,105 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-06 05:30:00,270 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39881
2023-10-06 05:30:00,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39881
2023-10-06 05:30:00,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39617
2023-10-06 05:30:00,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-06 05:30:00,271 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,270 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34825
2023-10-06 05:30:00,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34825
2023-10-06 05:30:00,271 - distributed.worker - INFO -               Threads:                          4
2023-10-06 05:30:00,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45323
2023-10-06 05:30:00,271 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-06 05:30:00,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-06 05:30:00,271 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gaedlax6
2023-10-06 05:30:00,271 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,271 - distributed.worker - INFO -               Threads:                          4
2023-10-06 05:30:00,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f405ac3-5d28-4bd9-ac89-3e51685546d5
2023-10-06 05:30:00,272 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-06 05:30:00,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-an7wncl5
2023-10-06 05:30:00,272 - distributed.worker - INFO - Starting Worker plugin PreImport-b525ef81-ac1c-47e4-a545-dc3fe578aa32
2023-10-06 05:30:00,272 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5c6f568-9af9-444f-b85a-f39ae9ae669e
2023-10-06 05:30:00,272 - distributed.worker - INFO - Starting Worker plugin PreImport-d7f842ba-c9b4-4d71-af95-7dc8c4f0284a
2023-10-06 05:30:00,272 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f45ae9a-79a1-4bd7-95b5-93e84f08b79e
2023-10-06 05:30:00,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b906fb5-a7e3-4c75-b0ea-0dd3c1f2c64b
2023-10-06 05:30:00,273 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,293 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34825', status: init, memory: 0, processing: 0>
2023-10-06 05:30:00,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34825
2023-10-06 05:30:00,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47520
2023-10-06 05:30:00,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:00,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-06 05:30:00,296 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,297 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39881', status: init, memory: 0, processing: 0>
2023-10-06 05:30:00,297 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39881
2023-10-06 05:30:00,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47516
2023-10-06 05:30:00,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-06 05:30:00,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:00,299 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-06 05:30:00,299 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:00,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-06 05:30:00,330 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-06 05:30:00,330 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-06 05:30:00,330 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-06 05:30:00,330 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-06 05:30:00,334 - distributed.scheduler - INFO - Remove client Client-61be300a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:00,335 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58470; closing.
2023-10-06 05:30:00,335 - distributed.scheduler - INFO - Remove client Client-61be300a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:00,336 - distributed.scheduler - INFO - Close client connection: Client-61be300a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:00,336 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44299'. Reason: nanny-close
2023-10-06 05:30:00,337 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:00,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43501'. Reason: nanny-close
2023-10-06 05:30:00,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38047'. Reason: nanny-close
2023-10-06 05:30:00,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36513'. Reason: nanny-close
2023-10-06 05:30:00,338 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36155. Reason: nanny-close
2023-10-06 05:30:00,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:00,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33199. Reason: nanny-close
2023-10-06 05:30:00,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47498; closing.
2023-10-06 05:30:00,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-06 05:30:00,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570200.3406975')
2023-10-06 05:30:00,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-06 05:30:00,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58488; closing.
2023-10-06 05:30:00,342 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:00,342 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:00,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570200.3421726')
2023-10-06 05:30:00,346 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:00,346 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:00,347 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39881. Reason: nanny-close
2023-10-06 05:30:00,347 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34825. Reason: nanny-close
2023-10-06 05:30:00,348 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-06 05:30:00,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47516; closing.
2023-10-06 05:30:00,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570200.349263')
2023-10-06 05:30:00,349 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-06 05:30:00,349 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47520; closing.
2023-10-06 05:30:00,350 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570200.3500228')
2023-10-06 05:30:00,350 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:00,350 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:00,350 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:01,453 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:01,453 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:01,453 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:01,455 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-06 05:30:01,455 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-06 05:30:03,475 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:03,480 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46115 instead
  warnings.warn(
2023-10-06 05:30:03,484 - distributed.scheduler - INFO - State start
2023-10-06 05:30:03,507 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:03,508 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:03,509 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46115/status
2023-10-06 05:30:03,509 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:03,561 - distributed.scheduler - INFO - Receive client connection: Client-65a35f96-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:03,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58760
2023-10-06 05:30:04,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42351'
2023-10-06 05:30:04,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43595'
2023-10-06 05:30:04,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40993'
2023-10-06 05:30:04,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37965'
2023-10-06 05:30:04,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37033'
2023-10-06 05:30:04,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36487'
2023-10-06 05:30:04,103 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36705'
2023-10-06 05:30:04,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45787'
2023-10-06 05:30:05,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:05,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:05,880 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:05,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:05,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:05,935 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:05,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:05,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:05,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:05,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:05,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:05,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:06,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:06,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:06,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:06,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:06,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:06,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:06,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:06,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:06,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:06,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:06,213 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:06,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:07,521 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36611
2023-10-06 05:30:07,522 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36611
2023-10-06 05:30:07,522 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43843
2023-10-06 05:30:07,522 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:07,522 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:07,522 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:07,522 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:07,522 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y7q9hypj
2023-10-06 05:30:07,522 - distributed.worker - INFO - Starting Worker plugin PreImport-6b9ff1b5-d746-4915-a1f9-7fbe127e36b1
2023-10-06 05:30:07,523 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c852dd1-271a-4ce3-89a9-dd7ec1d93238
2023-10-06 05:30:07,523 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ff78e35-78ea-4608-8a86-191051f11403
2023-10-06 05:30:07,843 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:07,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36611', status: init, memory: 0, processing: 0>
2023-10-06 05:30:07,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36611
2023-10-06 05:30:07,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58836
2023-10-06 05:30:07,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:07,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:07,875 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:07,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42711
2023-10-06 05:30:08,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42711
2023-10-06 05:30:08,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37503
2023-10-06 05:30:08,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,549 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,549 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zwyto8rw
2023-10-06 05:30:08,550 - distributed.worker - INFO - Starting Worker plugin PreImport-81562a29-de22-4332-9564-327746ad7d87
2023-10-06 05:30:08,550 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a65a160-f642-474d-863e-4223805d5990
2023-10-06 05:30:08,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43985
2023-10-06 05:30:08,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43985
2023-10-06 05:30:08,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34841
2023-10-06 05:30:08,642 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,642 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,642 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,642 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,642 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m5gp1o29
2023-10-06 05:30:08,642 - distributed.worker - INFO - Starting Worker plugin PreImport-a63f7280-208c-4cfe-99bb-66f9fa19f24b
2023-10-06 05:30:08,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2faa9dd-99cc-48f0-8591-dacc13a19603
2023-10-06 05:30:08,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bdfd2d1-d568-4353-95b0-e363a08515c6
2023-10-06 05:30:08,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33845
2023-10-06 05:30:08,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33845
2023-10-06 05:30:08,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42469
2023-10-06 05:30:08,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,645 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,645 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k8zhu024
2023-10-06 05:30:08,646 - distributed.worker - INFO - Starting Worker plugin PreImport-f90a1cfa-ae98-44dd-a578-62ef7261d967
2023-10-06 05:30:08,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5aa5c9d9-1a3d-4b3c-8e7a-f6d256523071
2023-10-06 05:30:08,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38191
2023-10-06 05:30:08,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38191
2023-10-06 05:30:08,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40041
2023-10-06 05:30:08,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,650 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,650 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,651 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fh7dd3ey
2023-10-06 05:30:08,651 - distributed.worker - INFO - Starting Worker plugin PreImport-3345d1c4-3c07-4baa-b3bc-d4f9b978c71f
2023-10-06 05:30:08,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8e096c7-d1d1-400c-a916-39bf6acf47f3
2023-10-06 05:30:08,680 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46655
2023-10-06 05:30:08,681 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46655
2023-10-06 05:30:08,681 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41961
2023-10-06 05:30:08,681 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,681 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,681 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,681 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mcdnh1cq
2023-10-06 05:30:08,682 - distributed.worker - INFO - Starting Worker plugin PreImport-2858f39e-6327-4177-b4bb-8a048ba66379
2023-10-06 05:30:08,682 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f59b41c1-0a75-4645-82d0-681b54ada6db
2023-10-06 05:30:08,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1797ea95-e32e-4fb2-95e7-dcb0dd4b7ea0
2023-10-06 05:30:08,687 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38511
2023-10-06 05:30:08,693 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38511
2023-10-06 05:30:08,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39127
2023-10-06 05:30:08,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,693 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,693 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,693 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xfrxe88o
2023-10-06 05:30:08,694 - distributed.worker - INFO - Starting Worker plugin PreImport-044eb603-72c2-4ada-bfa0-4b1db4429aa6
2023-10-06 05:30:08,694 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-adf1077b-13a4-4117-b807-52684697a394
2023-10-06 05:30:08,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8d6a168-e745-4f0c-8b8c-752b1835dbb5
2023-10-06 05:30:08,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37675
2023-10-06 05:30:08,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37675
2023-10-06 05:30:08,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43681
2023-10-06 05:30:08,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,709 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,709 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:08,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:08,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h61qin0_
2023-10-06 05:30:08,710 - distributed.worker - INFO - Starting Worker plugin PreImport-cc86efdb-8112-4ec0-a235-22063dbd2d7c
2023-10-06 05:30:08,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51ff33c8-5b6e-496c-8c67-b3f172da07f4
2023-10-06 05:30:08,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42711', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,711 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42711
2023-10-06 05:30:08,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58846
2023-10-06 05:30:08,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,713 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,713 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,795 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f569bda-df9a-44b9-a3c3-e633619bc002
2023-10-06 05:30:08,804 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,814 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03b9a5a7-e1ac-4e9f-b6c4-8d239627dc5e
2023-10-06 05:30:08,815 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ddeb2c2-4b2a-4bbb-a39c-5f373925749b
2023-10-06 05:30:08,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a95c93e4-4aa4-46f6-89eb-371dbabda194
2023-10-06 05:30:08,830 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,830 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,830 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,834 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43985', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,835 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43985
2023-10-06 05:30:08,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58852
2023-10-06 05:30:08,836 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38191', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38191
2023-10-06 05:30:08,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58856
2023-10-06 05:30:08,836 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,837 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,837 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,838 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46655', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,855 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46655
2023-10-06 05:30:08,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58878
2023-10-06 05:30:08,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33845', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33845
2023-10-06 05:30:08,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58870
2023-10-06 05:30:08,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38511', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38511
2023-10-06 05:30:08,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58892
2023-10-06 05:30:08,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,859 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,860 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37675', status: init, memory: 0, processing: 0>
2023-10-06 05:30:08,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37675
2023-10-06 05:30:08,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58896
2023-10-06 05:30:08,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:08,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:08,871 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:08,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:08,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:09,042 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:09,045 - distributed.scheduler - INFO - Remove client Client-65a35f96-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:09,046 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58760; closing.
2023-10-06 05:30:09,046 - distributed.scheduler - INFO - Remove client Client-65a35f96-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:09,046 - distributed.scheduler - INFO - Close client connection: Client-65a35f96-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:09,047 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42351'. Reason: nanny-close
2023-10-06 05:30:09,047 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43595'. Reason: nanny-close
2023-10-06 05:30:09,049 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40993'. Reason: nanny-close
2023-10-06 05:30:09,049 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,049 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33845. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37965'. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37675. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38191. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37033'. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,050 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42711. Reason: nanny-close
2023-10-06 05:30:09,051 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36487'. Reason: nanny-close
2023-10-06 05:30:09,051 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,051 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36705'. Reason: nanny-close
2023-10-06 05:30:09,052 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,052 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58870; closing.
2023-10-06 05:30:09,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45787'. Reason: nanny-close
2023-10-06 05:30:09,052 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43985. Reason: nanny-close
2023-10-06 05:30:09,052 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58856; closing.
2023-10-06 05:30:09,052 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:09,052 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38511. Reason: nanny-close
2023-10-06 05:30:09,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,052 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33845', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.05272')
2023-10-06 05:30:09,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46655. Reason: nanny-close
2023-10-06 05:30:09,053 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.0532668')
2023-10-06 05:30:09,053 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,054 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58896; closing.
2023-10-06 05:30:09,054 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,054 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,054 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,054 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,054 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,054 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36611. Reason: nanny-close
2023-10-06 05:30:09,055 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.0549748')
2023-10-06 05:30:09,055 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58846; closing.
2023-10-06 05:30:09,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,056 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.056294')
2023-10-06 05:30:09,056 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58852; closing.
2023-10-06 05:30:09,056 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58892; closing.
2023-10-06 05:30:09,057 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43985', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.05726')
2023-10-06 05:30:09,057 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:09,057 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.0575497')
2023-10-06 05:30:09,057 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58878; closing.
2023-10-06 05:30:09,058 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.0582426')
2023-10-06 05:30:09,062 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58836; closing.
2023-10-06 05:30:09,062 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570209.062602')
2023-10-06 05:30:09,062 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:09,062 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:09,065 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:10,664 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:10,665 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:10,665 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:10,666 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:30:10,667 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-06 05:30:12,972 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:12,977 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33083 instead
  warnings.warn(
2023-10-06 05:30:12,982 - distributed.scheduler - INFO - State start
2023-10-06 05:30:13,004 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:13,005 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:13,006 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33083/status
2023-10-06 05:30:13,006 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:13,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41779'
2023-10-06 05:30:13,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40337'
2023-10-06 05:30:13,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33041'
2023-10-06 05:30:13,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39219'
2023-10-06 05:30:13,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38809'
2023-10-06 05:30:13,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34037'
2023-10-06 05:30:13,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41997'
2023-10-06 05:30:13,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36659'
2023-10-06 05:30:13,643 - distributed.scheduler - INFO - Receive client connection: Client-6b32b89a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:13,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49388
2023-10-06 05:30:14,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:14,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:14,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:14,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:14,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:14,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:14,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:14,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:14,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:14,967 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:14,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:14,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:15,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:15,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:15,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:15,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:15,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:15,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:15,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:15,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:15,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:15,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:15,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:15,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:17,493 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39967
2023-10-06 05:30:17,494 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39967
2023-10-06 05:30:17,494 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34217
2023-10-06 05:30:17,494 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,494 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,494 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,494 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-64w726yt
2023-10-06 05:30:17,495 - distributed.worker - INFO - Starting Worker plugin PreImport-7df01474-2314-4560-a512-07c65a890171
2023-10-06 05:30:17,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad3d81dc-95d5-4c57-b6f3-a87a60f2dae1
2023-10-06 05:30:17,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d77d5994-5882-42fa-9e79-5e8917cab321
2023-10-06 05:30:17,500 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,526 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39967', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,528 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39967
2023-10-06 05:30:17,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49400
2023-10-06 05:30:17,529 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,530 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,530 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,557 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35913
2023-10-06 05:30:17,558 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35913
2023-10-06 05:30:17,558 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43749
2023-10-06 05:30:17,558 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,558 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,558 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,558 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1iiai3w1
2023-10-06 05:30:17,559 - distributed.worker - INFO - Starting Worker plugin PreImport-464a78f1-ff6e-4a2d-9d2c-cd50325dee18
2023-10-06 05:30:17,559 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-030d6cf8-9318-45ec-8d1e-ba6809d38288
2023-10-06 05:30:17,560 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32e21123-7b63-4b54-bbe1-69c88a16f203
2023-10-06 05:30:17,559 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40377
2023-10-06 05:30:17,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40377
2023-10-06 05:30:17,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38423
2023-10-06 05:30:17,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,560 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,560 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,561 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,561 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kmxeiw9m
2023-10-06 05:30:17,561 - distributed.worker - INFO - Starting Worker plugin PreImport-bf68995d-a463-4a4a-9583-d500c3c35e60
2023-10-06 05:30:17,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46e5b5e8-0433-4a5d-b186-775b8f1e49fd
2023-10-06 05:30:17,569 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56425085-b426-4820-b530-f4a80673423e
2023-10-06 05:30:17,571 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35913', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,596 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35913
2023-10-06 05:30:17,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49416
2023-10-06 05:30:17,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40377', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40377
2023-10-06 05:30:17,598 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49430
2023-10-06 05:30:17,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,600 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35241
2023-10-06 05:30:17,795 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35241
2023-10-06 05:30:17,795 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45341
2023-10-06 05:30:17,795 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,796 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,796 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,796 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sutxa1pr
2023-10-06 05:30:17,797 - distributed.worker - INFO - Starting Worker plugin PreImport-4cf5237c-e0f1-489f-90ca-53b07ba54709
2023-10-06 05:30:17,797 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fafc48d-d2e0-4fa8-910a-6c77bdb587b9
2023-10-06 05:30:17,798 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c82cd688-3929-4f76-ab17-fd4f1807bae9
2023-10-06 05:30:17,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46791
2023-10-06 05:30:17,803 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46791
2023-10-06 05:30:17,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36681
2023-10-06 05:30:17,803 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,803 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,803 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,803 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hhw9ts26
2023-10-06 05:30:17,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d62663d-c0ba-4182-93bc-24336b26b086
2023-10-06 05:30:17,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13a29fef-208b-4492-8ac6-c50d63c28593
2023-10-06 05:30:17,803 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34695
2023-10-06 05:30:17,804 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34695
2023-10-06 05:30:17,804 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45805
2023-10-06 05:30:17,805 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,805 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,805 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,805 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,805 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-049drubh
2023-10-06 05:30:17,805 - distributed.worker - INFO - Starting Worker plugin PreImport-2ef1858c-21a6-4bd0-a7ca-2a8d9fef3acf
2023-10-06 05:30:17,805 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39853
2023-10-06 05:30:17,805 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39853
2023-10-06 05:30:17,805 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8da61782-7b19-4ba3-b650-ee5208b47904
2023-10-06 05:30:17,806 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36669
2023-10-06 05:30:17,806 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,806 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,806 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,806 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,806 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i0gob6f7
2023-10-06 05:30:17,806 - distributed.worker - INFO - Starting Worker plugin PreImport-8dffa79f-9cf1-49e7-a389-4f168b524875
2023-10-06 05:30:17,807 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-652cbdb0-cb9d-4b22-b7c6-70c953a9d532
2023-10-06 05:30:17,807 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2fde0004-fd3b-4e21-b99a-262ec24dae49
2023-10-06 05:30:17,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46453
2023-10-06 05:30:17,809 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46453
2023-10-06 05:30:17,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45519
2023-10-06 05:30:17,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,809 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,809 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:17,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:17,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zkxjm1vt
2023-10-06 05:30:17,810 - distributed.worker - INFO - Starting Worker plugin PreImport-e9396c76-dd9c-4070-bdc2-a651be42fedf
2023-10-06 05:30:17,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10612656-d891-480d-8d75-c59f5008ddb5
2023-10-06 05:30:17,823 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,827 - distributed.worker - INFO - Starting Worker plugin PreImport-eda84b0d-b088-4634-aac4-4be4921c88b6
2023-10-06 05:30:17,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1fbdd9a-fd42-40f0-b4ef-8b8401263462
2023-10-06 05:30:17,827 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,827 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,828 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9b14e33-9fb7-466d-8f5b-56ba97311cf7
2023-10-06 05:30:17,829 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34695', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,852 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34695
2023-10-06 05:30:17,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49450
2023-10-06 05:30:17,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35241', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,854 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,854 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35241
2023-10-06 05:30:17,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49444
2023-10-06 05:30:17,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46791', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46791
2023-10-06 05:30:17,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49462
2023-10-06 05:30:17,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46453', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46453
2023-10-06 05:30:17,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49484
2023-10-06 05:30:17,864 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,864 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39853', status: init, memory: 0, processing: 0>
2023-10-06 05:30:17,865 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,865 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39853
2023-10-06 05:30:17,865 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49470
2023-10-06 05:30:17,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,866 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,866 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:17,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:17,867 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:17,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:17,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:17,896 - distributed.scheduler - INFO - Remove client Client-6b32b89a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:17,896 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49388; closing.
2023-10-06 05:30:17,896 - distributed.scheduler - INFO - Remove client Client-6b32b89a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:17,897 - distributed.scheduler - INFO - Close client connection: Client-6b32b89a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:17,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41779'. Reason: nanny-close
2023-10-06 05:30:17,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40337'. Reason: nanny-close
2023-10-06 05:30:17,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33041'. Reason: nanny-close
2023-10-06 05:30:17,899 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39219'. Reason: nanny-close
2023-10-06 05:30:17,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,900 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39967. Reason: nanny-close
2023-10-06 05:30:17,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38809'. Reason: nanny-close
2023-10-06 05:30:17,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40377. Reason: nanny-close
2023-10-06 05:30:17,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34037'. Reason: nanny-close
2023-10-06 05:30:17,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41997'. Reason: nanny-close
2023-10-06 05:30:17,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36659'. Reason: nanny-close
2023-10-06 05:30:17,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35913. Reason: nanny-close
2023-10-06 05:30:17,902 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49400; closing.
2023-10-06 05:30:17,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34695. Reason: nanny-close
2023-10-06 05:30:17,903 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9032445')
2023-10-06 05:30:17,903 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,903 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49430; closing.
2023-10-06 05:30:17,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,904 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,904 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9046633')
2023-10-06 05:30:17,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46791. Reason: nanny-close
2023-10-06 05:30:17,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,904 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,905 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46453. Reason: nanny-close
2023-10-06 05:30:17,906 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,906 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49416; closing.
2023-10-06 05:30:17,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,907 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,908 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:17,908 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,908 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39853. Reason: nanny-close
2023-10-06 05:30:17,908 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35241. Reason: nanny-close
2023-10-06 05:30:17,907 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49430>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-06 05:30:17,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35913', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9091768')
2023-10-06 05:30:17,909 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49450; closing.
2023-10-06 05:30:17,910 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9102635')
2023-10-06 05:30:17,910 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,910 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:17,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49462; closing.
2023-10-06 05:30:17,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49484; closing.
2023-10-06 05:30:17,912 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,912 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:17,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9130955')
2023-10-06 05:30:17,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9134789')
2023-10-06 05:30:17,913 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49470; closing.
2023-10-06 05:30:17,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49444; closing.
2023-10-06 05:30:17,914 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39853', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.914862')
2023-10-06 05:30:17,915 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570217.9153352')
2023-10-06 05:30:17,915 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:17,915 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49484>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-06 05:30:17,916 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49462>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-06 05:30:19,917 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:19,917 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:19,918 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:19,919 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:30:19,920 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-06 05:30:22,150 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:22,154 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-06 05:30:22,158 - distributed.scheduler - INFO - State start
2023-10-06 05:30:22,234 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:22,235 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:22,236 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-06 05:30:22,236 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:22,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34597'
2023-10-06 05:30:22,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41689'
2023-10-06 05:30:22,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36755'
2023-10-06 05:30:22,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34009'
2023-10-06 05:30:22,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40015'
2023-10-06 05:30:22,407 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40623'
2023-10-06 05:30:22,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45491'
2023-10-06 05:30:22,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39087'
2023-10-06 05:30:23,577 - distributed.scheduler - INFO - Receive client connection: Client-70bcbf4d-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:23,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47514
2023-10-06 05:30:24,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,287 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:24,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:24,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:24,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:27,004 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35199
2023-10-06 05:30:27,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35199
2023-10-06 05:30:27,005 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44411
2023-10-06 05:30:27,005 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,005 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,005 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,005 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,005 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_qd3mt46
2023-10-06 05:30:27,006 - distributed.worker - INFO - Starting Worker plugin PreImport-69b97ca5-f200-4846-a7b9-14552b88d1f7
2023-10-06 05:30:27,006 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b28c6e6-ccb7-4aea-ab5a-041f8b4ee7e1
2023-10-06 05:30:27,004 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41011
2023-10-06 05:30:27,006 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41011
2023-10-06 05:30:27,007 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41989
2023-10-06 05:30:27,007 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,007 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,007 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,007 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qx8lbwr5
2023-10-06 05:30:27,008 - distributed.worker - INFO - Starting Worker plugin PreImport-128578bc-13c6-4dc7-8429-c0c7391a00b4
2023-10-06 05:30:27,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a8f5677-da11-4c9a-bf23-309d78d829ca
2023-10-06 05:30:27,008 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44711
2023-10-06 05:30:27,009 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44711
2023-10-06 05:30:27,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33569
2023-10-06 05:30:27,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,009 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,009 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5zvi33_d
2023-10-06 05:30:27,010 - distributed.worker - INFO - Starting Worker plugin PreImport-1ceb4c0c-f568-40c5-9753-8e91a01eedc9
2023-10-06 05:30:27,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d25058c-6829-41a0-989e-dedff812ff0e
2023-10-06 05:30:27,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bb21174-9163-4103-ad29-694d6991c65e
2023-10-06 05:30:27,018 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35779
2023-10-06 05:30:27,019 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35779
2023-10-06 05:30:27,019 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40583
2023-10-06 05:30:27,019 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,019 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,019 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,020 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,020 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-88sfhvh5
2023-10-06 05:30:27,020 - distributed.worker - INFO - Starting Worker plugin PreImport-610e87ee-b9dc-4b63-8e08-2da60b6efb5c
2023-10-06 05:30:27,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22985549-1395-4679-b1af-e6b7a531d313
2023-10-06 05:30:27,075 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39407
2023-10-06 05:30:27,076 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39407
2023-10-06 05:30:27,076 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39691
2023-10-06 05:30:27,076 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,076 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,076 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,076 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,076 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j9eo04wr
2023-10-06 05:30:27,077 - distributed.worker - INFO - Starting Worker plugin PreImport-09da29e4-976e-4ad4-9e3a-53ffaad297c6
2023-10-06 05:30:27,077 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90889029-3d8b-49e1-b962-c26df41e935a
2023-10-06 05:30:27,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44739
2023-10-06 05:30:27,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44739
2023-10-06 05:30:27,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40313
2023-10-06 05:30:27,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,079 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,080 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,080 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5_aiqltd
2023-10-06 05:30:27,080 - distributed.worker - INFO - Starting Worker plugin PreImport-4a14c10c-9caa-4c53-a5f3-751b430aaeca
2023-10-06 05:30:27,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81bdba0d-54cd-4221-82d9-748427bb9517
2023-10-06 05:30:27,081 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44115
2023-10-06 05:30:27,081 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44115
2023-10-06 05:30:27,081 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35045
2023-10-06 05:30:27,082 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35615
2023-10-06 05:30:27,082 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,082 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35045
2023-10-06 05:30:27,082 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,082 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33381
2023-10-06 05:30:27,082 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,082 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,082 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,082 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,082 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h1l8dt4z
2023-10-06 05:30:27,082 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:27,082 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:27,082 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8kun0365
2023-10-06 05:30:27,082 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c1a118e-d98d-4f11-9547-5672692617cf
2023-10-06 05:30:27,083 - distributed.worker - INFO - Starting Worker plugin PreImport-8e532773-d22b-4cb0-872e-0f2de5032a69
2023-10-06 05:30:27,083 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1fa6930e-3306-4911-ae9a-e1a86c7277b1
2023-10-06 05:30:27,083 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57a7a730-472b-43ed-987b-843f10efd9a1
2023-10-06 05:30:27,227 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c5f4dac-a99b-4420-9eb6-18d09df73e34
2023-10-06 05:30:27,227 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ef7e581-b446-4a2b-82bc-425fa615ce5b
2023-10-06 05:30:27,249 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,261 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,265 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41011', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,267 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41011
2023-10-06 05:30:27,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47522
2023-10-06 05:30:27,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,269 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,269 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27f78cf1-76be-4435-b15e-0aea933002ca
2023-10-06 05:30:27,270 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,281 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35199', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35199
2023-10-06 05:30:27,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47536
2023-10-06 05:30:27,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,283 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,283 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52e7b5f3-0aee-414d-836a-99e2a37f0041
2023-10-06 05:30:27,305 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,306 - distributed.worker - INFO - Starting Worker plugin PreImport-084323eb-df90-45e5-977d-090662137029
2023-10-06 05:30:27,306 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72dd25d3-8358-4cb5-885e-d5656f2ded59
2023-10-06 05:30:27,309 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b894318-21c3-4766-a38c-c85e13a37bc6
2023-10-06 05:30:27,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44711', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,310 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,311 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44711
2023-10-06 05:30:27,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47544
2023-10-06 05:30:27,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,313 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35779', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,316 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35779
2023-10-06 05:30:27,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47560
2023-10-06 05:30:27,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,320 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,320 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44115', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44115
2023-10-06 05:30:27,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47588
2023-10-06 05:30:27,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,343 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,344 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35045', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,345 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35045
2023-10-06 05:30:27,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47574
2023-10-06 05:30:27,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,346 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,347 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,347 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44739', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44739
2023-10-06 05:30:27,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47610
2023-10-06 05:30:27,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39407', status: init, memory: 0, processing: 0>
2023-10-06 05:30:27,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39407
2023-10-06 05:30:27,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47598
2023-10-06 05:30:27,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:27,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,353 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:27,353 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:27,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:27,362 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,363 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,364 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:27,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,377 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,377 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:27,383 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:27,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:27,387 - distributed.scheduler - INFO - Remove client Client-70bcbf4d-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:27,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47514; closing.
2023-10-06 05:30:27,388 - distributed.scheduler - INFO - Remove client Client-70bcbf4d-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:27,388 - distributed.scheduler - INFO - Close client connection: Client-70bcbf4d-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:27,389 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34597'. Reason: nanny-close
2023-10-06 05:30:27,389 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41689'. Reason: nanny-close
2023-10-06 05:30:27,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,391 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44115. Reason: nanny-close
2023-10-06 05:30:27,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36755'. Reason: nanny-close
2023-10-06 05:30:27,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44739. Reason: nanny-close
2023-10-06 05:30:27,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34009'. Reason: nanny-close
2023-10-06 05:30:27,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44711. Reason: nanny-close
2023-10-06 05:30:27,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40015'. Reason: nanny-close
2023-10-06 05:30:27,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35779. Reason: nanny-close
2023-10-06 05:30:27,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40623'. Reason: nanny-close
2023-10-06 05:30:27,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,394 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47588; closing.
2023-10-06 05:30:27,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35199. Reason: nanny-close
2023-10-06 05:30:27,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45491'. Reason: nanny-close
2023-10-06 05:30:27,394 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.3944411')
2023-10-06 05:30:27,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41011. Reason: nanny-close
2023-10-06 05:30:27,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39087'. Reason: nanny-close
2023-10-06 05:30:27,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:27,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39407. Reason: nanny-close
2023-10-06 05:30:27,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35045. Reason: nanny-close
2023-10-06 05:30:27,396 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47560; closing.
2023-10-06 05:30:27,396 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,396 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47610; closing.
2023-10-06 05:30:27,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47544; closing.
2023-10-06 05:30:27,396 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35779', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.3975422')
2023-10-06 05:30:27,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44739', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.3979337')
2023-10-06 05:30:27,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:27,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.3983827')
2023-10-06 05:30:27,399 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,399 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47536; closing.
2023-10-06 05:30:27,399 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,399 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,399 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:27,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.4000666')
2023-10-06 05:30:27,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47522; closing.
2023-10-06 05:30:27,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47598; closing.
2023-10-06 05:30:27,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47574; closing.
2023-10-06 05:30:27,401 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.4011874')
2023-10-06 05:30:27,401 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.4017289')
2023-10-06 05:30:27,402 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35045', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570227.4021459')
2023-10-06 05:30:27,402 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:28,807 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:28,807 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:28,808 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:28,809 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:30:28,809 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-06 05:30:30,868 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:30,872 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42881 instead
  warnings.warn(
2023-10-06 05:30:30,876 - distributed.scheduler - INFO - State start
2023-10-06 05:30:30,897 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:30,898 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:30,899 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42881/status
2023-10-06 05:30:30,899 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:31,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33177'
2023-10-06 05:30:31,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45971'
2023-10-06 05:30:31,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46545'
2023-10-06 05:30:31,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39661'
2023-10-06 05:30:31,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36973'
2023-10-06 05:30:31,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44659'
2023-10-06 05:30:31,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35223'
2023-10-06 05:30:31,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33729'
2023-10-06 05:30:32,840 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:32,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:32,845 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:32,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:32,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:32,886 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:33,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:33,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:33,133 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:33,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:33,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:33,167 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:33,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:33,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:33,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:33,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:33,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:33,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:33,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:33,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:33,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:33,173 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:33,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:33,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:34,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44791
2023-10-06 05:30:34,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44791
2023-10-06 05:30:34,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44077
2023-10-06 05:30:34,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:34,758 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:34,758 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:34,758 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:34,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-31jwbpex
2023-10-06 05:30:34,759 - distributed.worker - INFO - Starting Worker plugin PreImport-0bcb5a00-cef7-4624-9794-aecbb4683fb9
2023-10-06 05:30:34,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-057bb509-fdd1-4d18-ab17-f1be6e214e02
2023-10-06 05:30:35,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-289e7fad-355e-4096-9246-c23f10a862ba
2023-10-06 05:30:35,084 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,111 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44791', status: init, memory: 0, processing: 0>
2023-10-06 05:30:35,125 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44791
2023-10-06 05:30:35,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45304
2023-10-06 05:30:35,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:35,127 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,127 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:35,460 - distributed.scheduler - INFO - Receive client connection: Client-760ac589-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:35,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45310
2023-10-06 05:30:35,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46069
2023-10-06 05:30:35,527 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46069
2023-10-06 05:30:35,527 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39295
2023-10-06 05:30:35,527 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,528 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,528 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,528 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,528 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s3z38owv
2023-10-06 05:30:35,529 - distributed.worker - INFO - Starting Worker plugin PreImport-85c469d8-3142-407b-b6fa-08529aca768f
2023-10-06 05:30:35,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f352354-a3cc-47ad-bf66-5f2f31deb8cf
2023-10-06 05:30:35,659 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a35fd199-8f8d-4255-aa6f-d23832ab249f
2023-10-06 05:30:35,660 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,692 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46069', status: init, memory: 0, processing: 0>
2023-10-06 05:30:35,693 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46069
2023-10-06 05:30:35,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45324
2023-10-06 05:30:35,694 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:35,695 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,695 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:35,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34411
2023-10-06 05:30:35,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34411
2023-10-06 05:30:35,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38643
2023-10-06 05:30:35,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,769 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,769 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,769 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,769 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s2v55gp0
2023-10-06 05:30:35,770 - distributed.worker - INFO - Starting Worker plugin PreImport-3a944308-9b61-44d6-970b-d79dee0b3cbb
2023-10-06 05:30:35,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9f5e9dd-e186-4604-af21-91660026491e
2023-10-06 05:30:35,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46143
2023-10-06 05:30:35,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46143
2023-10-06 05:30:35,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38305
2023-10-06 05:30:35,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,774 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,774 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g59vf8sk
2023-10-06 05:30:35,774 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45501
2023-10-06 05:30:35,775 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45501
2023-10-06 05:30:35,775 - distributed.worker - INFO - Starting Worker plugin PreImport-9f8ce4fd-18ab-4a13-84b2-ec651fb7d5a0
2023-10-06 05:30:35,775 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38687
2023-10-06 05:30:35,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b13465c3-6ab3-4e51-998a-748913199c10
2023-10-06 05:30:35,775 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,775 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,775 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,775 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,775 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tzl7_68r
2023-10-06 05:30:35,776 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5c9878f-6e14-46be-a9ab-f38bfee16f2e
2023-10-06 05:30:35,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8bf18ba-72c8-4401-9ce2-39df0855a30d
2023-10-06 05:30:35,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38751
2023-10-06 05:30:35,828 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38751
2023-10-06 05:30:35,828 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43981
2023-10-06 05:30:35,828 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,828 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,828 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,828 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,828 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-govc2rgt
2023-10-06 05:30:35,829 - distributed.worker - INFO - Starting Worker plugin PreImport-7cfca372-0517-4dd4-9b94-923526e812c7
2023-10-06 05:30:35,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b219059-1847-4178-8748-1a1103b73ef6
2023-10-06 05:30:35,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afcfc696-76d9-45e7-b591-90f324491ce1
2023-10-06 05:30:35,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35011
2023-10-06 05:30:35,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35011
2023-10-06 05:30:35,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43331
2023-10-06 05:30:35,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43331
2023-10-06 05:30:35,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37365
2023-10-06 05:30:35,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37377
2023-10-06 05:30:35,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:35,833 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,833 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:35,833 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,833 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:35,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:35,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vkexn8ln
2023-10-06 05:30:35,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-is9qp9z0
2023-10-06 05:30:35,834 - distributed.worker - INFO - Starting Worker plugin PreImport-3012fd05-177e-478f-80e3-7c28b3004c56
2023-10-06 05:30:35,834 - distributed.worker - INFO - Starting Worker plugin PreImport-2177f12d-5837-49b5-805d-8afa0a92202f
2023-10-06 05:30:35,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0d7f38c-77b2-4747-83d8-4083acd21014
2023-10-06 05:30:35,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6843071-44cf-47b1-9cda-2a839cb662cc
2023-10-06 05:30:36,151 - distributed.worker - INFO - Starting Worker plugin PreImport-85b76078-3695-442f-9a24-5dedccc33c7b
2023-10-06 05:30:36,152 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80050134-e0f7-41fe-b0a8-545e16e460c2
2023-10-06 05:30:36,167 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,183 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45501', status: init, memory: 0, processing: 0>
2023-10-06 05:30:36,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45501
2023-10-06 05:30:36,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45346
2023-10-06 05:30:36,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:36,185 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6a7b385-eb1b-40a3-9e7a-59b3db2d63c5
2023-10-06 05:30:36,186 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,186 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:36,186 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:36,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39ed4d9e-7707-449c-b49e-21d860c54568
2023-10-06 05:30:36,192 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46143', status: init, memory: 0, processing: 0>
2023-10-06 05:30:36,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb648c77-3572-4502-9f33-c3042afa174b
2023-10-06 05:30:36,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46143
2023-10-06 05:30:36,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45356
2023-10-06 05:30:36,199 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,200 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:36,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:36,201 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:36,210 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34411', status: init, memory: 0, processing: 0>
2023-10-06 05:30:36,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34411
2023-10-06 05:30:36,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45362
2023-10-06 05:30:36,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:36,213 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:36,213 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:36,222 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38751', status: init, memory: 0, processing: 0>
2023-10-06 05:30:36,223 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38751
2023-10-06 05:30:36,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45380
2023-10-06 05:30:36,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:36,224 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43331', status: init, memory: 0, processing: 0>
2023-10-06 05:30:36,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:36,225 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43331
2023-10-06 05:30:36,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45374
2023-10-06 05:30:36,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:36,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:36,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:36,228 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:36,233 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35011', status: init, memory: 0, processing: 0>
2023-10-06 05:30:36,233 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35011
2023-10-06 05:30:36,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45392
2023-10-06 05:30:36,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:36,235 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:36,235 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:36,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:36,308 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,310 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,310 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,310 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,320 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,320 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,320 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,321 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:30:36,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:36,331 - distributed.scheduler - INFO - Remove client Client-760ac589-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:36,331 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45310; closing.
2023-10-06 05:30:36,331 - distributed.scheduler - INFO - Remove client Client-760ac589-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:36,332 - distributed.scheduler - INFO - Close client connection: Client-760ac589-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:36,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33177'. Reason: nanny-close
2023-10-06 05:30:36,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45971'. Reason: nanny-close
2023-10-06 05:30:36,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45501. Reason: nanny-close
2023-10-06 05:30:36,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46545'. Reason: nanny-close
2023-10-06 05:30:36,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46069. Reason: nanny-close
2023-10-06 05:30:36,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39661'. Reason: nanny-close
2023-10-06 05:30:36,336 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,336 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38751. Reason: nanny-close
2023-10-06 05:30:36,336 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36973'. Reason: nanny-close
2023-10-06 05:30:36,336 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,336 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34411. Reason: nanny-close
2023-10-06 05:30:36,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44659'. Reason: nanny-close
2023-10-06 05:30:36,337 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35223'. Reason: nanny-close
2023-10-06 05:30:36,337 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46143. Reason: nanny-close
2023-10-06 05:30:36,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45346; closing.
2023-10-06 05:30:36,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,338 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44791. Reason: nanny-close
2023-10-06 05:30:36,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45501', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3383703')
2023-10-06 05:30:36,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33729'. Reason: nanny-close
2023-10-06 05:30:36,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:36,338 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35011. Reason: nanny-close
2023-10-06 05:30:36,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45380; closing.
2023-10-06 05:30:36,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43331. Reason: nanny-close
2023-10-06 05:30:36,339 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,340 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,340 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3404548')
2023-10-06 05:30:36,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,340 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45324; closing.
2023-10-06 05:30:36,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45362; closing.
2023-10-06 05:30:36,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46069', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3419807')
2023-10-06 05:30:36,342 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,342 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,342 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:36,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34411', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3424315')
2023-10-06 05:30:36,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45356; closing.
2023-10-06 05:30:36,343 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,343 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3434522')
2023-10-06 05:30:36,343 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45304; closing.
2023-10-06 05:30:36,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45392; closing.
2023-10-06 05:30:36,344 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3445807')
2023-10-06 05:30:36,344 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:36,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3449864')
2023-10-06 05:30:36,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45374; closing.
2023-10-06 05:30:36,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43331', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570236.3458014')
2023-10-06 05:30:36,346 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:37,851 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:37,851 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:37,852 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:37,853 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:30:37,853 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-06 05:30:40,023 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:40,028 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46399 instead
  warnings.warn(
2023-10-06 05:30:40,032 - distributed.scheduler - INFO - State start
2023-10-06 05:30:40,053 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:40,054 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:40,055 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46399/status
2023-10-06 05:30:40,055 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:40,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38467'
2023-10-06 05:30:40,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40929'
2023-10-06 05:30:40,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38501'
2023-10-06 05:30:40,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35541'
2023-10-06 05:30:40,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35447'
2023-10-06 05:30:40,210 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42643'
2023-10-06 05:30:40,218 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34191'
2023-10-06 05:30:40,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33427'
2023-10-06 05:30:40,274 - distributed.scheduler - INFO - Receive client connection: Client-7b649f02-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:40,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44876
2023-10-06 05:30:41,901 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:41,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:41,905 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:41,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:41,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:41,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:42,199 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:42,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:42,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:42,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:42,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:42,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:42,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:42,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:42,202 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:42,202 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:42,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:42,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:42,204 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:42,204 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:42,205 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:42,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:42,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:42,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:43,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36789
2023-10-06 05:30:43,261 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36789
2023-10-06 05:30:43,261 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40111
2023-10-06 05:30:43,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:43,261 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:43,261 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:43,261 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:43,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tjufts6s
2023-10-06 05:30:43,262 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7368b7e9-5f92-4d26-a892-aedd80f97524
2023-10-06 05:30:43,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afecd77c-29a1-489d-b21c-4df0d0599c2b
2023-10-06 05:30:43,918 - distributed.worker - INFO - Starting Worker plugin PreImport-9d4ddd4c-c071-4127-acea-1a40a578860b
2023-10-06 05:30:43,918 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:43,948 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36789', status: init, memory: 0, processing: 0>
2023-10-06 05:30:43,950 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36789
2023-10-06 05:30:43,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44886
2023-10-06 05:30:43,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:43,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:43,953 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:43,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:44,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39615
2023-10-06 05:30:44,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39615
2023-10-06 05:30:44,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36265
2023-10-06 05:30:44,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:44,873 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:44,873 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:44,874 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:44,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e8flbh1e
2023-10-06 05:30:44,874 - distributed.worker - INFO - Starting Worker plugin PreImport-3b8705f9-70c9-436e-af47-21fadf4b3b9d
2023-10-06 05:30:44,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8600d8b-e091-4705-a211-b251214a4a95
2023-10-06 05:30:45,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34215
2023-10-06 05:30:45,054 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34215
2023-10-06 05:30:45,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37615
2023-10-06 05:30:45,054 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,054 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,055 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:45,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:45,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l7_ccb9v
2023-10-06 05:30:45,055 - distributed.worker - INFO - Starting Worker plugin PreImport-1aa6081a-c647-4c6c-976b-6b3bd29e0524
2023-10-06 05:30:45,055 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47f81c5d-5cfe-43f3-a8ce-ed5e982cd35f
2023-10-06 05:30:45,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63df107e-a9d6-40a5-aeab-15b25ac56d4f
2023-10-06 05:30:45,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36039
2023-10-06 05:30:45,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36039
2023-10-06 05:30:45,056 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40491
2023-10-06 05:30:45,056 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,056 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,054 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43003
2023-10-06 05:30:45,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43003
2023-10-06 05:30:45,056 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:45,056 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:45,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cja1aanr
2023-10-06 05:30:45,057 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40801
2023-10-06 05:30:45,057 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,057 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,057 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:45,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:45,057 - distributed.worker - INFO - Starting Worker plugin PreImport-30fecd2c-da17-43fd-b904-b9f57f8b04a3
2023-10-06 05:30:45,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cgu_t_un
2023-10-06 05:30:45,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02302099-3b5b-4106-af6e-997e15c69997
2023-10-06 05:30:45,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42485
2023-10-06 05:30:45,057 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42485
2023-10-06 05:30:45,057 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44635
2023-10-06 05:30:45,057 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,057 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,057 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:45,058 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:45,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uke_v4c4
2023-10-06 05:30:45,058 - distributed.worker - INFO - Starting Worker plugin PreImport-42f3335b-99d8-495e-8acf-3a23986b2974
2023-10-06 05:30:45,058 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2af227fa-70f1-4c3a-a798-aff2aa4c9aa1
2023-10-06 05:30:45,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08ee7127-05d7-41cd-aa11-d29a6d56ebb6
2023-10-06 05:30:45,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09ccb8f0-af95-47f5-982f-dd9066f921be
2023-10-06 05:30:45,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41871
2023-10-06 05:30:45,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41871
2023-10-06 05:30:45,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32835
2023-10-06 05:30:45,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,060 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,060 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:45,060 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:45,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cs275jyl
2023-10-06 05:30:45,061 - distributed.worker - INFO - Starting Worker plugin PreImport-5433f630-c98b-44ea-97a4-1c35edd3d2d9
2023-10-06 05:30:45,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26ca5794-7343-46af-97b1-e5c78b37a840
2023-10-06 05:30:45,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39241
2023-10-06 05:30:45,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39241
2023-10-06 05:30:45,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37895
2023-10-06 05:30:45,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,062 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,062 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:45,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:30:45,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ph0y543a
2023-10-06 05:30:45,063 - distributed.worker - INFO - Starting Worker plugin PreImport-1369d493-24ce-47f8-a4e4-3d1ffcbfcbdf
2023-10-06 05:30:45,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-405c196a-8245-41c8-9e54-7b86de30a844
2023-10-06 05:30:45,207 - distributed.worker - INFO - Starting Worker plugin PreImport-69a93d08-ea26-4baa-b43e-87e78a704062
2023-10-06 05:30:45,208 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,231 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-365570bf-85f6-4e13-a9ce-1b951af3667b
2023-10-06 05:30:45,231 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,235 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43003', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,236 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43003
2023-10-06 05:30:45,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44902
2023-10-06 05:30:45,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,238 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,238 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44ef3362-eff2-482c-96b3-d1141abb2d33
2023-10-06 05:30:45,241 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,252 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a0e07a5-dd82-4130-8e46-433ea3e4fa90
2023-10-06 05:30:45,252 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34215', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34215
2023-10-06 05:30:45,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44912
2023-10-06 05:30:45,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,255 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,255 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90e2d7a4-40e1-4807-a46d-000694ceb616
2023-10-06 05:30:45,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6c3dc98-e16c-4915-a86b-d40924c07ce3
2023-10-06 05:30:45,260 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39615', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39615
2023-10-06 05:30:45,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44916
2023-10-06 05:30:45,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,264 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,264 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36039', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,270 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36039
2023-10-06 05:30:45,270 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44918
2023-10-06 05:30:45,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,272 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,272 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42485', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,292 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42485
2023-10-06 05:30:45,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44934
2023-10-06 05:30:45,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39241', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,295 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39241
2023-10-06 05:30:45,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44950
2023-10-06 05:30:45,295 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,295 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,296 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,298 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41871', status: init, memory: 0, processing: 0>
2023-10-06 05:30:45,298 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,298 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41871
2023-10-06 05:30:45,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44944
2023-10-06 05:30:45,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:45,301 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:45,301 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:45,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:45,382 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,382 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:30:45,388 - distributed.scheduler - INFO - Remove client Client-7b649f02-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:45,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44876; closing.
2023-10-06 05:30:45,389 - distributed.scheduler - INFO - Remove client Client-7b649f02-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:45,389 - distributed.scheduler - INFO - Close client connection: Client-7b649f02-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:45,390 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38467'. Reason: nanny-close
2023-10-06 05:30:45,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40929'. Reason: nanny-close
2023-10-06 05:30:45,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39241. Reason: nanny-close
2023-10-06 05:30:45,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38501'. Reason: nanny-close
2023-10-06 05:30:45,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35541'. Reason: nanny-close
2023-10-06 05:30:45,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43003. Reason: nanny-close
2023-10-06 05:30:45,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36039. Reason: nanny-close
2023-10-06 05:30:45,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35447'. Reason: nanny-close
2023-10-06 05:30:45,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34215. Reason: nanny-close
2023-10-06 05:30:45,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42643'. Reason: nanny-close
2023-10-06 05:30:45,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,394 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44950; closing.
2023-10-06 05:30:45,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42485. Reason: nanny-close
2023-10-06 05:30:45,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34191'. Reason: nanny-close
2023-10-06 05:30:45,395 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.3950386')
2023-10-06 05:30:45,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41871. Reason: nanny-close
2023-10-06 05:30:45,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,395 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33427'. Reason: nanny-close
2023-10-06 05:30:45,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:45,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39615. Reason: nanny-close
2023-10-06 05:30:45,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,396 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36789. Reason: nanny-close
2023-10-06 05:30:45,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44912; closing.
2023-10-06 05:30:45,397 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44902; closing.
2023-10-06 05:30:45,397 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44918; closing.
2023-10-06 05:30:45,397 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34215', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.3983362')
2023-10-06 05:30:45,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.398903')
2023-10-06 05:30:45,399 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.399369')
2023-10-06 05:30:45,400 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,400 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44934; closing.
2023-10-06 05:30:45,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:45,400 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,401 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.401152')
2023-10-06 05:30:45,401 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44944; closing.
2023-10-06 05:30:45,401 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44916; closing.
2023-10-06 05:30:45,402 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.402325')
2023-10-06 05:30:45,402 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.4027643')
2023-10-06 05:30:45,403 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:45,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44886; closing.
2023-10-06 05:30:45,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36789', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570245.4037693')
2023-10-06 05:30:45,403 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:45,404 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44886>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-06 05:30:46,908 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:46,908 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:46,909 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:46,910 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:30:46,911 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-06 05:30:49,222 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:49,226 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33165 instead
  warnings.warn(
2023-10-06 05:30:49,230 - distributed.scheduler - INFO - State start
2023-10-06 05:30:49,251 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:49,252 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:49,253 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33165/status
2023-10-06 05:30:49,253 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:49,257 - distributed.scheduler - INFO - Receive client connection: Client-80de3cbd-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:49,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45046
2023-10-06 05:30:49,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43955'
2023-10-06 05:30:50,897 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:50,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:51,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:30:52,358 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39633
2023-10-06 05:30:52,358 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39633
2023-10-06 05:30:52,359 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-06 05:30:52,359 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:30:52,359 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:52,359 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:30:52,359 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-06 05:30:52,359 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mt0_hkch
2023-10-06 05:30:52,359 - distributed.worker - INFO - Starting Worker plugin PreImport-99963862-1d67-4492-872c-e7c0117855f4
2023-10-06 05:30:52,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b372f974-0459-47fd-a8cb-47c33b9bb672
2023-10-06 05:30:52,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbf7ce3f-14c0-4b2d-9ad2-29773a4405a9
2023-10-06 05:30:52,360 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:52,382 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39633', status: init, memory: 0, processing: 0>
2023-10-06 05:30:52,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39633
2023-10-06 05:30:52,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47664
2023-10-06 05:30:52,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:30:52,385 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:30:52,385 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:30:52,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:30:52,427 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:30:52,429 - distributed.scheduler - INFO - Remove client Client-80de3cbd-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:52,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45046; closing.
2023-10-06 05:30:52,430 - distributed.scheduler - INFO - Remove client Client-80de3cbd-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:52,430 - distributed.scheduler - INFO - Close client connection: Client-80de3cbd-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:52,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43955'. Reason: nanny-close
2023-10-06 05:30:52,431 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:30:52,433 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39633. Reason: nanny-close
2023-10-06 05:30:52,434 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:30:52,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47664; closing.
2023-10-06 05:30:52,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570252.4353447')
2023-10-06 05:30:52,435 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:30:52,436 - distributed.nanny - INFO - Worker closed
2023-10-06 05:30:53,397 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:30:53,397 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:30:53,398 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:30:53,399 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:30:53,399 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-06 05:30:57,135 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:57,139 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40361 instead
  warnings.warn(
2023-10-06 05:30:57,142 - distributed.scheduler - INFO - State start
2023-10-06 05:30:57,163 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:30:57,164 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:30:57,164 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40361/status
2023-10-06 05:30:57,165 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:30:57,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37699'
2023-10-06 05:30:58,047 - distributed.scheduler - INFO - Receive client connection: Client-85acb464-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:30:58,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47740
2023-10-06 05:30:58,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:30:58,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:30:59,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:00,317 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46815
2023-10-06 05:31:00,318 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46815
2023-10-06 05:31:00,318 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38079
2023-10-06 05:31:00,318 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:00,318 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:00,318 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:00,319 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-06 05:31:00,319 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6bz2io_t
2023-10-06 05:31:00,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78dfa4c1-e73d-4c55-9492-7928e6c1bd0b
2023-10-06 05:31:00,320 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8befa71d-5d84-456e-ad62-baec35c20edc
2023-10-06 05:31:00,320 - distributed.worker - INFO - Starting Worker plugin PreImport-ff4426cf-618e-4eb6-ac37-4a7201b44ea9
2023-10-06 05:31:00,322 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:00,344 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46815', status: init, memory: 0, processing: 0>
2023-10-06 05:31:00,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46815
2023-10-06 05:31:00,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46092
2023-10-06 05:31:00,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:00,347 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:00,347 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:00,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:00,394 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:00,397 - distributed.scheduler - INFO - Remove client Client-85acb464-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:00,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47740; closing.
2023-10-06 05:31:00,397 - distributed.scheduler - INFO - Remove client Client-85acb464-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:00,398 - distributed.scheduler - INFO - Close client connection: Client-85acb464-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:00,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37699'. Reason: nanny-close
2023-10-06 05:31:00,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:00,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46815. Reason: nanny-close
2023-10-06 05:31:00,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:00,402 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46092; closing.
2023-10-06 05:31:00,402 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570260.4024718')
2023-10-06 05:31:00,402 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:31:00,403 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:01,465 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:31:01,466 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:31:01,466 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:31:01,467 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:31:01,468 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-06 05:31:03,503 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:03,507 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39879 instead
  warnings.warn(
2023-10-06 05:31:03,511 - distributed.scheduler - INFO - State start
2023-10-06 05:31:03,531 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:03,532 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:31:03,533 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39879/status
2023-10-06 05:31:03,533 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:31:07,182 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:46102'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46102>: Stream is closed
2023-10-06 05:31:07,541 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:31:07,541 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:31:07,542 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:31:07,543 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:31:07,543 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-06 05:31:09,628 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:09,632 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44213 instead
  warnings.warn(
2023-10-06 05:31:09,635 - distributed.scheduler - INFO - State start
2023-10-06 05:31:09,655 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:09,656 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-06 05:31:09,657 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44213/status
2023-10-06 05:31:09,657 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:31:09,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37781'
2023-10-06 05:31:09,897 - distributed.scheduler - INFO - Receive client connection: Client-8d1a9a94-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:09,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47360
2023-10-06 05:31:11,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:11,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:11,454 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:12,309 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34877
2023-10-06 05:31:12,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34877
2023-10-06 05:31:12,309 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44825
2023-10-06 05:31:12,309 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-06 05:31:12,309 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:12,310 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:12,310 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-06 05:31:12,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-7hxgswpu
2023-10-06 05:31:12,310 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fad55a9a-acbc-41d0-9751-70c072758ea0
2023-10-06 05:31:12,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b11b6d44-bbfc-4312-84e6-5e9650f79fb0
2023-10-06 05:31:12,310 - distributed.worker - INFO - Starting Worker plugin PreImport-ce32d8b2-032e-4d41-8ace-cd800dc1d77b
2023-10-06 05:31:12,311 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:12,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34877', status: init, memory: 0, processing: 0>
2023-10-06 05:31:12,334 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34877
2023-10-06 05:31:12,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36862
2023-10-06 05:31:12,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:12,335 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-06 05:31:12,335 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:12,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-06 05:31:12,348 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:12,350 - distributed.scheduler - INFO - Remove client Client-8d1a9a94-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:12,350 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47360; closing.
2023-10-06 05:31:12,351 - distributed.scheduler - INFO - Remove client Client-8d1a9a94-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:12,351 - distributed.scheduler - INFO - Close client connection: Client-8d1a9a94-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:12,352 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37781'. Reason: nanny-close
2023-10-06 05:31:12,387 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:12,389 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34877. Reason: nanny-close
2023-10-06 05:31:12,390 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-06 05:31:12,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36862; closing.
2023-10-06 05:31:12,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570272.391191')
2023-10-06 05:31:12,391 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:31:12,392 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:13,318 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:31:13,318 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:31:13,319 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:31:13,320 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-06 05:31:13,320 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-06 05:31:15,276 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:15,280 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40309 instead
  warnings.warn(
2023-10-06 05:31:15,284 - distributed.scheduler - INFO - State start
2023-10-06 05:31:15,306 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:15,307 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:31:15,308 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40309/status
2023-10-06 05:31:15,308 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:31:15,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39123'
2023-10-06 05:31:15,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45139'
2023-10-06 05:31:15,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46375'
2023-10-06 05:31:15,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37631'
2023-10-06 05:31:15,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43165'
2023-10-06 05:31:15,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40913'
2023-10-06 05:31:15,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45361'
2023-10-06 05:31:15,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39313'
2023-10-06 05:31:16,243 - distributed.scheduler - INFO - Receive client connection: Client-9079761a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:16,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52192
2023-10-06 05:31:17,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,414 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,414 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,416 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,419 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,422 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:17,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:17,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:17,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:20,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33383
2023-10-06 05:31:20,246 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33383
2023-10-06 05:31:20,246 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39729
2023-10-06 05:31:20,246 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,246 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,246 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,246 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,246 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sel0ctk2
2023-10-06 05:31:20,246 - distributed.worker - INFO - Starting Worker plugin PreImport-c5715e62-617c-4d1b-a2be-27029685d30a
2023-10-06 05:31:20,247 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1835466-f578-4362-9c4e-4999d47b2024
2023-10-06 05:31:20,261 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44349
2023-10-06 05:31:20,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44349
2023-10-06 05:31:20,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45147
2023-10-06 05:31:20,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,263 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,263 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-akjx5jmh
2023-10-06 05:31:20,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46833
2023-10-06 05:31:20,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46833
2023-10-06 05:31:20,264 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41245
2023-10-06 05:31:20,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,265 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,265 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2d11472-16c1-4af7-ac1f-8b1088c24c98
2023-10-06 05:31:20,265 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,265 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,265 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5fc1ddee-f7cd-4ed8-97be-7470d3ae323c
2023-10-06 05:31:20,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ce0c_la_
2023-10-06 05:31:20,265 - distributed.worker - INFO - Starting Worker plugin PreImport-b9765431-ac2c-44d6-bafe-aa2f2ec9ea32
2023-10-06 05:31:20,265 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2cca869-bc40-400e-8e85-de4db72db6cc
2023-10-06 05:31:20,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34487
2023-10-06 05:31:20,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34487
2023-10-06 05:31:20,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34351
2023-10-06 05:31:20,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,267 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,267 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qboc84ms
2023-10-06 05:31:20,268 - distributed.worker - INFO - Starting Worker plugin PreImport-925c260b-b57e-494d-8397-a25df8ea92db
2023-10-06 05:31:20,268 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdabb189-f3fe-485e-a5f0-eddc6dc27bb1
2023-10-06 05:31:20,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29d98c61-3b12-40d9-8f0e-a9eb9054c8e2
2023-10-06 05:31:20,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44101
2023-10-06 05:31:20,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44101
2023-10-06 05:31:20,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34925
2023-10-06 05:31:20,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,273 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,273 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3vgio8m4
2023-10-06 05:31:20,274 - distributed.worker - INFO - Starting Worker plugin PreImport-cc705d25-8337-48ee-9934-61314389d9cb
2023-10-06 05:31:20,274 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8fe87712-1622-4c25-a9f7-32e485a35c76
2023-10-06 05:31:20,276 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46147
2023-10-06 05:31:20,276 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46147
2023-10-06 05:31:20,276 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45701
2023-10-06 05:31:20,276 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,277 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,277 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,277 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,277 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sovny2ua
2023-10-06 05:31:20,277 - distributed.worker - INFO - Starting Worker plugin PreImport-0bcae052-1854-4c25-b1c9-0ea2be519de3
2023-10-06 05:31:20,277 - distributed.worker - INFO - Starting Worker plugin RMMSetup-884d9c8b-5d7f-42c0-a25b-c55a13f56876
2023-10-06 05:31:20,280 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41079
2023-10-06 05:31:20,281 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41079
2023-10-06 05:31:20,281 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44067
2023-10-06 05:31:20,281 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,281 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,282 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,282 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w211sxn3
2023-10-06 05:31:20,282 - distributed.worker - INFO - Starting Worker plugin PreImport-4af276a4-fda1-40cf-b526-f3562398af48
2023-10-06 05:31:20,282 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81f6be49-3b4c-474f-8764-fde15460730a
2023-10-06 05:31:20,283 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40289
2023-10-06 05:31:20,283 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40289
2023-10-06 05:31:20,283 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36757
2023-10-06 05:31:20,284 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,284 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,284 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:20,284 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-06 05:31:20,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9luqwrn3
2023-10-06 05:31:20,284 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37ac4014-9686-4129-9b38-afd1e10a46d6
2023-10-06 05:31:20,284 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0dd316d-3eed-4fc4-bbcb-f5f0b975cd7e
2023-10-06 05:31:20,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64219236-1575-4f43-ad36-fcdde3b5ad9f
2023-10-06 05:31:20,411 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8cea2b8d-f0ec-43e9-a955-d654e3b7b15e
2023-10-06 05:31:20,424 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,429 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00bf94bb-d6fc-4aad-87ae-9658b8458ace
2023-10-06 05:31:20,429 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6257f805-9240-4c9a-9b25-648769e79c6f
2023-10-06 05:31:20,429 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2cfd8320-2c9b-411d-b820-d10e5be44cc5
2023-10-06 05:31:20,429 - distributed.worker - INFO - Starting Worker plugin PreImport-57f7e2b0-be33-469a-953e-463826cc690c
2023-10-06 05:31:20,429 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,429 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,429 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,430 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,437 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33383', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,438 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33383
2023-10-06 05:31:20,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59762
2023-10-06 05:31:20,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,440 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,449 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,454 - distributed.worker - INFO - Starting Worker plugin PreImport-4b30e8b6-9d66-427e-bf26-dc4da30aa54d
2023-10-06 05:31:20,454 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,456 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40289', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,456 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40289
2023-10-06 05:31:20,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59778
2023-10-06 05:31:20,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46147', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,457 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46147
2023-10-06 05:31:20,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59788
2023-10-06 05:31:20,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,459 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41079', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,459 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,459 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41079
2023-10-06 05:31:20,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59776
2023-10-06 05:31:20,460 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,460 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,461 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,461 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46833', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,463 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46833
2023-10-06 05:31:20,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59768
2023-10-06 05:31:20,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,466 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,466 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,469 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44101', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,470 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44101
2023-10-06 05:31:20,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59804
2023-10-06 05:31:20,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,472 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,472 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,488 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34487', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,489 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34487
2023-10-06 05:31:20,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59820
2023-10-06 05:31:20,491 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,492 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,492 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,493 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44349', status: init, memory: 0, processing: 0>
2023-10-06 05:31:20,494 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44349
2023-10-06 05:31:20,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59828
2023-10-06 05:31:20,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:20,496 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:20,496 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:20,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:20,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,602 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,602 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,602 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,602 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,603 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-06 05:31:20,614 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,614 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,614 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,614 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,614 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,615 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,615 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,615 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:20,618 - distributed.scheduler - INFO - Remove client Client-9079761a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:20,618 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52192; closing.
2023-10-06 05:31:20,619 - distributed.scheduler - INFO - Remove client Client-9079761a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:20,619 - distributed.scheduler - INFO - Close client connection: Client-9079761a-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:20,620 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39123'. Reason: nanny-close
2023-10-06 05:31:20,620 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,621 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45139'. Reason: nanny-close
2023-10-06 05:31:20,622 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46147. Reason: nanny-close
2023-10-06 05:31:20,622 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,622 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46375'. Reason: nanny-close
2023-10-06 05:31:20,622 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,622 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40289. Reason: nanny-close
2023-10-06 05:31:20,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37631'. Reason: nanny-close
2023-10-06 05:31:20,623 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,623 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46833. Reason: nanny-close
2023-10-06 05:31:20,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43165'. Reason: nanny-close
2023-10-06 05:31:20,623 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,623 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,623 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59788; closing.
2023-10-06 05:31:20,624 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34487. Reason: nanny-close
2023-10-06 05:31:20,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40913'. Reason: nanny-close
2023-10-06 05:31:20,624 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6241941')
2023-10-06 05:31:20,624 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,624 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,624 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33383. Reason: nanny-close
2023-10-06 05:31:20,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45361'. Reason: nanny-close
2023-10-06 05:31:20,624 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,625 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41079. Reason: nanny-close
2023-10-06 05:31:20,625 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39313'. Reason: nanny-close
2023-10-06 05:31:20,625 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:20,625 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59778; closing.
2023-10-06 05:31:20,625 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,625 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,625 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44101. Reason: nanny-close
2023-10-06 05:31:20,626 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6264548')
2023-10-06 05:31:20,626 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,626 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,626 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59768; closing.
2023-10-06 05:31:20,626 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,626 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44349. Reason: nanny-close
2023-10-06 05:31:20,627 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.627303')
2023-10-06 05:31:20,627 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59820; closing.
2023-10-06 05:31:20,628 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,628 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,628 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34487', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6284254')
2023-10-06 05:31:20,628 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,628 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,628 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,628 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59762; closing.
2023-10-06 05:31:20,628 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59776; closing.
2023-10-06 05:31:20,629 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33383', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6295044')
2023-10-06 05:31:20,629 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:20,629 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41079', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6299055')
2023-10-06 05:31:20,630 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59804; closing.
2023-10-06 05:31:20,630 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:20,630 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6308277')
2023-10-06 05:31:20,631 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59828; closing.
2023-10-06 05:31:20,631 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570280.6315677')
2023-10-06 05:31:20,631 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:31:20,631 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:22,187 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:31:22,187 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:31:22,188 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:31:22,189 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:31:22,189 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-06 05:31:24,149 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:24,153 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43489 instead
  warnings.warn(
2023-10-06 05:31:24,157 - distributed.scheduler - INFO - State start
2023-10-06 05:31:24,177 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:24,178 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:31:24,179 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43489/status
2023-10-06 05:31:24,179 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:31:24,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44893'
2023-10-06 05:31:24,486 - distributed.scheduler - INFO - Receive client connection: Client-95be63cb-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:24,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59910
2023-10-06 05:31:25,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:25,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:25,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:26,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43775
2023-10-06 05:31:26,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43775
2023-10-06 05:31:26,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38831
2023-10-06 05:31:26,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:26,764 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:26,764 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:26,765 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-06 05:31:26,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9840jvfy
2023-10-06 05:31:26,765 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ba7419b-d89d-4add-93a8-00628b01142c
2023-10-06 05:31:26,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73b37115-88f4-4d1d-863d-0a7302867f80
2023-10-06 05:31:26,859 - distributed.worker - INFO - Starting Worker plugin PreImport-bbd8f88d-11a5-42f2-bd71-840ca5e81d8c
2023-10-06 05:31:26,860 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:26,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43775', status: init, memory: 0, processing: 0>
2023-10-06 05:31:26,883 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43775
2023-10-06 05:31:26,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59922
2023-10-06 05:31:26,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:26,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:26,885 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:26,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:26,938 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:31:26,941 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:26,942 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:26,945 - distributed.scheduler - INFO - Remove client Client-95be63cb-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:26,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59910; closing.
2023-10-06 05:31:26,945 - distributed.scheduler - INFO - Remove client Client-95be63cb-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:26,946 - distributed.scheduler - INFO - Close client connection: Client-95be63cb-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:26,946 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44893'. Reason: nanny-close
2023-10-06 05:31:26,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:26,948 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43775. Reason: nanny-close
2023-10-06 05:31:26,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59922; closing.
2023-10-06 05:31:26,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:26,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43775', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570286.9505439')
2023-10-06 05:31:26,950 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:31:26,951 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:27,812 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:31:27,813 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:31:27,813 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:31:27,814 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:31:27,815 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-06 05:31:29,800 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:29,805 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37105 instead
  warnings.warn(
2023-10-06 05:31:29,809 - distributed.scheduler - INFO - State start
2023-10-06 05:31:29,869 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-06 05:31:29,870 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-06 05:31:29,871 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37105/status
2023-10-06 05:31:29,871 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-06 05:31:29,893 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44095'
2023-10-06 05:31:31,630 - distributed.scheduler - INFO - Receive client connection: Client-991ae368-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:31,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42604
2023-10-06 05:31:31,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-06 05:31:31,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-06 05:31:31,685 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-06 05:31:32,594 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45659
2023-10-06 05:31:32,594 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45659
2023-10-06 05:31:32,594 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41395
2023-10-06 05:31:32,594 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-06 05:31:32,594 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:32,595 - distributed.worker - INFO -               Threads:                          1
2023-10-06 05:31:32,595 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-06 05:31:32,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ggz9jyfs
2023-10-06 05:31:32,595 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c6258ac-f2ad-4773-a800-36dd4f5a0be5
2023-10-06 05:31:32,595 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97bea1d0-7bbc-46d3-af9a-e2376fb578ea
2023-10-06 05:31:32,742 - distributed.worker - INFO - Starting Worker plugin PreImport-52a3b424-cea1-44e8-a6a5-bb91620e6d01
2023-10-06 05:31:32,742 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:32,769 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45659', status: init, memory: 0, processing: 0>
2023-10-06 05:31:32,770 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45659
2023-10-06 05:31:32,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42626
2023-10-06 05:31:32,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-06 05:31:32,773 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-06 05:31:32,773 - distributed.worker - INFO - -------------------------------------------------
2023-10-06 05:31:32,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-06 05:31:32,867 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-06 05:31:32,872 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-06 05:31:32,875 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:32,876 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-06 05:31:32,878 - distributed.scheduler - INFO - Remove client Client-991ae368-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:32,879 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42604; closing.
2023-10-06 05:31:32,879 - distributed.scheduler - INFO - Remove client Client-991ae368-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:32,879 - distributed.scheduler - INFO - Close client connection: Client-991ae368-6409-11ee-b817-d8c49764f6bb
2023-10-06 05:31:32,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44095'. Reason: nanny-close
2023-10-06 05:31:32,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-06 05:31:32,882 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45659. Reason: nanny-close
2023-10-06 05:31:32,883 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42626; closing.
2023-10-06 05:31:32,883 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-06 05:31:32,884 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696570292.8841827')
2023-10-06 05:31:32,884 - distributed.scheduler - INFO - Lost all workers
2023-10-06 05:31:32,885 - distributed.nanny - INFO - Worker closed
2023-10-06 05:31:33,896 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-06 05:31:33,896 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-06 05:31:33,897 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-06 05:31:33,898 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-06 05:31:33,898 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35275 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45027 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40897 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33751 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36203 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44259 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34041 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43985 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34195 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] 2023-10-06 05:35:33,084 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-06 05:35:33,108 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:45089', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42815 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35549 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41693 instead
  warnings.warn(
[1696570578.931553] [dgx13:72593:0]            sock.c:470  UCX  ERROR bind(fd=163 addr=0.0.0.0:59422) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41085 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44627 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39403 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34723 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35017 instead
  warnings.warn(
[1696570902.095719] [dgx13:77921:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:39496) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38531 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43681 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39143 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38439 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37579 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40145 instead
  warnings.warn(
2023-10-06 05:43:13,841 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-06 05:43:13,843 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45705 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41537 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45507 instead
  warnings.warn(
[1696571030.640507] [dgx13:78984:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:49737) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40993 instead
  warnings.warn(
2023-10-06 05:44:18,987 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-06 05:44:19,062 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,064 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,079 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,211 - distributed.core - ERROR - [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-06 05:44:19,237 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:48587'.
2023-10-06 05:44:19,238 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:48587'. Shutting down.
2023-10-06 05:44:19,299 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,312 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,320 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe31a8e3340>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-06 05:44:19,322 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,344 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:43484'.
2023-10-06 05:44:19,344 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:43484'. Shutting down.
2023-10-06 05:44:19,346 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:49607'.
2023-10-06 05:44:19,346 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:58818'.
2023-10-06 05:44:19,346 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:49607'. Shutting down.
2023-10-06 05:44:19,347 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:58818'. Shutting down.
2023-10-06 05:44:19,453 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7efd0c93e340>>, <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,467 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f8fc2320370>>, <Task finished name='Task-22' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-22' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:19,469 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f9ee2306370>>, <Task finished name='Task-22' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-22' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-06 05:44:21,323 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-06 05:44:21,457 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-06 05:44:21,471 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-06 05:44:21,472 - distributed.nanny - ERROR - Worker process died unexpectedly
