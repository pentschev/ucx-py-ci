============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-03 05:38:23,505 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:23,510 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32933 instead
  warnings.warn(
2023-06-03 05:38:23,513 - distributed.scheduler - INFO - State start
2023-06-03 05:38:23,536 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:23,536 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-03 05:38:23,537 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32933/status
2023-06-03 05:38:23,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42083'
2023-06-03 05:38:23,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42603'
2023-06-03 05:38:23,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42955'
2023-06-03 05:38:23,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46039'
2023-06-03 05:38:24,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:24,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:24,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:25,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:25,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:25,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:25,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:25,164 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:25,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:25,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:25,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:25,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-03 05:38:25,205 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34319
2023-06-03 05:38:25,205 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34319
2023-06-03 05:38:25,205 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34865
2023-06-03 05:38:25,205 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:38:25,205 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:25,205 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:38:25,205 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:38:25,206 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xese1p48
2023-06-03 05:38:25,206 - distributed.worker - INFO - Starting Worker plugin PreImport-ca957be2-bccd-419f-8609-eae246750a26
2023-06-03 05:38:25,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebb1a022-80a4-498e-9487-446439ddde50
2023-06-03 05:38:25,206 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b36361e-612e-468e-a00f-00c8390a50ae
2023-06-03 05:38:25,206 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:25,221 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34319', status: init, memory: 0, processing: 0>
2023-06-03 05:38:25,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34319
2023-06-03 05:38:25,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36204
2023-06-03 05:38:25,235 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:38:25,235 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:25,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:38:25,972 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44801
2023-06-03 05:38:25,972 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44801
2023-06-03 05:38:25,972 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36673
2023-06-03 05:38:25,972 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:38:25,972 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:25,972 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:38:25,972 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:38:25,972 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y328ni1g
2023-06-03 05:38:25,973 - distributed.worker - INFO - Starting Worker plugin PreImport-fb4ecba8-d3e3-4938-9241-70d8f3122801
2023-06-03 05:38:25,973 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e940801-eb2f-4de8-baae-644d9fb02cc8
2023-06-03 05:38:25,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61cd9be2-baeb-4bde-911a-974c396adcd4
2023-06-03 05:38:25,973 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:25,992 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44801', status: init, memory: 0, processing: 0>
2023-06-03 05:38:25,993 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44801
2023-06-03 05:38:25,993 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36210
2023-06-03 05:38:25,994 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:38:25,994 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:25,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:38:26,240 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35075
2023-06-03 05:38:26,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35075
2023-06-03 05:38:26,241 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40375
2023-06-03 05:38:26,241 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:38:26,241 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:26,241 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:38:26,241 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:38:26,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-14rcb1yi
2023-06-03 05:38:26,241 - distributed.worker - INFO - Starting Worker plugin PreImport-e3bf8ea1-6651-49fb-9e63-2890a03048ad
2023-06-03 05:38:26,242 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4294f645-f576-479b-b3fa-9b17b3528134
2023-06-03 05:38:26,242 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35e2f323-2752-4122-b73a-150880177f7f
2023-06-03 05:38:26,242 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:26,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43783
2023-06-03 05:38:26,248 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43783
2023-06-03 05:38:26,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44155
2023-06-03 05:38:26,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:38:26,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:26,248 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:38:26,248 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:38:26,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l72jft4p
2023-06-03 05:38:26,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-002cddb8-778d-48b5-9ddb-cffdb2ea0dba
2023-06-03 05:38:26,249 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d24101-4a72-4ded-ab71-c7418557138c
2023-06-03 05:38:26,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a44f8e97-47d1-47fb-9526-84f3ef0c1b83
2023-06-03 05:38:26,250 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:26,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35075', status: init, memory: 0, processing: 0>
2023-06-03 05:38:26,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35075
2023-06-03 05:38:26,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36230
2023-06-03 05:38:26,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:38:26,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:26,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:38:26,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43783', status: init, memory: 0, processing: 0>
2023-06-03 05:38:26,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43783
2023-06-03 05:38:26,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36244
2023-06-03 05:38:26,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:38:26,273 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:26,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:38:27,751 - distributed.scheduler - INFO - Receive client connection: Client-da2300bd-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:27,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36270
2023-06-03 05:38:27,760 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:38:27,760 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:38:27,760 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:38:27,760 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:38:27,764 - distributed.scheduler - INFO - Remove client Client-da2300bd-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:27,765 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36270; closing.
2023-06-03 05:38:27,765 - distributed.scheduler - INFO - Remove client Client-da2300bd-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:27,765 - distributed.scheduler - INFO - Close client connection: Client-da2300bd-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:27,766 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42083'. Reason: nanny-close
2023-06-03 05:38:27,766 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:27,767 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42603'. Reason: nanny-close
2023-06-03 05:38:27,767 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:27,768 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43783. Reason: nanny-close
2023-06-03 05:38:27,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46039'. Reason: nanny-close
2023-06-03 05:38:27,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:27,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42955'. Reason: nanny-close
2023-06-03 05:38:27,768 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44801. Reason: nanny-close
2023-06-03 05:38:27,769 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:27,769 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35075. Reason: nanny-close
2023-06-03 05:38:27,769 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36244; closing.
2023-06-03 05:38:27,769 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:38:27,770 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43783', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:27,770 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43783
2023-06-03 05:38:27,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34319. Reason: nanny-close
2023-06-03 05:38:27,771 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:38:27,771 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:27,771 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36230; closing.
2023-06-03 05:38:27,771 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35075', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:27,772 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35075
2023-06-03 05:38:27,772 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:27,772 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36204; closing.
2023-06-03 05:38:27,772 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43783
2023-06-03 05:38:27,772 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34319', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:27,772 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34319
2023-06-03 05:38:27,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:38:27,774 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36210; closing.
2023-06-03 05:38:27,775 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44801', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:27,775 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44801
2023-06-03 05:38:27,775 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:38:27,775 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:27,775 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43783
2023-06-03 05:38:27,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:38:27,777 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:28,883 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:38:28,883 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:38:28,883 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:38:28,884 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-03 05:38:28,884 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-03 05:38:30,631 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:30,635 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39261 instead
  warnings.warn(
2023-06-03 05:38:30,638 - distributed.scheduler - INFO - State start
2023-06-03 05:38:30,659 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:30,660 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:38:30,660 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39261/status
2023-06-03 05:38:30,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33255'
2023-06-03 05:38:31,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41729'
2023-06-03 05:38:31,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33161'
2023-06-03 05:38:31,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38803'
2023-06-03 05:38:31,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35675'
2023-06-03 05:38:31,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44499'
2023-06-03 05:38:31,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37871'
2023-06-03 05:38:31,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44921'
2023-06-03 05:38:32,305 - distributed.scheduler - INFO - Receive client connection: Client-de795261-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:32,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58878
2023-06-03 05:38:32,646 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,647 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,647 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,647 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,712 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,720 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:32,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:32,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:32,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:35,267 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44609
2023-06-03 05:38:35,268 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44609
2023-06-03 05:38:35,268 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45051
2023-06-03 05:38:35,268 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,268 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,268 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,268 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,268 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7bx7ypzb
2023-06-03 05:38:35,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17b87ad3-be33-4312-b5bb-92aa246ceab1
2023-06-03 05:38:35,275 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42087
2023-06-03 05:38:35,275 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42087
2023-06-03 05:38:35,276 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34729
2023-06-03 05:38:35,276 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,276 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,276 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,276 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5d7fi685
2023-06-03 05:38:35,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8536bf2b-d868-4b54-8cf3-12853db61f85
2023-06-03 05:38:35,409 - distributed.worker - INFO - Starting Worker plugin PreImport-5106eb11-0edc-4f41-bf74-b7a23dd39a98
2023-06-03 05:38:35,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd387a46-3041-444b-a644-c69371e75085
2023-06-03 05:38:35,410 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,412 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d90fa701-fa63-4e2c-9c40-d3d7da82173d
2023-06-03 05:38:35,413 - distributed.worker - INFO - Starting Worker plugin PreImport-8432bddc-1371-479b-b072-7aba67629418
2023-06-03 05:38:35,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44609', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,440 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44609
2023-06-03 05:38:35,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58884
2023-06-03 05:38:35,441 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,441 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,448 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42087', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,449 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42087
2023-06-03 05:38:35,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58894
2023-06-03 05:38:35,449 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,450 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37041
2023-06-03 05:38:35,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37041
2023-06-03 05:38:35,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35821
2023-06-03 05:38:35,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,457 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,457 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,457 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v5c1vj47
2023-06-03 05:38:35,457 - distributed.worker - INFO - Starting Worker plugin PreImport-bbf6f05c-fa37-464d-a73e-c375f4f997c9
2023-06-03 05:38:35,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2cec676-4782-4cdb-9e1e-b493b7d2054d
2023-06-03 05:38:35,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b25db89f-987d-44f0-8b1e-ebd1f752e8b2
2023-06-03 05:38:35,565 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,602 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37041', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,603 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37041
2023-06-03 05:38:35,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58902
2023-06-03 05:38:35,603 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,604 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35713
2023-06-03 05:38:35,646 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35713
2023-06-03 05:38:35,646 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39913
2023-06-03 05:38:35,646 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,646 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,646 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,646 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2od2qe40
2023-06-03 05:38:35,647 - distributed.worker - INFO - Starting Worker plugin PreImport-f2edc1bf-4fe5-44a9-8430-52fece589d6d
2023-06-03 05:38:35,647 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-294959d1-1922-4799-b567-22bea14cb1d1
2023-06-03 05:38:35,647 - distributed.worker - INFO - Starting Worker plugin RMMSetup-314adf1d-6340-4b02-af82-519bc2aaa4cd
2023-06-03 05:38:35,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41807
2023-06-03 05:38:35,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41807
2023-06-03 05:38:35,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36957
2023-06-03 05:38:35,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,650 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,650 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9v_hq91i
2023-06-03 05:38:35,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0b3478c-6d11-41e2-91b9-90e4ca650349
2023-06-03 05:38:35,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35693
2023-06-03 05:38:35,651 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35693
2023-06-03 05:38:35,651 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39775
2023-06-03 05:38:35,651 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,651 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,651 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,651 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9rqjhe_o
2023-06-03 05:38:35,652 - distributed.worker - INFO - Starting Worker plugin PreImport-ef83c61b-215f-4337-bf7b-928622d8b8b0
2023-06-03 05:38:35,652 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6aadf5c6-57f9-4d08-b1f9-3144a9e8bb8e
2023-06-03 05:38:35,652 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2245ef7e-53a7-409e-9c46-b7738d3428ed
2023-06-03 05:38:35,662 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45183
2023-06-03 05:38:35,662 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45183
2023-06-03 05:38:35,662 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36765
2023-06-03 05:38:35,662 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,662 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,663 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,663 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,663 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s4_5bzjd
2023-06-03 05:38:35,663 - distributed.worker - INFO - Starting Worker plugin PreImport-97b641ac-77c3-4978-a562-8b2be9acfe06
2023-06-03 05:38:35,663 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44007
2023-06-03 05:38:35,663 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ae56194-d745-4397-a1ef-711cc1490baa
2023-06-03 05:38:35,663 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44007
2023-06-03 05:38:35,663 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d97b34a-8d5e-43f1-b79d-638c90cc65b2
2023-06-03 05:38:35,663 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35099
2023-06-03 05:38:35,664 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,664 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,664 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:35,664 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:35,664 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cno84pnz
2023-06-03 05:38:35,664 - distributed.worker - INFO - Starting Worker plugin PreImport-86236ce9-f4f2-4f82-881f-2c6490644cb4
2023-06-03 05:38:35,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa4b5a31-f464-49b3-88a2-98be2e72676f
2023-06-03 05:38:35,665 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9de52246-7337-441c-bce7-a58d072997aa
2023-06-03 05:38:35,780 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,789 - distributed.worker - INFO - Starting Worker plugin PreImport-8ed057c0-cddd-4a84-ac6b-5c348ec2bddc
2023-06-03 05:38:35,789 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,789 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,789 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-989e9853-1540-4951-a82d-f10ee720f1fe
2023-06-03 05:38:35,789 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,790 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,812 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35713', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35713
2023-06-03 05:38:35,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58918
2023-06-03 05:38:35,813 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,813 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35693', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35693
2023-06-03 05:38:35,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58934
2023-06-03 05:38:35,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,815 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45183', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,815 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45183
2023-06-03 05:38:35,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58956
2023-06-03 05:38:35,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,816 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,816 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,816 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41807', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,817 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41807
2023-06-03 05:38:35,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58958
2023-06-03 05:38:35,817 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,818 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,819 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44007', status: init, memory: 0, processing: 0>
2023-06-03 05:38:35,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44007
2023-06-03 05:38:35,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58950
2023-06-03 05:38:35,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,820 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:35,820 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:35,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:35,880 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,882 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:35,885 - distributed.scheduler - INFO - Remove client Client-de795261-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:35,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58878; closing.
2023-06-03 05:38:35,886 - distributed.scheduler - INFO - Remove client Client-de795261-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:35,886 - distributed.scheduler - INFO - Close client connection: Client-de795261-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:35,887 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38803'. Reason: nanny-close
2023-06-03 05:38:35,887 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35675'. Reason: nanny-close
2023-06-03 05:38:35,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,889 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44609. Reason: nanny-close
2023-06-03 05:38:35,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33255'. Reason: nanny-close
2023-06-03 05:38:35,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37041. Reason: nanny-close
2023-06-03 05:38:35,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41729'. Reason: nanny-close
2023-06-03 05:38:35,890 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35693. Reason: nanny-close
2023-06-03 05:38:35,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33161'. Reason: nanny-close
2023-06-03 05:38:35,891 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,891 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45183. Reason: nanny-close
2023-06-03 05:38:35,891 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44499'. Reason: nanny-close
2023-06-03 05:38:35,891 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,891 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58884; closing.
2023-06-03 05:38:35,891 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,891 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44609', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,892 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44609
2023-06-03 05:38:35,892 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42087. Reason: nanny-close
2023-06-03 05:38:35,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37871'. Reason: nanny-close
2023-06-03 05:38:35,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,892 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,892 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35713. Reason: nanny-close
2023-06-03 05:38:35,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44921'. Reason: nanny-close
2023-06-03 05:38:35,893 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,893 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:35,893 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,893 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44007. Reason: nanny-close
2023-06-03 05:38:35,893 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,893 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44609
2023-06-03 05:38:35,893 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58902; closing.
2023-06-03 05:38:35,893 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,893 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58934; closing.
2023-06-03 05:38:35,894 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41807. Reason: nanny-close
2023-06-03 05:38:35,894 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,894 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44609
2023-06-03 05:38:35,894 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37041', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,894 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37041
2023-06-03 05:38:35,895 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,895 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35693', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,895 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44609
2023-06-03 05:38:35,895 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44609
2023-06-03 05:38:35,895 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35693
2023-06-03 05:38:35,895 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58956; closing.
2023-06-03 05:38:35,895 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,895 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,895 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:35,895 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45183', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,896 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45183
2023-06-03 05:38:35,896 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,896 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58894; closing.
2023-06-03 05:38:35,896 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,896 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58950; closing.
2023-06-03 05:38:35,896 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,897 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:35,897 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58918; closing.
2023-06-03 05:38:35,897 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58958; closing.
2023-06-03 05:38:35,897 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42087', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,897 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42087
2023-06-03 05:38:35,898 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44007', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,898 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44007
2023-06-03 05:38:35,898 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35713', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,898 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35713
2023-06-03 05:38:35,899 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41807', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:35,899 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41807
2023-06-03 05:38:35,899 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:38:37,404 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:38:37,405 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:38:37,405 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:38:37,406 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:38:37,407 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-03 05:38:39,250 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:39,254 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44321 instead
  warnings.warn(
2023-06-03 05:38:39,258 - distributed.scheduler - INFO - State start
2023-06-03 05:38:39,276 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:39,277 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:38:39,278 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44321/status
2023-06-03 05:38:39,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44173'
2023-06-03 05:38:39,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33443'
2023-06-03 05:38:39,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39977'
2023-06-03 05:38:39,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39721'
2023-06-03 05:38:39,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42779'
2023-06-03 05:38:39,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32817'
2023-06-03 05:38:39,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41519'
2023-06-03 05:38:39,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40127'
2023-06-03 05:38:40,359 - distributed.scheduler - INFO - Receive client connection: Client-e3853f35-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:40,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52532
2023-06-03 05:38:41,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,028 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,082 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,131 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,131 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,131 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,136 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:41,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:41,331 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,332 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:41,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:42,833 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45921
2023-06-03 05:38:42,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45921
2023-06-03 05:38:42,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38323
2023-06-03 05:38:42,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:42,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:42,833 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:42,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:42,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5m7f_35m
2023-06-03 05:38:42,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bdc64a6a-6b7c-4bd0-b191-23592f0b8f27
2023-06-03 05:38:42,875 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38413
2023-06-03 05:38:42,875 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38413
2023-06-03 05:38:42,875 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33345
2023-06-03 05:38:42,876 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:42,876 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:42,876 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:42,876 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:42,876 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l2rfbta0
2023-06-03 05:38:42,876 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49e3720e-d4f7-4a33-ba49-72fd25c82939
2023-06-03 05:38:42,921 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-893b2045-81d1-43be-9e5d-6326020ee11f
2023-06-03 05:38:42,921 - distributed.worker - INFO - Starting Worker plugin PreImport-55e9fca4-13f1-49f2-a6b7-9c336c3d35ba
2023-06-03 05:38:42,921 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:42,951 - distributed.worker - INFO - Starting Worker plugin PreImport-1a905bf9-a38f-408b-b3d8-29d29762c4f0
2023-06-03 05:38:42,951 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ee82d37-5083-4c42-9088-fede99ce350a
2023-06-03 05:38:42,952 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:42,956 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45921', status: init, memory: 0, processing: 0>
2023-06-03 05:38:42,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45921
2023-06-03 05:38:42,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52550
2023-06-03 05:38:42,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:42,958 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:42,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:42,984 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38413', status: init, memory: 0, processing: 0>
2023-06-03 05:38:42,984 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38413
2023-06-03 05:38:42,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52564
2023-06-03 05:38:42,985 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:42,986 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:42,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,031 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45677
2023-06-03 05:38:44,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45677
2023-06-03 05:38:44,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39343
2023-06-03 05:38:44,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,031 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,031 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:44,031 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:44,032 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vau7ucr4
2023-06-03 05:38:44,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-580934fd-8678-40b2-8b62-c0148a891e24
2023-06-03 05:38:44,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38619
2023-06-03 05:38:44,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38619
2023-06-03 05:38:44,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41791
2023-06-03 05:38:44,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,034 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,034 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:44,034 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:44,034 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tr4d0sco
2023-06-03 05:38:44,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68374439-2663-4e02-b8a2-a352dc98699e
2023-06-03 05:38:44,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44569
2023-06-03 05:38:44,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43169
2023-06-03 05:38:44,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44569
2023-06-03 05:38:44,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39761
2023-06-03 05:38:44,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43169
2023-06-03 05:38:44,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45855
2023-06-03 05:38:44,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39761
2023-06-03 05:38:44,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34069
2023-06-03 05:38:44,035 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35227
2023-06-03 05:38:44,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35125
2023-06-03 05:38:44,035 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:44,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,035 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:44,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35125
2023-06-03 05:38:44,035 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-esbh8yh0
2023-06-03 05:38:44,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39091
2023-06-03 05:38:44,035 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:44,035 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:44,036 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:44,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:44,036 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pnre16u9
2023-06-03 05:38:44,036 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:44,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-21zjat8z
2023-06-03 05:38:44,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:44,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zbzjyqn0
2023-06-03 05:38:44,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23b5e9e8-d7b2-4cdf-8f07-77914bae63ad
2023-06-03 05:38:44,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-facc51d5-a20f-4c91-add3-684523250840
2023-06-03 05:38:44,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa2e1e00-d143-43a3-b039-09292e279b4f
2023-06-03 05:38:44,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b06fa3b-e3df-43b9-9ead-ed7fe8a6d581
2023-06-03 05:38:44,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f62f7713-d22f-4b42-b3ad-3ac5c3340dc3
2023-06-03 05:38:44,054 - distributed.worker - INFO - Starting Worker plugin PreImport-88e2e3e5-1c0f-4fd7-a8e8-41b540baa5fe
2023-06-03 05:38:44,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4060b20b-ff98-4f36-92bf-f67a8491056c
2023-06-03 05:38:44,054 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,054 - distributed.worker - INFO - Starting Worker plugin PreImport-866acf16-c2ab-406f-bb7d-04e59bbced95
2023-06-03 05:38:44,055 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fec82f0-52a4-4681-882b-8324997335a5
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc812e8c-0438-4409-817c-bd1225561ef1
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin PreImport-89dbed5b-3e12-4d36-8d65-d6958bca383d
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2704c9f1-fa3c-481a-9f23-f147460a7dfc
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin PreImport-4b55f4c1-99c0-4988-9a84-ce114acb30df
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7f86b30-abc5-4be3-8721-d59d9ca4c839
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin PreImport-8c8d71e9-e93b-4a93-b356-ae8fb59e653d
2023-06-03 05:38:44,057 - distributed.worker - INFO - Starting Worker plugin PreImport-56aa45fd-8762-463c-ad86-a05643cac67b
2023-06-03 05:38:44,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,075 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45677', status: init, memory: 0, processing: 0>
2023-06-03 05:38:44,076 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45677
2023-06-03 05:38:44,076 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52594
2023-06-03 05:38:44,077 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,077 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38619', status: init, memory: 0, processing: 0>
2023-06-03 05:38:44,078 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38619
2023-06-03 05:38:44,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52598
2023-06-03 05:38:44,079 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,079 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,079 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35125', status: init, memory: 0, processing: 0>
2023-06-03 05:38:44,080 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35125
2023-06-03 05:38:44,080 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52612
2023-06-03 05:38:44,081 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,081 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,086 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43169', status: init, memory: 0, processing: 0>
2023-06-03 05:38:44,087 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43169
2023-06-03 05:38:44,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52630
2023-06-03 05:38:44,087 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,088 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,090 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44569', status: init, memory: 0, processing: 0>
2023-06-03 05:38:44,090 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44569
2023-06-03 05:38:44,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52622
2023-06-03 05:38:44,091 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,091 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39761', status: init, memory: 0, processing: 0>
2023-06-03 05:38:44,092 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39761
2023-06-03 05:38:44,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52624
2023-06-03 05:38:44,092 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:44,093 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:44,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:44,166 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,167 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,167 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,167 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,167 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,167 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,168 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,168 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:44,172 - distributed.scheduler - INFO - Remove client Client-e3853f35-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:44,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52532; closing.
2023-06-03 05:38:44,172 - distributed.scheduler - INFO - Remove client Client-e3853f35-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:44,172 - distributed.scheduler - INFO - Close client connection: Client-e3853f35-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:44,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39977'. Reason: nanny-close
2023-06-03 05:38:44,174 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44173'. Reason: nanny-close
2023-06-03 05:38:44,175 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,175 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38413. Reason: nanny-close
2023-06-03 05:38:44,175 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33443'. Reason: nanny-close
2023-06-03 05:38:44,176 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,176 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44569. Reason: nanny-close
2023-06-03 05:38:44,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39721'. Reason: nanny-close
2023-06-03 05:38:44,177 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,177 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45921. Reason: nanny-close
2023-06-03 05:38:44,177 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42779'. Reason: nanny-close
2023-06-03 05:38:44,177 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52564; closing.
2023-06-03 05:38:44,177 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38619. Reason: nanny-close
2023-06-03 05:38:44,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32817'. Reason: nanny-close
2023-06-03 05:38:44,178 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38413', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,178 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38413
2023-06-03 05:38:44,178 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41519'. Reason: nanny-close
2023-06-03 05:38:44,178 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43169. Reason: nanny-close
2023-06-03 05:38:44,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40127'. Reason: nanny-close
2023-06-03 05:38:44,179 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39761. Reason: nanny-close
2023-06-03 05:38:44,179 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38413
2023-06-03 05:38:44,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:44,179 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,179 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38413
2023-06-03 05:38:44,179 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45677. Reason: nanny-close
2023-06-03 05:38:44,179 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52622; closing.
2023-06-03 05:38:44,180 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,180 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35125. Reason: nanny-close
2023-06-03 05:38:44,180 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,180 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44569', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,180 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38413
2023-06-03 05:38:44,180 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,180 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44569
2023-06-03 05:38:44,181 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38413
2023-06-03 05:38:44,181 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52550; closing.
2023-06-03 05:38:44,181 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,181 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38413
2023-06-03 05:38:44,181 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,181 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45921', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,181 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45921
2023-06-03 05:38:44,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:44,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52598; closing.
2023-06-03 05:38:44,182 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38619', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,183 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,183 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,183 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38619
2023-06-03 05:38:44,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52630; closing.
2023-06-03 05:38:44,183 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,183 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:44,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52624; closing.
2023-06-03 05:38:44,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52594; closing.
2023-06-03 05:38:44,184 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43169', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,184 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43169
2023-06-03 05:38:44,184 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39761', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,184 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39761
2023-06-03 05:38:44,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45677', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,185 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45677
2023-06-03 05:38:44,185 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52612; closing.
2023-06-03 05:38:44,186 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35125', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:44,186 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35125
2023-06-03 05:38:44,186 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:38:45,691 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:38:45,692 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:38:45,692 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:38:45,693 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:38:45,694 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-03 05:38:47,629 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:47,633 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36169 instead
  warnings.warn(
2023-06-03 05:38:47,637 - distributed.scheduler - INFO - State start
2023-06-03 05:38:47,656 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:47,657 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:38:47,657 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36169/status
2023-06-03 05:38:47,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42037'
2023-06-03 05:38:47,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34753'
2023-06-03 05:38:47,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41211'
2023-06-03 05:38:47,860 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38287'
2023-06-03 05:38:47,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44657'
2023-06-03 05:38:47,875 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35977'
2023-06-03 05:38:47,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46883'
2023-06-03 05:38:47,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35463'
2023-06-03 05:38:49,314 - distributed.scheduler - INFO - Receive client connection: Client-e880f8af-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:49,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52786
2023-06-03 05:38:49,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,405 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,507 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,507 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,523 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:49,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:49,702 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,712 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,716 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:49,719 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:50,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36425
2023-06-03 05:38:50,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36425
2023-06-03 05:38:50,601 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34801
2023-06-03 05:38:50,601 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:50,601 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:50,601 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:50,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:50,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rpytlgq9
2023-06-03 05:38:50,602 - distributed.worker - INFO - Starting Worker plugin PreImport-b92fa87c-6688-42b6-a503-a1e3d242ca0e
2023-06-03 05:38:50,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b613f86-54c1-4e63-8e51-7b385dc00c0d
2023-06-03 05:38:50,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-118a675d-681c-439b-8c6a-920134956f76
2023-06-03 05:38:51,669 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:51,712 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36425', status: init, memory: 0, processing: 0>
2023-06-03 05:38:51,714 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36425
2023-06-03 05:38:51,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56818
2023-06-03 05:38:51,714 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:51,714 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:51,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,229 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43675
2023-06-03 05:38:52,229 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43675
2023-06-03 05:38:52,229 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33871
2023-06-03 05:38:52,229 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,229 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,229 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,229 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2vbtzktm
2023-06-03 05:38:52,230 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a888c86a-28cf-44d5-9cde-b03c53b45495
2023-06-03 05:38:52,348 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c71199a-9997-4b04-a03e-46d25e493807
2023-06-03 05:38:52,348 - distributed.worker - INFO - Starting Worker plugin PreImport-cc2cb3f3-b421-4c41-8e49-2f9dc8ef3f18
2023-06-03 05:38:52,348 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43675', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,381 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43675
2023-06-03 05:38:52,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56822
2023-06-03 05:38:52,381 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,381 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,460 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35603
2023-06-03 05:38:52,460 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41949
2023-06-03 05:38:52,460 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35603
2023-06-03 05:38:52,460 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41949
2023-06-03 05:38:52,460 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33957
2023-06-03 05:38:52,460 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,461 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35881
2023-06-03 05:38:52,461 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,461 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,461 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,461 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,461 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,461 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,461 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p4xacex1
2023-06-03 05:38:52,461 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,461 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-26lyf6hm
2023-06-03 05:38:52,461 - distributed.worker - INFO - Starting Worker plugin PreImport-f2a0bc55-5ea1-47b8-8d75-b22a881458fd
2023-06-03 05:38:52,461 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c9fc733-10af-4180-a7b2-f7f655726088
2023-06-03 05:38:52,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d83a329-0446-41b0-9200-51e57464d827
2023-06-03 05:38:52,462 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0f02f5f-5e0a-46e9-ba86-11ac04436e8d
2023-06-03 05:38:52,463 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43853
2023-06-03 05:38:52,463 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43853
2023-06-03 05:38:52,463 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39381
2023-06-03 05:38:52,463 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,464 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,464 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36261
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jg7l72_k
2023-06-03 05:38:52,464 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36261
2023-06-03 05:38:52,464 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34421
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35625
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33815
2023-06-03 05:38:52,464 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,464 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33815
2023-06-03 05:38:52,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,464 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35625
2023-06-03 05:38:52,464 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,464 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43227
2023-06-03 05:38:52,464 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36193
2023-06-03 05:38:52,464 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,464 - distributed.worker - INFO - Starting Worker plugin PreImport-a40d5ac3-0ce4-41f8-9bbd-776bc6fc3dbc
2023-06-03 05:38:52,464 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-niir8d79
2023-06-03 05:38:52,464 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,464 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,464 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b8bdae6b-b344-455a-a1ac-2404e88bc5e3
2023-06-03 05:38:52,464 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,464 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ffoop64m
2023-06-03 05:38:52,464 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:38:52,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y1j2_g9n
2023-06-03 05:38:52,464 - distributed.worker - INFO - Starting Worker plugin PreImport-afd588a3-a571-427f-892b-2ebf76004fed
2023-06-03 05:38:52,465 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-983f55d7-73d5-46d6-b82f-457f05300531
2023-06-03 05:38:52,465 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3024a6cb-1987-4ad5-a111-176d9c6e4876
2023-06-03 05:38:52,465 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8a42dcb-4e5d-48a5-a502-bb79bcb360ec
2023-06-03 05:38:52,465 - distributed.worker - INFO - Starting Worker plugin PreImport-069b22e5-9fda-4124-8b8d-9d5952abe7ff
2023-06-03 05:38:52,465 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce8184b8-a7a6-4fee-a3d2-e3dfe8334a6f
2023-06-03 05:38:52,465 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4977309-5bea-4338-9d8a-d91421a6d060
2023-06-03 05:38:52,466 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e749731-fed4-4b25-ab46-fad25608fa97
2023-06-03 05:38:52,673 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,684 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,686 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,687 - distributed.worker - INFO - Starting Worker plugin PreImport-47f57e52-d495-415a-94e7-fa574fd890af
2023-06-03 05:38:52,687 - distributed.worker - INFO - Starting Worker plugin PreImport-771638fd-c39d-44a9-a7d2-07b095036849
2023-06-03 05:38:52,687 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef127650-9a39-4f92-96d2-ae91d1cc70e4
2023-06-03 05:38:52,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3e5c60d-cff0-46ab-921c-5e2b58eb50ba
2023-06-03 05:38:52,687 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,687 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,700 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35603', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,701 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35603
2023-06-03 05:38:52,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56836
2023-06-03 05:38:52,702 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,702 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36261', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36261
2023-06-03 05:38:52,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56844
2023-06-03 05:38:52,712 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,712 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,714 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35625', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,715 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35625
2023-06-03 05:38:52,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56848
2023-06-03 05:38:52,715 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,716 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,716 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43853', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,717 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43853
2023-06-03 05:38:52,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56864
2023-06-03 05:38:52,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,718 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,718 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,718 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33815', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,719 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33815
2023-06-03 05:38:52,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56852
2023-06-03 05:38:52,720 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,720 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41949', status: init, memory: 0, processing: 0>
2023-06-03 05:38:52,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41949
2023-06-03 05:38:52,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56876
2023-06-03 05:38:52,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:38:52,725 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:38:52,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:38:52,757 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,757 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,757 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,757 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,757 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,758 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,758 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,758 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:38:52,770 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,770 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,770 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,770 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,770 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,770 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,771 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,771 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:38:52,777 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:38:52,778 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:38:52,781 - distributed.scheduler - INFO - Remove client Client-e880f8af-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:52,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52786; closing.
2023-06-03 05:38:52,781 - distributed.scheduler - INFO - Remove client Client-e880f8af-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:52,782 - distributed.scheduler - INFO - Close client connection: Client-e880f8af-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:52,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38287'. Reason: nanny-close
2023-06-03 05:38:52,783 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42037'. Reason: nanny-close
2023-06-03 05:38:52,784 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,784 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41949. Reason: nanny-close
2023-06-03 05:38:52,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34753'. Reason: nanny-close
2023-06-03 05:38:52,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,785 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43853. Reason: nanny-close
2023-06-03 05:38:52,785 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41211'. Reason: nanny-close
2023-06-03 05:38:52,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36261. Reason: nanny-close
2023-06-03 05:38:52,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44657'. Reason: nanny-close
2023-06-03 05:38:52,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35977'. Reason: nanny-close
2023-06-03 05:38:52,786 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56876; closing.
2023-06-03 05:38:52,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36425. Reason: nanny-close
2023-06-03 05:38:52,786 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,787 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41949', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,787 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-06-03 05:38:52,787 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46883'. Reason: nanny-close
2023-06-03 05:38:52,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43675. Reason: nanny-close
2023-06-03 05:38:52,787 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,787 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,787 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35463'. Reason: nanny-close
2023-06-03 05:38:52,787 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35603. Reason: nanny-close
2023-06-03 05:38:52,788 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:38:52,788 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,788 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35625. Reason: nanny-close
2023-06-03 05:38:52,788 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-06-03 05:38:52,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,788 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,789 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56864; closing.
2023-06-03 05:38:52,789 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,789 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-06-03 05:38:52,789 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33815. Reason: nanny-close
2023-06-03 05:38:52,789 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56844; closing.
2023-06-03 05:38:52,789 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,790 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-06-03 05:38:52,790 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-06-03 05:38:52,790 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,790 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43853', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,790 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,790 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,790 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43853
2023-06-03 05:38:52,791 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36261', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,791 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,791 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36261
2023-06-03 05:38:52,791 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:38:52,791 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,792 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,792 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56818; closing.
2023-06-03 05:38:52,792 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56822; closing.
2023-06-03 05:38:52,793 - distributed.nanny - INFO - Worker closed
2023-06-03 05:38:52,793 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36425', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,793 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36425
2023-06-03 05:38:52,794 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43675', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,794 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43675
2023-06-03 05:38:52,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56836; closing.
2023-06-03 05:38:52,795 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56848; closing.
2023-06-03 05:38:52,795 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56852; closing.
2023-06-03 05:38:52,795 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35603', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,796 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35603
2023-06-03 05:38:52,796 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35625', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,796 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35625
2023-06-03 05:38:52,797 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33815', status: closing, memory: 0, processing: 0>
2023-06-03 05:38:52,797 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33815
2023-06-03 05:38:52,797 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:38:54,851 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:38:54,852 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:38:54,852 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:38:54,853 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:38:54,854 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-03 05:38:56,760 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:56,764 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41207 instead
  warnings.warn(
2023-06-03 05:38:56,768 - distributed.scheduler - INFO - State start
2023-06-03 05:38:56,787 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:38:56,788 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:38:56,788 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41207/status
2023-06-03 05:38:57,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36157'
2023-06-03 05:38:57,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34367'
2023-06-03 05:38:57,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34571'
2023-06-03 05:38:57,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39725'
2023-06-03 05:38:57,113 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44257'
2023-06-03 05:38:57,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44895'
2023-06-03 05:38:57,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35395'
2023-06-03 05:38:57,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42571'
2023-06-03 05:38:57,425 - distributed.scheduler - INFO - Receive client connection: Client-edf7db0a-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:38:57,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57060
2023-06-03 05:38:58,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,872 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:38:58,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:38:58,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:38:58,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:00,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42837
2023-06-03 05:39:00,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42837
2023-06-03 05:39:00,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41717
2023-06-03 05:39:00,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:00,353 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:00,353 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:00,353 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:00,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-azyu18et
2023-06-03 05:39:00,354 - distributed.worker - INFO - Starting Worker plugin RMMSetup-775fb7a3-3dbe-4fb1-8c4e-1c1e439acf97
2023-06-03 05:39:00,688 - distributed.worker - INFO - Starting Worker plugin PreImport-5a54b852-d386-4a4e-8c93-01cd48b6c997
2023-06-03 05:39:00,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04a1ce09-909b-41d2-a296-e479ccf073ff
2023-06-03 05:39:00,688 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:00,715 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42837', status: init, memory: 0, processing: 0>
2023-06-03 05:39:00,717 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42837
2023-06-03 05:39:00,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35748
2023-06-03 05:39:00,717 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:00,717 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:00,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42421
2023-06-03 05:39:01,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42421
2023-06-03 05:39:01,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37331
2023-06-03 05:39:01,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,458 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,458 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dg8qysgy
2023-06-03 05:39:01,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df16960a-bbc7-4447-84b9-5d89a1dc57e9
2023-06-03 05:39:01,458 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41443
2023-06-03 05:39:01,458 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41443
2023-06-03 05:39:01,458 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38889
2023-06-03 05:39:01,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,459 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,459 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,459 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,459 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0iafolug
2023-06-03 05:39:01,459 - distributed.worker - INFO - Starting Worker plugin PreImport-cedc6f84-0fb4-4e60-916f-d30b33e32023
2023-06-03 05:39:01,459 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8adb21bf-2bbb-4bb8-967d-a5ff62d04050
2023-06-03 05:39:01,460 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9f71da2-173e-4ac8-9444-a05e1293ba87
2023-06-03 05:39:01,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c6493bb-8d60-4167-a50e-23c778005ff6
2023-06-03 05:39:01,580 - distributed.worker - INFO - Starting Worker plugin PreImport-96c5ed32-b6bc-4dbd-8492-d976f160fdb3
2023-06-03 05:39:01,580 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,583 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41443', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41443
2023-06-03 05:39:01,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35768
2023-06-03 05:39:01,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,607 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,612 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42421', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,613 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42421
2023-06-03 05:39:01,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35752
2023-06-03 05:39:01,613 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,614 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,699 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44353
2023-06-03 05:39:01,699 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46397
2023-06-03 05:39:01,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44353
2023-06-03 05:39:01,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46397
2023-06-03 05:39:01,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39207
2023-06-03 05:39:01,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37947
2023-06-03 05:39:01,699 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,699 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,699 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,699 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xwz8ykb_
2023-06-03 05:39:01,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ob_z8eh2
2023-06-03 05:39:01,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35179
2023-06-03 05:39:01,700 - distributed.worker - INFO - Starting Worker plugin PreImport-c0718408-012c-4e05-9da8-8f15b4a7c1e3
2023-06-03 05:39:01,700 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35179
2023-06-03 05:39:01,700 - distributed.worker - INFO - Starting Worker plugin PreImport-d69e50de-fb6c-4374-8bf8-b931f8fbfa82
2023-06-03 05:39:01,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e26dc922-5c95-45ce-9a58-b6266a66384b
2023-06-03 05:39:01,700 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38685
2023-06-03 05:39:01,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4da54a00-4de3-4fad-8ee5-a3200fb38196
2023-06-03 05:39:01,700 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,700 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,700 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,700 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,700 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2yblfkii
2023-06-03 05:39:01,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8676838c-4815-4d28-9924-c4c3f7677f90
2023-06-03 05:39:01,701 - distributed.worker - INFO - Starting Worker plugin PreImport-23ba0256-8db9-4975-a48a-8475a02c717c
2023-06-03 05:39:01,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca0816ed-a92b-4da1-90c5-7e509e9f7bfe
2023-06-03 05:39:01,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aef7c5d7-f500-4c36-a115-b03d8ffecdfc
2023-06-03 05:39:01,702 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ec7d52f-ce49-4413-9f38-a5e3aa143c36
2023-06-03 05:39:01,702 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41507
2023-06-03 05:39:01,703 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41507
2023-06-03 05:39:01,703 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44273
2023-06-03 05:39:01,703 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,703 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,703 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,703 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,703 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fzn5gmmg
2023-06-03 05:39:01,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5aca83ab-eb24-4240-b34c-1700f690e726
2023-06-03 05:39:01,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35301
2023-06-03 05:39:01,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35301
2023-06-03 05:39:01,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39605
2023-06-03 05:39:01,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,708 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,708 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:01,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:01,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-chyhvr3h
2023-06-03 05:39:01,709 - distributed.worker - INFO - Starting Worker plugin PreImport-3f6ebc8e-a4b6-4f02-a027-fd5e8652968d
2023-06-03 05:39:01,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a62fd605-2669-42f6-a341-36de624aef22
2023-06-03 05:39:01,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0716a76b-fe67-4040-bbad-efd8e3979129
2023-06-03 05:39:01,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,852 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,852 - distributed.worker - INFO - Starting Worker plugin PreImport-cfd0f787-abb7-4c57-9fdf-8f020c2d60db
2023-06-03 05:39:01,852 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,852 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,852 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3cf2aa8-f75f-4058-b4b1-cbf52acce354
2023-06-03 05:39:01,852 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35179', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35179
2023-06-03 05:39:01,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35782
2023-06-03 05:39:01,864 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,874 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41507', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,875 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41507
2023-06-03 05:39:01,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35800
2023-06-03 05:39:01,876 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,876 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,877 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35301', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35301
2023-06-03 05:39:01,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35788
2023-06-03 05:39:01,878 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,878 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,883 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44353', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,883 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44353
2023-06-03 05:39:01,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35818
2023-06-03 05:39:01,884 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,884 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46397', status: init, memory: 0, processing: 0>
2023-06-03 05:39:01,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46397
2023-06-03 05:39:01,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35804
2023-06-03 05:39:01,886 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:01,886 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:01,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:01,928 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,929 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,929 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,929 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,929 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,929 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,930 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,930 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:01,933 - distributed.scheduler - INFO - Remove client Client-edf7db0a-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:01,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57060; closing.
2023-06-03 05:39:01,934 - distributed.scheduler - INFO - Remove client Client-edf7db0a-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:01,934 - distributed.scheduler - INFO - Close client connection: Client-edf7db0a-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:01,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36157'. Reason: nanny-close
2023-06-03 05:39:01,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34367'. Reason: nanny-close
2023-06-03 05:39:01,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42837. Reason: nanny-close
2023-06-03 05:39:01,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34571'. Reason: nanny-close
2023-06-03 05:39:01,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39725'. Reason: nanny-close
2023-06-03 05:39:01,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46397. Reason: nanny-close
2023-06-03 05:39:01,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,938 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41443. Reason: nanny-close
2023-06-03 05:39:01,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44257'. Reason: nanny-close
2023-06-03 05:39:01,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35748; closing.
2023-06-03 05:39:01,938 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35179. Reason: nanny-close
2023-06-03 05:39:01,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44895'. Reason: nanny-close
2023-06-03 05:39:01,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42837', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,939 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42837
2023-06-03 05:39:01,939 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35395'. Reason: nanny-close
2023-06-03 05:39:01,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42421. Reason: nanny-close
2023-06-03 05:39:01,939 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44353. Reason: nanny-close
2023-06-03 05:39:01,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42571'. Reason: nanny-close
2023-06-03 05:39:01,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,940 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:01,940 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42837
2023-06-03 05:39:01,940 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42837
2023-06-03 05:39:01,940 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42837
2023-06-03 05:39:01,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,940 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35301. Reason: nanny-close
2023-06-03 05:39:01,940 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35768; closing.
2023-06-03 05:39:01,941 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35804; closing.
2023-06-03 05:39:01,941 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,941 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41507. Reason: nanny-close
2023-06-03 05:39:01,941 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,941 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42837
2023-06-03 05:39:01,941 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41443', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,941 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41443
2023-06-03 05:39:01,941 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,941 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42837
2023-06-03 05:39:01,942 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46397', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,942 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46397
2023-06-03 05:39:01,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35782; closing.
2023-06-03 05:39:01,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:01,942 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35179', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,942 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35179
2023-06-03 05:39:01,943 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35752; closing.
2023-06-03 05:39:01,943 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,943 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,943 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42421', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,943 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42421
2023-06-03 05:39:01,943 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,943 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:01,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35818; closing.
2023-06-03 05:39:01,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35788; closing.
2023-06-03 05:39:01,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35800; closing.
2023-06-03 05:39:01,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44353', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,945 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44353
2023-06-03 05:39:01,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35301', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,945 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35301
2023-06-03 05:39:01,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41507', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:01,945 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41507
2023-06-03 05:39:01,946 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:03,252 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:03,252 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:03,253 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:03,254 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:03,254 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-03 05:39:05,101 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:05,105 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46721 instead
  warnings.warn(
2023-06-03 05:39:05,109 - distributed.scheduler - INFO - State start
2023-06-03 05:39:05,128 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:05,129 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:39:05,130 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46721/status
2023-06-03 05:39:05,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33531'
2023-06-03 05:39:05,282 - distributed.scheduler - INFO - Receive client connection: Client-f2f633ce-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:05,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35910
2023-06-03 05:39:06,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:06,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:06,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:07,696 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43817
2023-06-03 05:39:07,696 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43817
2023-06-03 05:39:07,696 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-03 05:39:07,696 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:07,696 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:07,696 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:07,697 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:39:07,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bgd5fcsv
2023-06-03 05:39:07,697 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c3cf488-55a1-4af4-b0b7-f3955b6330c6
2023-06-03 05:39:07,697 - distributed.worker - INFO - Starting Worker plugin PreImport-01f930c8-1969-4095-98cf-51b3d61b31f3
2023-06-03 05:39:07,697 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2868b2d-8503-4472-baf5-bfd51c0537f6
2023-06-03 05:39:07,698 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:07,722 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43817', status: init, memory: 0, processing: 0>
2023-06-03 05:39:07,723 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43817
2023-06-03 05:39:07,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35934
2023-06-03 05:39:07,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:07,724 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:07,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:07,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:07,741 - distributed.scheduler - INFO - Remove client Client-f2f633ce-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:07,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35910; closing.
2023-06-03 05:39:07,742 - distributed.scheduler - INFO - Remove client Client-f2f633ce-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:07,742 - distributed.scheduler - INFO - Close client connection: Client-f2f633ce-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:07,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33531'. Reason: nanny-close
2023-06-03 05:39:07,743 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:07,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43817. Reason: nanny-close
2023-06-03 05:39:07,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35934; closing.
2023-06-03 05:39:07,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:07,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43817', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:07,746 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43817
2023-06-03 05:39:07,746 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:07,747 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:08,659 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:08,659 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:08,659 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:08,660 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:08,661 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-03 05:39:12,181 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:12,185 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45015 instead
  warnings.warn(
2023-06-03 05:39:12,188 - distributed.scheduler - INFO - State start
2023-06-03 05:39:12,206 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:12,207 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:39:12,207 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45015/status
2023-06-03 05:39:12,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34977'
2023-06-03 05:39:12,380 - distributed.scheduler - INFO - Receive client connection: Client-f7259041-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:12,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52210
2023-06-03 05:39:13,698 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:13,698 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:13,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:14,971 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38263
2023-06-03 05:39:14,971 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38263
2023-06-03 05:39:14,971 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38217
2023-06-03 05:39:14,971 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:14,971 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:14,972 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:14,972 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:39:14,972 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9whp8swy
2023-06-03 05:39:14,972 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-325a09c1-8ae0-4bdd-aea6-9c868a9ca9d9
2023-06-03 05:39:14,972 - distributed.worker - INFO - Starting Worker plugin PreImport-dc46a4e8-d7d8-4978-874c-8130f1e53af2
2023-06-03 05:39:14,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60593bca-60c3-4a79-8a9c-e402dcc9fe99
2023-06-03 05:39:14,974 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:14,997 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38263', status: init, memory: 0, processing: 0>
2023-06-03 05:39:14,997 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38263
2023-06-03 05:39:14,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52230
2023-06-03 05:39:14,998 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:14,998 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:15,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:15,039 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:15,041 - distributed.scheduler - INFO - Remove client Client-f7259041-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:15,042 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52210; closing.
2023-06-03 05:39:15,042 - distributed.scheduler - INFO - Remove client Client-f7259041-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:15,042 - distributed.scheduler - INFO - Close client connection: Client-f7259041-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:15,043 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34977'. Reason: nanny-close
2023-06-03 05:39:15,043 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:15,044 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38263. Reason: nanny-close
2023-06-03 05:39:15,046 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:15,046 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52230; closing.
2023-06-03 05:39:15,046 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38263', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:15,046 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38263
2023-06-03 05:39:15,046 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:15,047 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:16,009 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:16,009 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:16,009 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:16,010 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:16,010 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-03 05:39:17,841 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:17,844 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43311 instead
  warnings.warn(
2023-06-03 05:39:17,847 - distributed.scheduler - INFO - State start
2023-06-03 05:39:17,865 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:17,866 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:39:17,866 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43311/status
2023-06-03 05:39:21,198 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:21,198 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:21,198 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:21,199 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:21,199 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-03 05:39:22,995 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:22,999 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36237 instead
  warnings.warn(
2023-06-03 05:39:23,002 - distributed.scheduler - INFO - State start
2023-06-03 05:39:23,021 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:23,022 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-03 05:39:23,022 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36237/status
2023-06-03 05:39:23,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33887'
2023-06-03 05:39:23,865 - distributed.scheduler - INFO - Receive client connection: Client-fd979912-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:23,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59340
2023-06-03 05:39:24,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:24,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:24,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:25,103 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40819
2023-06-03 05:39:25,103 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40819
2023-06-03 05:39:25,103 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44497
2023-06-03 05:39:25,104 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:39:25,104 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:25,104 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:25,104 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:39:25,104 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mwe_fdq8
2023-06-03 05:39:25,104 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70d9768e-1203-4ffb-b0ee-d3557788ba8c
2023-06-03 05:39:25,104 - distributed.worker - INFO - Starting Worker plugin PreImport-55feb09c-6038-443b-9da4-395b7b66a6a8
2023-06-03 05:39:25,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22b5d3bb-3459-46ba-ba0f-2b6980eb23b6
2023-06-03 05:39:25,104 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:25,157 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40819', status: init, memory: 0, processing: 0>
2023-06-03 05:39:25,158 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40819
2023-06-03 05:39:25,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59356
2023-06-03 05:39:25,159 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:39:25,159 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:25,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:39:25,203 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:25,206 - distributed.scheduler - INFO - Remove client Client-fd979912-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:25,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59340; closing.
2023-06-03 05:39:25,206 - distributed.scheduler - INFO - Remove client Client-fd979912-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:25,207 - distributed.scheduler - INFO - Close client connection: Client-fd979912-01d0-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:25,207 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33887'. Reason: nanny-close
2023-06-03 05:39:25,208 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:25,209 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40819. Reason: nanny-close
2023-06-03 05:39:25,210 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59356; closing.
2023-06-03 05:39:25,210 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:39:25,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40819', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:25,211 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40819
2023-06-03 05:39:25,211 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:25,211 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:26,123 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:26,124 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:26,124 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:26,125 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-03 05:39:26,125 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-03 05:39:28,022 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:28,026 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35675 instead
  warnings.warn(
2023-06-03 05:39:28,029 - distributed.scheduler - INFO - State start
2023-06-03 05:39:28,084 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:28,085 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:39:28,086 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35675/status
2023-06-03 05:39:28,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32845'
2023-06-03 05:39:28,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45407'
2023-06-03 05:39:28,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42709'
2023-06-03 05:39:28,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38483'
2023-06-03 05:39:28,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33523'
2023-06-03 05:39:28,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34355'
2023-06-03 05:39:28,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35837'
2023-06-03 05:39:28,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40393'
2023-06-03 05:39:29,153 - distributed.scheduler - INFO - Receive client connection: Client-009dce73-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:29,165 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38820
2023-06-03 05:39:30,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,272 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,272 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,328 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,337 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:30,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:30,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:30,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:32,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38019
2023-06-03 05:39:32,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38019
2023-06-03 05:39:32,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37881
2023-06-03 05:39:32,926 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,926 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,926 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,926 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-evzr2p0i
2023-06-03 05:39:32,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d205265-7ebf-4ef1-a348-d7a58a16348e
2023-06-03 05:39:32,930 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41827
2023-06-03 05:39:32,930 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41827
2023-06-03 05:39:32,930 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32811
2023-06-03 05:39:32,930 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,930 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,930 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k_m9123v
2023-06-03 05:39:32,931 - distributed.worker - INFO - Starting Worker plugin PreImport-7ca2f956-d3be-4ad3-91d4-b6481c14d9e8
2023-06-03 05:39:32,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a2e0d8f-bdf1-4c2c-8da1-3da93a0f46c5
2023-06-03 05:39:32,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-754d99a0-cb39-4f66-a602-7259856516e0
2023-06-03 05:39:32,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34237
2023-06-03 05:39:32,932 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34237
2023-06-03 05:39:32,932 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42167
2023-06-03 05:39:32,932 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,932 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,932 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,932 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,932 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rahpq9u1
2023-06-03 05:39:32,933 - distributed.worker - INFO - Starting Worker plugin PreImport-1bd96de0-bcb1-4717-9a92-da4aa141847d
2023-06-03 05:39:32,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eecb5aeb-8872-47fa-b4c9-c677cd5d7f5f
2023-06-03 05:39:32,933 - distributed.worker - INFO - Starting Worker plugin RMMSetup-828b2c1e-8b13-4e8a-89ff-77a4370966cd
2023-06-03 05:39:32,936 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45919
2023-06-03 05:39:32,936 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45919
2023-06-03 05:39:32,936 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33817
2023-06-03 05:39:32,936 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,936 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,936 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,936 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,936 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k45n46ib
2023-06-03 05:39:32,937 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cefe409-64fb-4efa-8259-a390d187d239
2023-06-03 05:39:32,961 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38725
2023-06-03 05:39:32,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38725
2023-06-03 05:39:32,962 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36305
2023-06-03 05:39:32,962 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,962 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,962 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,962 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,962 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-inc_9x_f
2023-06-03 05:39:32,962 - distributed.worker - INFO - Starting Worker plugin PreImport-cda9e4d8-c702-4bb5-9fa9-9720e55a0cf0
2023-06-03 05:39:32,962 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-322bb6e0-2c55-4481-b5db-c377ab7fb471
2023-06-03 05:39:32,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9bf0b437-7b06-48cd-b452-72c6ad441c06
2023-06-03 05:39:32,963 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46395
2023-06-03 05:39:32,963 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46395
2023-06-03 05:39:32,963 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35627
2023-06-03 05:39:32,963 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45897
2023-06-03 05:39:32,963 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35627
2023-06-03 05:39:32,963 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,963 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34475
2023-06-03 05:39:32,963 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,963 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,963 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,963 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,963 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,963 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,963 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ycerp66l
2023-06-03 05:39:32,963 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,963 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j3luvyk8
2023-06-03 05:39:32,964 - distributed.worker - INFO - Starting Worker plugin PreImport-cf91d34b-1654-4f49-9349-c7acb2490ddb
2023-06-03 05:39:32,964 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c78ce9be-2b8d-42a9-b216-5688d004c118
2023-06-03 05:39:32,964 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd31c439-f7ae-451b-a280-33c5ad3d3a4c
2023-06-03 05:39:32,964 - distributed.worker - INFO - Starting Worker plugin RMMSetup-991525e9-523e-488b-b9c8-f0b1fdd3cd04
2023-06-03 05:39:32,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39069
2023-06-03 05:39:32,965 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39069
2023-06-03 05:39:32,965 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42413
2023-06-03 05:39:32,965 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:32,965 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:32,965 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:32,965 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:39:32,965 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fzn036k6
2023-06-03 05:39:32,966 - distributed.worker - INFO - Starting Worker plugin PreImport-0be52e6d-bac9-4a78-b31e-7be40a3e9492
2023-06-03 05:39:32,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07c69f09-1a50-413a-8a48-051c95a13491
2023-06-03 05:39:32,967 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8fc3fbad-5f52-4805-89ed-8d7385497a83
2023-06-03 05:39:33,098 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,115 - distributed.worker - INFO - Starting Worker plugin PreImport-b71ca29f-413f-44d2-a895-03450a81d34d
2023-06-03 05:39:33,115 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4386ead4-a645-44ac-bde0-3c679f8e27ac
2023-06-03 05:39:33,115 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a1174a7-ee37-4167-a9e6-0c4f4c524734
2023-06-03 05:39:33,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,115 - distributed.worker - INFO - Starting Worker plugin PreImport-09fc96be-dd1e-43f0-ace7-3c50240863b4
2023-06-03 05:39:33,115 - distributed.worker - INFO - Starting Worker plugin PreImport-ee1ea94a-5514-4b34-b45a-543f4c090f9d
2023-06-03 05:39:33,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,116 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ad60ca2-f679-4b32-9c20-554a67bb2e6d
2023-06-03 05:39:33,116 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,116 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,116 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41827', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41827
2023-06-03 05:39:33,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43598
2023-06-03 05:39:33,130 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,130 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,141 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38725', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,141 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38725
2023-06-03 05:39:33,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43622
2023-06-03 05:39:33,142 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,142 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,142 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46395', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,143 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46395
2023-06-03 05:39:33,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43630
2023-06-03 05:39:33,143 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,144 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,144 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34237', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,144 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34237
2023-06-03 05:39:33,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43608
2023-06-03 05:39:33,145 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,145 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,145 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35627', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,146 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,147 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,147 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35627
2023-06-03 05:39:33,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43636
2023-06-03 05:39:33,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,150 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39069', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,151 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39069
2023-06-03 05:39:33,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43646
2023-06-03 05:39:33,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45919', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,152 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,152 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,152 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45919
2023-06-03 05:39:33,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43674
2023-06-03 05:39:33,153 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,153 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38019', status: init, memory: 0, processing: 0>
2023-06-03 05:39:33,153 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,153 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38019
2023-06-03 05:39:33,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43660
2023-06-03 05:39:33,154 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:33,154 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:33,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:33,163 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,163 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,163 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:39:33,179 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,179 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:33,183 - distributed.scheduler - INFO - Remove client Client-009dce73-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:33,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38820; closing.
2023-06-03 05:39:33,184 - distributed.scheduler - INFO - Remove client Client-009dce73-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:33,184 - distributed.scheduler - INFO - Close client connection: Client-009dce73-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:33,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38483'. Reason: nanny-close
2023-06-03 05:39:33,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33523'. Reason: nanny-close
2023-06-03 05:39:33,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32845'. Reason: nanny-close
2023-06-03 05:39:33,187 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39069. Reason: nanny-close
2023-06-03 05:39:33,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45407'. Reason: nanny-close
2023-06-03 05:39:33,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46395. Reason: nanny-close
2023-06-03 05:39:33,188 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42709'. Reason: nanny-close
2023-06-03 05:39:33,188 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34355'. Reason: nanny-close
2023-06-03 05:39:33,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,189 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34237. Reason: nanny-close
2023-06-03 05:39:33,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35837'. Reason: nanny-close
2023-06-03 05:39:33,189 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,189 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43646; closing.
2023-06-03 05:39:33,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41827. Reason: nanny-close
2023-06-03 05:39:33,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40393'. Reason: nanny-close
2023-06-03 05:39:33,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,190 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39069', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,190 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39069
2023-06-03 05:39:33,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38725. Reason: nanny-close
2023-06-03 05:39:33,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,191 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,191 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43630; closing.
2023-06-03 05:39:33,191 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,191 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35627. Reason: nanny-close
2023-06-03 05:39:33,192 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,192 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39069
2023-06-03 05:39:33,192 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39069
2023-06-03 05:39:33,192 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39069
2023-06-03 05:39:33,192 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39069
2023-06-03 05:39:33,192 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46395', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,192 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46395
2023-06-03 05:39:33,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,192 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43608; closing.
2023-06-03 05:39:33,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,193 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39069
2023-06-03 05:39:33,193 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34237', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,193 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34237
2023-06-03 05:39:33,193 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,193 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,194 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43622; closing.
2023-06-03 05:39:33,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43598; closing.
2023-06-03 05:39:33,194 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38725', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,195 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38725
2023-06-03 05:39:33,195 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41827', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,195 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41827
2023-06-03 05:39:33,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43636; closing.
2023-06-03 05:39:33,196 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35627', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,196 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35627
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46395
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46395
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34237
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34237
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38725
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38725
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41827
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41827
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35627
2023-06-03 05:39:33,197 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35627
2023-06-03 05:39:33,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:33,205 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45919. Reason: nanny-close
2023-06-03 05:39:33,206 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38019. Reason: nanny-close
2023-06-03 05:39:33,207 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43674; closing.
2023-06-03 05:39:33,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,207 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45919', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,207 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45919
2023-06-03 05:39:33,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:33,208 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43660; closing.
2023-06-03 05:39:33,208 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38019', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:33,208 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38019
2023-06-03 05:39:33,208 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:33,208 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:33,209 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:34,452 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:34,452 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:34,453 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:34,453 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:34,454 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-03 05:39:36,298 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:36,303 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45161 instead
  warnings.warn(
2023-06-03 05:39:36,306 - distributed.scheduler - INFO - State start
2023-06-03 05:39:36,326 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:36,327 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:39:36,327 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45161/status
2023-06-03 05:39:36,526 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39405'
2023-06-03 05:39:36,920 - distributed.scheduler - INFO - Receive client connection: Client-058af701-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:36,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43784
2023-06-03 05:39:37,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:37,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:37,999 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:38,835 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40655
2023-06-03 05:39:38,835 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40655
2023-06-03 05:39:38,835 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40639
2023-06-03 05:39:38,835 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:38,835 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:38,835 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:38,835 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:39:38,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uo6y3lbi
2023-06-03 05:39:38,835 - distributed.worker - INFO - Starting Worker plugin PreImport-98b03fbb-3fe7-48ed-8c63-7706c9051d1d
2023-06-03 05:39:38,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a91215a7-6e55-4b34-b53f-3c9dc992f7b6
2023-06-03 05:39:38,836 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8531aac-1845-4466-9eda-e2eb5993cdeb
2023-06-03 05:39:39,025 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:39,051 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40655', status: init, memory: 0, processing: 0>
2023-06-03 05:39:39,052 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40655
2023-06-03 05:39:39,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43806
2023-06-03 05:39:39,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:39,053 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:39,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:39,071 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:39:39,074 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:39,075 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:39,077 - distributed.scheduler - INFO - Remove client Client-058af701-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:39,078 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43784; closing.
2023-06-03 05:39:39,078 - distributed.scheduler - INFO - Remove client Client-058af701-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:39,078 - distributed.scheduler - INFO - Close client connection: Client-058af701-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:39,079 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39405'. Reason: nanny-close
2023-06-03 05:39:39,079 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:39,081 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40655. Reason: nanny-close
2023-06-03 05:39:39,082 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:39,082 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43806; closing.
2023-06-03 05:39:39,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40655', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:39,083 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40655
2023-06-03 05:39:39,083 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:39,083 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:40,045 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:40,046 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:40,046 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:40,047 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:40,047 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-03 05:39:41,825 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:41,829 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34609 instead
  warnings.warn(
2023-06-03 05:39:41,833 - distributed.scheduler - INFO - State start
2023-06-03 05:39:41,861 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:39:41,862 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:39:41,863 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34609/status
2023-06-03 05:39:41,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33833'
2023-06-03 05:39:42,373 - distributed.scheduler - INFO - Receive client connection: Client-08da4f29-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:42,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52564
2023-06-03 05:39:43,325 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:43,325 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:43,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:39:44,111 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44933
2023-06-03 05:39:44,111 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44933
2023-06-03 05:39:44,111 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43997
2023-06-03 05:39:44,111 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:39:44,111 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:44,111 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:39:44,111 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:39:44,111 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_vbx4b51
2023-06-03 05:39:44,111 - distributed.worker - INFO - Starting Worker plugin PreImport-ba012722-483c-4084-8ec8-d6f314f47a89
2023-06-03 05:39:44,111 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d225000-7e40-4b6e-b2d3-0f5489c67df6
2023-06-03 05:39:44,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3d812fd-bfbc-4857-b2f1-4ff911fe251a
2023-06-03 05:39:44,206 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:44,227 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44933', status: init, memory: 0, processing: 0>
2023-06-03 05:39:44,227 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44933
2023-06-03 05:39:44,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52582
2023-06-03 05:39:44,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:39:44,228 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:39:44,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:39:44,314 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-06-03 05:39:44,318 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:39:44,321 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:44,322 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:39:44,325 - distributed.scheduler - INFO - Remove client Client-08da4f29-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:44,325 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52564; closing.
2023-06-03 05:39:44,325 - distributed.scheduler - INFO - Remove client Client-08da4f29-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:44,325 - distributed.scheduler - INFO - Close client connection: Client-08da4f29-01d1-11ee-8bc5-d8c49764f6bb
2023-06-03 05:39:44,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33833'. Reason: nanny-close
2023-06-03 05:39:44,327 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:39:44,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44933. Reason: nanny-close
2023-06-03 05:39:44,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:39:44,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52582; closing.
2023-06-03 05:39:44,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44933', status: closing, memory: 0, processing: 0>
2023-06-03 05:39:44,330 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44933
2023-06-03 05:39:44,330 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:39:44,331 - distributed.nanny - INFO - Worker closed
2023-06-03 05:39:45,242 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:39:45,243 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:39:45,243 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:39:45,244 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:39:45,244 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39535 instead
  warnings.warn(
2023-06-03 05:39:54,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:39:54,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:39:54,194 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43127 instead
  warnings.warn(
2023-06-03 05:40:03,531 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,531 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:03,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:03,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41123 instead
  warnings.warn(
2023-06-03 05:40:14,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,024 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,024 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,058 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,059 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,067 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,067 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:14,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:14,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35031 instead
  warnings.warn(
2023-06-03 05:40:26,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:26,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:26,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:26,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:26,994 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:26,994 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:27,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:27,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:27,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:27,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:27,013 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:27,013 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:27,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:27,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:27,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:27,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33563 instead
  warnings.warn(
2023-06-03 05:40:43,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,040 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:43,073 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:43,073 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] 2023-06-03 05:40:56,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:56,175 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:56,175 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38293 instead
  warnings.warn(
2023-06-03 05:41:11,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,392 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37911 instead
  warnings.warn(
2023-06-03 05:41:27,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,058 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:27,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:27,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44995 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35671 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37281 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39291 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42277 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46171 instead
  warnings.warn(
2023-06-03 05:43:13,913 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-03 05:43:13,922 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f30c55224c0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
