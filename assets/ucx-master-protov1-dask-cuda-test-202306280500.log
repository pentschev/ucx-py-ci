============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-28 05:37:58,716 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:37:58,720 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38921 instead
  warnings.warn(
2023-06-28 05:37:58,723 - distributed.scheduler - INFO - State start
2023-06-28 05:37:58,743 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:37:58,744 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-28 05:37:58,745 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38921/status
2023-06-28 05:37:58,893 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38035'
2023-06-28 05:37:58,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42559'
2023-06-28 05:37:58,914 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44155'
2023-06-28 05:37:58,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36361'
2023-06-28 05:38:00,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:00,405 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:00,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:00,414 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:00,414 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:00,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:00,417 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:00,421 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:00,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:00,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:00,424 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:00,430 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-28 05:38:00,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45691
2023-06-28 05:38:00,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45691
2023-06-28 05:38:00,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45255
2023-06-28 05:38:00,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:00,547 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:00,547 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:00,547 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:00,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ywpry5yd
2023-06-28 05:38:00,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0995ab45-3577-4085-9cee-6ce95a01f665
2023-06-28 05:38:00,548 - distributed.worker - INFO - Starting Worker plugin PreImport-8d8f8547-39da-4b63-b795-19c09ca07ddf
2023-06-28 05:38:00,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-444f67f5-fc93-449d-99fc-c0f70ed88206
2023-06-28 05:38:00,548 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:00,565 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45691', status: init, memory: 0, processing: 0>
2023-06-28 05:38:00,577 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45691
2023-06-28 05:38:00,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53844
2023-06-28 05:38:00,577 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:00,577 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:00,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:01,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34187
2023-06-28 05:38:01,866 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34187
2023-06-28 05:38:01,866 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42295
2023-06-28 05:38:01,866 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:01,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,867 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:01,867 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:01,867 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hoj3jlpf
2023-06-28 05:38:01,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-286cc4b7-91c1-4b1b-a640-ac671a934c02
2023-06-28 05:38:01,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af9260bb-1271-4318-adf8-607e1c892d33
2023-06-28 05:38:01,867 - distributed.worker - INFO - Starting Worker plugin PreImport-03ecc766-d25d-430e-8d09-44cadb0d9807
2023-06-28 05:38:01,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34815
2023-06-28 05:38:01,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34815
2023-06-28 05:38:01,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40341
2023-06-28 05:38:01,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:01,874 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,874 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:01,874 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:01,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l205telk
2023-06-28 05:38:01,875 - distributed.worker - INFO - Starting Worker plugin PreImport-55b8bd24-a6ff-4731-8e13-f786ba36f508
2023-06-28 05:38:01,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fdfdf116-3beb-42cb-b70b-e1d308902760
2023-06-28 05:38:01,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3c0d990-4317-4ae5-a28d-8f9105c77b01
2023-06-28 05:38:01,875 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,876 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40203
2023-06-28 05:38:01,876 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40203
2023-06-28 05:38:01,876 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36489
2023-06-28 05:38:01,877 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:01,877 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,877 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:01,877 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:01,877 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-24f19x9y
2023-06-28 05:38:01,877 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58b016e3-3316-438a-a523-01fb28741d06
2023-06-28 05:38:01,877 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bed2d17b-e0f5-43d1-b44e-d09eef92e35d
2023-06-28 05:38:01,877 - distributed.worker - INFO - Starting Worker plugin PreImport-2009a14a-900f-47ab-a8b6-bd949e6d6399
2023-06-28 05:38:01,878 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34187', status: init, memory: 0, processing: 0>
2023-06-28 05:38:01,889 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34187
2023-06-28 05:38:01,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53858
2023-06-28 05:38:01,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:01,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:01,897 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34815', status: init, memory: 0, processing: 0>
2023-06-28 05:38:01,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34815
2023-06-28 05:38:01,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53862
2023-06-28 05:38:01,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:01,898 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:01,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40203', status: init, memory: 0, processing: 0>
2023-06-28 05:38:01,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40203
2023-06-28 05:38:01,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53866
2023-06-28 05:38:01,911 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:01,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:01,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:03,204 - distributed.scheduler - INFO - Receive client connection: Client-efb321a4-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:03,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53912
2023-06-28 05:38:03,213 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:03,214 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:03,214 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:03,214 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:03,218 - distributed.scheduler - INFO - Remove client Client-efb321a4-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:03,218 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53912; closing.
2023-06-28 05:38:03,218 - distributed.scheduler - INFO - Remove client Client-efb321a4-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:03,219 - distributed.scheduler - INFO - Close client connection: Client-efb321a4-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:03,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42559'. Reason: nanny-close
2023-06-28 05:38:03,220 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:03,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38035'. Reason: nanny-close
2023-06-28 05:38:03,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:03,222 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34187. Reason: nanny-close
2023-06-28 05:38:03,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44155'. Reason: nanny-close
2023-06-28 05:38:03,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:03,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40203. Reason: nanny-close
2023-06-28 05:38:03,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36361'. Reason: nanny-close
2023-06-28 05:38:03,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:03,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34815. Reason: nanny-close
2023-06-28 05:38:03,224 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:03,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53858; closing.
2023-06-28 05:38:03,224 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34187', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:03,224 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34187
2023-06-28 05:38:03,224 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45691. Reason: nanny-close
2023-06-28 05:38:03,224 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:03,225 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:03,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:03,225 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53866; closing.
2023-06-28 05:38:03,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40203', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:03,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40203
2023-06-28 05:38:03,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34187
2023-06-28 05:38:03,226 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:03,226 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53862; closing.
2023-06-28 05:38:03,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:03,227 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:03,227 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34815', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:03,227 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34815
2023-06-28 05:38:03,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53844; closing.
2023-06-28 05:38:03,228 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:03,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45691', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:03,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45691
2023-06-28 05:38:03,229 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:04,237 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:04,237 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:04,237 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:04,238 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-28 05:38:04,239 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-28 05:38:06,193 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:06,198 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-28 05:38:06,201 - distributed.scheduler - INFO - State start
2023-06-28 05:38:06,222 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:06,223 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:06,224 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-28 05:38:06,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45963'
2023-06-28 05:38:06,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41937'
2023-06-28 05:38:06,413 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44211'
2023-06-28 05:38:06,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45107'
2023-06-28 05:38:06,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43029'
2023-06-28 05:38:06,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45527'
2023-06-28 05:38:06,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40995'
2023-06-28 05:38:06,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38813'
2023-06-28 05:38:06,625 - distributed.scheduler - INFO - Receive client connection: Client-f4204f54-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:06,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57948
2023-06-28 05:38:07,912 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:07,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:07,935 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,202 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,202 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:08,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:08,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,216 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,216 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:08,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:10,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33263
2023-06-28 05:38:10,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33263
2023-06-28 05:38:10,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36099
2023-06-28 05:38:10,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:10,095 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:10,095 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:10,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:10,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6kkrgmpy
2023-06-28 05:38:10,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d61bdce-fb4f-404d-8e0a-d0612b26e4d3
2023-06-28 05:38:10,453 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6eefd47b-0a9b-46ed-998c-0173d4628b0a
2023-06-28 05:38:10,457 - distributed.worker - INFO - Starting Worker plugin PreImport-b2ffc17d-dd3f-4966-8854-9bdde669d573
2023-06-28 05:38:10,458 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:10,495 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33263', status: init, memory: 0, processing: 0>
2023-06-28 05:38:10,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33263
2023-06-28 05:38:10,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46638
2023-06-28 05:38:10,498 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:10,498 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:10,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32865
2023-06-28 05:38:11,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32865
2023-06-28 05:38:11,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36071
2023-06-28 05:38:11,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,365 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,365 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,366 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,366 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eq0y6jyc
2023-06-28 05:38:11,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86f53f89-ec75-498c-a026-ab0e79d38f23
2023-06-28 05:38:11,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43101
2023-06-28 05:38:11,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43101
2023-06-28 05:38:11,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45693
2023-06-28 05:38:11,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45785
2023-06-28 05:38:11,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45693
2023-06-28 05:38:11,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43391
2023-06-28 05:38:11,369 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,369 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,369 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,369 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w387xcn3
2023-06-28 05:38:11,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4rlkhs_v
2023-06-28 05:38:11,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a4d1246-2901-4402-a52a-7c154c9b479a
2023-06-28 05:38:11,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73563710-2a4c-44e9-b22d-591b2ad2bf95
2023-06-28 05:38:11,370 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41049
2023-06-28 05:38:11,370 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41049
2023-06-28 05:38:11,371 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42491
2023-06-28 05:38:11,371 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,371 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,371 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,371 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,371 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-irlj3baf
2023-06-28 05:38:11,371 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7724d913-faaf-4631-aea6-9cd887c9d66f
2023-06-28 05:38:11,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43861
2023-06-28 05:38:11,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43861
2023-06-28 05:38:11,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32849
2023-06-28 05:38:11,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,372 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,372 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,372 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fhjy8q27
2023-06-28 05:38:11,372 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40537
2023-06-28 05:38:11,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40537
2023-06-28 05:38:11,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36341
2023-06-28 05:38:11,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,372 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-172903d9-00ff-4b67-a442-3657f1ee8741
2023-06-28 05:38:11,372 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q4_238_w
2023-06-28 05:38:11,373 - distributed.worker - INFO - Starting Worker plugin PreImport-2d7dfc17-7d7b-40d4-9ea0-b66cb269a8e4
2023-06-28 05:38:11,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42381
2023-06-28 05:38:11,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42381
2023-06-28 05:38:11,373 - distributed.worker - INFO - Starting Worker plugin PreImport-f73337d4-4b19-4c0a-9957-3a7592d144b1
2023-06-28 05:38:11,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38201
2023-06-28 05:38:11,373 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c449950-51a3-4172-8960-3e582ae3d226
2023-06-28 05:38:11,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,373 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,373 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1ab8d11-f07f-45c5-9d83-1b0ac348a6ee
2023-06-28 05:38:11,373 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:11,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:11,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7otpsk96
2023-06-28 05:38:11,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f061909-01aa-4639-ba83-e9740f93a794
2023-06-28 05:38:11,504 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,515 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-edcd59b7-9c7b-44e7-9387-7be0265799a1
2023-06-28 05:38:11,515 - distributed.worker - INFO - Starting Worker plugin PreImport-1c4e2cc0-5c5d-4054-af84-ed421d0523dc
2023-06-28 05:38:11,516 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f94c8323-b6e6-41cb-9c2f-ca6f224c49ba
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d957245-b1ba-476a-b3e6-7ad03d909508
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8810e04-0b20-4781-8f05-df661b72a9e3
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin PreImport-2cd27e67-d872-422d-9bdf-4bf26659c426
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8d0318b-f43c-4779-a24d-c48f0f28f77f
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin PreImport-caf8950d-daf2-4e94-9b6e-0a58844882bc
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab62426f-8fab-4389-8ca2-1f7e1c85ef3d
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin PreImport-9f6299b3-2385-48ce-a8b0-3af2c9e9b96f
2023-06-28 05:38:11,530 - distributed.worker - INFO - Starting Worker plugin PreImport-43493bb7-e356-4cc1-b976-5dd12827c571
2023-06-28 05:38:11,530 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,530 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,530 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,531 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,531 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43861', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43861
2023-06-28 05:38:11,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46654
2023-06-28 05:38:11,538 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,539 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,541 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43101', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,542 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43101
2023-06-28 05:38:11,542 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46656
2023-06-28 05:38:11,542 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,543 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,557 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41049', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,557 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41049
2023-06-28 05:38:11,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46660
2023-06-28 05:38:11,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,558 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,558 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45693', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,559 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45693
2023-06-28 05:38:11,559 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46672
2023-06-28 05:38:11,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,559 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42381', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,564 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42381
2023-06-28 05:38:11,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46676
2023-06-28 05:38:11,565 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32865', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,565 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,565 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32865
2023-06-28 05:38:11,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46680
2023-06-28 05:38:11,566 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,567 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,570 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40537', status: init, memory: 0, processing: 0>
2023-06-28 05:38:11,571 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40537
2023-06-28 05:38:11,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46684
2023-06-28 05:38:11,572 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:11,572 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:11,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:11,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,654 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:11,659 - distributed.scheduler - INFO - Remove client Client-f4204f54-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:11,659 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57948; closing.
2023-06-28 05:38:11,660 - distributed.scheduler - INFO - Remove client Client-f4204f54-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:11,660 - distributed.scheduler - INFO - Close client connection: Client-f4204f54-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:11,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45107'. Reason: nanny-close
2023-06-28 05:38:11,661 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43029'. Reason: nanny-close
2023-06-28 05:38:11,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45963'. Reason: nanny-close
2023-06-28 05:38:11,663 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40537. Reason: nanny-close
2023-06-28 05:38:11,663 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41937'. Reason: nanny-close
2023-06-28 05:38:11,663 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32865. Reason: nanny-close
2023-06-28 05:38:11,664 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44211'. Reason: nanny-close
2023-06-28 05:38:11,664 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33263. Reason: nanny-close
2023-06-28 05:38:11,664 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,665 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41049. Reason: nanny-close
2023-06-28 05:38:11,665 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45527'. Reason: nanny-close
2023-06-28 05:38:11,665 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46684; closing.
2023-06-28 05:38:11,665 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,665 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40995'. Reason: nanny-close
2023-06-28 05:38:11,665 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43861. Reason: nanny-close
2023-06-28 05:38:11,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40537', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,665 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,666 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,666 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40537
2023-06-28 05:38:11,666 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38813'. Reason: nanny-close
2023-06-28 05:38:11,666 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42381. Reason: nanny-close
2023-06-28 05:38:11,666 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:11,666 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,666 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,666 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45693. Reason: nanny-close
2023-06-28 05:38:11,666 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46680; closing.
2023-06-28 05:38:11,667 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,667 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,667 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43101. Reason: nanny-close
2023-06-28 05:38:11,667 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32865', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,667 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,668 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32865
2023-06-28 05:38:11,668 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40537
2023-06-28 05:38:11,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46660; closing.
2023-06-28 05:38:11,668 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40537
2023-06-28 05:38:11,668 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,668 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40537
2023-06-28 05:38:11,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46638; closing.
2023-06-28 05:38:11,668 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,669 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,669 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,669 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40537
2023-06-28 05:38:11,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41049', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,669 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41049
2023-06-28 05:38:11,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33263', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,669 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33263
2023-06-28 05:38:11,669 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,670 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:11,670 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,670 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,670 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46654; closing.
2023-06-28 05:38:11,670 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46676; closing.
2023-06-28 05:38:11,671 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46672; closing.
2023-06-28 05:38:11,671 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:11,671 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43861', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,671 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43861
2023-06-28 05:38:11,672 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42381', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,672 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42381
2023-06-28 05:38:11,672 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45693', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,672 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45693
2023-06-28 05:38:11,672 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46656; closing.
2023-06-28 05:38:11,673 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43101', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:11,673 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43101
2023-06-28 05:38:11,673 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:13,128 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:13,129 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:13,129 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:13,130 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:13,130 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-28 05:38:15,059 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:15,063 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-28 05:38:15,066 - distributed.scheduler - INFO - State start
2023-06-28 05:38:15,174 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:15,175 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:15,175 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-28 05:38:15,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36359'
2023-06-28 05:38:15,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42775'
2023-06-28 05:38:15,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45697'
2023-06-28 05:38:15,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33845'
2023-06-28 05:38:15,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38175'
2023-06-28 05:38:15,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38901'
2023-06-28 05:38:15,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44305'
2023-06-28 05:38:15,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36485'
2023-06-28 05:38:17,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,095 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,124 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,145 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,151 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:17,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:17,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:17,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:19,427 - distributed.scheduler - INFO - Receive client connection: Client-f96f8731-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:19,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46894
2023-06-28 05:38:19,739 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45765
2023-06-28 05:38:19,740 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45765
2023-06-28 05:38:19,740 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44407
2023-06-28 05:38:19,740 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,740 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,740 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,740 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,740 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7csf7i_1
2023-06-28 05:38:19,741 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ed5b0d0-44c6-4aa7-8b84-a200d9fe0166
2023-06-28 05:38:19,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c0f0e97-c909-4570-9a66-c4c1a5f0ba90
2023-06-28 05:38:19,747 - distributed.worker - INFO - Starting Worker plugin PreImport-ef9d0f97-e868-4597-88e2-beb493a99e66
2023-06-28 05:38:19,747 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,780 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45765', status: init, memory: 0, processing: 0>
2023-06-28 05:38:19,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45765
2023-06-28 05:38:19,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46918
2023-06-28 05:38:19,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,782 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:19,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38099
2023-06-28 05:38:19,927 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38099
2023-06-28 05:38:19,927 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43921
2023-06-28 05:38:19,927 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,927 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,927 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,927 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vwj6yndc
2023-06-28 05:38:19,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7733384e-d5fc-4d55-956d-3ea0fd713ccc
2023-06-28 05:38:19,931 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38595
2023-06-28 05:38:19,931 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38595
2023-06-28 05:38:19,931 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37737
2023-06-28 05:38:19,931 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,931 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,931 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,931 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0nc1qimy
2023-06-28 05:38:19,932 - distributed.worker - INFO - Starting Worker plugin PreImport-889d9b1c-a275-41b5-90e9-13b3ded108e3
2023-06-28 05:38:19,932 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a325374-e51c-4d44-964a-9d62566cf686
2023-06-28 05:38:19,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45629
2023-06-28 05:38:19,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45629
2023-06-28 05:38:19,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33237
2023-06-28 05:38:19,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,933 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,933 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,933 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-riqvi78g
2023-06-28 05:38:19,934 - distributed.worker - INFO - Starting Worker plugin PreImport-eaff6c73-c0ef-4189-88d5-43003864df5a
2023-06-28 05:38:19,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9aa56c1d-bb1a-46fc-a3ec-2938e51a08b6
2023-06-28 05:38:19,936 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41139
2023-06-28 05:38:19,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41139
2023-06-28 05:38:19,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35359
2023-06-28 05:38:19,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,937 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,937 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,937 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pwk_c9jc
2023-06-28 05:38:19,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aaf14369-268e-4a99-8282-610df011a6ee
2023-06-28 05:38:19,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38c17d81-8923-4229-8964-6e126472e7bc
2023-06-28 05:38:19,940 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42565
2023-06-28 05:38:19,941 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42565
2023-06-28 05:38:19,940 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38361
2023-06-28 05:38:19,941 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32905
2023-06-28 05:38:19,941 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38361
2023-06-28 05:38:19,941 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,941 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45403
2023-06-28 05:38:19,941 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,941 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,941 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,941 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,941 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,941 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,941 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iy0h45ny
2023-06-28 05:38:19,941 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,941 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bz9rblzp
2023-06-28 05:38:19,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be8b863c-ef19-45cb-a328-e2e04abcd0e2
2023-06-28 05:38:19,942 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1537c51a-9c0d-4c74-9913-42497652c7c0
2023-06-28 05:38:19,950 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36735
2023-06-28 05:38:19,951 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36735
2023-06-28 05:38:19,951 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35473
2023-06-28 05:38:19,951 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:19,951 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,951 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:19,951 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:19,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4j98qhw6
2023-06-28 05:38:19,951 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b811e5d-ebde-47ef-a1f5-6b1811bd050c
2023-06-28 05:38:19,951 - distributed.worker - INFO - Starting Worker plugin RMMSetup-471a3be0-0fff-46b3-ad2f-d469bdcd3456
2023-06-28 05:38:19,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40188a4a-5791-4532-bf4c-b1b949b84a79
2023-06-28 05:38:19,988 - distributed.worker - INFO - Starting Worker plugin PreImport-d4ffa68a-be45-4240-8463-9e4348325707
2023-06-28 05:38:19,988 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,989 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56e4beb3-19eb-4fb2-a861-22e13ff5b551
2023-06-28 05:38:19,989 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,989 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7293224f-9547-49b2-b37b-f2bf2ec1f5d2
2023-06-28 05:38:19,990 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,990 - distributed.worker - INFO - Starting Worker plugin PreImport-00aab7ba-e52f-4f29-9d16-6336e2342915
2023-06-28 05:38:19,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,991 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5859c195-8b36-4384-8dd1-7e0c651570a4
2023-06-28 05:38:19,991 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e787b9c9-bd1b-4e6e-b047-4b4a5aa894d4
2023-06-28 05:38:19,991 - distributed.worker - INFO - Starting Worker plugin PreImport-252f19da-fdd2-425f-b02c-a5ed053d4be2
2023-06-28 05:38:19,991 - distributed.worker - INFO - Starting Worker plugin PreImport-6a60ba8b-855c-49b2-998a-e27fe4860646
2023-06-28 05:38:19,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:19,991 - distributed.worker - INFO - Starting Worker plugin PreImport-160cbf61-bbbd-4ea0-9148-76b4719e4545
2023-06-28 05:38:19,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42565', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42565
2023-06-28 05:38:20,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42686
2023-06-28 05:38:20,016 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36735', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,016 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36735
2023-06-28 05:38:20,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42710
2023-06-28 05:38:20,017 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,017 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,020 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38361', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,021 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38361
2023-06-28 05:38:20,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42692
2023-06-28 05:38:20,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,022 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38099', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38099
2023-06-28 05:38:20,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42648
2023-06-28 05:38:20,026 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38595', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,026 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,027 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38595
2023-06-28 05:38:20,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42662
2023-06-28 05:38:20,027 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,028 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,028 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41139', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41139
2023-06-28 05:38:20,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42698
2023-06-28 05:38:20,029 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45629', status: init, memory: 0, processing: 0>
2023-06-28 05:38:20,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,029 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,029 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45629
2023-06-28 05:38:20,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42670
2023-06-28 05:38:20,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,030 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:20,030 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:20,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,033 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:20,064 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,065 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,065 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,065 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,065 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,065 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,065 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,066 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:20,070 - distributed.scheduler - INFO - Remove client Client-f96f8731-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:20,070 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46894; closing.
2023-06-28 05:38:20,070 - distributed.scheduler - INFO - Remove client Client-f96f8731-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:20,070 - distributed.scheduler - INFO - Close client connection: Client-f96f8731-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:20,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33845'. Reason: nanny-close
2023-06-28 05:38:20,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38175'. Reason: nanny-close
2023-06-28 05:38:20,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,073 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45629. Reason: nanny-close
2023-06-28 05:38:20,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36359'. Reason: nanny-close
2023-06-28 05:38:20,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,074 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38099. Reason: nanny-close
2023-06-28 05:38:20,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42775'. Reason: nanny-close
2023-06-28 05:38:20,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,075 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38361. Reason: nanny-close
2023-06-28 05:38:20,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45697'. Reason: nanny-close
2023-06-28 05:38:20,075 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,075 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42565. Reason: nanny-close
2023-06-28 05:38:20,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38901'. Reason: nanny-close
2023-06-28 05:38:20,076 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,076 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42670; closing.
2023-06-28 05:38:20,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44305'. Reason: nanny-close
2023-06-28 05:38:20,076 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38595. Reason: nanny-close
2023-06-28 05:38:20,076 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45629', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,077 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45629
2023-06-28 05:38:20,077 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36485'. Reason: nanny-close
2023-06-28 05:38:20,077 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41139. Reason: nanny-close
2023-06-28 05:38:20,077 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:20,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,077 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,077 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45765. Reason: nanny-close
2023-06-28 05:38:20,077 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42648; closing.
2023-06-28 05:38:20,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,078 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,078 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45629
2023-06-28 05:38:20,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38099', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,078 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45629
2023-06-28 05:38:20,078 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38099
2023-06-28 05:38:20,078 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36735. Reason: nanny-close
2023-06-28 05:38:20,079 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42692; closing.
2023-06-28 05:38:20,079 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45629
2023-06-28 05:38:20,079 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45629
2023-06-28 05:38:20,079 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38361', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,080 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38361
2023-06-28 05:38:20,080 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,080 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42686; closing.
2023-06-28 05:38:20,080 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:20,080 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42565', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,081 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42565
2023-06-28 05:38:20,081 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42662; closing.
2023-06-28 05:38:20,081 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42698; closing.
2023-06-28 05:38:20,081 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:20,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46918; closing.
2023-06-28 05:38:20,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38595', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,082 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38595
2023-06-28 05:38:20,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41139', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,082 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41139
2023-06-28 05:38:20,083 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45765', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,083 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45765
2023-06-28 05:38:20,083 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42710; closing.
2023-06-28 05:38:20,084 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36735', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:20,084 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36735
2023-06-28 05:38:20,084 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:21,539 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:21,539 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:21,540 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:21,540 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:21,541 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-28 05:38:23,419 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:23,423 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45479 instead
  warnings.warn(
2023-06-28 05:38:23,427 - distributed.scheduler - INFO - State start
2023-06-28 05:38:23,446 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:23,447 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:23,447 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45479/status
2023-06-28 05:38:23,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35777'
2023-06-28 05:38:23,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38535'
2023-06-28 05:38:23,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35885'
2023-06-28 05:38:23,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45239'
2023-06-28 05:38:23,644 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35743'
2023-06-28 05:38:23,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36921'
2023-06-28 05:38:23,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44705'
2023-06-28 05:38:23,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33209'
2023-06-28 05:38:24,259 - distributed.scheduler - INFO - Receive client connection: Client-fe67dd93-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:24,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42860
2023-06-28 05:38:25,333 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:25,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:25,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,425 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:25,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:28,101 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43271
2023-06-28 05:38:28,101 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43271
2023-06-28 05:38:28,101 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38697
2023-06-28 05:38:28,101 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,101 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,101 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,101 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,101 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3j45z6mp
2023-06-28 05:38:28,102 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11a951fc-02d4-43a0-afbe-52420c6c4422
2023-06-28 05:38:28,105 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42035
2023-06-28 05:38:28,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42035
2023-06-28 05:38:28,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35771
2023-06-28 05:38:28,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,106 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,106 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,106 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gl963yio
2023-06-28 05:38:28,106 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1731cb3c-9cbb-42d7-b4aa-a21b67490e81
2023-06-28 05:38:28,113 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38103
2023-06-28 05:38:28,114 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38103
2023-06-28 05:38:28,114 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44079
2023-06-28 05:38:28,114 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,114 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,114 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,114 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-edlvi535
2023-06-28 05:38:28,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f679a25e-d9fa-4ba2-b588-15e5798d14ed
2023-06-28 05:38:28,166 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45105
2023-06-28 05:38:28,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45105
2023-06-28 05:38:28,167 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36153
2023-06-28 05:38:28,167 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34347
2023-06-28 05:38:28,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36153
2023-06-28 05:38:28,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,167 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,167 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40311
2023-06-28 05:38:28,167 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,167 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ccp4suuw
2023-06-28 05:38:28,167 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zvh7q997
2023-06-28 05:38:28,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-512faad1-4c8d-4da3-ab7e-beec54631e88
2023-06-28 05:38:28,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d74c871-c2de-41b8-9d3d-9efdec3590f8
2023-06-28 05:38:28,168 - distributed.worker - INFO - Starting Worker plugin PreImport-b9114755-3302-4716-8e36-7d7f63c2bf16
2023-06-28 05:38:28,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21b42fa2-3036-454e-bf15-50fc21eb3899
2023-06-28 05:38:28,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40703
2023-06-28 05:38:28,175 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40703
2023-06-28 05:38:28,175 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35863
2023-06-28 05:38:28,175 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,175 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,175 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,175 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xz1hkepo
2023-06-28 05:38:28,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d3e778b-90fa-4740-a8ee-332fd9f8c8c4
2023-06-28 05:38:28,176 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45779
2023-06-28 05:38:28,176 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45779
2023-06-28 05:38:28,176 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34881
2023-06-28 05:38:28,176 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,176 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,177 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,177 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,177 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lfiefxe4
2023-06-28 05:38:28,177 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65d53cc0-5a66-4b2a-b362-356b6774212a
2023-06-28 05:38:28,178 - distributed.worker - INFO - Starting Worker plugin PreImport-b170789a-6812-4766-9e1b-310341ee14f8
2023-06-28 05:38:28,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c1802b6-fb83-4aaa-b3c4-fd59df83b39d
2023-06-28 05:38:28,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36487
2023-06-28 05:38:28,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36487
2023-06-28 05:38:28,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34767
2023-06-28 05:38:28,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,182 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,182 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:28,182 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:28,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-moku9sfh
2023-06-28 05:38:28,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d66e1b9c-83c5-4340-bce0-7ac3bd3fb273
2023-06-28 05:38:28,370 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-046af628-6fcf-48f2-9ba6-556ee511d603
2023-06-28 05:38:28,371 - distributed.worker - INFO - Starting Worker plugin PreImport-ef529668-2fab-4385-ba96-24fd4eecfd21
2023-06-28 05:38:28,371 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-343c0963-25aa-4545-be92-836cf9342086
2023-06-28 05:38:28,380 - distributed.worker - INFO - Starting Worker plugin PreImport-ba7b2e5d-9641-4b48-bfd7-70eec49363bb
2023-06-28 05:38:28,380 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37f9b06f-fece-4ad8-b150-6050f6f66408
2023-06-28 05:38:28,386 - distributed.worker - INFO - Starting Worker plugin PreImport-ef6607db-511f-441e-9486-f6b5c5ad94aa
2023-06-28 05:38:28,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43271', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,398 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,399 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43271
2023-06-28 05:38:28,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42888
2023-06-28 05:38:28,399 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,400 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,405 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42035', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,406 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42035
2023-06-28 05:38:28,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42904
2023-06-28 05:38:28,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-885fb3a2-9ca0-4a41-bbf4-701f557bc24a
2023-06-28 05:38:28,407 - distributed.worker - INFO - Starting Worker plugin PreImport-05d1c610-0743-450f-b651-daaad2c5a8dc
2023-06-28 05:38:28,408 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,410 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-581ab0c4-1bae-48a6-ac23-accbe26f5994
2023-06-28 05:38:28,411 - distributed.worker - INFO - Starting Worker plugin PreImport-86ba2191-4bed-4b44-9813-0ce75e7acb0a
2023-06-28 05:38:28,412 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d42c39d2-0746-413e-847a-288d33592f34
2023-06-28 05:38:28,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,415 - distributed.worker - INFO - Starting Worker plugin PreImport-8cacf9a1-e7ef-440b-8a04-66365618bcfb
2023-06-28 05:38:28,415 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38103', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,433 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38103
2023-06-28 05:38:28,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42918
2023-06-28 05:38:28,434 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,434 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,435 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45105', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,435 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45105
2023-06-28 05:38:28,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42946
2023-06-28 05:38:28,436 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,436 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,436 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45779', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,437 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45779
2023-06-28 05:38:28,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42934
2023-06-28 05:38:28,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,438 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,438 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40703', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,440 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40703
2023-06-28 05:38:28,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42964
2023-06-28 05:38:28,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,441 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,441 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,448 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36487', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,449 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36487
2023-06-28 05:38:28,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42962
2023-06-28 05:38:28,449 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,450 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36153', status: init, memory: 0, processing: 0>
2023-06-28 05:38:28,458 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36153
2023-06-28 05:38:28,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42976
2023-06-28 05:38:28,459 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:28,459 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:28,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:28,506 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,506 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,506 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,506 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,507 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,507 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,507 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,507 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:28,517 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,518 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:38:28,524 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:38:28,526 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:38:28,528 - distributed.scheduler - INFO - Remove client Client-fe67dd93-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:28,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42860; closing.
2023-06-28 05:38:28,528 - distributed.scheduler - INFO - Remove client Client-fe67dd93-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:28,529 - distributed.scheduler - INFO - Close client connection: Client-fe67dd93-1575-11ee-8780-d8c49764f6bb
2023-06-28 05:38:28,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45239'. Reason: nanny-close
2023-06-28 05:38:28,530 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35777'. Reason: nanny-close
2023-06-28 05:38:28,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36153. Reason: nanny-close
2023-06-28 05:38:28,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38535'. Reason: nanny-close
2023-06-28 05:38:28,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35885'. Reason: nanny-close
2023-06-28 05:38:28,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38103. Reason: nanny-close
2023-06-28 05:38:28,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42035. Reason: nanny-close
2023-06-28 05:38:28,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35743'. Reason: nanny-close
2023-06-28 05:38:28,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43271. Reason: nanny-close
2023-06-28 05:38:28,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36921'. Reason: nanny-close
2023-06-28 05:38:28,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42976; closing.
2023-06-28 05:38:28,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44705'. Reason: nanny-close
2023-06-28 05:38:28,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45779. Reason: nanny-close
2023-06-28 05:38:28,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36153', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,535 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36153
2023-06-28 05:38:28,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36487. Reason: nanny-close
2023-06-28 05:38:28,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33209'. Reason: nanny-close
2023-06-28 05:38:28,536 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,536 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:28,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40703. Reason: nanny-close
2023-06-28 05:38:28,536 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,536 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42904; closing.
2023-06-28 05:38:28,537 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36153
2023-06-28 05:38:28,537 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,537 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42918; closing.
2023-06-28 05:38:28,537 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36153
2023-06-28 05:38:28,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42888; closing.
2023-06-28 05:38:28,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,537 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36153
2023-06-28 05:38:28,537 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36153
2023-06-28 05:38:28,538 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45105. Reason: nanny-close
2023-06-28 05:38:28,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42035', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,538 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42035
2023-06-28 05:38:28,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38103', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,538 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38103
2023-06-28 05:38:28,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43271', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,539 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,539 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43271
2023-06-28 05:38:28,539 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:28,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42934; closing.
2023-06-28 05:38:28,540 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42962; closing.
2023-06-28 05:38:28,540 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,540 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42964; closing.
2023-06-28 05:38:28,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45779', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,540 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:28,540 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45779
2023-06-28 05:38:28,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36487', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,541 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36487
2023-06-28 05:38:28,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40703', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,541 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40703
2023-06-28 05:38:28,542 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42946; closing.
2023-06-28 05:38:28,542 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45105', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:28,542 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45105
2023-06-28 05:38:28,542 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:29,997 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:29,997 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:29,998 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:29,999 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:29,999 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-28 05:38:31,759 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:31,763 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35637 instead
  warnings.warn(
2023-06-28 05:38:31,766 - distributed.scheduler - INFO - State start
2023-06-28 05:38:31,784 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:31,785 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:31,786 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35637/status
2023-06-28 05:38:32,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45389'
2023-06-28 05:38:32,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37751'
2023-06-28 05:38:32,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46335'
2023-06-28 05:38:32,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46525'
2023-06-28 05:38:32,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35919'
2023-06-28 05:38:32,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43537'
2023-06-28 05:38:32,171 - distributed.scheduler - INFO - Receive client connection: Client-037212bc-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:32,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38011'
2023-06-28 05:38:32,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45908
2023-06-28 05:38:32,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41753'
2023-06-28 05:38:33,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,880 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,882 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,896 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,929 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:33,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:33,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,935 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:33,988 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:36,474 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33863
2023-06-28 05:38:36,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33863
2023-06-28 05:38:36,474 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42287
2023-06-28 05:38:36,474 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,474 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,474 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,474 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_61o5v_k
2023-06-28 05:38:36,475 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c140b72-f3cb-47b0-81ea-d1f33b847777
2023-06-28 05:38:36,475 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44467
2023-06-28 05:38:36,475 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44467
2023-06-28 05:38:36,475 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41419
2023-06-28 05:38:36,475 - distributed.worker - INFO - Starting Worker plugin PreImport-1d374348-08af-416d-98ae-551438a7cd5e
2023-06-28 05:38:36,475 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,475 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e125724-0ffc-4872-b642-05ada5452287
2023-06-28 05:38:36,475 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,475 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,476 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8pcqhxqi
2023-06-28 05:38:36,476 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79996d33-cb9c-44f4-a6d0-1ffdc38a8d31
2023-06-28 05:38:36,478 - distributed.worker - INFO - Starting Worker plugin PreImport-5b2dbdaa-9fcc-441a-86f3-bc4e22c9e0fd
2023-06-28 05:38:36,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51da03aa-cede-448e-aa77-60f0dd1fcbd2
2023-06-28 05:38:36,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40435
2023-06-28 05:38:36,656 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40435
2023-06-28 05:38:36,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38039
2023-06-28 05:38:36,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,657 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,657 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dpt6jt__
2023-06-28 05:38:36,657 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26a1228f-3536-4e33-bab2-32722a51bef2
2023-06-28 05:38:36,660 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43213
2023-06-28 05:38:36,660 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43997
2023-06-28 05:38:36,660 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43213
2023-06-28 05:38:36,661 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43997
2023-06-28 05:38:36,661 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41181
2023-06-28 05:38:36,661 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41927
2023-06-28 05:38:36,661 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,661 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,661 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,661 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,661 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,661 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,661 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,661 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uyfhrgat
2023-06-28 05:38:36,661 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,661 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b50z6qed
2023-06-28 05:38:36,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e44e44cc-be5e-45d1-8e63-1709b20d9238
2023-06-28 05:38:36,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3326c3b-3743-4b87-8789-029938e801df
2023-06-28 05:38:36,662 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41431
2023-06-28 05:38:36,663 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41431
2023-06-28 05:38:36,663 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35259
2023-06-28 05:38:36,663 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36243
2023-06-28 05:38:36,663 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,663 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36243
2023-06-28 05:38:36,663 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,663 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36627
2023-06-28 05:38:36,663 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,663 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,663 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,663 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1e0atdw6
2023-06-28 05:38:36,663 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,663 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,663 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,663 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0v8nzhif
2023-06-28 05:38:36,663 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f80b373-939d-4d7d-a6ed-d8a0baecaefd
2023-06-28 05:38:36,663 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d0fb341-38a4-48e6-8b50-b5fb4129a486
2023-06-28 05:38:36,665 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45141
2023-06-28 05:38:36,665 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45141
2023-06-28 05:38:36,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34323
2023-06-28 05:38:36,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,666 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,666 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:36,666 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:38:36,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-frjv7y8n
2023-06-28 05:38:36,667 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2be9e109-944a-49ee-9dfa-8f734231193b
2023-06-28 05:38:36,683 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,684 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44467', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,725 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44467
2023-06-28 05:38:36,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45934
2023-06-28 05:38:36,726 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,726 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,726 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33863', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,727 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33863
2023-06-28 05:38:36,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45946
2023-06-28 05:38:36,728 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,728 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c955101e-f903-483e-b739-0fe9246883cc
2023-06-28 05:38:36,804 - distributed.worker - INFO - Starting Worker plugin PreImport-95a7592f-9246-4bc5-b91e-ece8f2608dc9
2023-06-28 05:38:36,805 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39d125dd-45da-4527-bdbf-6104da9e3814
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81aa3abe-6139-4605-b8cc-066bf3ff3d5a
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin PreImport-0f485467-37dc-4255-81ec-f4e20fcc2dc9
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f4fd54a-eb56-46aa-8662-37c8df95535b
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0261eab2-dc25-4027-ab84-5c12b752b911
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin PreImport-60fbf255-4f40-40de-8c7b-18f97d72821e
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de0d9153-369f-4a1c-ace8-db5f8eff5ced
2023-06-28 05:38:36,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin PreImport-f88292a6-c914-4f9f-9744-7b1afbfe2629
2023-06-28 05:38:36,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin PreImport-924994d7-f287-447c-b59d-0a5301ea836f
2023-06-28 05:38:36,825 - distributed.worker - INFO - Starting Worker plugin PreImport-4048859f-089c-4ffc-944d-2f3eb5b18a7b
2023-06-28 05:38:36,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,834 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45141', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,835 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45141
2023-06-28 05:38:36,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45954
2023-06-28 05:38:36,835 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,835 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,853 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43213', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,853 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43213
2023-06-28 05:38:36,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45958
2023-06-28 05:38:36,854 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,854 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43997', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43997
2023-06-28 05:38:36,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45970
2023-06-28 05:38:36,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,857 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41431', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41431
2023-06-28 05:38:36,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45982
2023-06-28 05:38:36,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,858 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40435', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40435
2023-06-28 05:38:36,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45992
2023-06-28 05:38:36,861 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36243', status: init, memory: 0, processing: 0>
2023-06-28 05:38:36,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36243
2023-06-28 05:38:36,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45996
2023-06-28 05:38:36,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:36,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:36,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:36,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:38:36,910 - distributed.scheduler - INFO - Remove client Client-037212bc-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:36,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45908; closing.
2023-06-28 05:38:36,910 - distributed.scheduler - INFO - Remove client Client-037212bc-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:36,911 - distributed.scheduler - INFO - Close client connection: Client-037212bc-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:36,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46335'. Reason: nanny-close
2023-06-28 05:38:36,912 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46525'. Reason: nanny-close
2023-06-28 05:38:36,913 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,914 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44467. Reason: nanny-close
2023-06-28 05:38:36,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38011'. Reason: nanny-close
2023-06-28 05:38:36,914 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,915 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45389'. Reason: nanny-close
2023-06-28 05:38:36,915 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40435. Reason: nanny-close
2023-06-28 05:38:36,915 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,915 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43997. Reason: nanny-close
2023-06-28 05:38:36,915 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37751'. Reason: nanny-close
2023-06-28 05:38:36,916 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,916 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41431. Reason: nanny-close
2023-06-28 05:38:36,916 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35919'. Reason: nanny-close
2023-06-28 05:38:36,916 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,916 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33863. Reason: nanny-close
2023-06-28 05:38:36,916 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45934; closing.
2023-06-28 05:38:36,916 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43537'. Reason: nanny-close
2023-06-28 05:38:36,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,917 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,917 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44467', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,917 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44467
2023-06-28 05:38:36,917 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36243. Reason: nanny-close
2023-06-28 05:38:36,917 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41753'. Reason: nanny-close
2023-06-28 05:38:36,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,917 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:36,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,918 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43213. Reason: nanny-close
2023-06-28 05:38:36,918 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45970; closing.
2023-06-28 05:38:36,918 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,918 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,918 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45141. Reason: nanny-close
2023-06-28 05:38:36,918 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44467
2023-06-28 05:38:36,919 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43997', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43997
2023-06-28 05:38:36,919 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44467
2023-06-28 05:38:36,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44467
2023-06-28 05:38:36,919 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45992; closing.
2023-06-28 05:38:36,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45982; closing.
2023-06-28 05:38:36,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,920 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44467
2023-06-28 05:38:36,920 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40435', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,920 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40435
2023-06-28 05:38:36,921 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41431', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,921 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:36,921 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41431
2023-06-28 05:38:36,921 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,921 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,921 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,921 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45946; closing.
2023-06-28 05:38:36,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45996; closing.
2023-06-28 05:38:36,922 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:36,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45958; closing.
2023-06-28 05:38:36,922 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33863', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,922 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33863
2023-06-28 05:38:36,923 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36243', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,923 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36243
2023-06-28 05:38:36,923 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43213', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,923 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43213
2023-06-28 05:38:36,923 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45954; closing.
2023-06-28 05:38:36,924 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45141', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:36,924 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45141
2023-06-28 05:38:36,924 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:38,479 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:38,480 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:38,480 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:38,481 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:38,482 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-28 05:38:40,371 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:40,375 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40383 instead
  warnings.warn(
2023-06-28 05:38:40,379 - distributed.scheduler - INFO - State start
2023-06-28 05:38:40,494 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:40,495 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:40,496 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40383/status
2023-06-28 05:38:40,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36733'
2023-06-28 05:38:41,301 - distributed.scheduler - INFO - Receive client connection: Client-088067b9-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:41,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46054
2023-06-28 05:38:42,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:42,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:38:42,567 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:43,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43331
2023-06-28 05:38:43,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43331
2023-06-28 05:38:43,384 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-28 05:38:43,384 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:43,384 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:43,384 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:43,385 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:38:43,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4964dhxp
2023-06-28 05:38:43,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a5c5294-b450-4278-8afe-f992959d5aa4
2023-06-28 05:38:43,385 - distributed.worker - INFO - Starting Worker plugin PreImport-b68f7f32-a3d5-46f7-a04b-8554f77bebd0
2023-06-28 05:38:43,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46b08259-4cc9-4fbc-b780-6a1013fe2571
2023-06-28 05:38:43,386 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:43,414 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43331', status: init, memory: 0, processing: 0>
2023-06-28 05:38:43,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43331
2023-06-28 05:38:43,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46066
2023-06-28 05:38:43,416 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:43,416 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:43,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:43,450 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:38:43,452 - distributed.scheduler - INFO - Remove client Client-088067b9-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:43,452 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46054; closing.
2023-06-28 05:38:43,453 - distributed.scheduler - INFO - Remove client Client-088067b9-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:43,453 - distributed.scheduler - INFO - Close client connection: Client-088067b9-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:43,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36733'. Reason: nanny-close
2023-06-28 05:38:43,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:43,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43331. Reason: nanny-close
2023-06-28 05:38:43,457 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:43,457 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46066; closing.
2023-06-28 05:38:43,458 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43331', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:43,458 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43331
2023-06-28 05:38:43,458 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:43,459 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:44,571 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:44,571 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:44,571 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:44,572 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:44,572 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-28 05:38:48,355 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:48,359 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45771 instead
  warnings.warn(
2023-06-28 05:38:48,363 - distributed.scheduler - INFO - State start
2023-06-28 05:38:48,384 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:48,385 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:48,386 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45771/status
2023-06-28 05:38:48,412 - distributed.scheduler - INFO - Receive client connection: Client-0d34175d-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:48,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46136
2023-06-28 05:38:48,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37391'
2023-06-28 05:38:50,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:50,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:38:50,588 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:51,431 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44649
2023-06-28 05:38:51,431 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44649
2023-06-28 05:38:51,431 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44543
2023-06-28 05:38:51,431 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:38:51,431 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:51,431 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:38:51,431 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:38:51,431 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2z1x9xsa
2023-06-28 05:38:51,431 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73b5fc6c-357d-4a78-922f-94844ebe021c
2023-06-28 05:38:51,432 - distributed.worker - INFO - Starting Worker plugin PreImport-9ec9d8f5-5945-48d8-9ea9-5147640927f5
2023-06-28 05:38:51,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13e21a41-4595-4e94-8540-2acb655d2aa9
2023-06-28 05:38:51,433 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:51,459 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44649', status: init, memory: 0, processing: 0>
2023-06-28 05:38:51,460 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44649
2023-06-28 05:38:51,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44378
2023-06-28 05:38:51,460 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:38:51,460 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:51,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:38:51,484 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:38:51,487 - distributed.scheduler - INFO - Remove client Client-0d34175d-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:51,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46136; closing.
2023-06-28 05:38:51,487 - distributed.scheduler - INFO - Remove client Client-0d34175d-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:51,488 - distributed.scheduler - INFO - Close client connection: Client-0d34175d-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:38:51,489 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37391'. Reason: nanny-close
2023-06-28 05:38:51,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:51,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44649. Reason: nanny-close
2023-06-28 05:38:51,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44378; closing.
2023-06-28 05:38:51,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:38:51,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44649', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:51,510 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44649
2023-06-28 05:38:51,510 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:51,511 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:52,706 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:52,706 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:52,707 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:52,708 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:52,709 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-28 05:38:54,761 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:54,766 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36143 instead
  warnings.warn(
2023-06-28 05:38:54,770 - distributed.scheduler - INFO - State start
2023-06-28 05:38:54,792 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:54,793 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:38:54,793 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36143/status
2023-06-28 05:38:59,104 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:59,105 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:59,105 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:59,105 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:38:59,106 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-28 05:39:00,993 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:00,997 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37567 instead
  warnings.warn(
2023-06-28 05:39:01,000 - distributed.scheduler - INFO - State start
2023-06-28 05:39:01,054 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:01,055 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-28 05:39:01,056 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37567/status
2023-06-28 05:39:01,141 - distributed.scheduler - INFO - Receive client connection: Client-14c6543f-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:01,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35658
2023-06-28 05:39:01,161 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37061'
2023-06-28 05:39:02,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,672 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:03,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42989
2023-06-28 05:39:03,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42989
2023-06-28 05:39:03,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43009
2023-06-28 05:39:03,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:39:03,688 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:03,688 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:03,688 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:39:03,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2229onn9
2023-06-28 05:39:03,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e36a5a3-c2f6-4507-a7cf-bc4d1ddeabdc
2023-06-28 05:39:03,689 - distributed.worker - INFO - Starting Worker plugin PreImport-59110a76-ec70-4ca4-a3dd-04da7d3a0957
2023-06-28 05:39:03,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75f393e3-787f-48f6-b000-1ea6e42a09a8
2023-06-28 05:39:03,689 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:03,717 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42989', status: init, memory: 0, processing: 0>
2023-06-28 05:39:03,718 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42989
2023-06-28 05:39:03,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35696
2023-06-28 05:39:03,719 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:39:03,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:03,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:39:03,794 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:03,800 - distributed.scheduler - INFO - Remove client Client-14c6543f-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:03,800 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35658; closing.
2023-06-28 05:39:03,800 - distributed.scheduler - INFO - Remove client Client-14c6543f-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:03,801 - distributed.scheduler - INFO - Close client connection: Client-14c6543f-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:03,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37061'. Reason: nanny-close
2023-06-28 05:39:03,802 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:03,803 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42989. Reason: nanny-close
2023-06-28 05:39:03,805 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:39:03,805 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35696; closing.
2023-06-28 05:39:03,805 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42989', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:03,805 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42989
2023-06-28 05:39:03,805 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:03,806 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:05,219 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:39:05,219 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:05,220 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:05,220 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-28 05:39:05,221 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-28 05:39:07,447 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:07,452 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42915 instead
  warnings.warn(
2023-06-28 05:39:07,456 - distributed.scheduler - INFO - State start
2023-06-28 05:39:07,536 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:07,537 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:07,538 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:07,538 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-28 05:39:07,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41519'
2023-06-28 05:39:07,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43131'
2023-06-28 05:39:07,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37219'
2023-06-28 05:39:07,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45585'
2023-06-28 05:39:07,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37827'
2023-06-28 05:39:07,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35379'
2023-06-28 05:39:07,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32969'
2023-06-28 05:39:07,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41965'
2023-06-28 05:39:09,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,690 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,699 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,713 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:09,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:09,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:10,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,057 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:10,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:15,434 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46817
2023-06-28 05:39:15,434 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46817
2023-06-28 05:39:15,434 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43187
2023-06-28 05:39:15,434 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,434 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,435 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,435 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,435 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aiymk8n_
2023-06-28 05:39:15,435 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d3253b9-f10a-4382-afea-46d75255001b
2023-06-28 05:39:15,455 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36119
2023-06-28 05:39:15,455 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36119
2023-06-28 05:39:15,455 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33517
2023-06-28 05:39:15,455 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33517
2023-06-28 05:39:15,455 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34571
2023-06-28 05:39:15,455 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34587
2023-06-28 05:39:15,455 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,455 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,455 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,455 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,455 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,455 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,455 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,455 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ixvrokc
2023-06-28 05:39:15,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kalu8_3g
2023-06-28 05:39:15,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cba46e40-fd4c-4c93-9110-5310b0868c98
2023-06-28 05:39:15,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fffc9f68-30f8-4108-861d-158021c39c78
2023-06-28 05:39:15,474 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38091
2023-06-28 05:39:15,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38091
2023-06-28 05:39:15,474 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41839
2023-06-28 05:39:15,474 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,474 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,474 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,474 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dzou46ls
2023-06-28 05:39:15,475 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76f0c315-c510-488c-9f09-0fb4b2404445
2023-06-28 05:39:15,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43079
2023-06-28 05:39:15,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43079
2023-06-28 05:39:15,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34283
2023-06-28 05:39:15,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,500 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,500 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sm790bsy
2023-06-28 05:39:15,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fc1c68d-801e-4854-9038-5f80d681c7b5
2023-06-28 05:39:15,501 - distributed.worker - INFO - Starting Worker plugin PreImport-92b965a4-7f1e-4d7d-848c-628d7b64886c
2023-06-28 05:39:15,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed5fca83-e189-48c0-8cc7-f9fc95d3c54f
2023-06-28 05:39:15,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33401
2023-06-28 05:39:15,512 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33401
2023-06-28 05:39:15,512 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37619
2023-06-28 05:39:15,512 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,512 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,512 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,512 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s7g_um7u
2023-06-28 05:39:15,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d1b1215-9175-4da1-8e07-22d38df35141
2023-06-28 05:39:15,540 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45913
2023-06-28 05:39:15,540 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45913
2023-06-28 05:39:15,541 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36163
2023-06-28 05:39:15,541 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,541 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,541 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,541 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,541 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mhk4qhp6
2023-06-28 05:39:15,541 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dde90f7e-076f-4686-bdf1-3e3183e1d41c
2023-06-28 05:39:15,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36711
2023-06-28 05:39:15,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36711
2023-06-28 05:39:15,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46403
2023-06-28 05:39:15,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,542 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,542 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:15,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:15,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v05qgju_
2023-06-28 05:39:15,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e29deaa-4512-4026-8ad3-d7711869af7a
2023-06-28 05:39:15,546 - distributed.worker - INFO - Starting Worker plugin PreImport-8f357479-f51c-47ef-a1d7-b2d0ed9b98d6
2023-06-28 05:39:15,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea5c7915-3555-4b4e-9c43-7952770fb931
2023-06-28 05:39:15,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91a8242e-9dca-4245-8ea7-199af53bb58e
2023-06-28 05:39:15,751 - distributed.worker - INFO - Starting Worker plugin PreImport-0ee334a8-d8d2-43d9-b61b-3f9c8fd6371d
2023-06-28 05:39:15,751 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92796bc9-8b01-4146-b563-5abc217b8595
2023-06-28 05:39:15,754 - distributed.worker - INFO - Starting Worker plugin PreImport-23608a2c-482f-41e3-ae82-9d26764402cb
2023-06-28 05:39:15,754 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,771 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0e1cdfb-1934-40d3-8c9a-c06dde4f5307
2023-06-28 05:39:15,772 - distributed.worker - INFO - Starting Worker plugin PreImport-d1de6583-2267-42d9-84ae-269941309fdd
2023-06-28 05:39:15,772 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b577637-2918-4a57-8d72-1b2e5aacb807
2023-06-28 05:39:15,779 - distributed.worker - INFO - Starting Worker plugin PreImport-84cc31bf-5b81-4ce1-a861-cba191397564
2023-06-28 05:39:15,779 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,786 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,786 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,795 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5980cbc9-7752-4385-bf52-d06c4f7a4087
2023-06-28 05:39:15,795 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,795 - distributed.worker - INFO - Starting Worker plugin PreImport-126ceb1a-6e91-4424-a824-e00c5c0a40d9
2023-06-28 05:39:15,795 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,799 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,799 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,814 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,816 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40386605-097c-4d3b-8395-3d2fa057bd60
2023-06-28 05:39:15,816 - distributed.worker - INFO - Starting Worker plugin PreImport-bf278d3a-dec9-450a-9b1d-9ebe17cb2804
2023-06-28 05:39:15,816 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,816 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,816 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,816 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,827 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,838 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,841 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:15,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:15,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,907 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:15,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41519'. Reason: nanny-close
2023-06-28 05:39:15,913 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45585'. Reason: nanny-close
2023-06-28 05:39:15,914 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35379'. Reason: nanny-close
2023-06-28 05:39:15,914 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36711. Reason: nanny-close
2023-06-28 05:39:15,915 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,915 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32969'. Reason: nanny-close
2023-06-28 05:39:15,915 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33517. Reason: nanny-close
2023-06-28 05:39:15,915 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,916 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41965'. Reason: nanny-close
2023-06-28 05:39:15,916 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38091. Reason: nanny-close
2023-06-28 05:39:15,916 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,916 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43131'. Reason: nanny-close
2023-06-28 05:39:15,916 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36119. Reason: nanny-close
2023-06-28 05:39:15,916 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37219'. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43079. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,917 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37827'. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33401. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:15,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,918 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46817. Reason: nanny-close
2023-06-28 05:39:15,918 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,918 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,918 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45913. Reason: nanny-close
2023-06-28 05:39:15,918 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,919 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,919 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36711
2023-06-28 05:39:15,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36711
2023-06-28 05:39:15,919 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,920 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,920 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36711
2023-06-28 05:39:15,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:15,921 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,921 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:15,921 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-28 05:39:19,409 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:19,414 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-28 05:39:19,417 - distributed.scheduler - INFO - State start
2023-06-28 05:39:19,436 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:19,437 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:39:19,438 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-28 05:39:19,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34033'
2023-06-28 05:39:19,677 - distributed.scheduler - INFO - Receive client connection: Client-20dca838-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:19,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60674
2023-06-28 05:39:20,342 - distributed.scheduler - INFO - Receive client connection: Client-1fbb5162-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:20,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60858
2023-06-28 05:39:21,073 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:21,073 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:21,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:21,958 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45095
2023-06-28 05:39:21,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45095
2023-06-28 05:39:21,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35317
2023-06-28 05:39:21,959 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:21,959 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:21,959 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:21,959 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:39:21,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bgc3axn5
2023-06-28 05:39:21,959 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33f85175-1ff0-4ba4-bd80-56d767a5659c
2023-06-28 05:39:22,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-718d0e5a-b0cc-4c0a-a7e4-0176c5d525eb
2023-06-28 05:39:22,081 - distributed.worker - INFO - Starting Worker plugin PreImport-892dbcbb-244d-4311-9508-81a126c1b9a1
2023-06-28 05:39:22,081 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:22,111 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45095', status: init, memory: 0, processing: 0>
2023-06-28 05:39:22,113 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45095
2023-06-28 05:39:22,113 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60942
2023-06-28 05:39:22,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:22,113 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:22,117 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:22,206 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:22,210 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:22,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:22,215 - distributed.scheduler - INFO - Remove client Client-1fbb5162-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:22,215 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60858; closing.
2023-06-28 05:39:22,216 - distributed.scheduler - INFO - Remove client Client-1fbb5162-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:22,216 - distributed.scheduler - INFO - Close client connection: Client-1fbb5162-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:22,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34033'. Reason: nanny-close
2023-06-28 05:39:22,217 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:22,218 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45095. Reason: nanny-close
2023-06-28 05:39:22,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:22,220 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60942; closing.
2023-06-28 05:39:22,220 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45095', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:22,221 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45095
2023-06-28 05:39:22,221 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:22,221 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:23,434 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:39:23,434 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:23,434 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:23,436 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:39:23,437 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-28 05:39:25,522 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:25,527 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-28 05:39:25,530 - distributed.scheduler - INFO - State start
2023-06-28 05:39:25,552 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:25,553 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:39:25,554 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-28 05:39:25,774 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33141', status: init, memory: 0, processing: 0>
2023-06-28 05:39:25,791 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33141
2023-06-28 05:39:25,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33180
2023-06-28 05:39:25,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39139'
2023-06-28 05:39:26,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40749', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,061 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40749
2023-06-28 05:39:26,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33206
2023-06-28 05:39:26,112 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42971', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,112 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42971
2023-06-28 05:39:26,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33216
2023-06-28 05:39:26,121 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45005', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,121 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45005
2023-06-28 05:39:26,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33232
2023-06-28 05:39:26,168 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41495', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,169 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41495
2023-06-28 05:39:26,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33238
2023-06-28 05:39:26,176 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42241', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,176 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42241
2023-06-28 05:39:26,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33254
2023-06-28 05:39:26,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41497', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41497
2023-06-28 05:39:26,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33264
2023-06-28 05:39:26,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39365', status: init, memory: 0, processing: 0>
2023-06-28 05:39:26,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39365
2023-06-28 05:39:26,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33280
2023-06-28 05:39:26,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:26,268 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:26,379 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33206; closing.
2023-06-28 05:39:26,380 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40749', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,380 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40749
2023-06-28 05:39:26,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33254; closing.
2023-06-28 05:39:26,382 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42241', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,382 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42241
2023-06-28 05:39:26,383 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33264; closing.
2023-06-28 05:39:26,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41497', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,384 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41497
2023-06-28 05:39:26,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33280; closing.
2023-06-28 05:39:26,384 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39365', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,384 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39365
2023-06-28 05:39:26,385 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33238; closing.
2023-06-28 05:39:26,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41495', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41495
2023-06-28 05:39:26,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33232; closing.
2023-06-28 05:39:26,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33180; closing.
2023-06-28 05:39:26,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45005', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45005
2023-06-28 05:39:26,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33141', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33141
2023-06-28 05:39:26,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33216; closing.
2023-06-28 05:39:26,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42971', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:26,388 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42971
2023-06-28 05:39:26,388 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:26,389 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33216>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-28 05:39:26,470 - distributed.scheduler - INFO - Receive client connection: Client-23535f4c-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:26,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33296
2023-06-28 05:39:27,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:27,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:27,469 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:27,861 - distributed.scheduler - INFO - Receive client connection: Client-25bd64c4-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:27,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33312
2023-06-28 05:39:28,363 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33003
2023-06-28 05:39:28,363 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33003
2023-06-28 05:39:28,363 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39305
2023-06-28 05:39:28,363 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:28,363 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:28,363 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:28,363 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:39:28,363 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4kxdb19c
2023-06-28 05:39:28,363 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f27b4060-702f-4714-bdee-a064a128b416
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-28 05:39:28,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a70c365d-3b22-4f93-8553-a355d382fa2d
2023-06-28 05:39:28,612 - distributed.worker - INFO - Starting Worker plugin PreImport-1fe5bf04-70e1-4c58-b7d1-a40b36ed8fd7
2023-06-28 05:39:28,613 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33003. Reason: worker-close
2023-06-28 05:39:28,613 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-28 05:39:28,616 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:39:28,660 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:39:28,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39139'. Reason: nanny-instantiate-failed
2023-06-28 05:39:28,663 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-28 05:39:29,047 - distributed.nanny - INFO - Worker process 54165 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-28 05:39:33,327 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33749', status: init, memory: 0, processing: 0>
2023-06-28 05:39:33,328 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33749
2023-06-28 05:39:33,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43334
2023-06-28 05:39:33,395 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:33,396 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:39:33,399 - distributed.scheduler - INFO - Remove client Client-23535f4c-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:33,399 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33296; closing.
2023-06-28 05:39:33,400 - distributed.scheduler - INFO - Remove client Client-23535f4c-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:33,400 - distributed.scheduler - INFO - Close client connection: Client-23535f4c-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:33,401 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:39:33,401 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:33,402 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:33,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:43334 has been closed.
2023-06-28 05:39:33,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33749', status: running, memory: 0, processing: 0>
2023-06-28 05:39:33,403 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33749
2023-06-28 05:39:33,403 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:33,406 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:39:33,407 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:39:42,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,840 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,842 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,842 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:42,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:42,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:39:52,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,136 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:52,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:52,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37677 instead
  warnings.warn(
2023-06-28 05:40:00,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,596 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,596 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,647 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,647 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:00,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:00,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34917 instead
  warnings.warn(
2023-06-28 05:40:09,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,767 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:09,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:09,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-28 05:40:11,870 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:40:11,892 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-40' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:118> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 119, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-28 05:40:12,704 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f03a7044d30>>, <Task finished name='Task-39' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:341> exception=RuntimeError('Worker failed to start.')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 382, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 525, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:40:14,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,686 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:14,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:14,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:17,788 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 6
2023-06-28 05:40:17,789 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 6
2023-06-28 05:40:17,789 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 5
2023-06-28 05:40:17,790 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 5
2023-06-28 05:40:17,792 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 3
2023-06-28 05:40:17,792 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 3
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-28 05:40:17,894 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:40:17,950 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:40:17,968 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 7
2023-06-28 05:40:17,969 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 7
2023-06-28 05:40:17,971 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 2
2023-06-28 05:40:17,971 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 2
2023-06-28 05:40:17,972 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 4
2023-06-28 05:40:17,973 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 4
2023-06-28 05:40:17,973 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 1
2023-06-28 05:40:17,973 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 1
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 90 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
