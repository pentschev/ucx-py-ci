============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-07-26 05:28:31,818 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:31,822 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:28:31,825 - distributed.scheduler - INFO - State start
2023-07-26 05:28:31,843 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:31,844 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-26 05:28:31,845 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:28:31,905 - distributed.scheduler - INFO - Receive client connection: Client-416dd08d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:31,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39980
2023-07-26 05:28:31,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39289'
2023-07-26 05:28:31,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33471'
2023-07-26 05:28:31,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43795'
2023-07-26 05:28:31,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35201'
2023-07-26 05:28:33,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:33,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:33,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:33,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:33,378 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:33,378 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:33,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:33,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:33,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:33,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:33,399 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:33,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-07-26 05:28:33,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45667
2023-07-26 05:28:33,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45667
2023-07-26 05:28:33,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34065
2023-07-26 05:28:33,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:28:33,416 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:33,416 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:28:33,416 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:28:33,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vwf2vgj8
2023-07-26 05:28:33,416 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d293369f-3192-4999-b286-fe8ec78ecf58
2023-07-26 05:28:33,416 - distributed.worker - INFO - Starting Worker plugin PreImport-64d06adc-5596-4bdc-b459-690490765e50
2023-07-26 05:28:33,417 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9dfcc451-b8a9-4f3a-bf3b-0b635e51a5af
2023-07-26 05:28:33,417 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:33,430 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45667', status: init, memory: 0, processing: 0>
2023-07-26 05:28:33,431 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45667
2023-07-26 05:28:33,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40020
2023-07-26 05:28:33,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:28:33,432 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:33,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:28:34,500 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46355
2023-07-26 05:28:34,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46355
2023-07-26 05:28:34,501 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41569
2023-07-26 05:28:34,501 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:28:34,501 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,501 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:28:34,501 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:28:34,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fuk3e4ir
2023-07-26 05:28:34,501 - distributed.worker - INFO - Starting Worker plugin PreImport-ed47152e-28d3-4514-8310-c79d7999ad3e
2023-07-26 05:28:34,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cf2ef00-363e-4c13-9897-59b1b63846aa
2023-07-26 05:28:34,502 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0424df1-fa66-426b-b231-64284c0eb369
2023-07-26 05:28:34,502 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,521 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46355', status: init, memory: 0, processing: 0>
2023-07-26 05:28:34,522 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46355
2023-07-26 05:28:34,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40034
2023-07-26 05:28:34,523 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:28:34,523 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:28:34,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39221
2023-07-26 05:28:34,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39221
2023-07-26 05:28:34,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39039
2023-07-26 05:28:34,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:28:34,535 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,535 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:28:34,535 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:28:34,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ia5lmpbk
2023-07-26 05:28:34,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e7ed5f6-c842-4ffe-a3ab-ca04549bcffb
2023-07-26 05:28:34,536 - distributed.worker - INFO - Starting Worker plugin PreImport-8d2556b3-756e-45f2-9d03-91980b5b7ae6
2023-07-26 05:28:34,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2eff550-b787-494f-81ed-a57348a1e434
2023-07-26 05:28:34,536 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39221', status: init, memory: 0, processing: 0>
2023-07-26 05:28:34,554 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39221
2023-07-26 05:28:34,554 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40050
2023-07-26 05:28:34,555 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:28:34,555 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:28:34,586 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40653
2023-07-26 05:28:34,587 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40653
2023-07-26 05:28:34,587 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41069
2023-07-26 05:28:34,587 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:28:34,587 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,587 - distributed.worker - INFO -               Threads:                          4
2023-07-26 05:28:34,587 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-26 05:28:34,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x9x5mc_o
2023-07-26 05:28:34,587 - distributed.worker - INFO - Starting Worker plugin PreImport-41cb1268-89c9-4d36-9ac2-6cc71b89271d
2023-07-26 05:28:34,587 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fc8568d-f30e-4bdb-bb2b-6f09c68be5ee
2023-07-26 05:28:34,588 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc196b39-160d-4cf0-8ae5-ec6d37b42084
2023-07-26 05:28:34,588 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40653', status: init, memory: 0, processing: 0>
2023-07-26 05:28:34,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40653
2023-07-26 05:28:34,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40052
2023-07-26 05:28:34,605 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:28:34,605 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:34,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:28:34,650 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:28:34,651 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:28:34,651 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:28:34,652 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-26 05:28:34,657 - distributed.scheduler - INFO - Remove client Client-416dd08d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:34,657 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39980; closing.
2023-07-26 05:28:34,657 - distributed.scheduler - INFO - Remove client Client-416dd08d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:34,658 - distributed.scheduler - INFO - Close client connection: Client-416dd08d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:34,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39289'. Reason: nanny-close
2023-07-26 05:28:34,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:34,660 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33471'. Reason: nanny-close
2023-07-26 05:28:34,661 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:34,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46355. Reason: nanny-close
2023-07-26 05:28:34,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43795'. Reason: nanny-close
2023-07-26 05:28:34,661 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:34,662 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40653. Reason: nanny-close
2023-07-26 05:28:34,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35201'. Reason: nanny-close
2023-07-26 05:28:34,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:34,662 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39221. Reason: nanny-close
2023-07-26 05:28:34,663 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:28:34,663 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40034; closing.
2023-07-26 05:28:34,663 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46355', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:34,663 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45667. Reason: nanny-close
2023-07-26 05:28:34,663 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46355
2023-07-26 05:28:34,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:28:34,664 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:34,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:28:34,664 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40052; closing.
2023-07-26 05:28:34,665 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46355
2023-07-26 05:28:34,665 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:34,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40653', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:34,665 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40653
2023-07-26 05:28:34,665 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:28:34,665 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:34,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40050; closing.
2023-07-26 05:28:34,666 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39221', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:34,666 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39221
2023-07-26 05:28:34,667 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40020; closing.
2023-07-26 05:28:34,667 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:34,667 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45667', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:34,667 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45667
2023-07-26 05:28:34,667 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:28:35,625 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:28:35,625 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:28:35,626 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:28:35,626 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-26 05:28:35,627 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-07-26 05:28:37,352 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:37,356 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:28:37,359 - distributed.scheduler - INFO - State start
2023-07-26 05:28:37,379 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:37,380 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:28:37,380 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:28:37,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44937'
2023-07-26 05:28:37,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39799'
2023-07-26 05:28:37,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40979'
2023-07-26 05:28:37,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33147'
2023-07-26 05:28:37,526 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38427'
2023-07-26 05:28:37,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41319'
2023-07-26 05:28:37,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34037'
2023-07-26 05:28:37,550 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42913'
2023-07-26 05:28:37,693 - distributed.scheduler - INFO - Receive client connection: Client-44b7b238-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:37,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54304
2023-07-26 05:28:38,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:38,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,058 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,058 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,064 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,082 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:39,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:39,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:39,272 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:40,446 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44795
2023-07-26 05:28:40,446 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44795
2023-07-26 05:28:40,446 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42847
2023-07-26 05:28:40,446 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:40,446 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:40,446 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:40,446 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:40,446 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ie9ill2o
2023-07-26 05:28:40,447 - distributed.worker - INFO - Starting Worker plugin PreImport-279d80ef-dac8-451d-8627-0c1d9a86ca24
2023-07-26 05:28:40,447 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7f9efc6-35b3-47a0-8d5c-900f374dafa2
2023-07-26 05:28:40,447 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44664ee7-b822-43ef-bc1f-c8a8dce98f64
2023-07-26 05:28:40,770 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:40,794 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44795', status: init, memory: 0, processing: 0>
2023-07-26 05:28:40,795 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44795
2023-07-26 05:28:40,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54322
2023-07-26 05:28:40,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:40,796 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:40,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,485 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36927
2023-07-26 05:28:41,485 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36927
2023-07-26 05:28:41,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33603
2023-07-26 05:28:41,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,486 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,486 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9kck88h7
2023-07-26 05:28:41,486 - distributed.worker - INFO - Starting Worker plugin PreImport-2824ca38-1875-42d3-9978-c0675d17d4e8
2023-07-26 05:28:41,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af2c4d17-a63d-4a25-a0a3-eb327b6b51bb
2023-07-26 05:28:41,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-309ee44b-fff4-47a2-bd48-580147c6567e
2023-07-26 05:28:41,528 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40691
2023-07-26 05:28:41,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40691
2023-07-26 05:28:41,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43969
2023-07-26 05:28:41,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,528 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,528 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,528 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,528 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eyrxs23n
2023-07-26 05:28:41,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-355a4343-fd5f-4d7f-bfbd-d10b397fce72
2023-07-26 05:28:41,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35843
2023-07-26 05:28:41,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35843
2023-07-26 05:28:41,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43507
2023-07-26 05:28:41,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,535 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,535 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-klfhofiz
2023-07-26 05:28:41,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7944253-a1bb-42df-a9e1-62b3269b3fb7
2023-07-26 05:28:41,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33991
2023-07-26 05:28:41,584 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33991
2023-07-26 05:28:41,584 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34895
2023-07-26 05:28:41,584 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,584 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,584 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,584 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wlt9fmk7
2023-07-26 05:28:41,585 - distributed.worker - INFO - Starting Worker plugin PreImport-0b61cb57-2e3f-40f8-a8ab-1579d3ac4260
2023-07-26 05:28:41,585 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05c659f6-5144-4488-877e-2feef9b313e1
2023-07-26 05:28:41,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0f632b6-1c66-4f1d-af29-c47f8b665b9f
2023-07-26 05:28:41,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34519
2023-07-26 05:28:41,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34519
2023-07-26 05:28:41,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42647
2023-07-26 05:28:41,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41073
2023-07-26 05:28:41,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42647
2023-07-26 05:28:41,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,590 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41911
2023-07-26 05:28:41,590 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,590 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,590 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h2p7n13s
2023-07-26 05:28:41,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h_kmnk69
2023-07-26 05:28:41,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bec86ac-71ba-4c77-9ea9-21b42ec2f7ff
2023-07-26 05:28:41,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e28e6b6-260e-40f9-88c0-8c79ff54e838
2023-07-26 05:28:41,591 - distributed.worker - INFO - Starting Worker plugin PreImport-16de6b80-62a3-4ee3-859e-0d7bb0719b81
2023-07-26 05:28:41,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef9fa478-1f61-4459-83e5-ba741e4b5d4c
2023-07-26 05:28:41,607 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,617 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36447
2023-07-26 05:28:41,617 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36447
2023-07-26 05:28:41,618 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43989
2023-07-26 05:28:41,618 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,618 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,618 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:41,618 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:41,618 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j8qra6x1
2023-07-26 05:28:41,618 - distributed.worker - INFO - Starting Worker plugin PreImport-fc974356-8af1-4963-88a7-06f4b2864272
2023-07-26 05:28:41,618 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bada5907-c923-4071-ad29-b6a5e5029ef0
2023-07-26 05:28:41,619 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22073e2a-519b-4a24-a22c-1ade9aaa1d44
2023-07-26 05:28:41,630 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36927', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,631 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36927
2023-07-26 05:28:41,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47570
2023-07-26 05:28:41,631 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,631 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,642 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f24b9e0-e0fa-4101-ab9d-a901d98d34bf
2023-07-26 05:28:41,642 - distributed.worker - INFO - Starting Worker plugin PreImport-4b661d0c-a539-402f-b112-8a86d52f2c86
2023-07-26 05:28:41,643 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,657 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb0b3ac4-629c-415d-b425-39d87a51ebac
2023-07-26 05:28:41,658 - distributed.worker - INFO - Starting Worker plugin PreImport-4750d56e-f55c-4edf-bf9e-f6dabbce235f
2023-07-26 05:28:41,659 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,667 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40691', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,668 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40691
2023-07-26 05:28:41,668 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47576
2023-07-26 05:28:41,669 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,669 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,685 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35843', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,686 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35843
2023-07-26 05:28:41,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47586
2023-07-26 05:28:41,686 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,686 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,703 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,711 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83007251-ae6a-40a2-a816-04d326baa3bf
2023-07-26 05:28:41,726 - distributed.worker - INFO - Starting Worker plugin PreImport-7748833f-97be-439c-9902-1249a6c9d6af
2023-07-26 05:28:41,726 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,728 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34519', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,728 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34519
2023-07-26 05:28:41,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47600
2023-07-26 05:28:41,729 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,729 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,733 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,736 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33991', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33991
2023-07-26 05:28:41,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47610
2023-07-26 05:28:41,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,737 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42647', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,752 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42647
2023-07-26 05:28:41,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47622
2023-07-26 05:28:41,752 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,752 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36447', status: init, memory: 0, processing: 0>
2023-07-26 05:28:41,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36447
2023-07-26 05:28:41,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47628
2023-07-26 05:28:41,755 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:41,755 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:41,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:41,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,784 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,785 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,785 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,785 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:41,788 - distributed.scheduler - INFO - Remove client Client-44b7b238-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:41,789 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54304; closing.
2023-07-26 05:28:41,789 - distributed.scheduler - INFO - Remove client Client-44b7b238-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:41,789 - distributed.scheduler - INFO - Close client connection: Client-44b7b238-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:41,790 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33147'. Reason: nanny-close
2023-07-26 05:28:41,790 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,791 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44937'. Reason: nanny-close
2023-07-26 05:28:41,791 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,792 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35843. Reason: nanny-close
2023-07-26 05:28:41,792 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39799'. Reason: nanny-close
2023-07-26 05:28:41,792 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,793 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40691. Reason: nanny-close
2023-07-26 05:28:41,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40979'. Reason: nanny-close
2023-07-26 05:28:41,793 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,793 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36447. Reason: nanny-close
2023-07-26 05:28:41,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38427'. Reason: nanny-close
2023-07-26 05:28:41,794 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47586; closing.
2023-07-26 05:28:41,794 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,794 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36927. Reason: nanny-close
2023-07-26 05:28:41,794 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35843', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41319'. Reason: nanny-close
2023-07-26 05:28:41,794 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35843
2023-07-26 05:28:41,794 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,794 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34037'. Reason: nanny-close
2023-07-26 05:28:41,795 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42647. Reason: nanny-close
2023-07-26 05:28:41,795 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,795 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,795 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,795 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35843
2023-07-26 05:28:41,795 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34519. Reason: nanny-close
2023-07-26 05:28:41,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42913'. Reason: nanny-close
2023-07-26 05:28:41,796 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35843
2023-07-26 05:28:41,796 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47576; closing.
2023-07-26 05:28:41,796 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:41,796 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35843
2023-07-26 05:28:41,796 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,796 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,796 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,796 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40691', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,796 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44795. Reason: nanny-close
2023-07-26 05:28:41,797 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40691
2023-07-26 05:28:41,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33991. Reason: nanny-close
2023-07-26 05:28:41,797 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35843
2023-07-26 05:28:41,797 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47628; closing.
2023-07-26 05:28:41,797 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,797 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35843
2023-07-26 05:28:41,797 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36447', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,797 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36447
2023-07-26 05:28:41,797 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,798 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47570; closing.
2023-07-26 05:28:41,798 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,798 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36927', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,798 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,798 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36927
2023-07-26 05:28:41,798 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:41,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47622; closing.
2023-07-26 05:28:41,799 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,799 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42647', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,799 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42647
2023-07-26 05:28:41,799 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,799 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47600; closing.
2023-07-26 05:28:41,800 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54322; closing.
2023-07-26 05:28:41,800 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:41,800 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34519', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,800 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34519
2023-07-26 05:28:41,801 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44795', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,801 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44795
2023-07-26 05:28:41,801 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47610; closing.
2023-07-26 05:28:41,802 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33991', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:41,802 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33991
2023-07-26 05:28:41,802 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:28:43,107 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:28:43,107 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:28:43,108 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:28:43,109 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:28:43,109 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-07-26 05:28:44,831 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:44,835 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:28:44,838 - distributed.scheduler - INFO - State start
2023-07-26 05:28:44,856 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:44,856 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:28:44,857 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:28:44,946 - distributed.scheduler - INFO - Receive client connection: Client-492e8cbc-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:44,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47708
2023-07-26 05:28:44,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43647'
2023-07-26 05:28:45,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40673'
2023-07-26 05:28:45,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39095'
2023-07-26 05:28:45,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42467'
2023-07-26 05:28:45,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44197'
2023-07-26 05:28:45,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36657'
2023-07-26 05:28:45,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46559'
2023-07-26 05:28:45,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36519'
2023-07-26 05:28:46,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,449 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,545 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,596 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:46,596 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:46,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:46,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:48,083 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32905
2023-07-26 05:28:48,083 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32905
2023-07-26 05:28:48,083 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43385
2023-07-26 05:28:48,083 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:48,083 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,083 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:48,083 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:48,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qjs6zejm
2023-07-26 05:28:48,083 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7433c090-a25e-447a-9f7d-7420e5690dee
2023-07-26 05:28:48,147 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6aebb31b-57f4-4a37-ad6d-695705044dd4
2023-07-26 05:28:48,147 - distributed.worker - INFO - Starting Worker plugin PreImport-b50da0b4-2b50-404e-8a78-7d15d4b75d93
2023-07-26 05:28:48,147 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,176 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32905', status: init, memory: 0, processing: 0>
2023-07-26 05:28:48,178 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32905
2023-07-26 05:28:48,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47820
2023-07-26 05:28:48,178 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:48,178 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:48,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35177
2023-07-26 05:28:48,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35177
2023-07-26 05:28:48,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42485
2023-07-26 05:28:48,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:48,953 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,954 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:48,954 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:48,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fdtil5lm
2023-07-26 05:28:48,954 - distributed.worker - INFO - Starting Worker plugin PreImport-7fe9a877-d448-47f2-be0c-237e39d9e958
2023-07-26 05:28:48,955 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fd6277d-e798-4457-9fb4-3ca9bd9d651e
2023-07-26 05:28:48,955 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da9bd328-b19e-4bb3-92b2-133b2eab2e16
2023-07-26 05:28:48,960 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,985 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35177', status: init, memory: 0, processing: 0>
2023-07-26 05:28:48,985 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35177
2023-07-26 05:28:48,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47836
2023-07-26 05:28:48,986 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:48,986 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:48,992 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32771
2023-07-26 05:28:48,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32771
2023-07-26 05:28:48,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44511
2023-07-26 05:28:48,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:48,993 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,993 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:48,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:48,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1zreghnt
2023-07-26 05:28:48,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44025
2023-07-26 05:28:48,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44025
2023-07-26 05:28:48,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38345
2023-07-26 05:28:48,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:48,993 - distributed.worker - INFO - Starting Worker plugin PreImport-1bb049aa-97df-4183-b5b7-e13f0c960930
2023-07-26 05:28:48,993 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:48,993 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:48,993 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29546f6b-4c4a-4db4-b946-a818b8861879
2023-07-26 05:28:48,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:48,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8boaz1bn
2023-07-26 05:28:48,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd9db8fa-9063-4764-b5d3-dbde5f7c8edd
2023-07-26 05:28:48,994 - distributed.worker - INFO - Starting Worker plugin PreImport-f2cb2128-bad1-460f-a58b-1c52681ed2db
2023-07-26 05:28:48,994 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-308dac25-4a66-4fa9-b04f-d3e200f51f48
2023-07-26 05:28:48,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0dde1245-b0d4-4bcd-bed7-250d24ea6989
2023-07-26 05:28:49,001 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,001 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,021 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44025', status: init, memory: 0, processing: 0>
2023-07-26 05:28:49,021 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44025
2023-07-26 05:28:49,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47868
2023-07-26 05:28:49,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,022 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,022 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32771', status: init, memory: 0, processing: 0>
2023-07-26 05:28:49,023 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32771
2023-07-26 05:28:49,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47852
2023-07-26 05:28:49,023 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,023 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:49,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:49,063 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37251
2023-07-26 05:28:49,064 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37251
2023-07-26 05:28:49,064 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42373
2023-07-26 05:28:49,064 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,064 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,064 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:49,064 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:49,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8akk4c_e
2023-07-26 05:28:49,065 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05a47253-25be-4bce-becc-7b07b2f2f118
2023-07-26 05:28:49,065 - distributed.worker - INFO - Starting Worker plugin PreImport-c49accd5-3cc7-4739-a413-22c8fea566a6
2023-07-26 05:28:49,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e16446b2-bb2c-4c1a-94d0-2a50fbb7420d
2023-07-26 05:28:49,066 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42185
2023-07-26 05:28:49,066 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42185
2023-07-26 05:28:49,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33047
2023-07-26 05:28:49,067 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,067 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,067 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:49,067 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:49,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7h4xyym5
2023-07-26 05:28:49,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41631
2023-07-26 05:28:49,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41631
2023-07-26 05:28:49,067 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59e7aa3a-080c-4370-bb1d-a0bfe43d9d91
2023-07-26 05:28:49,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34343
2023-07-26 05:28:49,067 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,067 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,067 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:49,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33037
2023-07-26 05:28:49,067 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:49,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33037
2023-07-26 05:28:49,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7atrdxyi
2023-07-26 05:28:49,068 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44059
2023-07-26 05:28:49,068 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,068 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,068 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:49,068 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:49,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g0xh2nsp
2023-07-26 05:28:49,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46ad4d5e-a593-4f15-9f48-a067f289716c
2023-07-26 05:28:49,068 - distributed.worker - INFO - Starting Worker plugin PreImport-cc3d70cd-8444-4b2d-b79a-e7b5c4a1df6a
2023-07-26 05:28:49,069 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e9adb21-a7a6-4afd-be7c-522cef632f37
2023-07-26 05:28:49,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0e35d22-5721-425e-93a2-2fa56170d4da
2023-07-26 05:28:49,080 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a6b9cad-b79e-4f19-a563-e89760aa11ea
2023-07-26 05:28:49,082 - distributed.worker - INFO - Starting Worker plugin PreImport-070d17a6-9b2e-439a-a506-83ef234c6caa
2023-07-26 05:28:49,082 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,082 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2b5a104-6755-49eb-8118-32485d42fad6
2023-07-26 05:28:49,083 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,083 - distributed.worker - INFO - Starting Worker plugin PreImport-0e90ce3e-043e-45ef-9dc4-dbda798b561b
2023-07-26 05:28:49,083 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,103 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33037', status: init, memory: 0, processing: 0>
2023-07-26 05:28:49,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33037
2023-07-26 05:28:49,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47878
2023-07-26 05:28:49,104 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,104 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,104 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37251', status: init, memory: 0, processing: 0>
2023-07-26 05:28:49,105 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37251
2023-07-26 05:28:49,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47876
2023-07-26 05:28:49,105 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,106 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:49,108 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:49,109 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42185', status: init, memory: 0, processing: 0>
2023-07-26 05:28:49,109 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42185
2023-07-26 05:28:49,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47888
2023-07-26 05:28:49,110 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,110 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41631', status: init, memory: 0, processing: 0>
2023-07-26 05:28:49,110 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41631
2023-07-26 05:28:49,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47892
2023-07-26 05:28:49,111 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:49,111 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:49,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:49,113 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:49,170 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,171 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,171 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,171 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,171 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,171 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,171 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,172 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:49,176 - distributed.scheduler - INFO - Remove client Client-492e8cbc-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:49,176 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47708; closing.
2023-07-26 05:28:49,176 - distributed.scheduler - INFO - Remove client Client-492e8cbc-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:49,176 - distributed.scheduler - INFO - Close client connection: Client-492e8cbc-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:49,177 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43647'. Reason: nanny-close
2023-07-26 05:28:49,178 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40673'. Reason: nanny-close
2023-07-26 05:28:49,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,179 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42185. Reason: nanny-close
2023-07-26 05:28:49,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39095'. Reason: nanny-close
2023-07-26 05:28:49,180 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,180 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42467'. Reason: nanny-close
2023-07-26 05:28:49,181 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,181 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44025. Reason: nanny-close
2023-07-26 05:28:49,181 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44197'. Reason: nanny-close
2023-07-26 05:28:49,181 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,181 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47888; closing.
2023-07-26 05:28:49,181 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,182 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35177. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42185', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36657'. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,182 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32905. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46559'. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41631. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,183 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,183 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36519'. Reason: nanny-close
2023-07-26 05:28:49,183 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,183 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37251. Reason: nanny-close
2023-07-26 05:28:49,183 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:49,183 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32771. Reason: nanny-close
2023-07-26 05:28:49,183 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47868; closing.
2023-07-26 05:28:49,184 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,184 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,184 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44025', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,184 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,184 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44025
2023-07-26 05:28:49,184 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33037. Reason: nanny-close
2023-07-26 05:28:49,184 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,184 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47836; closing.
2023-07-26 05:28:49,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,185 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42185
2023-07-26 05:28:49,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35177', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,185 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,185 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35177
2023-07-26 05:28:49,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,186 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:49,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47820; closing.
2023-07-26 05:28:49,186 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47892; closing.
2023-07-26 05:28:49,186 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,186 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32905', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,186 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32905
2023-07-26 05:28:49,187 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,187 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:49,187 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41631', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,187 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41631
2023-07-26 05:28:49,187 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47852; closing.
2023-07-26 05:28:49,188 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32771', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,188 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32771
2023-07-26 05:28:49,188 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47876; closing.
2023-07-26 05:28:49,189 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47878; closing.
2023-07-26 05:28:49,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37251', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,189 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37251
2023-07-26 05:28:49,190 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33037', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:49,190 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-07-26 05:28:49,190 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:28:49,191 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:32905'.
2023-07-26 05:28:49,191 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:32905'. Shutting down.
2023-07-26 05:28:49,192 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:50,494 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:28:50,495 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:28:50,495 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:28:50,496 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:28:50,496 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-07-26 05:28:52,220 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:52,224 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:28:52,227 - distributed.scheduler - INFO - State start
2023-07-26 05:28:52,245 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:28:52,246 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:28:52,246 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:28:52,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35793'
2023-07-26 05:28:52,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38269'
2023-07-26 05:28:52,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46363'
2023-07-26 05:28:52,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40767'
2023-07-26 05:28:52,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39739'
2023-07-26 05:28:52,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44655'
2023-07-26 05:28:52,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36241'
2023-07-26 05:28:52,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42289'
2023-07-26 05:28:53,590 - distributed.scheduler - INFO - Receive client connection: Client-4d945864-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:53,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43964
2023-07-26 05:28:53,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,874 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:53,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,939 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:53,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:53,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:53,956 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,956 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,957 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:53,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:53,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:28:53,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:28:54,128 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:54,149 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:54,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:28:55,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39073
2023-07-26 05:28:55,537 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39073
2023-07-26 05:28:55,537 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34723
2023-07-26 05:28:55,537 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:55,537 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:55,537 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:55,537 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:55,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lymzje97
2023-07-26 05:28:55,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc4157f7-2003-44c7-b519-95891c3743b4
2023-07-26 05:28:55,840 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ba41f60-656a-4f8d-87bf-91c63bd412ce
2023-07-26 05:28:55,840 - distributed.worker - INFO - Starting Worker plugin PreImport-3e0f2426-3c82-4a75-9048-4f8c9a97a512
2023-07-26 05:28:55,841 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:55,865 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39073', status: init, memory: 0, processing: 0>
2023-07-26 05:28:55,867 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39073
2023-07-26 05:28:55,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43986
2023-07-26 05:28:55,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:55,868 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:55,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44443
2023-07-26 05:28:56,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44443
2023-07-26 05:28:56,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33065
2023-07-26 05:28:56,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42435
2023-07-26 05:28:56,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33065
2023-07-26 05:28:56,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32887
2023-07-26 05:28:56,519 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,519 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,519 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yydj4rxx
2023-07-26 05:28:56,520 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,520 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xf6tlosp
2023-07-26 05:28:56,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ab64ac0-ccbb-452e-9db8-39dbcee69f9f
2023-07-26 05:28:56,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-234880e1-716d-4c60-ba5f-862e24388e86
2023-07-26 05:28:56,523 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34159
2023-07-26 05:28:56,523 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34159
2023-07-26 05:28:56,523 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37729
2023-07-26 05:28:56,523 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,523 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,523 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,523 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,523 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b3363l1f
2023-07-26 05:28:56,524 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9922998a-2497-4229-a83b-32ee29b17c6d
2023-07-26 05:28:56,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46025
2023-07-26 05:28:56,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46025
2023-07-26 05:28:56,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35747
2023-07-26 05:28:56,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,525 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,525 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,526 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ugfj7qds
2023-07-26 05:28:56,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3cb3ff7-bec5-4571-aa21-80c1e2401dc7
2023-07-26 05:28:56,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36205
2023-07-26 05:28:56,529 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36205
2023-07-26 05:28:56,529 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34699
2023-07-26 05:28:56,529 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,529 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,529 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z5nw1mtp
2023-07-26 05:28:56,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6be09775-4d64-4efe-a506-ca7b634bd1f7
2023-07-26 05:28:56,531 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46731
2023-07-26 05:28:56,532 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46731
2023-07-26 05:28:56,532 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33581
2023-07-26 05:28:56,532 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,532 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,532 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,532 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,532 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l52gtlql
2023-07-26 05:28:56,533 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56f7b0e9-7542-471d-80a5-2ad7343096fa
2023-07-26 05:28:56,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37973
2023-07-26 05:28:56,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37973
2023-07-26 05:28:56,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41935
2023-07-26 05:28:56,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,534 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,534 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:28:56,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:28:56,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p3rrpmvf
2023-07-26 05:28:56,535 - distributed.worker - INFO - Starting Worker plugin PreImport-f2022268-6d2a-4a8b-9b34-e1ee236fd9e4
2023-07-26 05:28:56,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-672b081b-5241-4974-a904-974d7eab3283
2023-07-26 05:28:56,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d69f7fe3-6c3b-45e5-870c-744593fdf8e5
2023-07-26 05:28:56,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26beac42-d22e-486d-ab90-7ea0f1bcf32d
2023-07-26 05:28:56,727 - distributed.worker - INFO - Starting Worker plugin PreImport-4fb21c64-d937-4464-b5f8-5c5525159e02
2023-07-26 05:28:56,727 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,739 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20d082c0-e467-4b92-a5e6-312766a43cf6
2023-07-26 05:28:56,739 - distributed.worker - INFO - Starting Worker plugin PreImport-1d596424-1da4-4f0a-9c5e-bca528c18072
2023-07-26 05:28:56,740 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,742 - distributed.worker - INFO - Starting Worker plugin PreImport-9bb69853-e91d-41e8-a8ea-84600bb9485f
2023-07-26 05:28:56,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a75671b-ffab-4245-af70-e558208b96a8
2023-07-26 05:28:56,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91b40d0f-20ef-4653-a083-51bc7bd96d1f
2023-07-26 05:28:56,742 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b69cf8d-c475-47f5-b15a-94373ff04f7e
2023-07-26 05:28:56,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3efa9b43-5667-46a0-92b8-26d23cf6817d
2023-07-26 05:28:56,744 - distributed.worker - INFO - Starting Worker plugin PreImport-f64bb6fd-e21e-49d8-aeb3-d753f3ca0c7f
2023-07-26 05:28:56,744 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,744 - distributed.worker - INFO - Starting Worker plugin PreImport-a24b7adf-11d3-4e56-908a-4d840af4db21
2023-07-26 05:28:56,744 - distributed.worker - INFO - Starting Worker plugin PreImport-894f2a7b-9a6b-4283-8a5b-f37822a8d7e6
2023-07-26 05:28:56,744 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,745 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,745 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44443', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44443
2023-07-26 05:28:56,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43988
2023-07-26 05:28:56,751 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,751 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,768 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46025', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46025
2023-07-26 05:28:56,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44034
2023-07-26 05:28:56,770 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,770 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,770 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46731', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,771 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46731
2023-07-26 05:28:56,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44044
2023-07-26 05:28:56,772 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36205', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,772 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,772 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,772 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36205
2023-07-26 05:28:56,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44018
2023-07-26 05:28:56,773 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,773 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,773 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37973', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,773 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37973
2023-07-26 05:28:56,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44058
2023-07-26 05:28:56,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,774 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,774 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,776 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,779 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33065', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,779 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33065
2023-07-26 05:28:56,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44004
2023-07-26 05:28:56,780 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,780 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,781 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34159', status: init, memory: 0, processing: 0>
2023-07-26 05:28:56,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34159
2023-07-26 05:28:56,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44066
2023-07-26 05:28:56,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:28:56,782 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:28:56,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,855 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,866 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,867 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:28:56,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:28:56,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:28:56,876 - distributed.scheduler - INFO - Remove client Client-4d945864-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:56,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43964; closing.
2023-07-26 05:28:56,876 - distributed.scheduler - INFO - Remove client Client-4d945864-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:56,877 - distributed.scheduler - INFO - Close client connection: Client-4d945864-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:28:56,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35793'. Reason: nanny-close
2023-07-26 05:28:56,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38269'. Reason: nanny-close
2023-07-26 05:28:56,879 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46363'. Reason: nanny-close
2023-07-26 05:28:56,880 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37973. Reason: nanny-close
2023-07-26 05:28:56,880 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,880 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36205. Reason: nanny-close
2023-07-26 05:28:56,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40767'. Reason: nanny-close
2023-07-26 05:28:56,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,881 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44443. Reason: nanny-close
2023-07-26 05:28:56,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39739'. Reason: nanny-close
2023-07-26 05:28:56,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,882 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46025. Reason: nanny-close
2023-07-26 05:28:56,882 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44058; closing.
2023-07-26 05:28:56,882 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44655'. Reason: nanny-close
2023-07-26 05:28:56,882 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37973', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,882 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,882 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37973
2023-07-26 05:28:56,882 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36241'. Reason: nanny-close
2023-07-26 05:28:56,882 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33065. Reason: nanny-close
2023-07-26 05:28:56,882 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,883 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42289'. Reason: nanny-close
2023-07-26 05:28:56,883 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44018; closing.
2023-07-26 05:28:56,883 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,883 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:28:56,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34159. Reason: nanny-close
2023-07-26 05:28:56,883 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39073. Reason: nanny-close
2023-07-26 05:28:56,883 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,884 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,884 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36205', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,884 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37973
2023-07-26 05:28:56,884 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36205
2023-07-26 05:28:56,884 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,884 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43988; closing.
2023-07-26 05:28:56,884 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46731. Reason: nanny-close
2023-07-26 05:28:56,885 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37973
2023-07-26 05:28:56,885 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44443', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,885 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44443
2023-07-26 05:28:56,885 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37973
2023-07-26 05:28:56,885 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44034; closing.
2023-07-26 05:28:56,886 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46025', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,886 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46025
2023-07-26 05:28:56,886 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37973
2023-07-26 05:28:56,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,886 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43986; closing.
2023-07-26 05:28:56,887 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44004; closing.
2023-07-26 05:28:56,887 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:28:56,887 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39073', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,887 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39073
2023-07-26 05:28:56,887 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33065', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,888 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,888 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33065
2023-07-26 05:28:56,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44066; closing.
2023-07-26 05:28:56,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44044; closing.
2023-07-26 05:28:56,889 - distributed.nanny - INFO - Worker closed
2023-07-26 05:28:56,889 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34159', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,889 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34159
2023-07-26 05:28:56,890 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46731', status: closing, memory: 0, processing: 0>
2023-07-26 05:28:56,890 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46731
2023-07-26 05:28:56,890 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:28:58,245 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:28:58,245 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:28:58,245 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:28:58,246 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:28:58,246 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-07-26 05:29:00,008 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:00,011 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:00,014 - distributed.scheduler - INFO - State start
2023-07-26 05:29:00,034 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:00,034 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:00,035 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:00,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44333'
2023-07-26 05:29:00,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34659'
2023-07-26 05:29:00,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45259'
2023-07-26 05:29:00,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45521'
2023-07-26 05:29:00,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34821'
2023-07-26 05:29:00,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33907'
2023-07-26 05:29:00,295 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41671'
2023-07-26 05:29:00,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36253'
2023-07-26 05:29:00,587 - distributed.scheduler - INFO - Receive client connection: Client-523521d2-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:00,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44254
2023-07-26 05:29:01,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,789 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:01,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,883 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,883 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,885 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,885 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,891 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:01,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:01,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:02,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:02,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:02,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:02,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:02,050 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:02,050 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:02,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:03,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45449
2023-07-26 05:29:03,336 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45449
2023-07-26 05:29:03,336 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35431
2023-07-26 05:29:03,337 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:03,337 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:03,337 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:03,337 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:03,337 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ec675o9y
2023-07-26 05:29:03,337 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-576bcfe6-54ad-486f-8306-3b8ad3aec4f4
2023-07-26 05:29:03,337 - distributed.worker - INFO - Starting Worker plugin PreImport-d47d2bed-c287-4053-8afd-017ac0121e2c
2023-07-26 05:29:03,338 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2099653e-2c58-42cc-854c-9259feb80dc7
2023-07-26 05:29:03,648 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:03,672 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45449', status: init, memory: 0, processing: 0>
2023-07-26 05:29:03,674 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45449
2023-07-26 05:29:03,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53258
2023-07-26 05:29:03,675 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:03,675 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:03,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38687
2023-07-26 05:29:04,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38687
2023-07-26 05:29:04,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36633
2023-07-26 05:29:04,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,402 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,402 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3nth63iv
2023-07-26 05:29:04,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5f24352-a9c7-45b9-8241-b0e9811d998e
2023-07-26 05:29:04,412 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37877
2023-07-26 05:29:04,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37877
2023-07-26 05:29:04,412 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34471
2023-07-26 05:29:04,412 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,412 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,412 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pcy2t_tk
2023-07-26 05:29:04,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7adea3fd-c5e7-4f7a-9d39-54d522f7cf8d
2023-07-26 05:29:04,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42485
2023-07-26 05:29:04,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42485
2023-07-26 05:29:04,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36553
2023-07-26 05:29:04,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,414 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,414 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,414 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-47q15lqa
2023-07-26 05:29:04,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-320a794c-144a-40d2-8f75-1373fa4e14c2
2023-07-26 05:29:04,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44879
2023-07-26 05:29:04,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44879
2023-07-26 05:29:04,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39597
2023-07-26 05:29:04,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,416 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,416 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,416 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9vt3ywk2
2023-07-26 05:29:04,416 - distributed.worker - INFO - Starting Worker plugin PreImport-edfd0ccf-89c0-486c-8748-b71d1040b741
2023-07-26 05:29:04,417 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf836fc1-a481-48ac-8af8-af16ac7c76b5
2023-07-26 05:29:04,417 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dda207fc-0009-44df-9dea-39c1c55b366a
2023-07-26 05:29:04,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42671
2023-07-26 05:29:04,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42671
2023-07-26 05:29:04,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45887
2023-07-26 05:29:04,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,419 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,419 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e_l_ujbc
2023-07-26 05:29:04,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8636f88f-73c9-494c-bf76-08aa2f1750ab
2023-07-26 05:29:04,427 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46289
2023-07-26 05:29:04,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46289
2023-07-26 05:29:04,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45489
2023-07-26 05:29:04,428 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,428 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,428 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,428 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-263x5xlt
2023-07-26 05:29:04,428 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35271
2023-07-26 05:29:04,428 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35271
2023-07-26 05:29:04,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c92af4c-bd9f-4585-81a7-96f1f9bc6718
2023-07-26 05:29:04,428 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40861
2023-07-26 05:29:04,428 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,428 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,428 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:04,428 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:04,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ohmoh9kw
2023-07-26 05:29:04,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-293408ed-88a6-4d9d-a93b-679c7d599aa8
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c5ceb0d-5f32-4c2b-b378-4f9ae86d8603
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e532b109-8649-4306-8c08-370db3c67107
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2938c437-ae34-48bc-a6c1-939f69c07055
2023-07-26 05:29:04,570 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-647bcb00-c470-475a-afa5-0b3b3c3ac874
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin PreImport-25b8017f-1e7a-4b70-b637-88f2a429921d
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin PreImport-52fb9f9c-d34e-40b1-9eb3-1098691e2d11
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin PreImport-e2512c54-311f-4e26-8f35-9bba8a52412d
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin PreImport-17ec4715-3f6c-4a64-934a-9f75de8bab5d
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin PreImport-ef753ba8-6f5c-4b9b-813e-45941c2cf7e2
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86c28a24-56da-464a-b757-f628dcc443bd
2023-07-26 05:29:04,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2dfc0e8-eafd-4eb4-9217-7c18fbd59b7f
2023-07-26 05:29:04,570 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,570 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,571 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,571 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,571 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,571 - distributed.worker - INFO - Starting Worker plugin PreImport-9a7a7eda-4955-4aaa-aa33-330ab6424810
2023-07-26 05:29:04,571 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,594 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42485', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,595 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42485
2023-07-26 05:29:04,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53278
2023-07-26 05:29:04,595 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,596 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46289', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,597 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46289
2023-07-26 05:29:04,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53298
2023-07-26 05:29:04,597 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,597 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44879', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44879
2023-07-26 05:29:04,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53274
2023-07-26 05:29:04,599 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,599 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,599 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42671', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,600 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42671
2023-07-26 05:29:04,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53292
2023-07-26 05:29:04,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,600 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38687', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,604 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38687
2023-07-26 05:29:04,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53322
2023-07-26 05:29:04,605 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,605 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35271', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,606 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35271
2023-07-26 05:29:04,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53336
2023-07-26 05:29:04,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,607 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37877', status: init, memory: 0, processing: 0>
2023-07-26 05:29:04,607 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37877
2023-07-26 05:29:04,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53306
2023-07-26 05:29:04,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,608 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:04,608 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:04,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:04,711 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,711 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,711 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,711 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,711 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,712 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,715 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,715 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-26 05:29:04,720 - distributed.scheduler - INFO - Remove client Client-523521d2-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:04,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44254; closing.
2023-07-26 05:29:04,721 - distributed.scheduler - INFO - Remove client Client-523521d2-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:04,722 - distributed.scheduler - INFO - Close client connection: Client-523521d2-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:04,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45521'. Reason: nanny-close
2023-07-26 05:29:04,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,724 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44333'. Reason: nanny-close
2023-07-26 05:29:04,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44879. Reason: nanny-close
2023-07-26 05:29:04,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34659'. Reason: nanny-close
2023-07-26 05:29:04,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46289. Reason: nanny-close
2023-07-26 05:29:04,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45259'. Reason: nanny-close
2023-07-26 05:29:04,726 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34821'. Reason: nanny-close
2023-07-26 05:29:04,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35271. Reason: nanny-close
2023-07-26 05:29:04,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53274; closing.
2023-07-26 05:29:04,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37877. Reason: nanny-close
2023-07-26 05:29:04,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33907'. Reason: nanny-close
2023-07-26 05:29:04,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44879', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,728 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,728 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42671. Reason: nanny-close
2023-07-26 05:29:04,728 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41671'. Reason: nanny-close
2023-07-26 05:29:04,729 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,729 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42485. Reason: nanny-close
2023-07-26 05:29:04,729 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36253'. Reason: nanny-close
2023-07-26 05:29:04,730 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:04,730 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,730 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,730 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53298; closing.
2023-07-26 05:29:04,730 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,731 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,731 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45449. Reason: nanny-close
2023-07-26 05:29:04,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46289', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,731 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46289
2023-07-26 05:29:04,732 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,732 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38687. Reason: nanny-close
2023-07-26 05:29:04,732 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,732 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53292; closing.
2023-07-26 05:29:04,732 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53336; closing.
2023-07-26 05:29:04,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53306; closing.
2023-07-26 05:29:04,733 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42671', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,734 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42671
2023-07-26 05:29:04,734 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-26 05:29:04,734 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,734 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35271', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,735 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35271
2023-07-26 05:29:04,735 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:04,735 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37877', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,735 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37877
2023-07-26 05:29:04,736 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53278; closing.
2023-07-26 05:29:04,736 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,737 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42485', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,737 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42485
2023-07-26 05:29:04,737 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,737 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53258; closing.
2023-07-26 05:29:04,738 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:04,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45449', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,739 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45449
2023-07-26 05:29:04,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53322; closing.
2023-07-26 05:29:04,740 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38687', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:04,740 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38687
2023-07-26 05:29:04,741 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:04,741 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53322>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-26 05:29:06,090 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:06,090 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:06,091 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:06,092 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:06,092 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-07-26 05:29:07,911 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:07,915 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:07,918 - distributed.scheduler - INFO - State start
2023-07-26 05:29:08,184 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:08,185 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:08,186 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:08,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33759'
2023-07-26 05:29:08,978 - distributed.scheduler - INFO - Receive client connection: Client-56e63870-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:08,991 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53424
2023-07-26 05:29:09,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:09,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:29:10,222 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:10,241 - distributed.scheduler - INFO - Receive client connection: Client-592d0e75-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:10,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53444
2023-07-26 05:29:11,071 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40799
2023-07-26 05:29:11,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40799
2023-07-26 05:29:11,072 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-07-26 05:29:11,072 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:11,072 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:11,072 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:11,072 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:29:11,072 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g5dby47p
2023-07-26 05:29:11,072 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ea24ab5-8413-43c1-9ea2-c9a2fcc6f61b
2023-07-26 05:29:11,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25068989-8068-4517-ada7-8f9d9d96484a
2023-07-26 05:29:11,072 - distributed.worker - INFO - Starting Worker plugin PreImport-7bcbf95c-f13d-4533-b340-7cdbca316172
2023-07-26 05:29:11,073 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:11,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40799', status: init, memory: 0, processing: 0>
2023-07-26 05:29:11,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40799
2023-07-26 05:29:11,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38458
2023-07-26 05:29:11,094 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:11,095 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:11,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:11,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:11,133 - distributed.scheduler - INFO - Remove client Client-56e63870-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:11,133 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53424; closing.
2023-07-26 05:29:11,133 - distributed.scheduler - INFO - Remove client Client-56e63870-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:11,134 - distributed.scheduler - INFO - Close client connection: Client-56e63870-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:11,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33759'. Reason: nanny-close
2023-07-26 05:29:11,140 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:11,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40799. Reason: nanny-close
2023-07-26 05:29:11,142 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:11,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38458; closing.
2023-07-26 05:29:11,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40799', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:11,142 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40799
2023-07-26 05:29:11,143 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:11,143 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:12,051 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:12,051 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:12,052 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:12,053 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:12,054 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-07-26 05:29:15,472 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:15,476 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:15,479 - distributed.scheduler - INFO - State start
2023-07-26 05:29:15,550 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:15,551 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:15,552 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:15,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34933', status: init, memory: 0, processing: 0>
2023-07-26 05:29:15,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34933
2023-07-26 05:29:15,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38888
2023-07-26 05:29:15,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40659'
2023-07-26 05:29:16,009 - distributed.scheduler - INFO - Receive client connection: Client-592d0e75-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:16,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38924
2023-07-26 05:29:16,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39115', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39115
2023-07-26 05:29:16,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38930
2023-07-26 05:29:16,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34977', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34977
2023-07-26 05:29:16,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38944
2023-07-26 05:29:16,274 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43303', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43303
2023-07-26 05:29:16,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38950
2023-07-26 05:29:16,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36993', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,283 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36993
2023-07-26 05:29:16,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38954
2023-07-26 05:29:16,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35677', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35677
2023-07-26 05:29:16,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38962
2023-07-26 05:29:16,339 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35893', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,339 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35893
2023-07-26 05:29:16,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38972
2023-07-26 05:29:16,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41131', status: init, memory: 0, processing: 0>
2023-07-26 05:29:16,343 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41131
2023-07-26 05:29:16,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38976
2023-07-26 05:29:16,411 - distributed.scheduler - INFO - Remove client Client-592d0e75-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:16,411 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38924; closing.
2023-07-26 05:29:16,411 - distributed.scheduler - INFO - Remove client Client-592d0e75-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:16,412 - distributed.scheduler - INFO - Close client connection: Client-592d0e75-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:16,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38930; closing.
2023-07-26 05:29:16,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39115', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,417 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39115
2023-07-26 05:29:16,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38976; closing.
2023-07-26 05:29:16,419 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41131', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,419 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41131
2023-07-26 05:29:16,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38954; closing.
2023-07-26 05:29:16,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36993', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,420 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36993
2023-07-26 05:29:16,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38950; closing.
2023-07-26 05:29:16,421 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43303', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,421 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43303
2023-07-26 05:29:16,422 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38888; closing.
2023-07-26 05:29:16,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34933', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,422 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34933
2023-07-26 05:29:16,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38944; closing.
2023-07-26 05:29:16,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38962; closing.
2023-07-26 05:29:16,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38972; closing.
2023-07-26 05:29:16,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34977', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34977
2023-07-26 05:29:16,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35677', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35677
2023-07-26 05:29:16,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35893', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:16,425 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35893
2023-07-26 05:29:16,425 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:17,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:17,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:17,807 - distributed.scheduler - INFO - Receive client connection: Client-5b69be2f-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:17,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38988
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:29:17,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:17,862 - distributed.scheduler - INFO - Receive client connection: Client-5db7ceef-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:17,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39004
2023-07-26 05:29:18,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33933
2023-07-26 05:29:18,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33933
2023-07-26 05:29:18,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35781
2023-07-26 05:29:18,766 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:18,766 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:18,766 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:18,766 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:29:18,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7gvd39lp
2023-07-26 05:29:18,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4db6887e-6263-4560-9e39-37f5496bdc5b
2023-07-26 05:29:18,766 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f41e5dd-de19-449c-b9b9-f938a54cf8dc
2023-07-26 05:29:18,767 - distributed.worker - INFO - Starting Worker plugin PreImport-37e280ab-30a3-4192-8eab-982a167a97e0
2023-07-26 05:29:18,768 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:18,790 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33933', status: init, memory: 0, processing: 0>
2023-07-26 05:29:18,791 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33933
2023-07-26 05:29:18,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39030
2023-07-26 05:29:18,792 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:18,792 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:18,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:18,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:18,839 - distributed.scheduler - INFO - Remove client Client-5b69be2f-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:18,839 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38988; closing.
2023-07-26 05:29:18,840 - distributed.scheduler - INFO - Remove client Client-5b69be2f-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:18,840 - distributed.scheduler - INFO - Close client connection: Client-5b69be2f-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:18,841 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40659'. Reason: nanny-close
2023-07-26 05:29:18,841 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:18,842 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33933. Reason: nanny-close
2023-07-26 05:29:18,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:18,844 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39030; closing.
2023-07-26 05:29:18,844 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33933', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:18,844 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33933
2023-07-26 05:29:18,844 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:18,845 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:19,807 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:19,807 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:19,808 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:19,810 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:19,810 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-07-26 05:29:21,614 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:21,618 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:21,621 - distributed.scheduler - INFO - State start
2023-07-26 05:29:21,749 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:21,750 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:21,750 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:21,817 - distributed.scheduler - INFO - Receive client connection: Client-5db7ceef-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:21,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51106
2023-07-26 05:29:23,404 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34867', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,405 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34867
2023-07-26 05:29:23,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51160
2023-07-26 05:29:23,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37673', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37673
2023-07-26 05:29:23,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51164
2023-07-26 05:29:23,651 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40635', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40635
2023-07-26 05:29:23,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51178
2023-07-26 05:29:23,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40111', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40111
2023-07-26 05:29:23,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51172
2023-07-26 05:29:23,783 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36457', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,784 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36457
2023-07-26 05:29:23,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51182
2023-07-26 05:29:23,794 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38841', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,794 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38841
2023-07-26 05:29:23,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51192
2023-07-26 05:29:23,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43109', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,796 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43109
2023-07-26 05:29:23,796 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51202
2023-07-26 05:29:23,796 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38131', status: init, memory: 0, processing: 0>
2023-07-26 05:29:23,797 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38131
2023-07-26 05:29:23,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51198
2023-07-26 05:29:23,852 - distributed.scheduler - INFO - Remove client Client-5db7ceef-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:23,852 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51106; closing.
2023-07-26 05:29:23,853 - distributed.scheduler - INFO - Remove client Client-5db7ceef-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:23,853 - distributed.scheduler - INFO - Close client connection: Client-5db7ceef-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:23,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51198; closing.
2023-07-26 05:29:23,858 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38131', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,858 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38131
2023-07-26 05:29:23,861 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51202; closing.
2023-07-26 05:29:23,862 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43109', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,862 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43109
2023-07-26 05:29:23,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51164; closing.
2023-07-26 05:29:23,863 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37673', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37673
2023-07-26 05:29:23,864 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51160; closing.
2023-07-26 05:29:23,865 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34867', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,865 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34867
2023-07-26 05:29:23,866 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51178; closing.
2023-07-26 05:29:23,866 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51192; closing.
2023-07-26 05:29:23,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51172; closing.
2023-07-26 05:29:23,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51182; closing.
2023-07-26 05:29:23,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40635', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,868 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40635
2023-07-26 05:29:23,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38841', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,868 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38841
2023-07-26 05:29:23,869 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40111', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,869 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40111
2023-07-26 05:29:23,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36457', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:23,870 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36457
2023-07-26 05:29:23,870 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:25,294 - distributed.scheduler - INFO - Receive client connection: Client-62260afb-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:25,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51204
2023-07-26 05:29:25,736 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:25,736 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:25,737 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:25,738 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:25,738 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-07-26 05:29:27,559 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:27,563 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36885 instead
  warnings.warn(
2023-07-26 05:29:27,566 - distributed.scheduler - INFO - State start
2023-07-26 05:29:27,585 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:27,586 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-26 05:29:27,586 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36885/status
2023-07-26 05:29:27,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33901'
2023-07-26 05:29:28,671 - distributed.scheduler - INFO - Receive client connection: Client-6298c61d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:28,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52562
2023-07-26 05:29:29,414 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:29,414 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:29,421 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:31,690 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41963
2023-07-26 05:29:31,690 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41963
2023-07-26 05:29:31,690 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39721
2023-07-26 05:29:31,690 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-26 05:29:31,690 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,690 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:31,690 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:29:31,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0desdf3v
2023-07-26 05:29:31,691 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79c77c2e-2742-4f00-90c7-71cb1c002106
2023-07-26 05:29:31,691 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efa49011-55ee-458e-a99a-19f1f9bdd1f3
2023-07-26 05:29:31,691 - distributed.worker - INFO - Starting Worker plugin PreImport-64ed0222-ea08-4f95-92b8-df5c293e14e4
2023-07-26 05:29:31,692 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,714 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41963', status: init, memory: 0, processing: 0>
2023-07-26 05:29:31,715 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41963
2023-07-26 05:29:31,716 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42040
2023-07-26 05:29:31,716 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-26 05:29:31,716 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:31,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-26 05:29:31,791 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:31,793 - distributed.scheduler - INFO - Remove client Client-6298c61d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:31,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52562; closing.
2023-07-26 05:29:31,794 - distributed.scheduler - INFO - Remove client Client-6298c61d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:31,794 - distributed.scheduler - INFO - Close client connection: Client-6298c61d-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:31,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33901'. Reason: nanny-close
2023-07-26 05:29:31,796 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:31,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41963. Reason: nanny-close
2023-07-26 05:29:31,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42040; closing.
2023-07-26 05:29:31,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-26 05:29:31,799 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41963', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:31,799 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41963
2023-07-26 05:29:31,799 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:31,800 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:32,812 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:32,812 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:32,813 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:32,813 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-26 05:29:32,814 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-07-26 05:29:34,670 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:34,673 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:34,676 - distributed.scheduler - INFO - State start
2023-07-26 05:29:34,695 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:34,695 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:34,696 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:34,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46259'
2023-07-26 05:29:34,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36207'
2023-07-26 05:29:34,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38197'
2023-07-26 05:29:34,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40305'
2023-07-26 05:29:34,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38467'
2023-07-26 05:29:34,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46799'
2023-07-26 05:29:34,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40085'
2023-07-26 05:29:34,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40687'
2023-07-26 05:29:35,097 - distributed.scheduler - INFO - Receive client connection: Client-67147a40-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:35,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58478
2023-07-26 05:29:35,262 - distributed.scheduler - INFO - Receive client connection: Client-66dad571-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:35,263 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58502
2023-07-26 05:29:36,443 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,443 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,670 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,670 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:36,686 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:36,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,749 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:36,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:37,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33793
2023-07-26 05:29:37,778 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33793
2023-07-26 05:29:37,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46613
2023-07-26 05:29:37,778 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:37,778 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:37,778 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:37,778 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:37,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tjwvixu9
2023-07-26 05:29:37,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b734329-aaeb-4554-97b3-ad5199c13b33
2023-07-26 05:29:38,524 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c280bf98-953e-4a63-8354-d8749b9d818d
2023-07-26 05:29:38,524 - distributed.worker - INFO - Starting Worker plugin PreImport-22129def-a7dc-4232-9f6a-fecb88336d16
2023-07-26 05:29:38,524 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:38,551 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33793', status: init, memory: 0, processing: 0>
2023-07-26 05:29:38,552 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33793
2023-07-26 05:29:38,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58622
2023-07-26 05:29:38,553 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:38,553 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:38,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:40,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42257
2023-07-26 05:29:40,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42257
2023-07-26 05:29:40,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34267
2023-07-26 05:29:40,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,401 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,401 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fohngi0f
2023-07-26 05:29:40,401 - distributed.worker - INFO - Starting Worker plugin RMMSetup-effe7fbe-9a72-4854-9f8b-86398a53c927
2023-07-26 05:29:40,407 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42689
2023-07-26 05:29:40,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42689
2023-07-26 05:29:40,407 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34405
2023-07-26 05:29:40,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41133
2023-07-26 05:29:40,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34405
2023-07-26 05:29:40,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,407 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,407 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41383
2023-07-26 05:29:40,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dtoe8hqa
2023-07-26 05:29:40,407 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,407 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,408 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,408 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9db2dld6
2023-07-26 05:29:40,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f632040-3f8b-4518-815e-16cc148b9f5e
2023-07-26 05:29:40,408 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45857
2023-07-26 05:29:40,408 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45857
2023-07-26 05:29:40,408 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43727
2023-07-26 05:29:40,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f38fa83b-1108-4168-a3fb-21eaae00e85a
2023-07-26 05:29:40,409 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,409 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,409 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,409 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bb7_72i5
2023-07-26 05:29:40,409 - distributed.worker - INFO - Starting Worker plugin RMMSetup-681411b5-7211-4db8-8334-871d9159b1e6
2023-07-26 05:29:40,414 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35769
2023-07-26 05:29:40,414 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35769
2023-07-26 05:29:40,414 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43081
2023-07-26 05:29:40,414 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,415 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,415 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,415 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r4gs1bfr
2023-07-26 05:29:40,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e0bbc6d-8fe8-41fa-bcb1-8085af0312a1
2023-07-26 05:29:40,664 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf7e51ab-4aba-4e54-a2d4-929bb80b5113
2023-07-26 05:29:40,665 - distributed.worker - INFO - Starting Worker plugin PreImport-6f1aa61d-6514-42f1-a6d7-57d8289dd8f2
2023-07-26 05:29:40,665 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,681 - distributed.worker - INFO - Starting Worker plugin PreImport-bc29e92a-8fc5-471f-a499-03b0644f4be4
2023-07-26 05:29:40,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d7ed158-6ffc-4dc6-8726-ab9b7d3e7e93
2023-07-26 05:29:40,682 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,682 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce5711ba-c3f4-45d5-90eb-7f2b0e5bb030
2023-07-26 05:29:40,682 - distributed.worker - INFO - Starting Worker plugin PreImport-5b3bb375-e0f7-4f94-b3bb-f487dc327f26
2023-07-26 05:29:40,683 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,694 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-97e650b5-10e1-4bf3-9ae3-ca4cf4c6dacd
2023-07-26 05:29:40,694 - distributed.worker - INFO - Starting Worker plugin PreImport-082e81a4-604f-4b66-8b88-d5ac0fd8c932
2023-07-26 05:29:40,694 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,706 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34405', status: init, memory: 0, processing: 0>
2023-07-26 05:29:40,707 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34405
2023-07-26 05:29:40,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58648
2023-07-26 05:29:40,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,707 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:40,717 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3b15a70-1b49-4632-9119-c8c7e3e8b721
2023-07-26 05:29:40,717 - distributed.worker - INFO - Starting Worker plugin PreImport-0bbfaea6-ad05-4b63-9248-c563720db020
2023-07-26 05:29:40,717 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,720 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42257', status: init, memory: 0, processing: 0>
2023-07-26 05:29:40,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42257
2023-07-26 05:29:40,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58654
2023-07-26 05:29:40,721 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,721 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45857', status: init, memory: 0, processing: 0>
2023-07-26 05:29:40,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45857
2023-07-26 05:29:40,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58652
2023-07-26 05:29:40,723 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,723 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33931', status: init, memory: 0, processing: 0>
2023-07-26 05:29:40,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33931
2023-07-26 05:29:40,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58660
2023-07-26 05:29:40,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:40,725 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35769', status: init, memory: 0, processing: 0>
2023-07-26 05:29:40,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:40,726 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35769
2023-07-26 05:29:40,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58676
2023-07-26 05:29:40,726 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,726 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:40,741 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42689', status: init, memory: 0, processing: 0>
2023-07-26 05:29:40,741 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42689
2023-07-26 05:29:40,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58684
2023-07-26 05:29:40,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,742 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:40,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43515
2023-07-26 05:29:40,768 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43515
2023-07-26 05:29:40,768 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41727
2023-07-26 05:29:40,768 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,768 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,768 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,768 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,768 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rvz4qglf
2023-07-26 05:29:40,768 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e362f965-3378-4a84-81ae-d8fefbe602ba
2023-07-26 05:29:40,769 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44173
2023-07-26 05:29:40,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44173
2023-07-26 05:29:40,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35603
2023-07-26 05:29:40,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:40,769 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,769 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:40,770 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-26 05:29:40,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rlab67xv
2023-07-26 05:29:40,770 - distributed.worker - INFO - Starting Worker plugin PreImport-8e500c4c-a274-4895-8438-a9798d776b59
2023-07-26 05:29:40,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d48cfd4d-67cd-4e6c-8171-c49037375778
2023-07-26 05:29:40,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f00e7e2e-7353-465b-b82c-885f6e579062
2023-07-26 05:29:40,978 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e377969-5c18-436b-9ea3-75b8460000ef
2023-07-26 05:29:40,978 - distributed.worker - INFO - Starting Worker plugin PreImport-a4ef841c-0e50-412f-8725-69439dc10862
2023-07-26 05:29:40,979 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:40,979 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,002 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43515', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,003 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43515
2023-07-26 05:29:41,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37938
2023-07-26 05:29:41,003 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,004 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,005 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44173', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44173
2023-07-26 05:29:41,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37954
2023-07-26 05:29:41,014 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:41,014 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:41,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:41,607 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45339', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45339
2023-07-26 05:29:41,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37972
2023-07-26 05:29:41,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36665', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36665
2023-07-26 05:29:41,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37982
2023-07-26 05:29:41,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46655', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46655
2023-07-26 05:29:41,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37988
2023-07-26 05:29:41,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41913', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,662 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41913
2023-07-26 05:29:41,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37998
2023-07-26 05:29:41,680 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40115', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,680 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40115
2023-07-26 05:29:41,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38002
2023-07-26 05:29:41,681 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36283', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,682 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36283
2023-07-26 05:29:41,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38010
2023-07-26 05:29:41,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38171', status: init, memory: 0, processing: 0>
2023-07-26 05:29:41,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38171
2023-07-26 05:29:41,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38018
2023-07-26 05:29:49,201 - distributed.scheduler - INFO - Remove client Client-66dad571-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:49,202 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58502; closing.
2023-07-26 05:29:49,202 - distributed.scheduler - INFO - Remove client Client-66dad571-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:49,203 - distributed.scheduler - INFO - Close client connection: Client-66dad571-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:49,204 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36207'. Reason: nanny-close
2023-07-26 05:29:49,205 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,206 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38467'. Reason: nanny-close
2023-07-26 05:29:49,206 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,207 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46259'. Reason: nanny-close
2023-07-26 05:29:49,207 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44173. Reason: nanny-close
2023-07-26 05:29:49,207 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,208 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38197'. Reason: nanny-close
2023-07-26 05:29:49,208 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45857. Reason: nanny-close
2023-07-26 05:29:49,208 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,209 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43515. Reason: nanny-close
2023-07-26 05:29:49,209 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40305'. Reason: nanny-close
2023-07-26 05:29:49,209 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,210 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42689. Reason: nanny-close
2023-07-26 05:29:49,210 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46799'. Reason: nanny-close
2023-07-26 05:29:49,210 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37954; closing.
2023-07-26 05:29:49,210 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,210 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40085'. Reason: nanny-close
2023-07-26 05:29:49,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44173', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,211 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,211 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,211 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,211 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40687'. Reason: nanny-close
2023-07-26 05:29:49,211 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33793. Reason: nanny-close
2023-07-26 05:29:49,212 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:49,212 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42257. Reason: nanny-close
2023-07-26 05:29:49,212 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,213 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,213 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34405. Reason: nanny-close
2023-07-26 05:29:49,213 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,214 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,214 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,214 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,214 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35769. Reason: nanny-close
2023-07-26 05:29:49,215 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37938; closing.
2023-07-26 05:29:49,215 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,215 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58652; closing.
2023-07-26 05:29:49,215 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58684; closing.
2023-07-26 05:29:49,215 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,216 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
2023-07-26 05:29:49,216 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,216 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,216 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:49,217 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43515', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,217 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43515
2023-07-26 05:29:49,218 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45857', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,218 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45857
2023-07-26 05:29:49,218 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,218 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42689', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,218 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,219 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42689
2023-07-26 05:29:49,219 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,219 - distributed.nanny - INFO - Worker closed
2023-07-26 05:29:49,220 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58654; closing.
2023-07-26 05:29:49,221 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58622; closing.
2023-07-26 05:29:49,221 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58648; closing.
2023-07-26 05:29:49,222 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42257', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,222 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42257
2023-07-26 05:29:49,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33793', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,223 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33793
2023-07-26 05:29:49,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34405', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,224 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-07-26 05:29:49,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58676; closing.
2023-07-26 05:29:49,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35769', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,225 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35769
2023-07-26 05:29:49,227 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58676>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-26 05:29:49,318 - distributed.scheduler - INFO - Remove client Client-67147a40-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:49,318 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58478; closing.
2023-07-26 05:29:49,318 - distributed.scheduler - INFO - Remove client Client-67147a40-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:49,318 - distributed.scheduler - INFO - Close client connection: Client-67147a40-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:49,325 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38010; closing.
2023-07-26 05:29:49,325 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36283', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,325 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36283
2023-07-26 05:29:49,327 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37972; closing.
2023-07-26 05:29:49,327 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45339', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,327 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45339
2023-07-26 05:29:49,328 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37982; closing.
2023-07-26 05:29:49,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58660; closing.
2023-07-26 05:29:49,329 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36665', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,329 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36665
2023-07-26 05:29:49,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33931', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,330 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33931
2023-07-26 05:29:49,330 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38002; closing.
2023-07-26 05:29:49,331 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38018; closing.
2023-07-26 05:29:49,331 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40115', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,331 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40115
2023-07-26 05:29:49,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38171', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,332 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38171
2023-07-26 05:29:49,333 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37988; closing.
2023-07-26 05:29:49,333 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37998; closing.
2023-07-26 05:29:49,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46655', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,334 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46655
2023-07-26 05:29:49,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41913', status: closing, memory: 0, processing: 0>
2023-07-26 05:29:49,334 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41913
2023-07-26 05:29:49,335 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:29:49,335 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37998>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-26 05:29:49,335 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37988>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-26 05:29:50,772 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:29:50,773 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:50,773 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:50,775 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:29:50,775 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-07-26 05:29:52,778 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:52,782 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37199 instead
  warnings.warn(
2023-07-26 05:29:52,786 - distributed.scheduler - INFO - State start
2023-07-26 05:29:52,805 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:52,806 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:29:52,806 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:29:52,807 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-26 05:29:52,914 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36233'
2023-07-26 05:29:54,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:54,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:54,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:55,130 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40661
2023-07-26 05:29:55,130 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40661
2023-07-26 05:29:55,131 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45025
2023-07-26 05:29:55,131 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:55,131 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:55,131 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:55,131 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:29:55,131 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ka019n1h
2023-07-26 05:29:55,131 - distributed.worker - INFO - Starting Worker plugin PreImport-3949a26a-73fe-4c47-8aa3-f95805e8d04e
2023-07-26 05:29:55,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4129f7ae-0365-40ca-9816-7153003c1eab
2023-07-26 05:29:55,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9bc41f7-10bd-4122-81d0-f8b56ae486ad
2023-07-26 05:29:55,228 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:55,251 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:29:55,252 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:55,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:29:55,271 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:29:55,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:29:55,289 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36233'. Reason: nanny-close
2023-07-26 05:29:55,289 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:29:55,290 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40661. Reason: nanny-close
2023-07-26 05:29:55,292 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:29:55,293 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-07-26 05:29:57,679 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:57,683 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-26 05:29:57,685 - distributed.scheduler - INFO - State start
2023-07-26 05:29:57,703 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-26 05:29:57,704 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-26 05:29:57,704 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-26 05:29:57,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45619'
2023-07-26 05:29:57,933 - distributed.scheduler - INFO - Receive client connection: Client-749e77ad-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:29:57,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56328
2023-07-26 05:29:58,420 - distributed.scheduler - INFO - Receive client connection: Client-75e4aa7a-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:29:58,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56336
2023-07-26 05:29:59,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:29:59,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:29:59,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-26 05:29:59,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33411
2023-07-26 05:29:59,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33411
2023-07-26 05:29:59,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43345
2023-07-26 05:29:59,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-26 05:29:59,993 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:29:59,993 - distributed.worker - INFO -               Threads:                          1
2023-07-26 05:29:59,993 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-26 05:29:59,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r8eo9jos
2023-07-26 05:29:59,993 - distributed.worker - INFO - Starting Worker plugin PreImport-ccffc1f3-2192-4b53-8f06-5852e007f252
2023-07-26 05:29:59,993 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8b15293-f021-4bbf-b982-a593abef3f17
2023-07-26 05:29:59,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-393fecf7-9513-44ab-99aa-7cd73ad02e75
2023-07-26 05:30:00,094 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:00,119 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33411', status: init, memory: 0, processing: 0>
2023-07-26 05:30:00,120 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33411
2023-07-26 05:30:00,121 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56364
2023-07-26 05:30:00,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-26 05:30:00,121 - distributed.worker - INFO - -------------------------------------------------
2023-07-26 05:30:00,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-26 05:30:00,150 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:00,153 - distributed.scheduler - INFO - Remove client Client-75e4aa7a-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:00,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56336; closing.
2023-07-26 05:30:00,153 - distributed.scheduler - INFO - Remove client Client-75e4aa7a-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:00,153 - distributed.scheduler - INFO - Close client connection: Client-75e4aa7a-2b75-11ee-96ba-d8c49764f6bb
2023-07-26 05:30:00,200 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-07-26 05:30:00,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-26 05:30:00,209 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:00,210 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-26 05:30:00,212 - distributed.scheduler - INFO - Remove client Client-749e77ad-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:30:00,212 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56328; closing.
2023-07-26 05:30:00,212 - distributed.scheduler - INFO - Remove client Client-749e77ad-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:30:00,213 - distributed.scheduler - INFO - Close client connection: Client-749e77ad-2b75-11ee-9429-d8c49764f6bb
2023-07-26 05:30:00,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45619'. Reason: nanny-close
2023-07-26 05:30:00,214 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-26 05:30:00,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33411. Reason: nanny-close
2023-07-26 05:30:00,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-26 05:30:00,217 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56364; closing.
2023-07-26 05:30:00,217 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33411', status: closing, memory: 0, processing: 0>
2023-07-26 05:30:00,217 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33411
2023-07-26 05:30:00,217 - distributed.scheduler - INFO - Lost all workers
2023-07-26 05:30:00,218 - distributed.nanny - INFO - Worker closed
2023-07-26 05:30:01,079 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-26 05:30:01,079 - distributed.scheduler - INFO - Scheduler closing...
2023-07-26 05:30:01,080 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-26 05:30:01,081 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-26 05:30:01,081 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:30:09,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,491 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,491 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,515 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,515 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:09,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:09,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:30:17,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:17,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,014 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,014 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,077 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,077 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:18,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:18,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-26 05:30:25,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,276 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,276 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,353 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,353 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:25,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:25,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36509 instead
  warnings.warn(
2023-07-26 05:30:33,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,937 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,937 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:33,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:33,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33077 instead
  warnings.warn(
2023-07-26 05:30:43,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:43,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:43,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:43,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:43,970 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:43,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:44,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:44,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:44,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:44,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:44,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:44,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:44,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:44,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-26 05:30:44,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-26 05:30:44,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
