============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-11-16 06:36:56,575 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:36:56,579 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44405 instead
  warnings.warn(
2023-11-16 06:36:56,583 - distributed.scheduler - INFO - State start
2023-11-16 06:36:56,605 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:36:56,606 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-16 06:36:56,607 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44405/status
2023-11-16 06:36:56,608 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:36:56,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44963'
2023-11-16 06:36:56,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36485'
2023-11-16 06:36:56,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42587'
2023-11-16 06:36:56,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44699'
2023-11-16 06:36:57,243 - distributed.scheduler - INFO - Receive client connection: Client-88a46b9f-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:36:57,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41246
2023-11-16 06:36:58,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:36:58,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:36:58,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:36:58,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:36:58,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:36:58,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:36:58,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:36:58,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:36:58,463 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:36:58,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:36:58,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:36:58,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-11-16 06:36:58,507 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42241
2023-11-16 06:36:58,507 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42241
2023-11-16 06:36:58,507 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42037
2023-11-16 06:36:58,507 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-16 06:36:58,507 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:36:58,507 - distributed.worker - INFO -               Threads:                          4
2023-11-16 06:36:58,507 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-16 06:36:58,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-m67aslmy
2023-11-16 06:36:58,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14217772-c85d-4e12-b613-b63476f56405
2023-11-16 06:36:58,507 - distributed.worker - INFO - Starting Worker plugin PreImport-7863ed55-4ab0-458c-adae-9576e13d9d3f
2023-11-16 06:36:58,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42ff2f85-b93f-4590-bcbd-7a89a0a1b0fc
2023-11-16 06:36:58,508 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:36:59,259 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42241', status: init, memory: 0, processing: 0>
2023-11-16 06:36:59,260 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42241
2023-11-16 06:36:59,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41256
2023-11-16 06:36:59,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:36:59,262 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-16 06:36:59,262 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:36:59,263 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-16 06:37:00,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39295
2023-11-16 06:37:00,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39295
2023-11-16 06:37:00,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39431
2023-11-16 06:37:00,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-16 06:37:00,063 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,063 - distributed.worker - INFO -               Threads:                          4
2023-11-16 06:37:00,064 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-16 06:37:00,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-v4gnev6a
2023-11-16 06:37:00,064 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-812d6a53-6ab5-44cb-9cd4-04bd750402dc
2023-11-16 06:37:00,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f94e8122-9560-4fbf-b185-6164a76bf86f
2023-11-16 06:37:00,065 - distributed.worker - INFO - Starting Worker plugin PreImport-cc670a49-4e12-4709-a02a-4620318feb22
2023-11-16 06:37:00,065 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37053
2023-11-16 06:37:00,092 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37053
2023-11-16 06:37:00,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46343
2023-11-16 06:37:00,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-16 06:37:00,093 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,093 - distributed.worker - INFO -               Threads:                          4
2023-11-16 06:37:00,093 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-16 06:37:00,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ho74bq30
2023-11-16 06:37:00,093 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4642081c-6dda-4316-a2ee-1447387921ee
2023-11-16 06:37:00,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4aa72da3-704a-4940-8b21-2d5e4d51e18b
2023-11-16 06:37:00,096 - distributed.worker - INFO - Starting Worker plugin PreImport-fcefaf32-8b4c-4fdb-b822-f91253f8a475
2023-11-16 06:37:00,096 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,098 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38899
2023-11-16 06:37:00,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38899
2023-11-16 06:37:00,098 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44581
2023-11-16 06:37:00,098 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-16 06:37:00,098 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,099 - distributed.worker - INFO -               Threads:                          4
2023-11-16 06:37:00,099 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-16 06:37:00,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_xx53024
2023-11-16 06:37:00,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bee8d382-5943-455f-924a-95bb0fe74b9e
2023-11-16 06:37:00,099 - distributed.worker - INFO - Starting Worker plugin PreImport-e8285e53-e38b-4711-9162-570059500c33
2023-11-16 06:37:00,100 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1853a60-78e7-41a9-aedd-bf1f161294cf
2023-11-16 06:37:00,100 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,101 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39295', status: init, memory: 0, processing: 0>
2023-11-16 06:37:00,102 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39295
2023-11-16 06:37:00,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54366
2023-11-16 06:37:00,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:00,106 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-16 06:37:00,106 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-16 06:37:00,123 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38899', status: init, memory: 0, processing: 0>
2023-11-16 06:37:00,124 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38899
2023-11-16 06:37:00,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54386
2023-11-16 06:37:00,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:00,126 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-16 06:37:00,126 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-16 06:37:00,135 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37053', status: init, memory: 0, processing: 0>
2023-11-16 06:37:00,135 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37053
2023-11-16 06:37:00,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54370
2023-11-16 06:37:00,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:00,139 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-16 06:37:00,139 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:00,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-16 06:37:00,157 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-16 06:37:00,157 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-16 06:37:00,157 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-16 06:37:00,158 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-16 06:37:00,162 - distributed.scheduler - INFO - Remove client Client-88a46b9f-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:00,162 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41246; closing.
2023-11-16 06:37:00,162 - distributed.scheduler - INFO - Remove client Client-88a46b9f-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:00,162 - distributed.scheduler - INFO - Close client connection: Client-88a46b9f-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:00,163 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44963'. Reason: nanny-close
2023-11-16 06:37:00,164 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:00,165 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36485'. Reason: nanny-close
2023-11-16 06:37:00,165 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:00,165 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39295. Reason: nanny-close
2023-11-16 06:37:00,166 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38899. Reason: nanny-close
2023-11-16 06:37:00,167 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54366; closing.
2023-11-16 06:37:00,167 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-16 06:37:00,167 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-16 06:37:00,167 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116620.1678803')
2023-11-16 06:37:00,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42587'. Reason: nanny-close
2023-11-16 06:37:00,168 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:00,168 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54386; closing.
2023-11-16 06:37:00,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44699'. Reason: nanny-close
2023-11-16 06:37:00,169 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38899', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116620.16901')
2023-11-16 06:37:00,169 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:00,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:00,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37053. Reason: nanny-close
2023-11-16 06:37:00,169 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:00,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42241. Reason: nanny-close
2023-11-16 06:37:00,169 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:54386>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-16 06:37:00,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-16 06:37:00,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54370; closing.
2023-11-16 06:37:00,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-16 06:37:00,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41256; closing.
2023-11-16 06:37:00,172 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37053', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116620.1727486')
2023-11-16 06:37:00,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116620.1731257')
2023-11-16 06:37:00,173 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:00,173 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:00,174 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:01,330 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:01,330 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:01,330 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:01,332 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-16 06:37:01,332 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-11-16 06:37:03,379 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:03,383 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42097 instead
  warnings.warn(
2023-11-16 06:37:03,387 - distributed.scheduler - INFO - State start
2023-11-16 06:37:03,539 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:03,540 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:37:03,541 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42097/status
2023-11-16 06:37:03,541 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:37:03,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40041'
2023-11-16 06:37:03,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37275'
2023-11-16 06:37:03,684 - distributed.scheduler - INFO - Receive client connection: Client-8ca35eba-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:03,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51312
2023-11-16 06:37:03,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40937'
2023-11-16 06:37:03,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39567'
2023-11-16 06:37:03,714 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38485'
2023-11-16 06:37:03,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43413'
2023-11-16 06:37:03,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36043'
2023-11-16 06:37:03,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33679'
2023-11-16 06:37:05,420 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,420 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,424 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,571 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,575 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,587 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,615 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:05,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:05,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:05,788 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:07,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35787
2023-11-16 06:37:07,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35787
2023-11-16 06:37:07,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35999
2023-11-16 06:37:07,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:07,182 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:07,182 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:07,183 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:07,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ip35fr9j
2023-11-16 06:37:07,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f91f5b1a-bd64-4906-9023-13b111bc7112
2023-11-16 06:37:07,183 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d536c60b-bb18-4086-9737-2d7a45587030
2023-11-16 06:37:07,532 - distributed.worker - INFO - Starting Worker plugin PreImport-44c36f7c-7ad0-46ac-bcb8-d2eb98809cc5
2023-11-16 06:37:07,532 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:07,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35787', status: init, memory: 0, processing: 0>
2023-11-16 06:37:07,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35787
2023-11-16 06:37:07,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51338
2023-11-16 06:37:07,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:07,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:07,569 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:07,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34043
2023-11-16 06:37:08,403 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34043
2023-11-16 06:37:08,403 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33863
2023-11-16 06:37:08,403 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,403 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,403 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,403 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,403 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jg4_7rmj
2023-11-16 06:37:08,404 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-348c37d8-a708-4614-9e82-3e3445a7151f
2023-11-16 06:37:08,404 - distributed.worker - INFO - Starting Worker plugin PreImport-a09916dd-845a-44a7-bf5e-fd3562672f29
2023-11-16 06:37:08,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4826181c-1a20-4034-95e8-ac109905e538
2023-11-16 06:37:08,459 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42307
2023-11-16 06:37:08,459 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42307
2023-11-16 06:37:08,460 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39909
2023-11-16 06:37:08,460 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,460 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,460 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,460 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,460 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oiaosocx
2023-11-16 06:37:08,460 - distributed.worker - INFO - Starting Worker plugin PreImport-ee174d4e-2f6e-43e0-a385-5f801aee41ae
2023-11-16 06:37:08,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7aeba51-bb2f-460c-b673-73b35efb0f96
2023-11-16 06:37:08,461 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b8fa0e4-a7ff-47f5-b85d-a2c1b4c7d52a
2023-11-16 06:37:08,460 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46377
2023-11-16 06:37:08,461 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46377
2023-11-16 06:37:08,461 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39869
2023-11-16 06:37:08,461 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,461 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,461 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,462 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-22lkiyeu
2023-11-16 06:37:08,462 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aae1e575-fca9-4a2e-aedb-535f5b43a15b
2023-11-16 06:37:08,466 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44047
2023-11-16 06:37:08,466 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44047
2023-11-16 06:37:08,467 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34463
2023-11-16 06:37:08,467 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,467 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,467 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,467 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w0mmlg4h
2023-11-16 06:37:08,467 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96e1bee7-a16f-4b25-a321-1a39bdf17bed
2023-11-16 06:37:08,480 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46461
2023-11-16 06:37:08,481 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46461
2023-11-16 06:37:08,481 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41701
2023-11-16 06:37:08,481 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,481 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,481 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,482 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,482 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nc0tsb1l
2023-11-16 06:37:08,482 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e384e5d2-b02a-44c3-ac6e-289e9e0717af
2023-11-16 06:37:08,481 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33691
2023-11-16 06:37:08,482 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33691
2023-11-16 06:37:08,482 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34755
2023-11-16 06:37:08,482 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,482 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,483 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,483 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,483 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4b1xdlyk
2023-11-16 06:37:08,483 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0415acb-db47-4fed-aabe-5b342ad40d58
2023-11-16 06:37:08,483 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10b8ef6f-8aee-4339-bb4e-29c6745d6094
2023-11-16 06:37:08,484 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42005
2023-11-16 06:37:08,484 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42005
2023-11-16 06:37:08,485 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38573
2023-11-16 06:37:08,485 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,485 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,485 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:08,485 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:08,485 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l50ik8vf
2023-11-16 06:37:08,485 - distributed.worker - INFO - Starting Worker plugin PreImport-cbdf1567-a25b-4f75-9ac4-e90753f27a41
2023-11-16 06:37:08,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5aa159e8-88a5-4bda-9447-199474ad62fc
2023-11-16 06:37:08,486 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d792173e-383a-4bc7-9495-237716ab5086
2023-11-16 06:37:08,605 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4ff128c-e979-4f00-bdb5-80df6d8a8057
2023-11-16 06:37:08,622 - distributed.worker - INFO - Starting Worker plugin PreImport-a5db893b-cb1d-4d52-8373-8643479b4883
2023-11-16 06:37:08,623 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,625 - distributed.worker - INFO - Starting Worker plugin PreImport-c88bf3cc-546a-449d-ae65-d10df159f107
2023-11-16 06:37:08,625 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-292e7cac-b8fe-4e25-b447-1ddda2c4d558
2023-11-16 06:37:08,625 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,625 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,625 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c8132c1-3e79-48e5-9ea5-c75c4ffaddd2
2023-11-16 06:37:08,625 - distributed.worker - INFO - Starting Worker plugin PreImport-9484a46e-4708-4651-8242-b586e9ea8bcb
2023-11-16 06:37:08,625 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,625 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,626 - distributed.worker - INFO - Starting Worker plugin PreImport-d6e79019-093b-495f-ad3a-1fec6599ff8b
2023-11-16 06:37:08,627 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,632 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34043', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,633 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34043
2023-11-16 06:37:08,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51352
2023-11-16 06:37:08,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,638 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,638 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33691', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,654 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33691
2023-11-16 06:37:08,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51378
2023-11-16 06:37:08,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44047', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44047
2023-11-16 06:37:08,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51390
2023-11-16 06:37:08,656 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,656 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46377', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46377
2023-11-16 06:37:08,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51368
2023-11-16 06:37:08,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,660 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,660 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,661 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,661 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,662 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42005', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42005
2023-11-16 06:37:08,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51408
2023-11-16 06:37:08,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,666 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42307', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,666 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42307
2023-11-16 06:37:08,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51402
2023-11-16 06:37:08,667 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46461', status: init, memory: 0, processing: 0>
2023-11-16 06:37:08,667 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46461
2023-11-16 06:37:08,668 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51420
2023-11-16 06:37:08,668 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:08,671 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,671 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,674 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,674 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,675 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:08,675 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:08,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:08,755 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,755 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,756 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,756 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,756 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,756 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,756 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,756 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:08,761 - distributed.scheduler - INFO - Remove client Client-8ca35eba-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:08,761 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51312; closing.
2023-11-16 06:37:08,761 - distributed.scheduler - INFO - Remove client Client-8ca35eba-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:08,762 - distributed.scheduler - INFO - Close client connection: Client-8ca35eba-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:08,763 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40041'. Reason: nanny-close
2023-11-16 06:37:08,763 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,764 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37275'. Reason: nanny-close
2023-11-16 06:37:08,764 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,765 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42005. Reason: nanny-close
2023-11-16 06:37:08,765 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40937'. Reason: nanny-close
2023-11-16 06:37:08,765 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,765 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39567'. Reason: nanny-close
2023-11-16 06:37:08,765 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35787. Reason: nanny-close
2023-11-16 06:37:08,766 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,766 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46377. Reason: nanny-close
2023-11-16 06:37:08,766 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38485'. Reason: nanny-close
2023-11-16 06:37:08,766 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,766 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34043. Reason: nanny-close
2023-11-16 06:37:08,767 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43413'. Reason: nanny-close
2023-11-16 06:37:08,767 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,767 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,767 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51408; closing.
2023-11-16 06:37:08,767 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42307. Reason: nanny-close
2023-11-16 06:37:08,767 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36043'. Reason: nanny-close
2023-11-16 06:37:08,768 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,768 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42005', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.7681975')
2023-11-16 06:37:08,768 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46461. Reason: nanny-close
2023-11-16 06:37:08,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33679'. Reason: nanny-close
2023-11-16 06:37:08,768 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:08,768 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33691. Reason: nanny-close
2023-11-16 06:37:08,768 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,769 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51352; closing.
2023-11-16 06:37:08,769 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,769 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44047. Reason: nanny-close
2023-11-16 06:37:08,769 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51368; closing.
2023-11-16 06:37:08,769 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,770 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,770 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,770 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.7704248')
2023-11-16 06:37:08,770 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,770 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.7707891')
2023-11-16 06:37:08,770 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,771 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,771 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51338; closing.
2023-11-16 06:37:08,771 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:08,771 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,772 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,772 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,771 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51352>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51352>: Stream is closed
2023-11-16 06:37:08,773 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:08,773 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.7736044')
2023-11-16 06:37:08,774 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51402; closing.
2023-11-16 06:37:08,774 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.7745802')
2023-11-16 06:37:08,775 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51420; closing.
2023-11-16 06:37:08,775 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51378; closing.
2023-11-16 06:37:08,775 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.775748')
2023-11-16 06:37:08,776 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.7761242')
2023-11-16 06:37:08,776 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51390; closing.
2023-11-16 06:37:08,777 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116628.777135')
2023-11-16 06:37:08,777 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:08,777 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51390>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-16 06:37:10,831 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:10,831 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:10,832 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:10,834 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:37:10,835 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-11-16 06:37:13,187 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:13,192 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45219 instead
  warnings.warn(
2023-11-16 06:37:13,196 - distributed.scheduler - INFO - State start
2023-11-16 06:37:13,223 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:13,224 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:37:13,225 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45219/status
2023-11-16 06:37:13,225 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:37:13,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45273'
2023-11-16 06:37:13,478 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44411'
2023-11-16 06:37:13,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34549'
2023-11-16 06:37:13,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41327'
2023-11-16 06:37:13,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35117'
2023-11-16 06:37:13,515 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46195'
2023-11-16 06:37:13,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46581'
2023-11-16 06:37:13,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34409'
2023-11-16 06:37:14,437 - distributed.scheduler - INFO - Receive client connection: Client-926464de-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:14,451 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60010
2023-11-16 06:37:15,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,346 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,346 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:15,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:15,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:15,399 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:18,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38209
2023-11-16 06:37:18,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38209
2023-11-16 06:37:18,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32945
2023-11-16 06:37:18,054 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,054 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,054 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,054 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zq0jyea1
2023-11-16 06:37:18,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6d50172-4bb9-4ee0-bbdf-9f585bc6f340
2023-11-16 06:37:18,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7875ca92-9cb2-4b61-99a1-ecf073aa7941
2023-11-16 06:37:18,060 - distributed.worker - INFO - Starting Worker plugin PreImport-363de454-10da-4aed-b3e7-08c3df784541
2023-11-16 06:37:18,060 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38209', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38209
2023-11-16 06:37:18,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60032
2023-11-16 06:37:18,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,086 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,086 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,179 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44591
2023-11-16 06:37:18,180 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44591
2023-11-16 06:37:18,180 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42165
2023-11-16 06:37:18,180 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,180 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,180 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,180 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,180 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q5tgd8xi
2023-11-16 06:37:18,181 - distributed.worker - INFO - Starting Worker plugin PreImport-d176804c-c031-4e86-b7a5-307078a14bc4
2023-11-16 06:37:18,181 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d44c9060-b9ba-427d-88fa-03b647380ca5
2023-11-16 06:37:18,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a398694a-404e-497b-a3d2-ddcf24f76a4c
2023-11-16 06:37:18,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44219
2023-11-16 06:37:18,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44219
2023-11-16 06:37:18,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43265
2023-11-16 06:37:18,190 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,190 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,190 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,191 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nrik96_v
2023-11-16 06:37:18,191 - distributed.worker - INFO - Starting Worker plugin PreImport-8d531404-cd46-4e32-b111-74dd21728704
2023-11-16 06:37:18,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80763181-bb78-4aad-98c1-c809420651df
2023-11-16 06:37:18,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b82a17ae-69fd-4f26-9651-f14e23c68a81
2023-11-16 06:37:18,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33139
2023-11-16 06:37:18,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33139
2023-11-16 06:37:18,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40725
2023-11-16 06:37:18,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,219 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,219 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,219 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-97ci23u6
2023-11-16 06:37:18,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b40da8e-727a-4a02-a444-a41136997db3
2023-11-16 06:37:18,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39893
2023-11-16 06:37:18,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39893
2023-11-16 06:37:18,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41189
2023-11-16 06:37:18,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43565
2023-11-16 06:37:18,228 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43565
2023-11-16 06:37:18,228 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37559
2023-11-16 06:37:18,228 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,228 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,228 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,228 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,228 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4usotiy6
2023-11-16 06:37:18,228 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,228 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vx1dyxz7
2023-11-16 06:37:18,229 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28635d2a-752f-41dd-b124-ad685e4d0b09
2023-11-16 06:37:18,229 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd520ea3-bdf7-428e-b5a0-9e50a35f0d49
2023-11-16 06:37:18,230 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44995
2023-11-16 06:37:18,230 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44995
2023-11-16 06:37:18,230 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46565
2023-11-16 06:37:18,231 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,231 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,231 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,231 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,231 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-msolq0vi
2023-11-16 06:37:18,231 - distributed.worker - INFO - Starting Worker plugin PreImport-52ad9510-e6ff-409a-b870-374061f3a871
2023-11-16 06:37:18,231 - distributed.worker - INFO - Starting Worker plugin RMMSetup-068ad318-5109-48d1-bff2-5a833b45386b
2023-11-16 06:37:18,237 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45087
2023-11-16 06:37:18,238 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45087
2023-11-16 06:37:18,239 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43837
2023-11-16 06:37:18,239 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,239 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,239 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:18,239 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:18,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rhyfdfdl
2023-11-16 06:37:18,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-706d08a8-0ba8-438e-9ad2-e8835623e7cf
2023-11-16 06:37:18,241 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,248 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d117ee0-0394-4720-abde-3ae274a0ee29
2023-11-16 06:37:18,257 - distributed.worker - INFO - Starting Worker plugin PreImport-e8ee46bd-f763-4ff4-acde-a73366ac3d82
2023-11-16 06:37:18,257 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a25bf244-4595-497d-971b-2bb011843080
2023-11-16 06:37:18,258 - distributed.worker - INFO - Starting Worker plugin PreImport-986c504f-7709-449f-9ae6-e8b49bceba07
2023-11-16 06:37:18,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9bb06973-1dda-4d57-b65b-76f8f3b27924
2023-11-16 06:37:18,259 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,259 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7855d010-2820-4bd3-8653-58308f154dd7
2023-11-16 06:37:18,308 - distributed.worker - INFO - Starting Worker plugin PreImport-cbe50863-9eb1-4e40-810e-0c6c87eabb45
2023-11-16 06:37:18,308 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,308 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e95e9b4-9bab-4c53-acb8-46c180d3397b
2023-11-16 06:37:18,309 - distributed.worker - INFO - Starting Worker plugin PreImport-4bc60ebf-d4e6-42d2-a581-68aa03e308e3
2023-11-16 06:37:18,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33139', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,309 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33139
2023-11-16 06:37:18,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60070
2023-11-16 06:37:18,311 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44995', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,311 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44995
2023-11-16 06:37:18,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60078
2023-11-16 06:37:18,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,312 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,312 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,312 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44219', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,313 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44219
2023-11-16 06:37:18,313 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60048
2023-11-16 06:37:18,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,313 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,313 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39893', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,314 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39893
2023-11-16 06:37:18,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60086
2023-11-16 06:37:18,314 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,314 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44591', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,315 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44591
2023-11-16 06:37:18,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60058
2023-11-16 06:37:18,315 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,315 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,315 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,316 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,316 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,318 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,484 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45087', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,484 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45087
2023-11-16 06:37:18,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60100
2023-11-16 06:37:18,485 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43565', status: init, memory: 0, processing: 0>
2023-11-16 06:37:18,486 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43565
2023-11-16 06:37:18,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60102
2023-11-16 06:37:18,486 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,487 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:18,487 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,488 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:18,488 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:18,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:18,571 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,571 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,572 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,572 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,572 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,572 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,572 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,572 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:18,576 - distributed.scheduler - INFO - Remove client Client-926464de-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:18,577 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60010; closing.
2023-11-16 06:37:18,577 - distributed.scheduler - INFO - Remove client Client-926464de-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:18,577 - distributed.scheduler - INFO - Close client connection: Client-926464de-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:18,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45273'. Reason: nanny-close
2023-11-16 06:37:18,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44411'. Reason: nanny-close
2023-11-16 06:37:18,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44591. Reason: nanny-close
2023-11-16 06:37:18,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34549'. Reason: nanny-close
2023-11-16 06:37:18,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39893. Reason: nanny-close
2023-11-16 06:37:18,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41327'. Reason: nanny-close
2023-11-16 06:37:18,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38209. Reason: nanny-close
2023-11-16 06:37:18,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35117'. Reason: nanny-close
2023-11-16 06:37:18,582 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44995. Reason: nanny-close
2023-11-16 06:37:18,582 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46195'. Reason: nanny-close
2023-11-16 06:37:18,582 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60058; closing.
2023-11-16 06:37:18,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43565. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44591', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5829678')
2023-11-16 06:37:18,583 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46581'. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44219. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34409'. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,583 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33139. Reason: nanny-close
2023-11-16 06:37:18,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60032; closing.
2023-11-16 06:37:18,584 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,584 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45087. Reason: nanny-close
2023-11-16 06:37:18,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38209', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.585012')
2023-11-16 06:37:18,585 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,585 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60078; closing.
2023-11-16 06:37:18,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60086; closing.
2023-11-16 06:37:18,585 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,586 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,586 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,586 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:18,587 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,585 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60032>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60032>: Stream is closed
2023-11-16 06:37:18,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44995', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5878723')
2023-11-16 06:37:18,588 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5882683')
2023-11-16 06:37:18,588 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:18,589 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60102; closing.
2023-11-16 06:37:18,589 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43565', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5898583')
2023-11-16 06:37:18,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60048; closing.
2023-11-16 06:37:18,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60070; closing.
2023-11-16 06:37:18,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60100; closing.
2023-11-16 06:37:18,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5909953')
2023-11-16 06:37:18,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5914104')
2023-11-16 06:37:18,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116638.5917845')
2023-11-16 06:37:18,591 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:18,592 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60070>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-16 06:37:20,297 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:20,297 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:20,297 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:20,299 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:37:20,299 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-11-16 06:37:22,628 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:22,633 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42647 instead
  warnings.warn(
2023-11-16 06:37:22,637 - distributed.scheduler - INFO - State start
2023-11-16 06:37:22,659 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:22,660 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:37:22,661 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42647/status
2023-11-16 06:37:22,661 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:37:22,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34379'
2023-11-16 06:37:22,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36411'
2023-11-16 06:37:22,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39581'
2023-11-16 06:37:22,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42315'
2023-11-16 06:37:22,767 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38671'
2023-11-16 06:37:22,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39917'
2023-11-16 06:37:22,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40765'
2023-11-16 06:37:22,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37379'
2023-11-16 06:37:22,992 - distributed.scheduler - INFO - Receive client connection: Client-980286a8-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:23,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35230
2023-11-16 06:37:24,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,621 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,661 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,661 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,736 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:24,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:24,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:24,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:26,990 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34435
2023-11-16 06:37:26,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34435
2023-11-16 06:37:26,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35251
2023-11-16 06:37:26,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:26,992 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:26,992 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:26,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:26,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-se9mha5i
2023-11-16 06:37:26,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6aeb3df4-f4b5-4712-a775-46164b1b8f70
2023-11-16 06:37:27,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44009
2023-11-16 06:37:27,199 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44009
2023-11-16 06:37:27,199 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43997
2023-11-16 06:37:27,199 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,199 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,199 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,199 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,199 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dtxcbz61
2023-11-16 06:37:27,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46d18a69-ff63-47a3-b238-6f8fc603998e
2023-11-16 06:37:27,200 - distributed.worker - INFO - Starting Worker plugin PreImport-dea2592c-eb9e-40d8-a70d-22ecb52bc485
2023-11-16 06:37:27,200 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a667ff9-fe33-46e5-b7ee-4bb28394e1c4
2023-11-16 06:37:27,225 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38355
2023-11-16 06:37:27,226 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38355
2023-11-16 06:37:27,226 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36979
2023-11-16 06:37:27,226 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,226 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,226 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,227 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,227 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ae0_ynu5
2023-11-16 06:37:27,227 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed113018-f2b8-40fe-bcfc-5d79d9d518b4
2023-11-16 06:37:27,237 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43603
2023-11-16 06:37:27,238 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43603
2023-11-16 06:37:27,238 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33557
2023-11-16 06:37:27,238 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,238 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,238 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,238 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-77yfh5uq
2023-11-16 06:37:27,239 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a182af4b-1999-4234-bab7-7781bea51094
2023-11-16 06:37:27,239 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73f62082-41c9-4859-bdd2-4674c1c8073b
2023-11-16 06:37:27,246 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35963
2023-11-16 06:37:27,246 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35963
2023-11-16 06:37:27,247 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33237
2023-11-16 06:37:27,247 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,247 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,247 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,247 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zc0ohjhf
2023-11-16 06:37:27,247 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee08848f-c629-4b93-8605-9219b8e62bda
2023-11-16 06:37:27,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36611
2023-11-16 06:37:27,249 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36611
2023-11-16 06:37:27,249 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39019
2023-11-16 06:37:27,249 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,249 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,249 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,249 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,249 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6sk7qfun
2023-11-16 06:37:27,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4c2370d-0e8f-47cc-a03f-218fda9daad8
2023-11-16 06:37:27,251 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64fc3a30-bfb2-441e-a2e7-a7a7ea59e1b1
2023-11-16 06:37:27,252 - distributed.worker - INFO - Starting Worker plugin PreImport-34f42992-c1f3-4b5f-8531-c0108808ffbc
2023-11-16 06:37:27,252 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,251 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39233
2023-11-16 06:37:27,252 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39233
2023-11-16 06:37:27,252 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40829
2023-11-16 06:37:27,252 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,252 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,252 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,253 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t39p9saa
2023-11-16 06:37:27,253 - distributed.worker - INFO - Starting Worker plugin PreImport-ebab492d-cbfb-4646-b6c7-313c28dcc5cb
2023-11-16 06:37:27,253 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b7c1f79-242c-4566-8495-26dd69b3cd55
2023-11-16 06:37:27,254 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41005
2023-11-16 06:37:27,255 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41005
2023-11-16 06:37:27,255 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39635
2023-11-16 06:37:27,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,255 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,255 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a72091ba-bcb8-43fe-bc48-9e1b4086328d
2023-11-16 06:37:27,255 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:27,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:27,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xl6vwo32
2023-11-16 06:37:27,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b5b177a-6b96-401c-9902-54bd39516326
2023-11-16 06:37:27,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84c99c8e-e49c-464a-b4bb-09cf745f060f
2023-11-16 06:37:27,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34435', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,284 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34435
2023-11-16 06:37:27,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35242
2023-11-16 06:37:27,285 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,286 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,286 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,392 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,418 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44009', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,419 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44009
2023-11-16 06:37:27,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35252
2023-11-16 06:37:27,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,421 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,421 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d89b4cf-8673-41d6-b842-4f1fff161e7b
2023-11-16 06:37:27,443 - distributed.worker - INFO - Starting Worker plugin PreImport-98a8b935-b83c-4a26-a054-997a192cb8c9
2023-11-16 06:37:27,444 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,449 - distributed.worker - INFO - Starting Worker plugin PreImport-3fe6159a-91ab-47ca-8c0b-bed6845b2b6c
2023-11-16 06:37:27,449 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,456 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce4599b6-551d-4353-b0e5-b397b3618955
2023-11-16 06:37:27,456 - distributed.worker - INFO - Starting Worker plugin PreImport-2b28fdca-9534-481e-b10c-e941b9b5367e
2023-11-16 06:37:27,456 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3c35241-d69b-4895-8cf0-c15e3f71a16c
2023-11-16 06:37:27,458 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,459 - distributed.worker - INFO - Starting Worker plugin PreImport-5f7f7610-6ccd-4f5a-9bb2-5fc67c7c6546
2023-11-16 06:37:27,459 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,460 - distributed.worker - INFO - Starting Worker plugin PreImport-0aa3b3a2-bb54-4451-9b9b-e73a0d09c71f
2023-11-16 06:37:27,460 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38355', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38355
2023-11-16 06:37:27,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35266
2023-11-16 06:37:27,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43603', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,482 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,482 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43603
2023-11-16 06:37:27,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35282
2023-11-16 06:37:27,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,484 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,486 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35963', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,487 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35963
2023-11-16 06:37:27,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35298
2023-11-16 06:37:27,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,488 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41005', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,488 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,489 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,489 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41005
2023-11-16 06:37:27,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35312
2023-11-16 06:37:27,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,490 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,491 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39233', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39233
2023-11-16 06:37:27,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35314
2023-11-16 06:37:27,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36611', status: init, memory: 0, processing: 0>
2023-11-16 06:37:27,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,499 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36611
2023-11-16 06:37:27,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35322
2023-11-16 06:37:27,500 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,500 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,501 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:27,502 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:27,503 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:27,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,512 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:27,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,515 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,515 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,515 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,515 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,516 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:27,529 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,529 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,529 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,530 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,530 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,530 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,530 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,530 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:27,538 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:27,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:27,542 - distributed.scheduler - INFO - Remove client Client-980286a8-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:27,542 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35230; closing.
2023-11-16 06:37:27,543 - distributed.scheduler - INFO - Remove client Client-980286a8-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:27,543 - distributed.scheduler - INFO - Close client connection: Client-980286a8-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:27,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34379'. Reason: nanny-close
2023-11-16 06:37:27,545 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36411'. Reason: nanny-close
2023-11-16 06:37:27,546 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38355. Reason: nanny-close
2023-11-16 06:37:27,546 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39581'. Reason: nanny-close
2023-11-16 06:37:27,546 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,547 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43603. Reason: nanny-close
2023-11-16 06:37:27,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42315'. Reason: nanny-close
2023-11-16 06:37:27,547 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,547 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34435. Reason: nanny-close
2023-11-16 06:37:27,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38671'. Reason: nanny-close
2023-11-16 06:37:27,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,548 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44009. Reason: nanny-close
2023-11-16 06:37:27,548 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39917'. Reason: nanny-close
2023-11-16 06:37:27,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,549 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39233. Reason: nanny-close
2023-11-16 06:37:27,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40765'. Reason: nanny-close
2023-11-16 06:37:27,549 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35266; closing.
2023-11-16 06:37:27,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,549 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,549 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38355', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.5496862')
2023-11-16 06:37:27,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36611. Reason: nanny-close
2023-11-16 06:37:27,550 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,550 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37379'. Reason: nanny-close
2023-11-16 06:37:27,550 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:27,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41005. Reason: nanny-close
2023-11-16 06:37:27,550 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,550 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35963. Reason: nanny-close
2023-11-16 06:37:27,551 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35282; closing.
2023-11-16 06:37:27,551 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,551 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,552 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,552 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,553 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35252; closing.
2023-11-16 06:37:27,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43603', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.5534763')
2023-11-16 06:37:27,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,554 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:27,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35242; closing.
2023-11-16 06:37:27,554 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,555 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,555 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44009', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.5555475')
2023-11-16 06:37:27,556 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.5561442')
2023-11-16 06:37:27,556 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:27,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35314; closing.
2023-11-16 06:37:27,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.5576558')
2023-11-16 06:37:27,558 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35322; closing.
2023-11-16 06:37:27,558 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35312; closing.
2023-11-16 06:37:27,559 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.5589685')
2023-11-16 06:37:27,559 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41005', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.559433')
2023-11-16 06:37:27,560 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35298; closing.
2023-11-16 06:37:27,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116647.560659')
2023-11-16 06:37:27,560 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:27,561 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35298>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-16 06:37:29,112 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:29,112 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:29,113 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:29,114 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:37:29,114 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-11-16 06:37:31,108 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:31,112 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39279 instead
  warnings.warn(
2023-11-16 06:37:31,115 - distributed.scheduler - INFO - State start
2023-11-16 06:37:31,138 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:31,138 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:37:31,139 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39279/status
2023-11-16 06:37:31,139 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:37:31,245 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41483'
2023-11-16 06:37:31,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43401'
2023-11-16 06:37:31,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41223'
2023-11-16 06:37:31,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43935'
2023-11-16 06:37:31,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45029'
2023-11-16 06:37:31,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44709'
2023-11-16 06:37:31,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33029'
2023-11-16 06:37:31,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37979'
2023-11-16 06:37:31,588 - distributed.scheduler - INFO - Receive client connection: Client-9d311f34-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:31,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56534
2023-11-16 06:37:33,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,153 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,153 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,153 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,153 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,153 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,181 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,181 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:33,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:33,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:33,193 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:36,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39055
2023-11-16 06:37:36,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39055
2023-11-16 06:37:36,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43933
2023-11-16 06:37:36,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,095 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,095 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lwqagsbe
2023-11-16 06:37:36,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43a1cb9a-e0ad-4267-9878-fb3eab72fefc
2023-11-16 06:37:36,098 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39971
2023-11-16 06:37:36,099 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39971
2023-11-16 06:37:36,099 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37913
2023-11-16 06:37:36,099 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,099 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,099 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,099 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cnj1nebr
2023-11-16 06:37:36,100 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99dd85bc-9f56-49f0-8bc6-19235252bc4f
2023-11-16 06:37:36,100 - distributed.worker - INFO - Starting Worker plugin PreImport-71b0c710-b999-43e5-b89a-cd3a477e12da
2023-11-16 06:37:36,100 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8199918c-fdf6-4ab5-bbd6-d0cfa5835a22
2023-11-16 06:37:36,104 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33935
2023-11-16 06:37:36,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33935
2023-11-16 06:37:36,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45613
2023-11-16 06:37:36,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,105 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,105 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,105 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i6oc4dvg
2023-11-16 06:37:36,106 - distributed.worker - INFO - Starting Worker plugin PreImport-19171501-eea7-4b10-93b8-5dc7ecf29959
2023-11-16 06:37:36,106 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-095c42bd-88a3-4eb8-8729-145c7d4f66dd
2023-11-16 06:37:36,106 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba067c3e-c8c7-4640-a292-cf2f9ed736cd
2023-11-16 06:37:36,109 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40461
2023-11-16 06:37:36,110 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40461
2023-11-16 06:37:36,110 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34709
2023-11-16 06:37:36,110 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,110 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,110 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,111 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,111 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wgskfi0y
2023-11-16 06:37:36,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7f9c0d7-e7af-4578-83ad-07eef882b63d
2023-11-16 06:37:36,385 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41803
2023-11-16 06:37:36,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41803
2023-11-16 06:37:36,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37437
2023-11-16 06:37:36,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,386 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,386 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,386 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_96zkypx
2023-11-16 06:37:36,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e27225a-ca6d-4a09-bf4a-195b28ca7e17
2023-11-16 06:37:36,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7e1e41b-0384-4f84-bffc-3742fef5bf0d
2023-11-16 06:37:36,390 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33839
2023-11-16 06:37:36,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33839
2023-11-16 06:37:36,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34961
2023-11-16 06:37:36,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,392 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,392 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sxvhobml
2023-11-16 06:37:36,394 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0313854d-9ff1-410c-855f-03fa4f90aec7
2023-11-16 06:37:36,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9a7c124-5e9f-4d52-bfc1-5632cf99decb
2023-11-16 06:37:36,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44731
2023-11-16 06:37:36,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44731
2023-11-16 06:37:36,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46345
2023-11-16 06:37:36,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,539 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,539 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6u9ydt5o
2023-11-16 06:37:36,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63d57a85-ca04-48b3-8071-6fbd00fd3c55
2023-11-16 06:37:36,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33441
2023-11-16 06:37:36,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33441
2023-11-16 06:37:36,545 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38905
2023-11-16 06:37:36,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:36,546 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:36,546 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:36,546 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:36,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p4kbyiha
2023-11-16 06:37:36,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acbe4afe-58cc-4765-9eec-75caf5c8a062
2023-11-16 06:37:37,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49f5a537-82a6-4e66-8c25-11acee530917
2023-11-16 06:37:37,013 - distributed.worker - INFO - Starting Worker plugin PreImport-187f0405-3645-4a0f-95fc-82254205a625
2023-11-16 06:37:37,014 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,014 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75c9506e-4359-4c84-8802-955c76830aff
2023-11-16 06:37:37,015 - distributed.worker - INFO - Starting Worker plugin PreImport-005adcb9-8246-473a-8409-e573fcf0e3b0
2023-11-16 06:37:37,016 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,046 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39055', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,048 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39055
2023-11-16 06:37:37,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56552
2023-11-16 06:37:37,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,052 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,052 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,057 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40461', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40461
2023-11-16 06:37:37,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56564
2023-11-16 06:37:37,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,063 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,068 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,068 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39971', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39971
2023-11-16 06:37:37,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56578
2023-11-16 06:37:37,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,092 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,092 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,093 - distributed.worker - INFO - Starting Worker plugin PreImport-38b04537-63fb-487c-8c79-4db83b1cafea
2023-11-16 06:37:37,093 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,101 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33935', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,102 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33935
2023-11-16 06:37:37,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56588
2023-11-16 06:37:37,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,110 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,110 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,123 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33839', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,124 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33839
2023-11-16 06:37:37,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56594
2023-11-16 06:37:37,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,126 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,126 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88c4d2d5-06c4-4933-8232-5f72b267a619
2023-11-16 06:37:37,131 - distributed.worker - INFO - Starting Worker plugin PreImport-cd1be618-f97c-49fa-a627-c482809b945d
2023-11-16 06:37:37,131 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,147 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df4ef8b3-ba43-438e-82ca-3b683118200f
2023-11-16 06:37:37,147 - distributed.worker - INFO - Starting Worker plugin PreImport-6b840164-65cf-4258-b0b7-384fc4b53631
2023-11-16 06:37:37,147 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,149 - distributed.worker - INFO - Starting Worker plugin PreImport-4728d42d-c1c6-4394-acba-bd788baade90
2023-11-16 06:37:37,150 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,161 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33441', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,162 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33441
2023-11-16 06:37:37,162 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56602
2023-11-16 06:37:37,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,164 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,164 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,186 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44731', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,187 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44731
2023-11-16 06:37:37,187 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56604
2023-11-16 06:37:37,189 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41803', status: init, memory: 0, processing: 0>
2023-11-16 06:37:37,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41803
2023-11-16 06:37:37,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56610
2023-11-16 06:37:37,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,195 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:37,197 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:37,197 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:37,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:37,266 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,266 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,267 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,279 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:37:37,287 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,289 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:37,292 - distributed.scheduler - INFO - Remove client Client-9d311f34-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:37,292 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56534; closing.
2023-11-16 06:37:37,292 - distributed.scheduler - INFO - Remove client Client-9d311f34-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:37,293 - distributed.scheduler - INFO - Close client connection: Client-9d311f34-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:37,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41483'. Reason: nanny-close
2023-11-16 06:37:37,294 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43401'. Reason: nanny-close
2023-11-16 06:37:37,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,295 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44731. Reason: nanny-close
2023-11-16 06:37:37,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41223'. Reason: nanny-close
2023-11-16 06:37:37,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,295 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41803. Reason: nanny-close
2023-11-16 06:37:37,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43935'. Reason: nanny-close
2023-11-16 06:37:37,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39055. Reason: nanny-close
2023-11-16 06:37:37,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45029'. Reason: nanny-close
2023-11-16 06:37:37,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39971. Reason: nanny-close
2023-11-16 06:37:37,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44709'. Reason: nanny-close
2023-11-16 06:37:37,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,297 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33935. Reason: nanny-close
2023-11-16 06:37:37,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33029'. Reason: nanny-close
2023-11-16 06:37:37,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56604; closing.
2023-11-16 06:37:37,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40461. Reason: nanny-close
2023-11-16 06:37:37,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56610; closing.
2023-11-16 06:37:37,298 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37979'. Reason: nanny-close
2023-11-16 06:37:37,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56552; closing.
2023-11-16 06:37:37,298 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:37,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33839. Reason: nanny-close
2023-11-16 06:37:37,298 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.2986932')
2023-11-16 06:37:37,299 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33441. Reason: nanny-close
2023-11-16 06:37:37,299 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.2992897')
2023-11-16 06:37:37,299 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,299 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,299 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,299 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,299 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39055', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.2997112')
2023-11-16 06:37:37,299 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,300 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56578; closing.
2023-11-16 06:37:37,301 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,301 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:37,301 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56588; closing.
2023-11-16 06:37:37,301 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,302 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39971', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.301934')
2023-11-16 06:37:37,302 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,302 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:37,303 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.3031816')
2023-11-16 06:37:37,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56564; closing.
2023-11-16 06:37:37,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56594; closing.
2023-11-16 06:37:37,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56602; closing.
2023-11-16 06:37:37,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.3044112')
2023-11-16 06:37:37,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.3048203')
2023-11-16 06:37:37,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116657.3052218')
2023-11-16 06:37:37,305 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:39,262 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:39,262 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:39,263 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:39,264 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:37:39,264 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-11-16 06:37:41,525 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:41,530 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39791 instead
  warnings.warn(
2023-11-16 06:37:41,534 - distributed.scheduler - INFO - State start
2023-11-16 06:37:41,556 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:41,557 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:37:41,558 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39791/status
2023-11-16 06:37:41,559 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:37:41,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45649'
2023-11-16 06:37:41,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39671'
2023-11-16 06:37:41,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46427'
2023-11-16 06:37:41,863 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40849'
2023-11-16 06:37:41,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41123'
2023-11-16 06:37:41,875 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39287'
2023-11-16 06:37:41,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37185'
2023-11-16 06:37:41,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38489'
2023-11-16 06:37:43,562 - distributed.scheduler - INFO - Receive client connection: Client-a35709c6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:43,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46708
2023-11-16 06:37:43,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,725 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,736 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,737 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,742 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:43,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:43,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:43,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:47,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45455
2023-11-16 06:37:47,867 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45455
2023-11-16 06:37:47,867 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44991
2023-11-16 06:37:47,867 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,867 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,867 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,867 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,867 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xvc0xo2n
2023-11-16 06:37:47,868 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbf80468-e1b8-4d4a-813d-fd0ef346cf4c
2023-11-16 06:37:47,901 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35285
2023-11-16 06:37:47,902 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35285
2023-11-16 06:37:47,902 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33177
2023-11-16 06:37:47,902 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,902 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,903 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,903 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gniye5ib
2023-11-16 06:37:47,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cebe06a0-039c-46ec-96bc-229ce4a1e74c
2023-11-16 06:37:47,903 - distributed.worker - INFO - Starting Worker plugin PreImport-39be201e-c44d-47dd-8aa9-bd5356f526d8
2023-11-16 06:37:47,904 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31f9ba0e-34ab-47d3-9261-a2b0ac30c75e
2023-11-16 06:37:47,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44357
2023-11-16 06:37:47,931 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44357
2023-11-16 06:37:47,931 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44495
2023-11-16 06:37:47,931 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,931 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,931 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,931 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l1uqiqgk
2023-11-16 06:37:47,932 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50f93e17-862b-4fbd-84cb-a82aa3023b90
2023-11-16 06:37:47,933 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffc99d62-5a5c-43dd-889f-8e02e3124256
2023-11-16 06:37:47,937 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44131
2023-11-16 06:37:47,938 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44131
2023-11-16 06:37:47,937 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34103
2023-11-16 06:37:47,938 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38483
2023-11-16 06:37:47,938 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34103
2023-11-16 06:37:47,938 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,938 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,938 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42769
2023-11-16 06:37:47,938 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,938 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,938 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,938 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,938 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,938 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jka_30qo
2023-11-16 06:37:47,939 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,939 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yafsb2vd
2023-11-16 06:37:47,939 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d71a7c97-c6ba-4560-8602-c2480df3419c
2023-11-16 06:37:47,939 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b82d2810-11c0-470d-b072-7a279c8c97d7
2023-11-16 06:37:47,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37011
2023-11-16 06:37:47,946 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37011
2023-11-16 06:37:47,946 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34893
2023-11-16 06:37:47,946 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,946 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,946 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,947 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g6y1vchs
2023-11-16 06:37:47,946 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43821
2023-11-16 06:37:47,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43821
2023-11-16 06:37:47,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37873
2023-11-16 06:37:47,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38367
2023-11-16 06:37:47,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38367
2023-11-16 06:37:47,947 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,947 - distributed.worker - INFO - Starting Worker plugin PreImport-d01cde7e-a2ed-4e20-a921-09fb3f9a69f6
2023-11-16 06:37:47,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40179
2023-11-16 06:37:47,947 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bedc1e5-3e41-443a-828f-0fce4566fa37
2023-11-16 06:37:47,947 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:47,947 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,947 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,948 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5426a919-511f-43eb-bd12-43287fbf6847
2023-11-16 06:37:47,948 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,948 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9xuayb_a
2023-11-16 06:37:47,949 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7039ecfa-6a45-4ce1-926d-46a7ecc668dc
2023-11-16 06:37:47,949 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59a3fcee-34ba-47d1-b903-0fc7af152d46
2023-11-16 06:37:47,947 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:47,951 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:47,952 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:37:47,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yhwdisye
2023-11-16 06:37:47,952 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e421a220-fc13-4911-8724-be96d937b4c4
2023-11-16 06:37:48,177 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee92f49e-0fc6-4226-aa21-35a92965b13f
2023-11-16 06:37:48,177 - distributed.worker - INFO - Starting Worker plugin PreImport-3bbfda6a-5cc2-4334-8411-e1fa9e7291f2
2023-11-16 06:37:48,177 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45455', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45455
2023-11-16 06:37:48,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46732
2023-11-16 06:37:48,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,220 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,229 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0bd2d5ce-054b-45d7-afe0-01dbee36a931
2023-11-16 06:37:48,229 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,230 - distributed.worker - INFO - Starting Worker plugin PreImport-b3c8e870-8850-4939-bf36-fcdc83fdeda7
2023-11-16 06:37:48,230 - distributed.worker - INFO - Starting Worker plugin PreImport-6c387490-24f7-4e8b-9eed-79b3cecf9949
2023-11-16 06:37:48,231 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,231 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bb99001-3cdb-4d9a-b43c-e6a349b0e9a0
2023-11-16 06:37:48,251 - distributed.worker - INFO - Starting Worker plugin PreImport-cbdcbadf-22e9-4e4d-bf04-ce2f580da38b
2023-11-16 06:37:48,252 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,262 - distributed.worker - INFO - Starting Worker plugin PreImport-c4ae18ca-f3a6-4473-b9fa-7f3445593105
2023-11-16 06:37:48,263 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,263 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ae52c63-ed58-4717-9e26-f88a1738fbe6
2023-11-16 06:37:48,264 - distributed.worker - INFO - Starting Worker plugin PreImport-06d6787d-8ac9-4281-97f0-f7755d3c39b6
2023-11-16 06:37:48,264 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,264 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,275 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35285', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,275 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35285
2023-11-16 06:37:48,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46734
2023-11-16 06:37:48,276 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44357', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,276 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44357
2023-11-16 06:37:48,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46736
2023-11-16 06:37:48,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43821', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,278 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43821
2023-11-16 06:37:48,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46740
2023-11-16 06:37:48,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,280 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,280 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,280 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,280 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,282 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,286 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,286 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,287 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34103', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34103
2023-11-16 06:37:48,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46782
2023-11-16 06:37:48,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,296 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,299 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44131', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,299 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44131
2023-11-16 06:37:48,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46754
2023-11-16 06:37:48,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,301 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,302 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,302 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,308 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37011', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,309 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37011
2023-11-16 06:37:48,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46798
2023-11-16 06:37:48,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38367', status: init, memory: 0, processing: 0>
2023-11-16 06:37:48,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38367
2023-11-16 06:37:48,310 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46770
2023-11-16 06:37:48,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:48,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,313 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,319 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:48,319 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:48,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:48,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:37:48,373 - distributed.scheduler - INFO - Remove client Client-a35709c6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:48,373 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46708; closing.
2023-11-16 06:37:48,373 - distributed.scheduler - INFO - Remove client Client-a35709c6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:48,373 - distributed.scheduler - INFO - Close client connection: Client-a35709c6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:48,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45649'. Reason: nanny-close
2023-11-16 06:37:48,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39671'. Reason: nanny-close
2023-11-16 06:37:48,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44131. Reason: nanny-close
2023-11-16 06:37:48,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46427'. Reason: nanny-close
2023-11-16 06:37:48,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38367. Reason: nanny-close
2023-11-16 06:37:48,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40849'. Reason: nanny-close
2023-11-16 06:37:48,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45455. Reason: nanny-close
2023-11-16 06:37:48,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41123'. Reason: nanny-close
2023-11-16 06:37:48,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35285. Reason: nanny-close
2023-11-16 06:37:48,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39287'. Reason: nanny-close
2023-11-16 06:37:48,378 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46754; closing.
2023-11-16 06:37:48,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,379 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.3790967')
2023-11-16 06:37:48,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37011. Reason: nanny-close
2023-11-16 06:37:48,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37185'. Reason: nanny-close
2023-11-16 06:37:48,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43821. Reason: nanny-close
2023-11-16 06:37:48,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38489'. Reason: nanny-close
2023-11-16 06:37:48,380 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:48,380 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44357. Reason: nanny-close
2023-11-16 06:37:48,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46770; closing.
2023-11-16 06:37:48,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46732; closing.
2023-11-16 06:37:48,380 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,381 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,381 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34103. Reason: nanny-close
2023-11-16 06:37:48,381 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.381608')
2023-11-16 06:37:48,381 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,382 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45455', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.3819625')
2023-11-16 06:37:48,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46734; closing.
2023-11-16 06:37:48,382 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.382647')
2023-11-16 06:37:48,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,383 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46798; closing.
2023-11-16 06:37:48,383 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,384 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:48,384 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.3840966')
2023-11-16 06:37:48,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46740; closing.
2023-11-16 06:37:48,384 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,384 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:48,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46782; closing.
2023-11-16 06:37:48,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46736; closing.
2023-11-16 06:37:48,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.3850775')
2023-11-16 06:37:48,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.3854325')
2023-11-16 06:37:48,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116668.385778')
2023-11-16 06:37:48,385 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:48,386 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:50,042 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:50,042 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:50,043 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:50,044 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:37:50,044 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-11-16 06:37:52,242 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:52,247 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46405 instead
  warnings.warn(
2023-11-16 06:37:52,250 - distributed.scheduler - INFO - State start
2023-11-16 06:37:52,271 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:37:52,272 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:37:52,273 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46405/status
2023-11-16 06:37:52,273 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:37:52,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41749'
2023-11-16 06:37:53,939 - distributed.scheduler - INFO - Receive client connection: Client-a9b002ae-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:53,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55618
2023-11-16 06:37:54,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:37:54,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:37:54,653 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:37:55,611 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42485
2023-11-16 06:37:55,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42485
2023-11-16 06:37:55,612 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-11-16 06:37:55,612 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:37:55,612 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:55,612 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:37:55,612 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 06:37:55,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztxnpz40
2023-11-16 06:37:55,612 - distributed.worker - INFO - Starting Worker plugin RMMSetup-999cdb3e-b466-441f-87fb-9b13367bae27
2023-11-16 06:37:55,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5caccd9-154f-401d-8b24-af06b646d18a
2023-11-16 06:37:55,613 - distributed.worker - INFO - Starting Worker plugin PreImport-a40a9e1a-0d6b-4a23-86d9-1c7436796d27
2023-11-16 06:37:55,614 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:55,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42485', status: init, memory: 0, processing: 0>
2023-11-16 06:37:55,645 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42485
2023-11-16 06:37:55,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55634
2023-11-16 06:37:55,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:37:55,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:37:55,648 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:37:55,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:37:55,686 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:37:55,688 - distributed.scheduler - INFO - Remove client Client-a9b002ae-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:55,689 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55618; closing.
2023-11-16 06:37:55,689 - distributed.scheduler - INFO - Remove client Client-a9b002ae-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:55,689 - distributed.scheduler - INFO - Close client connection: Client-a9b002ae-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:37:55,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41749'. Reason: nanny-close
2023-11-16 06:37:55,690 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:37:55,692 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42485. Reason: nanny-close
2023-11-16 06:37:55,694 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:37:55,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55634; closing.
2023-11-16 06:37:55,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116675.6944816')
2023-11-16 06:37:55,694 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:37:55,695 - distributed.nanny - INFO - Worker closed
2023-11-16 06:37:56,957 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:37:56,957 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:37:56,957 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:37:56,958 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:37:56,959 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-11-16 06:38:00,878 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:00,882 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39683 instead
  warnings.warn(
2023-11-16 06:38:00,886 - distributed.scheduler - INFO - State start
2023-11-16 06:38:00,910 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:00,911 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:38:00,912 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39683/status
2023-11-16 06:38:00,912 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:38:00,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40865'
2023-11-16 06:38:02,044 - distributed.scheduler - INFO - Receive client connection: Client-aee9c3a1-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:02,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43962
2023-11-16 06:38:02,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:02,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:02,923 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:04,025 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32867
2023-11-16 06:38:04,026 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32867
2023-11-16 06:38:04,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39481
2023-11-16 06:38:04,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:04,026 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:04,026 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:04,026 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 06:38:04,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lmcakmb4
2023-11-16 06:38:04,026 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88167f71-4eb3-45ef-8276-4f164fa8ae9c
2023-11-16 06:38:04,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-598b8dc0-ad76-47a1-b2bd-34db7dd14d65
2023-11-16 06:38:04,027 - distributed.worker - INFO - Starting Worker plugin PreImport-52d479b9-223d-44e2-a84b-599d97127501
2023-11-16 06:38:04,029 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:04,066 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32867', status: init, memory: 0, processing: 0>
2023-11-16 06:38:04,067 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32867
2023-11-16 06:38:04,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43982
2023-11-16 06:38:04,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:04,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:04,069 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:04,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:04,102 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:04,105 - distributed.scheduler - INFO - Remove client Client-aee9c3a1-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:04,105 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43962; closing.
2023-11-16 06:38:04,105 - distributed.scheduler - INFO - Remove client Client-aee9c3a1-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:04,106 - distributed.scheduler - INFO - Close client connection: Client-aee9c3a1-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:04,107 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40865'. Reason: nanny-close
2023-11-16 06:38:04,110 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:04,111 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32867. Reason: nanny-close
2023-11-16 06:38:04,113 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:04,113 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43982; closing.
2023-11-16 06:38:04,114 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116684.1142664')
2023-11-16 06:38:04,114 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:38:04,115 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:05,324 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:38:05,324 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:38:05,325 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:38:05,326 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:38:05,327 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-11-16 06:38:07,600 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:07,606 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-16 06:38:07,610 - distributed.scheduler - INFO - State start
2023-11-16 06:38:07,714 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:07,715 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:38:07,716 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-16 06:38:07,716 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:38:11,748 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44008'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44008>: Stream is closed
2023-11-16 06:38:12,003 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:38:12,004 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:38:12,004 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:38:12,005 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:38:12,006 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-11-16 06:38:14,401 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:14,405 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-16 06:38:14,409 - distributed.scheduler - INFO - State start
2023-11-16 06:38:14,650 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:14,651 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-16 06:38:14,651 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-16 06:38:14,651 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:38:14,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33161'
2023-11-16 06:38:15,198 - distributed.scheduler - INFO - Receive client connection: Client-b6e038d5-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:15,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57592
2023-11-16 06:38:16,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:16,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:16,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:17,129 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44389
2023-11-16 06:38:17,129 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44389
2023-11-16 06:38:17,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44325
2023-11-16 06:38:17,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-16 06:38:17,130 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:17,130 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:17,130 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 06:38:17,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-7e7iuzkv
2023-11-16 06:38:17,130 - distributed.worker - INFO - Starting Worker plugin PreImport-1a7f9c08-3a15-4eb0-b1fa-09080d3d5a02
2023-11-16 06:38:17,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dce865f8-c10a-4830-8b7e-d6c67fee296f
2023-11-16 06:38:17,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb0fb479-4279-4840-8091-2582179d916e
2023-11-16 06:38:17,131 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:17,162 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44389', status: init, memory: 0, processing: 0>
2023-11-16 06:38:17,163 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44389
2023-11-16 06:38:17,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57600
2023-11-16 06:38:17,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:17,165 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-16 06:38:17,165 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:17,167 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-16 06:38:17,264 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:17,267 - distributed.scheduler - INFO - Remove client Client-b6e038d5-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:17,267 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57592; closing.
2023-11-16 06:38:17,268 - distributed.scheduler - INFO - Remove client Client-b6e038d5-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:17,268 - distributed.scheduler - INFO - Close client connection: Client-b6e038d5-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:17,269 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33161'. Reason: nanny-close
2023-11-16 06:38:17,269 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:17,271 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44389. Reason: nanny-close
2023-11-16 06:38:17,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57600; closing.
2023-11-16 06:38:17,272 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-16 06:38:17,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116697.2731926')
2023-11-16 06:38:17,273 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:38:17,274 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:18,586 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:38:18,586 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:38:18,587 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:38:18,588 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-16 06:38:18,588 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-11-16 06:38:21,059 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:21,064 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34849 instead
  warnings.warn(
2023-11-16 06:38:21,067 - distributed.scheduler - INFO - State start
2023-11-16 06:38:21,694 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:21,695 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:38:21,697 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34849/status
2023-11-16 06:38:21,697 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:38:21,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41685'
2023-11-16 06:38:22,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41683'
2023-11-16 06:38:22,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45627'
2023-11-16 06:38:22,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38267'
2023-11-16 06:38:22,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39473'
2023-11-16 06:38:22,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41495'
2023-11-16 06:38:22,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46411'
2023-11-16 06:38:22,093 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46533'
2023-11-16 06:38:22,660 - distributed.scheduler - INFO - Receive client connection: Client-bace48f6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:22,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57670
2023-11-16 06:38:23,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:23,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:23,845 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:23,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:23,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:23,877 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:23,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:23,893 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:23,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:23,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:23,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:23,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:23,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:23,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:23,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:23,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:23,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:23,995 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:24,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:24,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:24,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:24,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:24,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:24,031 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:25,540 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37815
2023-11-16 06:38:25,541 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37815
2023-11-16 06:38:25,541 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46035
2023-11-16 06:38:25,541 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:25,541 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:25,541 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:25,541 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:25,541 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ej7lhey
2023-11-16 06:38:25,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0034b7ca-1637-460d-b916-ec55f4936b53
2023-11-16 06:38:25,542 - distributed.worker - INFO - Starting Worker plugin PreImport-c0b2f9f2-bf2e-42cd-9eda-a9f23dffeae8
2023-11-16 06:38:25,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-076b47cd-8b95-482b-a44c-3c23d3f49fcf
2023-11-16 06:38:26,314 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:26,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37815', status: init, memory: 0, processing: 0>
2023-11-16 06:38:26,508 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37815
2023-11-16 06:38:26,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57684
2023-11-16 06:38:26,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:26,514 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:26,514 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:26,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,551 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33153
2023-11-16 06:38:27,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33153
2023-11-16 06:38:27,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34651
2023-11-16 06:38:27,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,552 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,552 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xnvx5o1r
2023-11-16 06:38:27,553 - distributed.worker - INFO - Starting Worker plugin PreImport-c2514341-480c-44f5-a74d-0343a98e4ce7
2023-11-16 06:38:27,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c0e791f-73c9-4a90-bd61-6108dad60943
2023-11-16 06:38:27,553 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33399
2023-11-16 06:38:27,554 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33399
2023-11-16 06:38:27,554 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46021
2023-11-16 06:38:27,554 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,554 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,554 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,554 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,554 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8vhx0c26
2023-11-16 06:38:27,555 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d58e8729-c062-4cd5-b498-8fea603ac9f2
2023-11-16 06:38:27,554 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41415
2023-11-16 06:38:27,556 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41415
2023-11-16 06:38:27,556 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41535
2023-11-16 06:38:27,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,557 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,557 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rm_0u_d_
2023-11-16 06:38:27,558 - distributed.worker - INFO - Starting Worker plugin PreImport-49a8c147-76c3-4a91-853a-f02717a8e799
2023-11-16 06:38:27,558 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb27e340-8730-499a-bf3f-8632698b1bb6
2023-11-16 06:38:27,559 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8684ff0a-0bbb-41d1-99cc-aa586379ddbd
2023-11-16 06:38:27,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70eb882b-9df4-4906-aedc-7c5e70eb4379
2023-11-16 06:38:27,665 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,676 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfd903b7-4d51-48d2-a809-9aae734d150d
2023-11-16 06:38:27,676 - distributed.worker - INFO - Starting Worker plugin PreImport-b5c5d3a8-371e-4c4a-bed5-c83b162f36c3
2023-11-16 06:38:27,676 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,686 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33153', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,703 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33153
2023-11-16 06:38:27,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57700
2023-11-16 06:38:27,704 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,705 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,705 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,708 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33399', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,708 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33399
2023-11-16 06:38:27,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57710
2023-11-16 06:38:27,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,710 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,710 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41415', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41415
2023-11-16 06:38:27,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57712
2023-11-16 06:38:27,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,724 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40759
2023-11-16 06:38:27,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45425
2023-11-16 06:38:27,751 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40759
2023-11-16 06:38:27,751 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45425
2023-11-16 06:38:27,751 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41779
2023-11-16 06:38:27,751 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41855
2023-11-16 06:38:27,751 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,751 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,752 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,752 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ovi0cpu
2023-11-16 06:38:27,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lrmzoww1
2023-11-16 06:38:27,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff2bcf88-9f1e-4bf9-8ebc-1a36da8b77d1
2023-11-16 06:38:27,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7f44f4e-e355-4613-9cd8-b616f57fe1cd
2023-11-16 06:38:27,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48244b2a-7b0e-4020-bf54-1f685201503e
2023-11-16 06:38:27,752 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34195
2023-11-16 06:38:27,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34195
2023-11-16 06:38:27,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34365
2023-11-16 06:38:27,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,753 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,753 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nqw5bhd1
2023-11-16 06:38:27,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33707
2023-11-16 06:38:27,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33707
2023-11-16 06:38:27,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e0c1a9b-40d9-4135-8dc0-45e2756abc26
2023-11-16 06:38:27,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44249
2023-11-16 06:38:27,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,754 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,754 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:27,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-16 06:38:27,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8pf6g3nr
2023-11-16 06:38:27,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c36c831c-47b1-416d-b078-e126c9a64dca
2023-11-16 06:38:27,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3938a013-6dd6-48c3-b819-61b35ad0adfc
2023-11-16 06:38:27,861 - distributed.worker - INFO - Starting Worker plugin PreImport-df14f317-1916-4b68-af26-3a948476c1ef
2023-11-16 06:38:27,862 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,873 - distributed.worker - INFO - Starting Worker plugin PreImport-f37145e2-b61d-4864-83ba-82aa36d71cdd
2023-11-16 06:38:27,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e39d1fc7-8e36-41eb-8410-08adbb892202
2023-11-16 06:38:27,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa5383b7-5768-4baf-abb3-c5d98ca444cb
2023-11-16 06:38:27,874 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,874 - distributed.worker - INFO - Starting Worker plugin PreImport-46341411-5101-4b13-8311-59d459e85f4b
2023-11-16 06:38:27,874 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,874 - distributed.worker - INFO - Starting Worker plugin PreImport-5b693abb-47fa-423a-a3ef-83ae57ce120c
2023-11-16 06:38:27,875 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33707', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33707
2023-11-16 06:38:27,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57728
2023-11-16 06:38:27,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,896 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45425', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45425
2023-11-16 06:38:27,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57744
2023-11-16 06:38:27,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40759', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,901 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,901 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40759
2023-11-16 06:38:27,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57760
2023-11-16 06:38:27,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,903 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,903 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34195', status: init, memory: 0, processing: 0>
2023-11-16 06:38:27,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34195
2023-11-16 06:38:27,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57776
2023-11-16 06:38:27,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:27,919 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:27,919 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:27,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:27,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-16 06:38:27,964 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,964 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,964 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,964 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,965 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,965 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,965 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,965 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:27,969 - distributed.scheduler - INFO - Remove client Client-bace48f6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:27,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57670; closing.
2023-11-16 06:38:27,969 - distributed.scheduler - INFO - Remove client Client-bace48f6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:27,969 - distributed.scheduler - INFO - Close client connection: Client-bace48f6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:27,970 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41685'. Reason: nanny-close
2023-11-16 06:38:27,971 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,971 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41683'. Reason: nanny-close
2023-11-16 06:38:27,972 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,972 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33399. Reason: nanny-close
2023-11-16 06:38:27,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45627'. Reason: nanny-close
2023-11-16 06:38:27,972 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,972 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45425. Reason: nanny-close
2023-11-16 06:38:27,973 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38267'. Reason: nanny-close
2023-11-16 06:38:27,973 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,973 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34195. Reason: nanny-close
2023-11-16 06:38:27,973 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39473'. Reason: nanny-close
2023-11-16 06:38:27,973 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,974 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,974 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57710; closing.
2023-11-16 06:38:27,974 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37815. Reason: nanny-close
2023-11-16 06:38:27,974 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.974417')
2023-11-16 06:38:27,974 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41495'. Reason: nanny-close
2023-11-16 06:38:27,974 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,974 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,975 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41415. Reason: nanny-close
2023-11-16 06:38:27,975 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46411'. Reason: nanny-close
2023-11-16 06:38:27,975 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,975 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,975 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40759. Reason: nanny-close
2023-11-16 06:38:27,975 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46533'. Reason: nanny-close
2023-11-16 06:38:27,976 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,976 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:27,976 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57744; closing.
2023-11-16 06:38:27,976 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33153. Reason: nanny-close
2023-11-16 06:38:27,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33707. Reason: nanny-close
2023-11-16 06:38:27,977 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,977 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45425', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.9770765')
2023-11-16 06:38:27,977 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,977 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57776; closing.
2023-11-16 06:38:27,977 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,977 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,978 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.9781044')
2023-11-16 06:38:27,978 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57684; closing.
2023-11-16 06:38:27,978 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,978 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:27,978 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,979 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,979 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.9792635')
2023-11-16 06:38:27,979 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,979 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57712; closing.
2023-11-16 06:38:27,979 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57760; closing.
2023-11-16 06:38:27,980 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,980 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41415', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.9806483')
2023-11-16 06:38:27,980 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:27,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.9810445')
2023-11-16 06:38:27,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57700; closing.
2023-11-16 06:38:27,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57728; closing.
2023-11-16 06:38:27,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.9820936')
2023-11-16 06:38:27,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116707.982563')
2023-11-16 06:38:27,982 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:38:29,588 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:38:29,588 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:38:29,589 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:38:29,590 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:38:29,591 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-11-16 06:38:31,949 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:31,954 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43803 instead
  warnings.warn(
2023-11-16 06:38:31,957 - distributed.scheduler - INFO - State start
2023-11-16 06:38:32,537 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:32,538 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:38:32,539 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43803/status
2023-11-16 06:38:32,539 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:38:32,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43259'
2023-11-16 06:38:34,209 - distributed.scheduler - INFO - Receive client connection: Client-c15f1ca6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:34,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37326
2023-11-16 06:38:34,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:34,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:34,397 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:35,447 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41827
2023-11-16 06:38:35,448 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41827
2023-11-16 06:38:35,448 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40111
2023-11-16 06:38:35,449 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:35,449 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:35,449 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:35,449 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 06:38:35,449 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zr0oeq5x
2023-11-16 06:38:35,449 - distributed.worker - INFO - Starting Worker plugin PreImport-aca9ecfa-a3e6-4d03-b099-3b43cd4d1c03
2023-11-16 06:38:35,449 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0353b48c-5694-422f-8ea6-e10ee0a687b5
2023-11-16 06:38:35,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0918affb-17f0-4e44-a1ba-8a83e5429808
2023-11-16 06:38:35,540 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:35,571 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41827', status: init, memory: 0, processing: 0>
2023-11-16 06:38:35,572 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41827
2023-11-16 06:38:35,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37334
2023-11-16 06:38:35,573 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:35,574 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:35,574 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:35,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:35,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:38:35,658 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:35,659 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:35,662 - distributed.scheduler - INFO - Remove client Client-c15f1ca6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:35,662 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37326; closing.
2023-11-16 06:38:35,663 - distributed.scheduler - INFO - Remove client Client-c15f1ca6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:35,663 - distributed.scheduler - INFO - Close client connection: Client-c15f1ca6-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:35,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43259'. Reason: nanny-close
2023-11-16 06:38:35,664 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:35,665 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41827. Reason: nanny-close
2023-11-16 06:38:35,667 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:35,667 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37334; closing.
2023-11-16 06:38:35,668 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116715.6680524')
2023-11-16 06:38:35,668 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:38:35,669 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:36,831 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:38:36,831 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:38:36,832 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:38:36,833 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:38:36,834 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-11-16 06:38:39,218 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:39,222 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42809 instead
  warnings.warn(
2023-11-16 06:38:39,226 - distributed.scheduler - INFO - State start
2023-11-16 06:38:39,279 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-16 06:38:39,279 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-16 06:38:39,280 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42809/status
2023-11-16 06:38:39,280 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-16 06:38:39,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40139'
2023-11-16 06:38:40,039 - distributed.scheduler - INFO - Receive client connection: Client-c5bb8a8d-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:40,054 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33524
2023-11-16 06:38:41,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 06:38:41,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 06:38:41,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 06:38:41,806 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36595
2023-11-16 06:38:41,807 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36595
2023-11-16 06:38:41,807 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44851
2023-11-16 06:38:41,807 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-16 06:38:41,807 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:41,807 - distributed.worker - INFO -               Threads:                          1
2023-11-16 06:38:41,807 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 06:38:41,807 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1e7u2v71
2023-11-16 06:38:41,808 - distributed.worker - INFO - Starting Worker plugin PreImport-36ba0928-c4a7-4217-bee8-0897e308807b
2023-11-16 06:38:41,808 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61505d5a-adcb-4060-93b1-fbe08f2b7aba
2023-11-16 06:38:41,902 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01989e21-1122-451c-abee-53d99aa29723
2023-11-16 06:38:41,902 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:41,933 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36595', status: init, memory: 0, processing: 0>
2023-11-16 06:38:41,934 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36595
2023-11-16 06:38:41,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33538
2023-11-16 06:38:41,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-16 06:38:41,936 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-16 06:38:41,936 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 06:38:41,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-16 06:38:41,949 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-11-16 06:38:41,953 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-16 06:38:41,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:41,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-16 06:38:41,960 - distributed.scheduler - INFO - Remove client Client-c5bb8a8d-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:41,961 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33524; closing.
2023-11-16 06:38:41,961 - distributed.scheduler - INFO - Remove client Client-c5bb8a8d-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:41,961 - distributed.scheduler - INFO - Close client connection: Client-c5bb8a8d-844a-11ee-b452-d8c49764f6bb
2023-11-16 06:38:41,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40139'. Reason: nanny-close
2023-11-16 06:38:41,962 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-16 06:38:41,964 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36595. Reason: nanny-close
2023-11-16 06:38:41,965 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33538; closing.
2023-11-16 06:38:41,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-16 06:38:41,966 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700116721.965952')
2023-11-16 06:38:41,966 - distributed.scheduler - INFO - Lost all workers
2023-11-16 06:38:41,967 - distributed.nanny - INFO - Worker closed
2023-11-16 06:38:43,028 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-16 06:38:43,029 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-16 06:38:43,029 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-16 06:38:43,030 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-16 06:38:43,031 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38321 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42091 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45721 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33529 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36535 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37311 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38155 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44207 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41829 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41829 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41999 instead
  warnings.warn(
[1700117015.323328] [dgx13:71218:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:49139) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36885 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33271 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34477 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43527 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44307 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37615 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] Process SpawnProcess-50:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 266, in _test_jit_unspill
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] Process SpawnProcess-51:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 266, in _test_jit_unspill
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] 2023-11-16 06:51:10,448 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-11-16 06:51:10,449 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-11-16 06:51:10,452 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:10,458 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:13,905 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-11-16 06:51:13,910 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:14,425 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-11-16 06:51:14,430 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:15,352 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-11-16 06:51:15,355 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:15,579 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-11-16 06:51:15,582 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:15,603 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 5
2023-11-16 06:51:15,612 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 5', 'time': 1700117475.6024284}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:16,057 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 4
2023-11-16 06:51:16,064 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 4', 'time': 1700117476.0564902}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:16,104 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 6
2023-11-16 06:51:16,110 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 7
2023-11-16 06:51:16,112 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 6', 'time': 1700117476.1037502}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 06:51:16,115 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 7', 'time': 1700117476.1074214}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-11-16 06:51:22,300 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:22,308 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:32799'. Shutting down.
2023-11-16 06:51:22,310 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe597b07d60>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:23,210 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:23,216 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:35575'. Shutting down.
2023-11-16 06:51:23,218 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f06ee08cd60>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:23,279 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:23,285 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f0c784aebb0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:23,412 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
2023-11-16 06:51:23,420 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:44183'. Shutting down.
2023-11-16 06:51:23,422 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe746d12d60>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=OutOfMemoryError('Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 1426, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1447, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1118, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 1139, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 1384, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
  File "cupy/cuda/memory.pyx", line 1387, in cupy.cuda.memory.SingleDeviceMemoryPool._try_malloc
cupy.cuda.memory.OutOfMemoryError: Out of memory allocating 80,384 bytes (allocated so far: 0 bytes).
FAILED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41435 instead
  warnings.warn(
2023-11-16 06:51:26,711 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-95:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 26, in _test_initialize_ucx_tcp
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34895 instead
  warnings.warn(
2023-11-16 06:51:29,082 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-96:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 61, in _test_initialize_ucx_nvlink
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45493 instead
  warnings.warn(
2023-11-16 06:51:31,426 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-97:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 97, in _test_initialize_ucx_infiniband
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45847 instead
  warnings.warn(
2023-11-16 06:51:33,801 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-98:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 135, in _test_initialize_ucx_all
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2023-11-16 07:01:20,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 07:01:20,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 07:01:20,591 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 07:01:21,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37639
2023-11-16 07:01:21,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37639
2023-11-16 07:01:21,354 - distributed.worker - INFO -           Worker name:                          0
2023-11-16 07:01:21,354 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39637
2023-11-16 07:01:21,354 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43847
2023-11-16 07:01:21,354 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 07:01:21,354 - distributed.worker - INFO -               Threads:                          1
2023-11-16 07:01:21,354 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 07:01:21,354 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-klu3tggd
2023-11-16 07:01:21,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5155874-c892-4084-a827-e3e9aaf6a32f
2023-11-16 07:01:21,355 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffa253c6-cb22-4def-b19b-42c6df2297b6
2023-11-16 07:01:21,355 - distributed.worker - INFO - Starting Worker plugin PreImport-665656fd-2645-42a8-bd65-b863c644d13a
No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 148, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2023-11-16 07:01:21,529 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37639. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2023-11-16 07:01:21,529 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-11-16 07:01:21,531 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 148, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-11-16 07:01:23,599 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-16 07:01:23,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-16 07:01:23,603 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-16 07:01:24,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38593
2023-11-16 07:01:24,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38593
2023-11-16 07:01:24,374 - distributed.worker - INFO -           Worker name:                          0
2023-11-16 07:01:24,374 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45233
2023-11-16 07:01:24,374 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43847
2023-11-16 07:01:24,374 - distributed.worker - INFO - -------------------------------------------------
2023-11-16 07:01:24,374 - distributed.worker - INFO -               Threads:                          1
2023-11-16 07:01:24,374 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-16 07:01:24,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2nc0dteh
2023-11-16 07:01:24,374 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4850eacd-2440-4c36-9a51-e208ecc62da1
2023-11-16 07:01:24,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a9367c7-89a2-4e5e-ac1a-c926a70d293a
2023-11-16 07:01:24,375 - distributed.worker - INFO - Starting Worker plugin PreImport-a9c2253e-2561-4b4c-a03a-eb312d0ba7b9
No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 148, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2023-11-16 07:01:24,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38593. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2023-11-16 07:01:24,553 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-11-16 07:01:24,558 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 148, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
