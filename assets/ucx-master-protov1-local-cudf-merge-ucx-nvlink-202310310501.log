[dgx13:73574:0:73574] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  73574) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f2341ce506d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a264) [0x7f2341ce5264]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a42a) [0x7f2341ce542a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f23e6e94420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f2341d636a4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f2341d8c829]
 6  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x2146f) [0x7f235402f46f]
 7  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x248c8) [0x7f23540328c8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f2341cee499]
 9  /opt/conda/envs/gdf/lib/./libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f235403166d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f2341d608ca]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f2341e1906a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x558da11e36fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x558da11df094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x558da11f0519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x558da11fde83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x558da1308b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x558da119ad05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x558da11e77f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x558da11e5929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x558da11df094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x558da11f0519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x558da11e1128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x558da11df094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x558da11fdccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x558da11fe44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x558da12c110e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x558da11e877c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x558da11e36fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x558da11fddac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x558da11e36fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x558da11df094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x558da11f0519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x558da11e05c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x558da11f07c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x558da11e0312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x558da11df094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x558da11f0519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x558da11e1128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x558da11df094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x558da11ded68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x558da11ded19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x558da128c07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x558da12b8fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x558da12b5353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x558da12ad16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x558da12ad05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x558da12ac297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x558da127ff07]
=================================
[dgx13:73580:0:73580] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  73580) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f0f14a4606d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a264) [0x7f0f14a46264]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a42a) [0x7f0f14a4642a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f0fb5bc1420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f0f14ac46a4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f0f14aed829]
 6  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x2146f) [0x7f0f14a0046f]
 7  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x248c8) [0x7f0f14a038c8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f0f14a4f499]
 9  /opt/conda/envs/gdf/lib/./libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f0f14a0266d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f0f14ac18ca]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f0f14b7a06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55586e7506fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55586e74c094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55586e75d519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55586e800162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f0f35d441e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55586e75577c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55586e707d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55586e7547f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55586e752929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55586e74c094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55586e75d519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55586e74e128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55586e74c094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55586e76accb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55586e76b44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55586e82e10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55586e75577c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55586e7506fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55586e76adac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55586e7506fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55586e74c094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55586e75d519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55586e74d5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55586e75d7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55586e74d312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55586e74c094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55586e75d519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55586e74e128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55586e74c094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55586e74bd68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55586e74bd19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55586e7f907b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55586e825fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55586e822353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55586e81a16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55586e81a05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55586e819297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55586e7ecf07]
=================================
2023-10-31 05:56:52,115 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:55549 -> ucx://127.0.0.1:42545
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8f46bb3100, tag: 0xb971b60580dc0431, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-31 05:56:52,146 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47685 -> ucx://127.0.0.1:56657
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f1a06a6b100, tag: 0xeb6d94b0c4be0159, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
[dgx13:73583:0:73583] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  73583) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f043312506d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a264) [0x7f0433125264]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a42a) [0x7f043312542a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f04d427b420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f04331a36a4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f04331cc829]
 6  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x2146f) [0x7f04330df46f]
 7  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x248c8) [0x7f04330e28c8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f043312e499]
 9  /opt/conda/envs/gdf/lib/./libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f04330e166d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f04331a08ca]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f043325906a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55f7eff026fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55f7efefe094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f7eff0f519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55f7effb2162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f04543fa1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55f7eff0777c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55f7efeb9d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55f7eff067f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55f7eff04929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55f7efefe094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f7eff0f519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55f7eff00128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55f7efefe094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55f7eff1cccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55f7eff1d44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55f7effe010e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55f7eff0777c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55f7eff026fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55f7eff1cdac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55f7eff026fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55f7efefe094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f7eff0f519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f7efeff5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55f7eff0f7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55f7efeff312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55f7efefe094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f7eff0f519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55f7eff00128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55f7efefe094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55f7efefdd68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55f7efefdd19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55f7effab07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55f7effd7fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55f7effd4353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55f7effcc16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55f7effcc05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55f7effcb297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55f7eff9ef07]
=================================
[dgx13:73577:0:73577] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  73577) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f92387e406d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a264) [0x7f92387e4264]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a42a) [0x7f92387e442a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f92d993c420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f92388626a4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f923888b829]
 6  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x2146f) [0x7f923879e46f]
 7  /opt/conda/envs/gdf/lib/./libuct.so.0(+0x248c8) [0x7f92387a18c8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f92387ed499]
 9  /opt/conda/envs/gdf/lib/./libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f92387a066d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f923885f8ca]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f923891806a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55b3827026fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b3826fe094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b38270f519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55b3827b2162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f9259abb1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55b38270777c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55b3826b9d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55b3827067f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55b382704929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b3826fe094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b38270f519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55b382700128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b3826fe094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55b38271cccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55b38271d44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55b3827e010e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55b38270777c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55b3827026fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55b38271cdac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55b3827026fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b3826fe094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b38270f519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b3826ff5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55b38270f7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55b3826ff312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b3826fe094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b38270f519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55b382700128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55b3826fe094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55b3826fdd68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55b3826fdd19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55b3827ab07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55b3827d7fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55b3827d4353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55b3827cc16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55b3827cc05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55b3827cb297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55b38279ef07]
=================================
2023-10-31 05:56:54,798 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47685 -> ucx://127.0.0.1:33301
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f1a06a6b180, tag: 0x51c480ea741db7bf, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-31 05:56:54,798 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33301
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fb930768140, tag: 0xb947ebeaa63ddd0a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fb930768140, tag: 0xb947ebeaa63ddd0a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-31 05:56:54,798 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33301
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fa03009b140, tag: 0x6a10bfc5dde8a6a4, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fa03009b140, tag: 0x6a10bfc5dde8a6a4, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-31 05:56:54,805 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33301
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-10-31 05:56:54,804 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33301
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-10-31 05:56:55,701 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44127 -> ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa03009b380, tag: 0xa541099b721ebe37, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-31 05:56:55,702 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f1a06a6b300, tag: 0x66d7532bc12be502, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f1a06a6b300, tag: 0x66d7532bc12be502, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-31 05:56:55,702 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f8f46bb3280, tag: 0x3762b7ecfa968120, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f8f46bb3280, tag: 0x3762b7ecfa968120, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-10-31 05:56:55,702 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47859 -> ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fb930768380, tag: 0xb8a53257c6be66bf, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-31 05:56:55,702 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47685 -> ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f1a06a6b340, tag: 0xfd443e294fcedf5b, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-31 05:56:55,702 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fa03009b200, tag: 0x3267b3a23cdaad26, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fa03009b200, tag: 0x3267b3a23cdaad26, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-31 05:56:55,703 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40057
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fb930768200, tag: 0x1478ad7210d48b92, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fb930768200, tag: 0x1478ad7210d48b92, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-31 05:56:57,010 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 1)
Function:  shuffle_group
args:      (               key   payload
shuffle                     
1           104008  35781238
1           104010  82352438
1           104012  73004779
1           104025  80876875
1           104026  22958152
...            ...       ...
1        799987875  92047839
1        799987882  59684010
1        799987892  91458766
1        799987902  92917534
1        799987903  29446595

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:57,021 - distributed.worker - ERROR - Exception during execution of task ('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6, 7).
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2382, in _prepare_args_for_execution
    data[k] = self.data[k]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/device_host_file.py", line 273, in __getitem__
    raise KeyError(key)
KeyError: ('group-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2234, in execute
    args2, kwargs2 = self._prepare_args_for_execution(ts, args, kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2386, in _prepare_args_for_execution
    data[k] = Actor(type(self.state.actors[k]), self.address, k, self)
KeyError: ('group-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
2023-10-31 05:56:57,088 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 6)
Function:  shuffle_group
args:      (               key   payload
shuffle                     
6           104001  71397967
6           104002  56920929
6           104004  25405039
6           104006  60030999
6           104015  87829185
...            ...       ...
6        799987871  69019480
6        799987879  33934337
6        799987886  91931955
6        799987896  40818041
6        799987899   6633392

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:58,581 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  _concat
args:      ([                key   payload
72712     851304898  23958880
59971     800275470  91870154
59972     816046605  75711051
104641    851449864  82814904
35040     815895327  54833085
...             ...       ...
99960105  818873186  46505591
99960119  102194112  33480601
99960065    6015631  72710515
99960085  849463350  33828731
99960095  802273421  44478463

[12498632 rows x 2 columns],                 key   payload
23117     964414959  42249312
23124     935475971  46849209
75845     968188591  50348979
75854     957372461  71522172
75856      21127267  86568807
...             ...       ...
99987722  120561464  69365112
99987724  914938041  34471987
99987725  928121394  83723809
99987736  910420241  32013690
99987737  925704825  30010469

[12502237 rows x 2 columns],                  key   payload
22018     1054195119  13726404
123077    1068971409  82280188
42727     1069795030  32563579
123078    1054577653  28657800
22027      731812113  75951478
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:58,748 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 0)
Function:  _concat
args:      ([               key   payload
shuffle                     
0          1032711  58126192
0           898861  23202318
0          1022517  28620757
0          1325673  67102722
0          1053237  72075688
...            ...       ...
0        799882338  17955160
0        799918264  57128043
0        799976930  30532909
0        799972755  62038702
0        799943612  98996290

[12505522 rows x 2 columns],                key   payload
shuffle                     
1           248147  54069484
1            26444  45078083
1           224189  19885977
1           156007  71692001
1          1110649  56447613
...            ...       ...
1        799710949  24876677
1        799666402  85732419
1        799726768  37770509
1        799701688  82783548
1        799778098  72856058

[12497568 rows x 2 columns],                key   payload
shuffle                     
2           602895  28077269
2           410031  32509532
2           601864  65077105
2           651205   2930751
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:58,780 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
Function:  _concat
args:      ([                key   payload
72704     402235452  37338982
72717     855977912  27422668
59970     843490742  93753108
72723     302858605  99967626
59976     310809101  72283579
...             ...       ...
99960067  851935651   2402605
99960068  868804825  76760875
99960069  820415591  67986182
99960075  834812455  89601819
99960087  503109209  62912680

[12500893 rows x 2 columns],                 key   payload
23110     912688039  10193501
23112     958623699  72223425
75840     938171483  47381529
23118     967232294  43121654
75846     957543222  61190961
...             ...       ...
99987759  918627408  49808270
99987762  945436680  84092041
99987717  950070687  51471893
99987720  620276417  87684791
99987733  613219733  31769548

[12495890 rows x 2 columns],                  key   payload
22023     1055671254  37987686
123075     127314748  84638981
42720     1035707980  45242553
123079    1003643378  25562731
22028     1059609229  45220130
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:58,972 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-31 05:56:58,972 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-31 05:56:58,987 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 6)
Function:  _concat
args:      ([               key   payload
shuffle                     
0          1050669    687340
0          1072760  11339305
0           882480   5630284
0           908613   7608106
0          1050678   8385935
...            ...       ...
0        799911394  31855143
0        799948587   6367173
0        799944059   3360854
0        799800937  73764503
0        799878760  98845094

[12498811 rows x 2 columns],                key   payload
shuffle                     
1            31136  26417248
1           194417  34746950
1           165787   4322597
1          1114280  59650070
1           531113  11451200
...            ...       ...
1        799966210  27142362
1        799852930  89573772
1        799851906  86035214
1        799858164  50727676
1        799796533  75285912

[12498510 rows x 2 columns],                key   payload
shuffle                     
2          1088717  83164202
2          1117776  13584014
2          1190052  70014902
2          1118819  92226559
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:59,007 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 2)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           877483  99984670
0           882124  17823420
0           954156   8172371
0          1308364   4720266
0          1039404  82065186
...            ...       ...
0        799986115  59011167
0        799970668  27941843
0        799969064  13136449
0        799918268  71017897
0        799970678  46518137

[12497244 rows x 2 columns],                key   payload
shuffle                     
1           189360  90837068
1           188319  41915130
1           291287  86843813
1            33682  96174474
1           979833   3809735
...            ...       ...
1        799756452   1583037
1        799790586  59379688
1        799691829  87965247
1        799849754  35641950
1        799672001  93723581

[12500558 rows x 2 columns],                key   payload
shuffle                     
2          1090812   4274734
2          1111075  67303309
2          1134328  29006689
2          1106564  40822680
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:59,081 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 1)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           924031   6004605
0           852746  55890239
0           929245  49851653
0          1316483   4768131
0           915214  60105795
...            ...       ...
0        799871103  86755390
0        799957987  53013887
0        799968626  97358657
0        799832135  46140709
0        799973242  72649324

[12497508 rows x 2 columns],                key   payload
shuffle                     
1           295886   2840911
1          1164848  13022527
1           596056  75891116
1          1130362  31958376
1          1071448  33701788
...            ...       ...
1        799999137  36773093
1        799944750   4616799
1        799797148  78003055
1        799835110  75603867
1        799862267  52929596

[12503907 rows x 2 columns],                key   payload
shuffle                     
2          1128503  55752614
2          1167433  89088575
2           946483  74802472
2          1028427  38757449
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:56:59,081 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-10-31 05:56:59,122 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 4)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           926653  44015057
0           967025   4413751
0           944898   5999552
0           965425  72326671
0           993785  49630821
...            ...       ...
0        799920538  66044877
0        799778309  69186125
0        799972780  79276304
0        799882343  59243486
0        799911397  71197909

[12503392 rows x 2 columns],                key   payload
shuffle                     
1           201523  57690570
1           191997   4506659
1          1169966  34483971
1           587936   4760178
1          1115088  39702052
...            ...       ...
1        799926901  50315775
1        799741876  26528260
1        799701685  81345316
1        799916362  63371962
1        799821123  82678729

[12500389 rows x 2 columns],                key   payload
shuffle                     
2          1129185  43387178
2          1154802  41458661
2          1094906  54760253
2          1147917  64021589
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-31 05:57:05,341 - distributed.nanny - WARNING - Restarting worker
2023-10-31 05:57:05,459 - distributed.nanny - WARNING - Restarting worker
2023-10-31 05:57:07,867 - distributed.nanny - WARNING - Restarting worker
2023-10-31 05:57:08,215 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
