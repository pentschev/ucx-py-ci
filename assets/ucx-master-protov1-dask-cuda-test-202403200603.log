============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-20 07:00:41,031 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:00:41,035 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39603 instead
  warnings.warn(
2024-03-20 07:00:41,038 - distributed.scheduler - INFO - State start
2024-03-20 07:00:41,064 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:00:41,065 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-20 07:00:41,066 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39603/status
2024-03-20 07:00:41,066 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:00:41,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38493'
2024-03-20 07:00:41,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40493'
2024-03-20 07:00:41,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36189'
2024-03-20 07:00:41,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35825'
2024-03-20 07:00:43,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:43,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:43,227 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:43,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34521
2024-03-20 07:00:43,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34521
2024-03-20 07:00:43,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43423
2024-03-20 07:00:43,228 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,228 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,228 - distributed.worker - INFO -               Threads:                          4
2024-03-20 07:00:43,228 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-20 07:00:43,228 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-c2vvwvtb
2024-03-20 07:00:43,228 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e64fc421-b76b-4dd2-9dd6-ea825cfc1ade
2024-03-20 07:00:43,228 - distributed.worker - INFO - Starting Worker plugin PreImport-03157608-8892-4382-8167-b3a0162fe8dc
2024-03-20 07:00:43,228 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59b4c8fc-e288-4342-9d4f-b6a532e0392b
2024-03-20 07:00:43,228 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34521', status: init, memory: 0, processing: 0>
2024-03-20 07:00:43,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:43,286 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:43,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:43,291 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42881
2024-03-20 07:00:43,291 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42881
2024-03-20 07:00:43,292 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32987
2024-03-20 07:00:43,292 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,292 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,292 - distributed.worker - INFO -               Threads:                          4
2024-03-20 07:00:43,292 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-20 07:00:43,292 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6ksec73q
2024-03-20 07:00:43,292 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34521
2024-03-20 07:00:43,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35522
2024-03-20 07:00:43,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d060bd7-ce18-45f0-a2f6-658c4f0634d6
2024-03-20 07:00:43,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:43,293 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:43,294 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:43,294 - distributed.worker - INFO - Starting Worker plugin PreImport-566d82d9-0b75-4115-860c-caecf5ac6346
2024-03-20 07:00:43,294 - distributed.worker - INFO - Starting Worker plugin RMMSetup-468ddfcb-13b7-4a45-9b6c-8403f6c235c6
2024-03-20 07:00:43,295 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-20 07:00:43,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:43,299 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44023
2024-03-20 07:00:43,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44023
2024-03-20 07:00:43,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45167
2024-03-20 07:00:43,299 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,299 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,299 - distributed.worker - INFO -               Threads:                          4
2024-03-20 07:00:43,299 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-20 07:00:43,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-oquz6f_p
2024-03-20 07:00:43,299 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e09d829d-33db-449b-91b4-5b6f41c58303
2024-03-20 07:00:43,300 - distributed.worker - INFO - Starting Worker plugin PreImport-4df60d95-9e93-4f3f-a5c7-06a497d1f176
2024-03-20 07:00:43,300 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4aa756ec-c8ca-4c72-bee9-1e9c4dac9238
2024-03-20 07:00:43,300 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:43,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:43,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:43,311 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41955
2024-03-20 07:00:43,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41955
2024-03-20 07:00:43,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45039
2024-03-20 07:00:43,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,312 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,312 - distributed.worker - INFO -               Threads:                          4
2024-03-20 07:00:43,312 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-20 07:00:43,312 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-b7x5z5js
2024-03-20 07:00:43,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-98cc5095-7cfe-41a9-86ce-759636a35ecd
2024-03-20 07:00:43,312 - distributed.worker - INFO - Starting Worker plugin PreImport-ce3da4f1-9f42-4daf-aea1-f387929d39f4
2024-03-20 07:00:43,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c28e4b4-8a3b-4be8-a2d1-8a9132fca239
2024-03-20 07:00:43,313 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,398 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42881', status: init, memory: 0, processing: 0>
2024-03-20 07:00:43,398 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42881
2024-03-20 07:00:43,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35526
2024-03-20 07:00:43,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:43,400 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,400 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-20 07:00:43,409 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44023', status: init, memory: 0, processing: 0>
2024-03-20 07:00:43,409 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44023
2024-03-20 07:00:43,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35538
2024-03-20 07:00:43,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:43,411 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,411 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-20 07:00:43,421 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41955', status: init, memory: 0, processing: 0>
2024-03-20 07:00:43,421 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41955
2024-03-20 07:00:43,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35546
2024-03-20 07:00:43,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:43,423 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-20 07:00:43,423 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:43,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-20 07:00:43,930 - distributed.scheduler - INFO - Receive client connection: Client-8f445db6-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:43,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35554
2024-03-20 07:00:43,940 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-20 07:00:43,940 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-20 07:00:43,940 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-20 07:00:43,940 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-20 07:00:43,945 - distributed.scheduler - INFO - Remove client Client-8f445db6-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:43,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35554; closing.
2024-03-20 07:00:43,946 - distributed.scheduler - INFO - Remove client Client-8f445db6-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:43,946 - distributed.scheduler - INFO - Close client connection: Client-8f445db6-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:43,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38493'. Reason: nanny-close
2024-03-20 07:00:43,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:43,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40493'. Reason: nanny-close
2024-03-20 07:00:43,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:43,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36189'. Reason: nanny-close
2024-03-20 07:00:43,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:43,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35825'. Reason: nanny-close
2024-03-20 07:00:43,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34521. Reason: nanny-close
2024-03-20 07:00:43,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41955. Reason: nanny-close
2024-03-20 07:00:43,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:43,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42881. Reason: nanny-close
2024-03-20 07:00:43,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44023. Reason: nanny-close
2024-03-20 07:00:43,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35546; closing.
2024-03-20 07:00:43,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-20 07:00:43,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41955', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918043.9958062')
2024-03-20 07:00:43,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-20 07:00:43,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-20 07:00:43,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-20 07:00:43,997 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:43,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35538; closing.
2024-03-20 07:00:43,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35526; closing.
2024-03-20 07:00:43,997 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:43,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918043.9977946')
2024-03-20 07:00:43,997 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:43,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918043.9981523')
2024-03-20 07:00:43,998 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35522; closing.
2024-03-20 07:00:43,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34521', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918043.998778')
2024-03-20 07:00:43,998 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:00:43,999 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:44,662 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:00:44,663 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:00:44,663 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:00:44,664 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-20 07:00:44,665 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-20 07:00:47,011 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:00:47,016 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40219 instead
  warnings.warn(
2024-03-20 07:00:47,020 - distributed.scheduler - INFO - State start
2024-03-20 07:00:47,049 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:00:47,050 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:00:47,050 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40219/status
2024-03-20 07:00:47,050 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:00:47,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35565'
2024-03-20 07:00:47,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42227'
2024-03-20 07:00:47,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46291'
2024-03-20 07:00:47,266 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44373'
2024-03-20 07:00:47,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32891'
2024-03-20 07:00:47,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46109'
2024-03-20 07:00:47,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39609'
2024-03-20 07:00:47,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43343'
2024-03-20 07:00:49,024 - distributed.scheduler - INFO - Receive client connection: Client-92b346c7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:49,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35176
2024-03-20 07:00:49,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,374 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40967
2024-03-20 07:00:49,374 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40967
2024-03-20 07:00:49,374 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33565
2024-03-20 07:00:49,374 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,374 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,374 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y37mqbbe
2024-03-20 07:00:49,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f6c4b67-c655-48f1-ba5c-c82f1ad296fa
2024-03-20 07:00:49,375 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,377 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40879
2024-03-20 07:00:49,377 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40879
2024-03-20 07:00:49,377 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45323
2024-03-20 07:00:49,377 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,377 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,377 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,377 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tadbehiq
2024-03-20 07:00:49,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-700be572-38bb-40bf-a958-4b19aa709363
2024-03-20 07:00:49,378 - distributed.worker - INFO - Starting Worker plugin PreImport-7485cab9-a3c5-42ac-ba33-06ffade211e4
2024-03-20 07:00:49,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40395
2024-03-20 07:00:49,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40395
2024-03-20 07:00:49,378 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb7322ed-dcca-485b-8510-20880073d680
2024-03-20 07:00:49,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46711
2024-03-20 07:00:49,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,378 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,378 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,378 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38903
2024-03-20 07:00:49,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38903
2024-03-20 07:00:49,378 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ai6oxsp
2024-03-20 07:00:49,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38855
2024-03-20 07:00:49,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,378 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,379 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-it8ra7t1
2024-03-20 07:00:49,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2ba5fd5-86e1-4468-9ed0-baf01672de2c
2024-03-20 07:00:49,379 - distributed.worker - INFO - Starting Worker plugin PreImport-114a89a3-1ed7-4362-96a1-a616ac56e9b4
2024-03-20 07:00:49,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bea157c3-5eb4-4c47-a3e1-9b6fa4fff4a8
2024-03-20 07:00:49,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8e5748f-7d4f-461c-a56c-6d340b6d2242
2024-03-20 07:00:49,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45449
2024-03-20 07:00:49,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45449
2024-03-20 07:00:49,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46071
2024-03-20 07:00:49,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,388 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,388 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,388 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nv7dxzsn
2024-03-20 07:00:49,389 - distributed.worker - INFO - Starting Worker plugin PreImport-5b0de584-bb39-4d5f-9d9f-740c15210f5e
2024-03-20 07:00:49,389 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a47f5b6f-f596-4e42-ad50-5fcdedaf9554
2024-03-20 07:00:49,389 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fea1f848-b5c7-4f8e-8aa9-17d43f24a600
2024-03-20 07:00:49,478 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,478 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,484 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,485 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39063
2024-03-20 07:00:49,485 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39063
2024-03-20 07:00:49,485 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46067
2024-03-20 07:00:49,485 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,485 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,485 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,485 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,485 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xyd3ufva
2024-03-20 07:00:49,485 - distributed.worker - INFO - Starting Worker plugin RMMSetup-beef9066-7122-4ee0-b306-d0822ce6b5e0
2024-03-20 07:00:49,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,491 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39815
2024-03-20 07:00:49,491 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39815
2024-03-20 07:00:49,491 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36333
2024-03-20 07:00:49,491 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,491 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,491 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,492 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,492 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vvvjm11x
2024-03-20 07:00:49,492 - distributed.worker - INFO - Starting Worker plugin PreImport-7aead699-889f-45cd-989d-1d897991388f
2024-03-20 07:00:49,492 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f885828-13e9-45b9-bc2f-c282029abb6b
2024-03-20 07:00:49,492 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4d737bc-e02c-42de-b3a8-750956723d94
2024-03-20 07:00:49,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:49,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:49,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:49,511 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33521
2024-03-20 07:00:49,511 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33521
2024-03-20 07:00:49,511 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45127
2024-03-20 07:00:49,511 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:49,511 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:49,511 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:49,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:49,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b1y8082d
2024-03-20 07:00:49,512 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca2aeb2d-391f-4f9e-9fbf-35305026ae98
2024-03-20 07:00:51,454 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40879', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,477 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40879
2024-03-20 07:00:51,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45774
2024-03-20 07:00:51,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,478 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,478 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,479 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b255c13-cb66-4c89-a00c-02ab6806a1cc
2024-03-20 07:00:51,480 - distributed.worker - INFO - Starting Worker plugin PreImport-9ce77f67-426e-461d-8617-a8a348418430
2024-03-20 07:00:51,481 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89a381b4-7445-4e5b-8e1f-38ae44907695
2024-03-20 07:00:51,506 - distributed.worker - INFO - Starting Worker plugin PreImport-2e1ec900-315d-427a-bba3-b822603b2187
2024-03-20 07:00:51,506 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,516 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40395', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40395
2024-03-20 07:00:51,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45790
2024-03-20 07:00:51,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,521 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,521 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,524 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,532 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40967', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40967
2024-03-20 07:00:51,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45798
2024-03-20 07:00:51,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,539 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38903', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,544 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38903
2024-03-20 07:00:51,544 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45808
2024-03-20 07:00:51,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,546 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,546 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5c32d67-9fc7-407e-8a19-9c7548a96be0
2024-03-20 07:00:51,552 - distributed.worker - INFO - Starting Worker plugin PreImport-f4da4c62-eb07-4dc0-b05d-32256efc6ef9
2024-03-20 07:00:51,553 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,557 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45449', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,557 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45449
2024-03-20 07:00:51,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45814
2024-03-20 07:00:51,558 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,559 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,564 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5c96f1c-35d7-48f4-a57d-fac22254032b
2024-03-20 07:00:51,584 - distributed.worker - INFO - Starting Worker plugin PreImport-700a2d25-692e-4c4d-a85d-e6a9b58909e9
2024-03-20 07:00:51,584 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,585 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39063', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39063
2024-03-20 07:00:51,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45822
2024-03-20 07:00:51,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39815', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,588 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39815
2024-03-20 07:00:51,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45838
2024-03-20 07:00:51,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,589 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,589 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,590 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,590 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,610 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33521', status: init, memory: 0, processing: 0>
2024-03-20 07:00:51,611 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33521
2024-03-20 07:00:51,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45844
2024-03-20 07:00:51,612 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:51,613 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:51,613 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:51,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:00:51,634 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:00:51,641 - distributed.scheduler - INFO - Remove client Client-92b346c7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:51,641 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35176; closing.
2024-03-20 07:00:51,642 - distributed.scheduler - INFO - Remove client Client-92b346c7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:51,643 - distributed.scheduler - INFO - Close client connection: Client-92b346c7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:51,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35565'. Reason: nanny-close
2024-03-20 07:00:51,644 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,644 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42227'. Reason: nanny-close
2024-03-20 07:00:51,645 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46291'. Reason: nanny-close
2024-03-20 07:00:51,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38903. Reason: nanny-close
2024-03-20 07:00:51,645 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44373'. Reason: nanny-close
2024-03-20 07:00:51,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33521. Reason: nanny-close
2024-03-20 07:00:51,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,646 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32891'. Reason: nanny-close
2024-03-20 07:00:51,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40879. Reason: nanny-close
2024-03-20 07:00:51,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,646 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46109'. Reason: nanny-close
2024-03-20 07:00:51,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40967. Reason: nanny-close
2024-03-20 07:00:51,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39609'. Reason: nanny-close
2024-03-20 07:00:51,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,647 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45808; closing.
2024-03-20 07:00:51,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39063. Reason: nanny-close
2024-03-20 07:00:51,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43343'. Reason: nanny-close
2024-03-20 07:00:51,647 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38903', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.6478071')
2024-03-20 07:00:51,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:00:51,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40395. Reason: nanny-close
2024-03-20 07:00:51,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39815. Reason: nanny-close
2024-03-20 07:00:51,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45844; closing.
2024-03-20 07:00:51,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45449. Reason: nanny-close
2024-03-20 07:00:51,649 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,649 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,649 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,649 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33521', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.64963')
2024-03-20 07:00:51,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45774; closing.
2024-03-20 07:00:51,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45798; closing.
2024-03-20 07:00:51,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40879', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.6509728')
2024-03-20 07:00:51,651 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,651 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.6516864')
2024-03-20 07:00:51,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:00:51,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45822; closing.
2024-03-20 07:00:51,652 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,652 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,652 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,652 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39063', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.6528544')
2024-03-20 07:00:51,653 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45838; closing.
2024-03-20 07:00:51,653 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45790; closing.
2024-03-20 07:00:51,653 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45814; closing.
2024-03-20 07:00:51,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.654046')
2024-03-20 07:00:51,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.6543386')
2024-03-20 07:00:51,654 - distributed.nanny - INFO - Worker closed
2024-03-20 07:00:51,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918051.654693')
2024-03-20 07:00:51,654 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:00:52,760 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:00:52,760 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:00:52,761 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:00:52,763 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:00:52,764 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-20 07:00:55,057 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:00:55,062 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37265 instead
  warnings.warn(
2024-03-20 07:00:55,066 - distributed.scheduler - INFO - State start
2024-03-20 07:00:55,089 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:00:55,089 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:00:55,090 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37265/status
2024-03-20 07:00:55,090 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:00:55,139 - distributed.scheduler - INFO - Receive client connection: Client-9779002d-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:00:55,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45932
2024-03-20 07:00:55,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41883'
2024-03-20 07:00:55,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42535'
2024-03-20 07:00:55,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45961'
2024-03-20 07:00:55,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33087'
2024-03-20 07:00:55,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40299'
2024-03-20 07:00:55,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33791'
2024-03-20 07:00:55,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33677'
2024-03-20 07:00:55,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45119'
2024-03-20 07:00:57,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,515 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35625
2024-03-20 07:00:57,517 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35625
2024-03-20 07:00:57,517 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41291
2024-03-20 07:00:57,517 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,517 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,517 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,517 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,517 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y7s_hgfi
2024-03-20 07:00:57,517 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96d2a9eb-6ce9-4bab-9fd8-16de4d131d3e
2024-03-20 07:00:57,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,531 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35269
2024-03-20 07:00:57,531 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35269
2024-03-20 07:00:57,531 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46515
2024-03-20 07:00:57,531 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,531 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,532 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,532 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,532 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_4_t8md0
2024-03-20 07:00:57,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cabf493a-aeca-4231-a19c-9b76ad60e8ad
2024-03-20 07:00:57,532 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35655
2024-03-20 07:00:57,532 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35655
2024-03-20 07:00:57,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36127
2024-03-20 07:00:57,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,533 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,533 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,533 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,533 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3nz8rw_1
2024-03-20 07:00:57,533 - distributed.worker - INFO - Starting Worker plugin RMMSetup-93fe3b66-3bee-4deb-9399-4d86743511e1
2024-03-20 07:00:57,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33591
2024-03-20 07:00:57,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33591
2024-03-20 07:00:57,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43387
2024-03-20 07:00:57,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,536 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,536 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6vyslmi1
2024-03-20 07:00:57,536 - distributed.worker - INFO - Starting Worker plugin PreImport-7d174a14-dd9e-4ee9-91d1-fb5090083e47
2024-03-20 07:00:57,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00ca9a6c-0c75-4716-8570-973e96924f4a
2024-03-20 07:00:57,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da68da27-bc1a-48ff-a354-03e7e6d2cb7d
2024-03-20 07:00:57,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,590 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,590 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,590 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,591 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39329
2024-03-20 07:00:57,591 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39329
2024-03-20 07:00:57,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36851
2024-03-20 07:00:57,592 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,592 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,592 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,592 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_dpyprq6
2024-03-20 07:00:57,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daaa737e-3e5e-48e9-bf84-59bae4f6ae98
2024-03-20 07:00:57,594 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:00:57,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:00:57,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,598 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43703
2024-03-20 07:00:57,598 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43703
2024-03-20 07:00:57,598 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41385
2024-03-20 07:00:57,598 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,598 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,598 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,598 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-735aaf68
2024-03-20 07:00:57,598 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38827
2024-03-20 07:00:57,598 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38827
2024-03-20 07:00:57,598 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45561
2024-03-20 07:00:57,599 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-235920ae-ebd2-4634-8492-ff2eaa72e3d9
2024-03-20 07:00:57,599 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,599 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,599 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4cxz4qgf
2024-03-20 07:00:57,599 - distributed.worker - INFO - Starting Worker plugin PreImport-84b254b6-9d25-4fef-a2e2-a1a976fe3679
2024-03-20 07:00:57,599 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62427785-175e-49d7-abf7-22a8f0e44a6f
2024-03-20 07:00:57,599 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a58bd47c-c3ab-4d2d-9816-4dcc900cf396
2024-03-20 07:00:57,602 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:00:57,604 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40017
2024-03-20 07:00:57,604 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40017
2024-03-20 07:00:57,604 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32815
2024-03-20 07:00:57,604 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:00:57,604 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:57,604 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:00:57,604 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:00:57,605 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-orn5bk79
2024-03-20 07:00:57,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ff664cd-7f4d-4ca6-9e87-62098269ce6c
2024-03-20 07:00:59,954 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:59,974 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33591', status: init, memory: 0, processing: 0>
2024-03-20 07:00:59,976 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33591
2024-03-20 07:00:59,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46728
2024-03-20 07:00:59,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:00:59,978 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:00:59,978 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:00:59,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,092 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c39aacfe-e228-443d-afa5-8d7ce4e2d983
2024-03-20 07:01:00,093 - distributed.worker - INFO - Starting Worker plugin PreImport-cf434512-3105-4c01-8ade-7645692a668c
2024-03-20 07:01:00,093 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,115 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35269', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,115 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35269
2024-03-20 07:01:00,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46730
2024-03-20 07:01:00,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,117 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,117 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,141 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c3d0611-0094-4915-9d39-19357c0892fd
2024-03-20 07:01:00,151 - distributed.worker - INFO - Starting Worker plugin PreImport-8803b2d5-9740-4590-b081-83ed4083401e
2024-03-20 07:01:00,151 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,161 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61e2b3f1-616a-4f92-887e-6fa9be5a544f
2024-03-20 07:01:00,162 - distributed.worker - INFO - Starting Worker plugin PreImport-e29469eb-1cd4-46e8-bb58-2a2ed0397c4c
2024-03-20 07:01:00,162 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6fa2c9a-79ab-4e49-8e5a-5d66522d2a1b
2024-03-20 07:01:00,165 - distributed.worker - INFO - Starting Worker plugin PreImport-76ec5661-3736-4e89-8c32-64595699d7ae
2024-03-20 07:01:00,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2aa2f78-1024-458f-933e-b1413141574d
2024-03-20 07:01:00,165 - distributed.worker - INFO - Starting Worker plugin PreImport-beebe652-aeec-4432-9369-5b77f193792b
2024-03-20 07:01:00,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2964f5e-83b1-481b-b4ec-946e3bb0ebf7
2024-03-20 07:01:00,167 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,167 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,170 - distributed.worker - INFO - Starting Worker plugin PreImport-b17e4be1-cf84-4fc4-8966-e915cdddc102
2024-03-20 07:01:00,172 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,173 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43703', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,173 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43703
2024-03-20 07:01:00,173 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46760
2024-03-20 07:01:00,174 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,175 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,175 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,178 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38827', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38827
2024-03-20 07:01:00,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46746
2024-03-20 07:01:00,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,182 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,182 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,183 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40017', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,183 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40017
2024-03-20 07:01:00,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46770
2024-03-20 07:01:00,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,185 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,185 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,200 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39329', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,201 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39329
2024-03-20 07:01:00,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46782
2024-03-20 07:01:00,202 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35625', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,203 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35625
2024-03-20 07:01:00,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46778
2024-03-20 07:01:00,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,204 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,204 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,204 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,205 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,205 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35655', status: init, memory: 0, processing: 0>
2024-03-20 07:01:00,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35655
2024-03-20 07:01:00,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46788
2024-03-20 07:01:00,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:00,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:00,210 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:00,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:00,283 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,283 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,285 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:00,289 - distributed.scheduler - INFO - Remove client Client-9779002d-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:00,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45932; closing.
2024-03-20 07:01:00,289 - distributed.scheduler - INFO - Remove client Client-9779002d-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:00,289 - distributed.scheduler - INFO - Close client connection: Client-9779002d-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:00,290 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41883'. Reason: nanny-close
2024-03-20 07:01:00,291 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,291 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42535'. Reason: nanny-close
2024-03-20 07:01:00,292 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45961'. Reason: nanny-close
2024-03-20 07:01:00,292 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35625. Reason: nanny-close
2024-03-20 07:01:00,292 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33087'. Reason: nanny-close
2024-03-20 07:01:00,293 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,293 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35655. Reason: nanny-close
2024-03-20 07:01:00,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40299'. Reason: nanny-close
2024-03-20 07:01:00,293 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33591. Reason: nanny-close
2024-03-20 07:01:00,293 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,294 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35269. Reason: nanny-close
2024-03-20 07:01:00,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33791'. Reason: nanny-close
2024-03-20 07:01:00,294 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33677'. Reason: nanny-close
2024-03-20 07:01:00,294 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39329. Reason: nanny-close
2024-03-20 07:01:00,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45119'. Reason: nanny-close
2024-03-20 07:01:00,295 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,295 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,295 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38827. Reason: nanny-close
2024-03-20 07:01:00,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:00,295 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,295 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43703. Reason: nanny-close
2024-03-20 07:01:00,295 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46778; closing.
2024-03-20 07:01:00,296 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46728; closing.
2024-03-20 07:01:00,296 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,296 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46730; closing.
2024-03-20 07:01:00,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40017. Reason: nanny-close
2024-03-20 07:01:00,296 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,296 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.2967439')
2024-03-20 07:01:00,297 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,297 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33591', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.2974136')
2024-03-20 07:01:00,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,297 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46788; closing.
2024-03-20 07:01:00,298 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,298 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:00,298 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.298308')
2024-03-20 07:01:00,299 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,299 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.2992666')
2024-03-20 07:01:00,299 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,299 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,299 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:00,301 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46782; closing.
2024-03-20 07:01:00,301 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46788>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-20 07:01:00,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39329', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.3045366')
2024-03-20 07:01:00,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46746; closing.
2024-03-20 07:01:00,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46760; closing.
2024-03-20 07:01:00,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46770; closing.
2024-03-20 07:01:00,306 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.3061478')
2024-03-20 07:01:00,306 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.3067906')
2024-03-20 07:01:00,307 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40017', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918060.3073223')
2024-03-20 07:01:00,307 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:01:01,106 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:01:01,107 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:01:01,107 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:01:01,108 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:01:01,109 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-20 07:01:03,541 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:03,546 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33259 instead
  warnings.warn(
2024-03-20 07:01:03,550 - distributed.scheduler - INFO - State start
2024-03-20 07:01:03,571 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:03,572 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:01:03,573 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33259/status
2024-03-20 07:01:03,573 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:01:03,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40235'
2024-03-20 07:01:03,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33429'
2024-03-20 07:01:03,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35227'
2024-03-20 07:01:03,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35527'
2024-03-20 07:01:03,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32805'
2024-03-20 07:01:03,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45597'
2024-03-20 07:01:03,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37897'
2024-03-20 07:01:03,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46063'
2024-03-20 07:01:04,829 - distributed.scheduler - INFO - Receive client connection: Client-9c7f0708-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:04,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47134
2024-03-20 07:01:05,565 - distributed.scheduler - INFO - Receive client connection: Client-9cf43d6d-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:01:05,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47152
2024-03-20 07:01:05,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,667 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,671 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43549
2024-03-20 07:01:05,671 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43549
2024-03-20 07:01:05,671 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45393
2024-03-20 07:01:05,671 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,671 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,671 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,671 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,671 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,671 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fhufgrue
2024-03-20 07:01:05,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a605628-0019-473a-8e16-a818b0937d34
2024-03-20 07:01:05,672 - distributed.worker - INFO - Starting Worker plugin PreImport-7af8bfbe-b62d-47f9-970a-1ac61f367104
2024-03-20 07:01:05,672 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40201
2024-03-20 07:01:05,672 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40201
2024-03-20 07:01:05,672 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35677
2024-03-20 07:01:05,672 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53204a14-b777-4eac-a561-52c4959e1cca
2024-03-20 07:01:05,672 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,672 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,672 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,672 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,672 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wmct30ah
2024-03-20 07:01:05,672 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3771f0b0-a341-42b5-a9a7-f5e5c625fb53
2024-03-20 07:01:05,675 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9197af4a-1313-4c05-bb38-c4659acfe9ca
2024-03-20 07:01:05,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,691 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42541
2024-03-20 07:01:05,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42541
2024-03-20 07:01:05,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34947
2024-03-20 07:01:05,692 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,692 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,693 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,693 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-geixf9p_
2024-03-20 07:01:05,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b2d4bc5-9074-4ebb-b7bf-dee67671b6c3
2024-03-20 07:01:05,707 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,708 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,710 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,712 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,713 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35621
2024-03-20 07:01:05,713 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35621
2024-03-20 07:01:05,713 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35319
2024-03-20 07:01:05,713 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,713 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,713 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,713 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,713 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rkfd3r8u
2024-03-20 07:01:05,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d176952e-16ff-4ffc-b953-db2c27d5ac55
2024-03-20 07:01:05,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,715 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,715 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43525
2024-03-20 07:01:05,715 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43525
2024-03-20 07:01:05,715 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44791
2024-03-20 07:01:05,715 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,715 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,715 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,715 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,715 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f9ba7odv
2024-03-20 07:01:05,715 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,715 - distributed.worker - INFO - Starting Worker plugin PreImport-8f9ff8a5-583d-4c85-8e82-296f8d6b2797
2024-03-20 07:01:05,715 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1dda8d5-abf7-4eee-bff1-d9333887b53b
2024-03-20 07:01:05,715 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34185
2024-03-20 07:01:05,716 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1a169bf-d029-495b-b8bd-dc56f3ad0026
2024-03-20 07:01:05,716 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34185
2024-03-20 07:01:05,716 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41687
2024-03-20 07:01:05,716 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,716 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,716 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,716 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,716 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gbhb3byr
2024-03-20 07:01:05,716 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38123
2024-03-20 07:01:05,716 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f4204a8-2c91-4478-b371-f6443d8967c0
2024-03-20 07:01:05,716 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38123
2024-03-20 07:01:05,716 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45805
2024-03-20 07:01:05,716 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,716 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,716 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,716 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,716 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fawql4ld
2024-03-20 07:01:05,717 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-485bb515-7058-470a-8b32-8fc7b9f6d216
2024-03-20 07:01:05,718 - distributed.worker - INFO - Starting Worker plugin PreImport-52971262-4487-4e34-a8da-c3fc253a1ed8
2024-03-20 07:01:05,718 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aff435b2-6149-42cc-a580-324bce2e6d55
2024-03-20 07:01:05,772 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:05,772 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:05,779 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:05,780 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44799
2024-03-20 07:01:05,780 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44799
2024-03-20 07:01:05,780 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40381
2024-03-20 07:01:05,780 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:05,781 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:05,781 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:05,781 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:05,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b1p8cnvz
2024-03-20 07:01:05,781 - distributed.worker - INFO - Starting Worker plugin PreImport-5dfb0107-c240-478e-a892-e0ba7c351918
2024-03-20 07:01:05,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c39e092c-8187-41b0-8c62-977c3d7ab84b
2024-03-20 07:01:05,781 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe3af0e3-45e8-4455-9165-1bdc800df447
2024-03-20 07:01:08,045 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3282f73-7202-4f49-beb6-7a5d20dd5cc5
2024-03-20 07:01:08,045 - distributed.worker - INFO - Starting Worker plugin PreImport-b5342023-e4cf-4f1a-be40-05a0f36028bd
2024-03-20 07:01:08,046 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,049 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,050 - distributed.worker - INFO - Starting Worker plugin PreImport-eed45248-202e-4dfd-bd1f-03dd4921d9f8
2024-03-20 07:01:08,052 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42541', status: init, memory: 0, processing: 0>
2024-03-20 07:01:08,078 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42541
2024-03-20 07:01:08,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47174
2024-03-20 07:01:08,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:08,080 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:08,080 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:08,085 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43549', status: init, memory: 0, processing: 0>
2024-03-20 07:01:08,086 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43549
2024-03-20 07:01:08,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47180
2024-03-20 07:01:08,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40201', status: init, memory: 0, processing: 0>
2024-03-20 07:01:08,088 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:08,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40201
2024-03-20 07:01:08,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47188
2024-03-20 07:01:08,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:08,089 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:08,090 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:08,090 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:08,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:08,342 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,365 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43525', status: init, memory: 0, processing: 0>
2024-03-20 07:01:08,366 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43525
2024-03-20 07:01:08,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47192
2024-03-20 07:01:08,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:08,368 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:08,368 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:08,369 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:10,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6790541-97db-4912-9a9a-4da03e8eace2
2024-03-20 07:01:10,014 - distributed.worker - INFO - Starting Worker plugin PreImport-2dd5eb7b-1cb0-430d-970e-507350ee73e3
2024-03-20 07:01:10,014 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,039 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35621', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35621
2024-03-20 07:01:10,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46286
2024-03-20 07:01:10,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:10,041 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:10,041 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:10,042 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,081 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38123', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,082 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38123
2024-03-20 07:01:10,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46288
2024-03-20 07:01:10,083 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:10,084 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:10,085 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:10,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af1d93ae-b5aa-4b3d-8165-651291f001c8
2024-03-20 07:01:10,102 - distributed.worker - INFO - Starting Worker plugin PreImport-25688c0c-e1ea-40db-aa17-960cf2cf553d
2024-03-20 07:01:10,103 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,135 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34185', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,136 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34185
2024-03-20 07:01:10,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46304
2024-03-20 07:01:10,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:10,138 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:10,138 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:10,154 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44799', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,155 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44799
2024-03-20 07:01:10,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46312
2024-03-20 07:01:10,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:10,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:10,157 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:10,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:10,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,185 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,185 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,185 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,185 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:10,194 - distributed.scheduler - INFO - Remove client Client-9cf43d6d-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:01:10,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47152; closing.
2024-03-20 07:01:10,194 - distributed.scheduler - INFO - Remove client Client-9cf43d6d-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:01:10,198 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,198 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,198 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,198 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,198 - distributed.scheduler - INFO - Close client connection: Client-9cf43d6d-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:01:10,198 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,199 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,199 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,199 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:10,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:10,209 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:10,212 - distributed.scheduler - INFO - Remove client Client-9c7f0708-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:10,212 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47134; closing.
2024-03-20 07:01:10,212 - distributed.scheduler - INFO - Remove client Client-9c7f0708-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:10,212 - distributed.scheduler - INFO - Close client connection: Client-9c7f0708-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:10,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40235'. Reason: nanny-close
2024-03-20 07:01:10,214 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33429'. Reason: nanny-close
2024-03-20 07:01:10,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35227'. Reason: nanny-close
2024-03-20 07:01:10,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40201. Reason: nanny-close
2024-03-20 07:01:10,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35527'. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43549. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32805'. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42541. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45597'. Reason: nanny-close
2024-03-20 07:01:10,216 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35621. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37897'. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34185. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46063'. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,217 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47188; closing.
2024-03-20 07:01:10,217 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38123. Reason: nanny-close
2024-03-20 07:01:10,217 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:10,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,218 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40201', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.2182775')
2024-03-20 07:01:10,218 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43525. Reason: nanny-close
2024-03-20 07:01:10,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,219 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44799. Reason: nanny-close
2024-03-20 07:01:10,219 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46286; closing.
2024-03-20 07:01:10,219 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,219 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,219 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47174; closing.
2024-03-20 07:01:10,219 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,220 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,220 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,220 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.2205944')
2024-03-20 07:01:10,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:10,221 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.2209625')
2024-03-20 07:01:10,221 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47180; closing.
2024-03-20 07:01:10,221 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,221 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,222 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,222 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:10,222 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46286>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46286>: Stream is closed
2024-03-20 07:01:10,224 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43549', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.2240486')
2024-03-20 07:01:10,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46304; closing.
2024-03-20 07:01:10,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46288; closing.
2024-03-20 07:01:10,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34185', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.2252665')
2024-03-20 07:01:10,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38123', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.22556')
2024-03-20 07:01:10,225 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47192; closing.
2024-03-20 07:01:10,226 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46312; closing.
2024-03-20 07:01:10,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43525', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.2264519')
2024-03-20 07:01:10,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.226814')
2024-03-20 07:01:10,227 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:01:10,330 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46715', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,330 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46715
2024-03-20 07:01:10,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46328
2024-03-20 07:01:10,364 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46328; closing.
2024-03-20 07:01:10,364 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.3643112')
2024-03-20 07:01:10,364 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:01:10,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34975', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,457 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34975
2024-03-20 07:01:10,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46332
2024-03-20 07:01:10,458 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38351', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38351
2024-03-20 07:01:10,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46336
2024-03-20 07:01:10,473 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39601', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,474 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39601
2024-03-20 07:01:10,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46356
2024-03-20 07:01:10,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42203', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,475 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42203
2024-03-20 07:01:10,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46342
2024-03-20 07:01:10,480 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44611', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,481 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44611
2024-03-20 07:01:10,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46366
2024-03-20 07:01:10,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38485', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38485
2024-03-20 07:01:10,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46370
2024-03-20 07:01:10,500 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37279', status: init, memory: 0, processing: 0>
2024-03-20 07:01:10,501 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37279
2024-03-20 07:01:10,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46382
2024-03-20 07:01:10,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46382; closing.
2024-03-20 07:01:10,517 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37279', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.5173204')
2024-03-20 07:01:10,517 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46332; closing.
2024-03-20 07:01:10,518 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.518345')
2024-03-20 07:01:10,519 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46332>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-20 07:01:10,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46370; closing.
2024-03-20 07:01:10,521 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46336; closing.
2024-03-20 07:01:10,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.5216725')
2024-03-20 07:01:10,522 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38351', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.5220258')
2024-03-20 07:01:10,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46366; closing.
2024-03-20 07:01:10,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46342; closing.
2024-03-20 07:01:10,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.5231419')
2024-03-20 07:01:10,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42203', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.5234368')
2024-03-20 07:01:10,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46356; closing.
2024-03-20 07:01:10,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918070.5241783')
2024-03-20 07:01:10,524 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:01:11,430 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:01:11,430 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:01:11,431 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:01:11,432 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:01:11,433 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-20 07:01:13,651 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:13,655 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36019 instead
  warnings.warn(
2024-03-20 07:01:13,658 - distributed.scheduler - INFO - State start
2024-03-20 07:01:13,680 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:13,681 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-20 07:01:13,681 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:01:13,682 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-20 07:01:13,847 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33139'
2024-03-20 07:01:13,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43775'
2024-03-20 07:01:13,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44495'
2024-03-20 07:01:13,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45195'
2024-03-20 07:01:13,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43305'
2024-03-20 07:01:13,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33881'
2024-03-20 07:01:13,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41839'
2024-03-20 07:01:13,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43143'
2024-03-20 07:01:15,937 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,937 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,942 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,943 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38163
2024-03-20 07:01:15,943 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38163
2024-03-20 07:01:15,943 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32793
2024-03-20 07:01:15,943 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,943 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,943 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,943 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ykeq_xq
2024-03-20 07:01:15,944 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2cd9e1ae-28da-44b4-9fbe-7a4fdc71edc6
2024-03-20 07:01:15,944 - distributed.worker - INFO - Starting Worker plugin PreImport-8c915190-38c9-4d97-9a43-d1409b4dff2a
2024-03-20 07:01:15,944 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa182780-10f8-4b63-968b-d0c95e82cfdd
2024-03-20 07:01:15,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,976 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,977 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46849
2024-03-20 07:01:15,977 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46849
2024-03-20 07:01:15,977 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45707
2024-03-20 07:01:15,977 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,977 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,977 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,977 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,977 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hymofzje
2024-03-20 07:01:15,977 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2aa4acd3-9b0e-4615-bd7b-072e853284fc
2024-03-20 07:01:15,980 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,981 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,981 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36991
2024-03-20 07:01:15,982 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36991
2024-03-20 07:01:15,982 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38351
2024-03-20 07:01:15,982 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,982 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,982 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,982 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,982 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zebknass
2024-03-20 07:01:15,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,982 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0a5f4a2-8da6-4589-8e51-cf9f552e1a03
2024-03-20 07:01:15,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,983 - distributed.worker - INFO - Starting Worker plugin PreImport-f250db32-04d9-486c-bc28-9bdd347e0e19
2024-03-20 07:01:15,983 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91195b5d-c836-4632-ab9a-dc007630914b
2024-03-20 07:01:15,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,987 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,988 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45827
2024-03-20 07:01:15,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45827
2024-03-20 07:01:15,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35601
2024-03-20 07:01:15,988 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,988 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,988 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,988 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b8c71via
2024-03-20 07:01:15,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16e7ac2e-f207-4ea7-9697-3c3901db9757
2024-03-20 07:01:15,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:15,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:15,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,990 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44801
2024-03-20 07:01:15,990 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44801
2024-03-20 07:01:15,990 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40801
2024-03-20 07:01:15,990 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,990 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,990 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,990 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,990 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kdkuzefx
2024-03-20 07:01:15,990 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39495
2024-03-20 07:01:15,991 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39495
2024-03-20 07:01:15,991 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42529
2024-03-20 07:01:15,991 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,991 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,991 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60e04645-ecf4-417c-9d6f-6c01dbe88953
2024-03-20 07:01:15,991 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,991 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2gjyacvk
2024-03-20 07:01:15,991 - distributed.worker - INFO - Starting Worker plugin PreImport-adee15b4-2c98-4a6f-aaf9-372a9967f5c7
2024-03-20 07:01:15,991 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c201654-aac6-4fe5-8968-1344cf6dca9b
2024-03-20 07:01:15,991 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1fad664-ae9e-4167-905b-e1e828768130
2024-03-20 07:01:15,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:15,995 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41701
2024-03-20 07:01:15,995 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41701
2024-03-20 07:01:15,995 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37849
2024-03-20 07:01:15,995 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:15,995 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:15,995 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:15,995 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:15,995 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jtieh424
2024-03-20 07:01:15,995 - distributed.worker - INFO - Starting Worker plugin PreImport-092f8865-6369-46a4-9ba6-edc11c3f241b
2024-03-20 07:01:15,995 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8dc12c2d-6598-4a3a-aa75-480ce4de2412
2024-03-20 07:01:15,995 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1274303a-3f1d-4057-a437-9bde8caa497a
2024-03-20 07:01:16,002 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:16,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40969
2024-03-20 07:01:16,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40969
2024-03-20 07:01:16,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40357
2024-03-20 07:01:16,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:16,006 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:16,006 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:16,006 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:16,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p53r1vnb
2024-03-20 07:01:16,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebd93d90-0fdb-4245-9cc3-d3ee607297cc
2024-03-20 07:01:19,805 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ebbea9f-d08d-4f31-9a97-d294e37827d8
2024-03-20 07:01:19,807 - distributed.worker - INFO - Starting Worker plugin PreImport-79824ea8-afff-426e-bee7-7266b2ee1b0c
2024-03-20 07:01:19,808 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,845 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,845 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,925 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,925 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,927 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,928 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe08207f-084c-4a83-93df-4b1e064e3eaf
2024-03-20 07:01:19,937 - distributed.worker - INFO - Starting Worker plugin PreImport-2d528ede-2dba-4aba-aece-71571ab519eb
2024-03-20 07:01:19,939 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b15cf79e-e616-48a9-9ed0-ed1b22546653
2024-03-20 07:01:19,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca2c1b6d-28be-4a6b-9d81-b5a377b4ba85
2024-03-20 07:01:19,942 - distributed.worker - INFO - Starting Worker plugin PreImport-3dd306c5-286f-4720-b88b-a68c018e916a
2024-03-20 07:01:19,943 - distributed.worker - INFO - Starting Worker plugin PreImport-19dd0225-7cc3-455b-9776-41bc7be4e978
2024-03-20 07:01:19,943 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,943 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,952 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,952 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,954 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,965 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,965 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,971 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,971 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,972 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,973 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,973 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,977 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:19,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:19,980 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:19,981 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:19,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:29,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,959 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,959 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,959 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,959 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:29,972 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,972 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,972 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,972 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,972 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,972 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:29,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:01:30,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33139'. Reason: nanny-close
2024-03-20 07:01:30,000 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43775'. Reason: nanny-close
2024-03-20 07:01:30,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44495'. Reason: nanny-close
2024-03-20 07:01:30,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45195'. Reason: nanny-close
2024-03-20 07:01:30,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38163. Reason: nanny-close
2024-03-20 07:01:30,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43305'. Reason: nanny-close
2024-03-20 07:01:30,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36991. Reason: nanny-close
2024-03-20 07:01:30,003 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46849. Reason: nanny-close
2024-03-20 07:01:30,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33881'. Reason: nanny-close
2024-03-20 07:01:30,003 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41839'. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40969. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43143'. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45827. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44801. Reason: nanny-close
2024-03-20 07:01:30,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39495. Reason: nanny-close
2024-03-20 07:01:30,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41701. Reason: nanny-close
2024-03-20 07:01:30,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,006 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,007 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:30,008 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,008 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,008 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,009 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,009 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:30,009 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-20 07:01:33,236 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:33,240 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33697 instead
  warnings.warn(
2024-03-20 07:01:33,244 - distributed.scheduler - INFO - State start
2024-03-20 07:01:33,267 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:33,268 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-20 07:01:33,269 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:01:33,270 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-20 07:01:33,483 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46787'
2024-03-20 07:01:33,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44213'
2024-03-20 07:01:33,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46473'
2024-03-20 07:01:33,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45853'
2024-03-20 07:01:33,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38317'
2024-03-20 07:01:33,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38895'
2024-03-20 07:01:33,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40085'
2024-03-20 07:01:33,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42793'
2024-03-20 07:01:35,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,434 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46419
2024-03-20 07:01:35,434 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46419
2024-03-20 07:01:35,434 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33547
2024-03-20 07:01:35,434 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,434 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,434 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,434 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,434 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ydziutml
2024-03-20 07:01:35,434 - distributed.worker - INFO - Starting Worker plugin RMMSetup-881a889b-f7a9-4136-9081-87a5c55a280d
2024-03-20 07:01:35,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,449 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,453 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,454 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38507
2024-03-20 07:01:35,454 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38507
2024-03-20 07:01:35,454 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36775
2024-03-20 07:01:35,454 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,454 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,454 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,454 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a9p8pg8m
2024-03-20 07:01:35,454 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91889f72-3e19-48f9-90d6-b57cabdaccc8
2024-03-20 07:01:35,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41643
2024-03-20 07:01:35,456 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41643
2024-03-20 07:01:35,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45081
2024-03-20 07:01:35,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,457 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,457 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,457 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1r72mlzh
2024-03-20 07:01:35,457 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a5bad73-00bf-4dfb-94e1-99ca54ef307b
2024-03-20 07:01:35,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,671 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35645
2024-03-20 07:01:35,671 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35645
2024-03-20 07:01:35,671 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41179
2024-03-20 07:01:35,671 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,671 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,671 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,671 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,671 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lotpu1nt
2024-03-20 07:01:35,672 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47e83b4b-ea3e-4b0c-a76d-af0d0e78877a
2024-03-20 07:01:35,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,858 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,858 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,863 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35215
2024-03-20 07:01:35,863 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35215
2024-03-20 07:01:35,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45503
2024-03-20 07:01:35,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,864 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,864 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-laaohhcc
2024-03-20 07:01:35,864 - distributed.worker - INFO - Starting Worker plugin PreImport-575f878d-3563-43da-b543-831118309a3a
2024-03-20 07:01:35,864 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebdf5856-5fcd-4d63-a8e6-ca5f85e88666
2024-03-20 07:01:35,864 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,865 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58c47d48-0c32-4797-aae2-82a9e48d28d8
2024-03-20 07:01:35,865 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42489
2024-03-20 07:01:35,866 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42489
2024-03-20 07:01:35,866 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45851
2024-03-20 07:01:35,866 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,866 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,866 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,866 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_sk0t289
2024-03-20 07:01:35,866 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb086c4c-1c09-4825-b563-f0d5ec25af75
2024-03-20 07:01:35,866 - distributed.worker - INFO - Starting Worker plugin PreImport-67398a9a-07ae-4ae2-bde3-4a05d42b9c23
2024-03-20 07:01:35,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-121f6d27-7db8-491a-b0f8-89c6a25981e6
2024-03-20 07:01:35,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,877 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:35,878 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33231
2024-03-20 07:01:35,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:35,878 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33231
2024-03-20 07:01:35,878 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45229
2024-03-20 07:01:35,878 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,878 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,878 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,878 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,878 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3xpwfxou
2024-03-20 07:01:35,879 - distributed.worker - INFO - Starting Worker plugin PreImport-1249a79b-55c9-4162-a0dc-bf0fb9137e47
2024-03-20 07:01:35,879 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da7ea6d2-7a4e-48b7-a884-de81267d3842
2024-03-20 07:01:35,879 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b8c20a0-4703-471e-852d-ec95de4c2c57
2024-03-20 07:01:35,882 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:35,883 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38379
2024-03-20 07:01:35,883 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38379
2024-03-20 07:01:35,883 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37809
2024-03-20 07:01:35,883 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:35,883 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:35,883 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:35,883 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:01:35,883 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-02tiivro
2024-03-20 07:01:35,884 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d2edb37-4225-4f7f-8e52-ba40a05a2946
2024-03-20 07:01:35,884 - distributed.worker - INFO - Starting Worker plugin PreImport-b788c4a6-332a-4bb4-acc9-e3f67ecadffb
2024-03-20 07:01:35,884 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1788e34-c4d9-48db-a78a-73af4376cd0e
2024-03-20 07:01:39,779 - distributed.worker - INFO - Starting Worker plugin PreImport-92542602-fb32-424a-ac30-389674212571
2024-03-20 07:01:39,780 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81fd1ef8-2b6a-4192-afb7-8e523d8aa568
2024-03-20 07:01:39,781 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:39,817 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:39,817 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:39,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85b13050-17cb-430c-a4a6-ebfa47171215
2024-03-20 07:01:39,919 - distributed.worker - INFO - Starting Worker plugin PreImport-3d56041f-cd5c-43de-aa26-9b1567b9140a
2024-03-20 07:01:39,920 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:39,956 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:39,956 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:39,970 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,982 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12537749-dfb7-4f61-a6b5-ae17e76dd61d
2024-03-20 07:01:39,982 - distributed.worker - INFO - Starting Worker plugin PreImport-2029f7ea-b47b-4949-ba40-4e184528bccc
2024-03-20 07:01:39,982 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,986 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b26dfb64-b364-4bab-9571-4781b981fd65
2024-03-20 07:01:39,987 - distributed.worker - INFO - Starting Worker plugin PreImport-452c88f0-773b-43d1-bc3e-df95618072f2
2024-03-20 07:01:39,988 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,993 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:39,996 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:40,007 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:40,007 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,008 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:40,010 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:40,011 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:40,011 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:40,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:40,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:40,019 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:40,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:40,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:40,022 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:40,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:40,027 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:40,027 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:40,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:40,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:40,036 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:40,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:49,547 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,547 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,548 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,549 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,549 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,549 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:01:49,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46787'. Reason: nanny-close
2024-03-20 07:01:49,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44213'. Reason: nanny-close
2024-03-20 07:01:49,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,559 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46473'. Reason: nanny-close
2024-03-20 07:01:49,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46419. Reason: nanny-close
2024-03-20 07:01:49,560 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,560 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45853'. Reason: nanny-close
2024-03-20 07:01:49,560 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,560 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41643. Reason: nanny-close
2024-03-20 07:01:49,560 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38317'. Reason: nanny-close
2024-03-20 07:01:49,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33231. Reason: nanny-close
2024-03-20 07:01:49,561 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,561 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38895'. Reason: nanny-close
2024-03-20 07:01:49,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35645. Reason: nanny-close
2024-03-20 07:01:49,561 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40085'. Reason: nanny-close
2024-03-20 07:01:49,562 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,562 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38507. Reason: nanny-close
2024-03-20 07:01:49,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42793'. Reason: nanny-close
2024-03-20 07:01:49,563 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:49,563 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35215. Reason: nanny-close
2024-03-20 07:01:49,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,563 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42489. Reason: nanny-close
2024-03-20 07:01:49,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,564 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38379. Reason: nanny-close
2024-03-20 07:01:49,564 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,564 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,565 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,565 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,565 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,565 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,566 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,566 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:49,567 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,567 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,567 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,568 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:49,568 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-20 07:01:52,936 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:52,941 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41147 instead
  warnings.warn(
2024-03-20 07:01:52,944 - distributed.scheduler - INFO - State start
2024-03-20 07:01:52,967 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:01:52,967 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:01:52,968 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41147/status
2024-03-20 07:01:52,968 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:01:53,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42313'
2024-03-20 07:01:53,788 - distributed.scheduler - INFO - Receive client connection: Client-ba889e7b-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:01:53,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35604
2024-03-20 07:01:54,187 - distributed.scheduler - INFO - Receive client connection: Client-b9e5baa7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:54,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35674
2024-03-20 07:01:54,894 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:01:54,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:01:55,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:01:55,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41385
2024-03-20 07:01:55,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41385
2024-03-20 07:01:55,536 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-20 07:01:55,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:01:55,536 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:55,536 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:01:55,536 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-20 07:01:55,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-in5upy92
2024-03-20 07:01:55,537 - distributed.worker - INFO - Starting Worker plugin RMMSetup-772920a6-8e63-4175-b018-2a2505938a0a
2024-03-20 07:01:55,537 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67805ecc-6518-4b3b-a84e-2cc8f51fd477
2024-03-20 07:01:55,537 - distributed.worker - INFO - Starting Worker plugin PreImport-cbb4e5dd-90b2-4b82-a889-6f5f44ac3b8c
2024-03-20 07:01:55,537 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:55,590 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41385', status: init, memory: 0, processing: 0>
2024-03-20 07:01:55,591 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41385
2024-03-20 07:01:55,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35682
2024-03-20 07:01:55,592 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:01:55,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:01:55,593 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:01:55,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:01:55,642 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:01:55,645 - distributed.scheduler - INFO - Remove client Client-b9e5baa7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:55,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35674; closing.
2024-03-20 07:01:55,645 - distributed.scheduler - INFO - Remove client Client-b9e5baa7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:55,646 - distributed.scheduler - INFO - Close client connection: Client-b9e5baa7-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:01:55,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42313'. Reason: nanny-close
2024-03-20 07:01:55,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:01:55,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41385. Reason: nanny-close
2024-03-20 07:01:55,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35682; closing.
2024-03-20 07:01:55,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:01:55,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918115.6506956')
2024-03-20 07:01:55,650 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:01:55,651 - distributed.nanny - INFO - Worker closed
2024-03-20 07:01:56,412 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:01:56,412 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:01:56,413 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:01:56,415 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:01:56,415 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-20 07:02:00,597 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:00,601 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38331 instead
  warnings.warn(
2024-03-20 07:02:00,604 - distributed.scheduler - INFO - State start
2024-03-20 07:02:00,626 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:00,626 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:02:00,627 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38331/status
2024-03-20 07:02:00,627 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:02:00,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37679'
2024-03-20 07:02:00,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35991', status: init, memory: 0, processing: 0>
2024-03-20 07:02:00,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35991
2024-03-20 07:02:00,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33716
2024-03-20 07:02:00,756 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33716; closing.
2024-03-20 07:02:00,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918120.7566564')
2024-03-20 07:02:00,757 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:00,951 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37447', status: init, memory: 0, processing: 0>
2024-03-20 07:02:00,952 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37447
2024-03-20 07:02:00,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33724
2024-03-20 07:02:00,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33724; closing.
2024-03-20 07:02:00,961 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918120.9612675')
2024-03-20 07:02:00,961 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:00,999 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36767', status: init, memory: 0, processing: 0>
2024-03-20 07:02:01,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36767
2024-03-20 07:02:01,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33728
2024-03-20 07:02:01,013 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33728; closing.
2024-03-20 07:02:01,013 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918121.0135345')
2024-03-20 07:02:01,013 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:01,252 - distributed.scheduler - INFO - Receive client connection: Client-bea6c310-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:01,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33736
2024-03-20 07:02:02,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:02,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:03,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:03,052 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46069
2024-03-20 07:02:03,052 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46069
2024-03-20 07:02:03,052 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46731
2024-03-20 07:02:03,052 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:03,052 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:03,052 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:03,052 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-20 07:02:03,053 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oqu9tj17
2024-03-20 07:02:03,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa4084ac-e47c-4b40-a627-a1d302642626
2024-03-20 07:02:03,053 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f467f2ee-72b0-41fd-ac87-eec5ba104bdf
2024-03-20 07:02:03,053 - distributed.worker - INFO - Starting Worker plugin PreImport-82357fe2-8d20-4540-a51d-6e683273de3b
2024-03-20 07:02:03,057 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:03,167 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46069', status: init, memory: 0, processing: 0>
2024-03-20 07:02:03,168 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46069
2024-03-20 07:02:03,168 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33764
2024-03-20 07:02:03,169 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:03,170 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:03,171 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:03,173 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:03,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:03,198 - distributed.scheduler - INFO - Remove client Client-bea6c310-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:03,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33736; closing.
2024-03-20 07:02:03,198 - distributed.scheduler - INFO - Remove client Client-bea6c310-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:03,199 - distributed.scheduler - INFO - Close client connection: Client-bea6c310-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:03,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37679'. Reason: nanny-close
2024-03-20 07:02:03,209 - distributed.scheduler - INFO - Receive client connection: Client-ba889e7b-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:03,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33772
2024-03-20 07:02:03,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:03,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46069. Reason: nanny-close
2024-03-20 07:02:03,223 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:03,223 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33764; closing.
2024-03-20 07:02:03,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46069', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918123.2234323')
2024-03-20 07:02:03,223 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:03,224 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:04,015 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:02:04,016 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:02:04,016 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:02:04,018 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:02:04,018 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-20 07:02:06,262 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:06,267 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33745 instead
  warnings.warn(
2024-03-20 07:02:06,270 - distributed.scheduler - INFO - State start
2024-03-20 07:02:06,290 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:06,291 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:02:06,291 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33745/status
2024-03-20 07:02:06,291 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:02:06,349 - distributed.scheduler - INFO - Receive client connection: Client-ba889e7b-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:06,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33988
2024-03-20 07:02:08,991 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:34002'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34002>: Stream is closed
2024-03-20 07:02:09,310 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:02:09,311 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:02:09,311 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:02:09,313 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:02:09,313 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-20 07:02:11,616 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:11,620 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38145 instead
  warnings.warn(
2024-03-20 07:02:11,624 - distributed.scheduler - INFO - State start
2024-03-20 07:02:11,645 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:11,645 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-20 07:02:11,646 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38145/status
2024-03-20 07:02:11,646 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:02:11,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37369'
2024-03-20 07:02:12,791 - distributed.scheduler - INFO - Receive client connection: Client-c52c0366-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:12,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60732
2024-03-20 07:02:13,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:13,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:13,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:13,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44103
2024-03-20 07:02:13,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44103
2024-03-20 07:02:13,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44155
2024-03-20 07:02:13,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-20 07:02:13,413 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:13,413 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:13,414 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-20 07:02:13,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-k710hn6l
2024-03-20 07:02:13,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d6af137-9329-43b2-98ed-ae6a36755222
2024-03-20 07:02:13,414 - distributed.worker - INFO - Starting Worker plugin PreImport-483f6ec8-156c-4624-8a24-a325833b034e
2024-03-20 07:02:13,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-213f4466-a0cc-4fdf-8b25-8160d24db760
2024-03-20 07:02:13,414 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:13,467 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44103', status: init, memory: 0, processing: 0>
2024-03-20 07:02:13,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44103
2024-03-20 07:02:13,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60748
2024-03-20 07:02:13,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:13,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-20 07:02:13,470 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:13,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-20 07:02:13,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:13,527 - distributed.scheduler - INFO - Remove client Client-c52c0366-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:13,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60732; closing.
2024-03-20 07:02:13,528 - distributed.scheduler - INFO - Remove client Client-c52c0366-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:13,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37369'. Reason: nanny-close
2024-03-20 07:02:13,529 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:13,529 - distributed.scheduler - INFO - Close client connection: Client-c52c0366-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:13,531 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44103. Reason: nanny-close
2024-03-20 07:02:13,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60748; closing.
2024-03-20 07:02:13,536 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-20 07:02:13,536 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918133.5365279')
2024-03-20 07:02:13,537 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:13,539 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:14,144 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:02:14,144 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:02:14,145 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:02:14,146 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-20 07:02:14,146 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-20 07:02:16,264 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:16,268 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35911 instead
  warnings.warn(
2024-03-20 07:02:16,272 - distributed.scheduler - INFO - State start
2024-03-20 07:02:16,440 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:16,440 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:02:16,441 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35911/status
2024-03-20 07:02:16,441 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:02:16,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37139'
2024-03-20 07:02:16,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32887'
2024-03-20 07:02:16,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34233'
2024-03-20 07:02:16,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36311'
2024-03-20 07:02:16,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38377'
2024-03-20 07:02:16,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35625'
2024-03-20 07:02:16,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34967'
2024-03-20 07:02:16,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40309'
2024-03-20 07:02:17,702 - distributed.scheduler - INFO - Receive client connection: Client-c9e06b9f-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:17,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40050
2024-03-20 07:02:18,279 - distributed.scheduler - INFO - Receive client connection: Client-c8080f4a-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:18,279 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40070
2024-03-20 07:02:18,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,558 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41745
2024-03-20 07:02:18,558 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41745
2024-03-20 07:02:18,559 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39823
2024-03-20 07:02:18,559 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,559 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,559 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,559 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bdwbkyek
2024-03-20 07:02:18,559 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ddbce66-a359-4f0d-bce3-baea466dd2fe
2024-03-20 07:02:18,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,822 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35667
2024-03-20 07:02:18,827 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35667
2024-03-20 07:02:18,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44937
2024-03-20 07:02:18,827 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,827 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,827 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,827 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-407dr9ii
2024-03-20 07:02:18,828 - distributed.worker - INFO - Starting Worker plugin PreImport-4d239caa-8545-441f-8f53-3a0b42969211
2024-03-20 07:02:18,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-faba81a2-aa48-4c50-b6c5-bd582a19652c
2024-03-20 07:02:18,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44e930ff-19cd-4b51-a297-3a7bf02916c8
2024-03-20 07:02:18,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43645
2024-03-20 07:02:18,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43645
2024-03-20 07:02:18,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45583
2024-03-20 07:02:18,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,830 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,830 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,831 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r6huhoel
2024-03-20 07:02:18,831 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4f22db8-fa18-4b6b-b2c7-91bd86ec823b
2024-03-20 07:02:18,831 - distributed.worker - INFO - Starting Worker plugin PreImport-ad707c44-9081-4448-90ad-3c6f6e45ff17
2024-03-20 07:02:18,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-771b82ab-071f-497c-a045-ca78fcd561bd
2024-03-20 07:02:18,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:18,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:18,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,835 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46039
2024-03-20 07:02:18,835 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46039
2024-03-20 07:02:18,835 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39175
2024-03-20 07:02:18,835 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,835 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,835 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,835 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xvkp4_6s
2024-03-20 07:02:18,836 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b8c1dc2-8da2-4b2a-9317-453d9963c44a
2024-03-20 07:02:18,836 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,836 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34265
2024-03-20 07:02:18,837 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34265
2024-03-20 07:02:18,837 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40075
2024-03-20 07:02:18,837 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,837 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,837 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0o2w9lvv
2024-03-20 07:02:18,838 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37727
2024-03-20 07:02:18,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37727
2024-03-20 07:02:18,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35577
2024-03-20 07:02:18,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,838 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,838 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,838 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5d9b306-5324-44dc-b038-8e03ace521eb
2024-03-20 07:02:18,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jtm6gf29
2024-03-20 07:02:18,838 - distributed.worker - INFO - Starting Worker plugin PreImport-d0a83255-4828-48ce-aad8-0d7e834d7136
2024-03-20 07:02:18,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67f896a7-2c01-43a9-8e97-5b77edab499e
2024-03-20 07:02:18,839 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48b5af0f-f5b3-44de-bf68-a4c03dc085d7
2024-03-20 07:02:18,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,843 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42867
2024-03-20 07:02:18,844 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42867
2024-03-20 07:02:18,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35089
2024-03-20 07:02:18,844 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,844 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,844 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,844 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,844 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r__jdohl
2024-03-20 07:02:18,845 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e738dc4-bdae-4f0a-ac5a-4e8c03a3a79a
2024-03-20 07:02:18,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:18,852 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43789
2024-03-20 07:02:18,852 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43789
2024-03-20 07:02:18,852 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41031
2024-03-20 07:02:18,852 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:18,852 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:18,852 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:18,852 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:02:18,852 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n6jr2no1
2024-03-20 07:02:18,853 - distributed.worker - INFO - Starting Worker plugin PreImport-89018cec-da7f-4c53-8857-b42e54d310a0
2024-03-20 07:02:18,853 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9f4f4a3-3ee1-4461-a1fc-f5709d3c7217
2024-03-20 07:02:18,853 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec5cc65e-b3a5-46f2-a389-9eb2a90bb30e
2024-03-20 07:02:19,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-76cff664-f11c-46f0-a910-6858b988cd17
2024-03-20 07:02:19,010 - distributed.worker - INFO - Starting Worker plugin PreImport-1e3f342d-2f8c-40c4-8d3d-ccdf89687a13
2024-03-20 07:02:19,011 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:19,032 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41745', status: init, memory: 0, processing: 0>
2024-03-20 07:02:19,034 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41745
2024-03-20 07:02:19,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40088
2024-03-20 07:02:19,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:19,035 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:19,035 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:19,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,677 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,701 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35667', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,702 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35667
2024-03-20 07:02:20,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46526
2024-03-20 07:02:20,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,704 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,704 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,799 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43645', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,823 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43645
2024-03-20 07:02:20,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46616
2024-03-20 07:02:20,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,825 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,825 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,832 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,832 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88a41393-1480-473b-8812-c9805ee19fba
2024-03-20 07:02:20,833 - distributed.worker - INFO - Starting Worker plugin PreImport-620827d2-8fc5-411f-862e-8bcd51b7392b
2024-03-20 07:02:20,834 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,853 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-793f97a2-5046-44cf-964c-cfdb7953a057
2024-03-20 07:02:20,854 - distributed.worker - INFO - Starting Worker plugin PreImport-1e621893-7ed3-4da2-be1e-7f140805e36f
2024-03-20 07:02:20,854 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43789', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43789
2024-03-20 07:02:20,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46624
2024-03-20 07:02:20,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,860 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,864 - distributed.worker - INFO - Starting Worker plugin PreImport-f37132b4-9994-4ba4-a80d-673f4f1ee20e
2024-03-20 07:02:20,865 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f9cf052-8b34-4658-91ed-a8c8b2e87c15
2024-03-20 07:02:20,866 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46039', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,867 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46039
2024-03-20 07:02:20,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46628
2024-03-20 07:02:20,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,871 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,876 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42867', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42867
2024-03-20 07:02:20,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46640
2024-03-20 07:02:20,878 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,879 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,879 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,897 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37727', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37727
2024-03-20 07:02:20,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46658
2024-03-20 07:02:20,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34265', status: init, memory: 0, processing: 0>
2024-03-20 07:02:20,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34265
2024-03-20 07:02:20,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46654
2024-03-20 07:02:20,900 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,900 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:20,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,902 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:20,903 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:20,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:20,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-20 07:02:20,967 - distributed.scheduler - INFO - Remove client Client-c9e06b9f-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:20,967 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40050; closing.
2024-03-20 07:02:20,968 - distributed.scheduler - INFO - Remove client Client-c9e06b9f-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:20,968 - distributed.scheduler - INFO - Close client connection: Client-c9e06b9f-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:20,982 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,982 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,982 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,982 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,983 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,983 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,983 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,983 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:20,989 - distributed.scheduler - INFO - Remove client Client-c8080f4a-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:20,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40070; closing.
2024-03-20 07:02:20,990 - distributed.scheduler - INFO - Remove client Client-c8080f4a-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:20,991 - distributed.scheduler - INFO - Close client connection: Client-c8080f4a-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:20,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37139'. Reason: nanny-close
2024-03-20 07:02:20,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,992 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32887'. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34233'. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37727. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36311'. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34265. Reason: nanny-close
2024-03-20 07:02:20,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,994 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38377'. Reason: nanny-close
2024-03-20 07:02:20,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42867. Reason: nanny-close
2024-03-20 07:02:20,994 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,994 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35625'. Reason: nanny-close
2024-03-20 07:02:20,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43645. Reason: nanny-close
2024-03-20 07:02:20,994 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34967'. Reason: nanny-close
2024-03-20 07:02:20,995 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46039. Reason: nanny-close
2024-03-20 07:02:20,995 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40309'. Reason: nanny-close
2024-03-20 07:02:20,995 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:20,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,995 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41745. Reason: nanny-close
2024-03-20 07:02:20,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,996 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43789. Reason: nanny-close
2024-03-20 07:02:20,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46640; closing.
2024-03-20 07:02:20,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,996 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35667. Reason: nanny-close
2024-03-20 07:02:20,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46658; closing.
2024-03-20 07:02:20,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918140.9974024')
2024-03-20 07:02:20,997 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,997 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:20,997 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:20,997 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,997 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:20,998 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:20,998 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,998 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46616; closing.
2024-03-20 07:02:20,998 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:20,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918140.9986238')
2024-03-20 07:02:20,999 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:20,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46654; closing.
2024-03-20 07:02:20,999 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:20,999 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:21,000 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:21,000 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918141.0007153')
2024-03-20 07:02:21,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918141.001448')
2024-03-20 07:02:21,003 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46628; closing.
2024-03-20 07:02:21,004 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40088; closing.
2024-03-20 07:02:21,004 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46624; closing.
2024-03-20 07:02:21,005 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46654>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-20 07:02:21,008 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46616>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-20 07:02:21,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918141.0089908')
2024-03-20 07:02:21,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918141.0096815')
2024-03-20 07:02:21,010 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43789', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918141.0103307')
2024-03-20 07:02:21,011 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46526; closing.
2024-03-20 07:02:21,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918141.0117168')
2024-03-20 07:02:21,012 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:21,957 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:02:21,958 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:02:21,958 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:02:21,960 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:02:21,961 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-20 07:02:24,372 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:24,378 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37567 instead
  warnings.warn(
2024-03-20 07:02:24,388 - distributed.scheduler - INFO - State start
2024-03-20 07:02:24,535 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:24,536 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:02:24,538 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37567/status
2024-03-20 07:02:24,538 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:02:24,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44885'
2024-03-20 07:02:24,835 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42173', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,851 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42173
2024-03-20 07:02:24,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46898
2024-03-20 07:02:24,866 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38115', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,866 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38115
2024-03-20 07:02:24,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46906
2024-03-20 07:02:24,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32777', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32777
2024-03-20 07:02:24,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46920
2024-03-20 07:02:24,880 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33131', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,881 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33131
2024-03-20 07:02:24,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46942
2024-03-20 07:02:24,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41553', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,884 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41553
2024-03-20 07:02:24,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46944
2024-03-20 07:02:24,887 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34289', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,888 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34289
2024-03-20 07:02:24,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46932
2024-03-20 07:02:24,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45337', status: init, memory: 0, processing: 0>
2024-03-20 07:02:24,889 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45337
2024-03-20 07:02:24,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46956
2024-03-20 07:02:24,904 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46898; closing.
2024-03-20 07:02:24,905 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.9049592')
2024-03-20 07:02:24,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46920; closing.
2024-03-20 07:02:24,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46956; closing.
2024-03-20 07:02:24,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.9077735')
2024-03-20 07:02:24,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.9081447')
2024-03-20 07:02:24,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46906; closing.
2024-03-20 07:02:24,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.9091122')
2024-03-20 07:02:24,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46932; closing.
2024-03-20 07:02:24,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.9097817')
2024-03-20 07:02:24,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46942; closing.
2024-03-20 07:02:24,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.932156')
2024-03-20 07:02:24,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46944; closing.
2024-03-20 07:02:24,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41553', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918144.9330306')
2024-03-20 07:02:24,933 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:25,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33931', status: init, memory: 0, processing: 0>
2024-03-20 07:02:25,247 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33931
2024-03-20 07:02:25,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46962
2024-03-20 07:02:25,285 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46962; closing.
2024-03-20 07:02:25,285 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33931', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918145.2857106')
2024-03-20 07:02:25,285 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:26,069 - distributed.scheduler - INFO - Receive client connection: Client-cedd4c15-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:26,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46966
2024-03-20 07:02:26,170 - distributed.scheduler - INFO - Receive client connection: Client-cca784ea-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:26,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46972
2024-03-20 07:02:26,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:26,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:26,419 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:26,420 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44235
2024-03-20 07:02:26,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44235
2024-03-20 07:02:26,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40187
2024-03-20 07:02:26,420 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:26,420 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:26,420 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:26,420 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-20 07:02:26,420 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v4yrl1ex
2024-03-20 07:02:26,421 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e08ef739-0324-41a9-b28f-c368bd720091
2024-03-20 07:02:26,720 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20b90585-76e3-4d99-afdd-289ff2be7ee6
2024-03-20 07:02:26,720 - distributed.worker - INFO - Starting Worker plugin PreImport-d6874b46-6443-4f09-a156-b65d67aee3eb
2024-03-20 07:02:26,721 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:26,785 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44235', status: init, memory: 0, processing: 0>
2024-03-20 07:02:26,786 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44235
2024-03-20 07:02:26,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46992
2024-03-20 07:02:26,787 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:26,788 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:26,789 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:26,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:26,840 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:02:26,844 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:26,846 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:26,847 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:26,848 - distributed.scheduler - INFO - Remove client Client-cedd4c15-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:26,848 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46966; closing.
2024-03-20 07:02:26,848 - distributed.scheduler - INFO - Remove client Client-cedd4c15-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:26,849 - distributed.scheduler - INFO - Close client connection: Client-cedd4c15-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:26,850 - distributed.scheduler - INFO - Remove client Client-cca784ea-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:26,850 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46972; closing.
2024-03-20 07:02:26,850 - distributed.scheduler - INFO - Remove client Client-cca784ea-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:26,850 - distributed.scheduler - INFO - Close client connection: Client-cca784ea-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:26,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44885'. Reason: nanny-close
2024-03-20 07:02:26,851 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:26,853 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44235. Reason: nanny-close
2024-03-20 07:02:26,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:26,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46992; closing.
2024-03-20 07:02:26,855 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44235', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918146.855664')
2024-03-20 07:02:26,855 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:26,857 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:27,466 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:02:27,467 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:02:27,467 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:02:27,468 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:02:27,469 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-20 07:02:29,625 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:29,630 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41941 instead
  warnings.warn(
2024-03-20 07:02:29,634 - distributed.scheduler - INFO - State start
2024-03-20 07:02:29,655 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-20 07:02:29,656 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-20 07:02:29,657 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41941/status
2024-03-20 07:02:29,657 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-20 07:02:29,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33691'
2024-03-20 07:02:29,956 - distributed.scheduler - INFO - Receive client connection: Client-d0c4ccf1-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:29,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54478
2024-03-20 07:02:30,484 - distributed.scheduler - INFO - Receive client connection: Client-cfee2010-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:30,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54502
2024-03-20 07:02:31,603 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:02:31,603 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:02:31,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:02:31,607 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46275
2024-03-20 07:02:31,607 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46275
2024-03-20 07:02:31,607 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33019
2024-03-20 07:02:31,608 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-20 07:02:31,608 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:31,608 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:02:31,608 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-20 07:02:31,608 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jhptpd_2
2024-03-20 07:02:31,608 - distributed.worker - INFO - Starting Worker plugin PreImport-0c779235-7afa-4211-bff2-237766df77ec
2024-03-20 07:02:31,608 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c56c048-f8b7-48fa-9a5b-9420d583d1d8
2024-03-20 07:02:31,608 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6575eb05-311a-40d5-bb2f-d623fad3a5ad
2024-03-20 07:02:31,900 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:31,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46275', status: init, memory: 0, processing: 0>
2024-03-20 07:02:31,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46275
2024-03-20 07:02:31,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54542
2024-03-20 07:02:31,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:02:31,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-20 07:02:31,967 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:02:31,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-20 07:02:31,990 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:31,991 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-03-20 07:02:31,993 - distributed.scheduler - INFO - Remove client Client-d0c4ccf1-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:31,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54478; closing.
2024-03-20 07:02:31,993 - distributed.scheduler - INFO - Remove client Client-d0c4ccf1-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:31,993 - distributed.scheduler - INFO - Close client connection: Client-d0c4ccf1-e687-11ee-976e-d8c49764f6bb
2024-03-20 07:02:31,996 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-20 07:02:31,999 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:32,001 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:02:32,003 - distributed.scheduler - INFO - Remove client Client-cfee2010-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:32,003 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54502; closing.
2024-03-20 07:02:32,003 - distributed.scheduler - INFO - Remove client Client-cfee2010-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:32,004 - distributed.scheduler - INFO - Close client connection: Client-cfee2010-e687-11ee-9838-d8c49764f6bb
2024-03-20 07:02:32,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33691'. Reason: nanny-close
2024-03-20 07:02:32,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-20 07:02:32,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46275. Reason: nanny-close
2024-03-20 07:02:32,008 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-20 07:02:32,008 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54542; closing.
2024-03-20 07:02:32,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46275', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710918152.009072')
2024-03-20 07:02:32,009 - distributed.scheduler - INFO - Lost all workers
2024-03-20 07:02:32,010 - distributed.nanny - INFO - Worker closed
2024-03-20 07:02:32,620 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-20 07:02:32,620 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-20 07:02:32,621 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-20 07:02:32,622 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-20 07:02:32,622 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33117 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42967 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41059 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33165 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46101 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40587 instead
  warnings.warn(
[1710918267.334608] [dgx13:60999:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:38142) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40873 instead
  warnings.warn(
2024-03-20 07:04:32,807 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46619 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33829 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42551 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38929 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44823 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37441 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34235 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35727 instead
  warnings.warn(
2024-03-20 07:10:21,903 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] 2024-03-20 07:10:24,164 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41613 instead
  warnings.warn(
2024-03-20 07:10:26,499 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41049 instead
  warnings.warn(
2024-03-20 07:10:29,836 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42883 instead
  warnings.warn(
2024-03-20 07:10:33,091 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36669 instead
  warnings.warn(
2024-03-20 07:10:36,257 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39829 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38447 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33889 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38091 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39349 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39387 instead
  warnings.warn(
[1710918771.314332] [dgx13:69384:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:49766) failed: Address already in use
[1710918772.771226] [dgx13:69389:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:46162) failed: Address already in use
[1710918774.399087] [dgx13:69384:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:36284) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41737 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43513 instead
  warnings.warn(
[1710918810.994217] [dgx13:70198:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:33981) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34799 instead
  warnings.warn(
2024-03-20 07:14:24,280 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36877 instead
  warnings.warn(
2024-03-20 07:14:26,384 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-44:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] 2024-03-20 07:14:28,468 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-45:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38795 instead
  warnings.warn(
2024-03-20 07:14:31,714 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-46:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40871 instead
  warnings.warn(
2024-03-20 07:14:34,929 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39589 instead
  warnings.warn(
2024-03-20 07:14:38,714 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45709 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33401 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38675 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45857 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33885 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40317 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33209 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42963 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37387 instead
  warnings.warn(
[1710918983.985968] [dgx13:73483:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:35087) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42497 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39061 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42893 instead
  warnings.warn(
2024-03-20 07:17:07,978 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-63:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45211 instead
  warnings.warn(
2024-03-20 07:17:10,186 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-64:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37053 instead
  warnings.warn(
2024-03-20 07:17:12,329 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-65:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41913 instead
  warnings.warn(
2024-03-20 07:17:15,824 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-66:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44707 instead
  warnings.warn(
2024-03-20 07:17:19,051 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-67:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46675 instead
  warnings.warn(
2024-03-20 07:17:22,284 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-68:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36651 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43277 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37653 instead
  warnings.warn(
2024-03-20 07:17:49,685 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-71:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 302, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40823 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41249 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39185 instead
  warnings.warn(
[1710919121.955119] [dgx13:76250:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:56992) failed: Address already in use
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1710919138.705625] [dgx13:55352:0]            sock.c:481  UCX  ERROR bind(fd=174 addr=0.0.0.0:43890) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-20 07:19:16,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:16,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:16,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:16,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:16,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:16,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:17,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:17,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:17,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:17,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:17,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:17,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:17,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:17,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:17,129 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:17,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:17,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33775
2024-03-20 07:19:17,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33775
2024-03-20 07:19:17,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33777
2024-03-20 07:19:17,585 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,585 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,585 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u0x1cwkz
2024-03-20 07:19:17,586 - distributed.worker - INFO - Starting Worker plugin PreImport-507e9c6f-dbd2-4d9f-8b10-df35750a36da
2024-03-20 07:19:17,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d5d5fd3-07d2-420b-b1d8-8ad3d3c587be
2024-03-20 07:19:17,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-caa1ef7a-5702-4158-899f-a9bd840a9565
2024-03-20 07:19:17,587 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34201
2024-03-20 07:19:17,626 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34201
2024-03-20 07:19:17,626 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38571
2024-03-20 07:19:17,626 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,626 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,626 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h9q0dd1y
2024-03-20 07:19:17,626 - distributed.worker - INFO - Starting Worker plugin PreImport-b19315ec-7136-41fa-ad8a-0f8e84cf2fdb
2024-03-20 07:19:17,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b72adf11-982d-4801-b3c5-655d26ba24cc
2024-03-20 07:19:17,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f322d0e-981a-483f-a78b-2fe9bc6bcdeb
2024-03-20 07:19:17,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,646 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,646 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,653 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,654 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33597
2024-03-20 07:19:17,654 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33597
2024-03-20 07:19:17,654 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34207
2024-03-20 07:19:17,654 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,654 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,654 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wj76imyh
2024-03-20 07:19:17,654 - distributed.worker - INFO - Starting Worker plugin PreImport-568b01ae-0ee3-44cc-8c4f-36e35a552686
2024-03-20 07:19:17,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7437f32-d946-4d86-8432-2f848c005c4b
2024-03-20 07:19:17,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c618aa9-62aa-4db4-bb49-99370b4193c7
2024-03-20 07:19:17,655 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,679 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35861
2024-03-20 07:19:17,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35861
2024-03-20 07:19:17,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34225
2024-03-20 07:19:17,679 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,679 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,680 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,680 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mka_ky73
2024-03-20 07:19:17,680 - distributed.worker - INFO - Starting Worker plugin PreImport-29faaa22-4e7e-4742-a6a5-f9861a7706c4
2024-03-20 07:19:17,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-284056f3-0c90-496d-b0bf-ecb423177b6c
2024-03-20 07:19:17,680 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dde9f627-8abe-41f3-8fec-73f7225a83aa
2024-03-20 07:19:17,680 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,697 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,697 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,725 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,726 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35719
2024-03-20 07:19:17,726 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35719
2024-03-20 07:19:17,726 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41179
2024-03-20 07:19:17,726 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,726 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,726 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ajg8j9ru
2024-03-20 07:19:17,727 - distributed.worker - INFO - Starting Worker plugin PreImport-7dae4e67-b649-4391-812e-85b15920ccab
2024-03-20 07:19:17,727 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f8e7815-86a8-46af-a517-8040020fdc63
2024-03-20 07:19:17,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1634116a-f630-4b8a-8c0b-231f14a401e1
2024-03-20 07:19:17,727 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,741 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,742 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,742 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,749 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36613
2024-03-20 07:19:17,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36613
2024-03-20 07:19:17,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38871
2024-03-20 07:19:17,751 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,751 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,751 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qj39m_ed
2024-03-20 07:19:17,751 - distributed.worker - INFO - Starting Worker plugin PreImport-33f3aa30-6add-42ff-bad1-f7ed1cdc5464
2024-03-20 07:19:17,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45aacd58-13a1-4715-a496-29d21dbe2f35
2024-03-20 07:19:17,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3744be37-5fb9-4f96-bcb3-def24011b1ba
2024-03-20 07:19:17,752 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,767 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,767 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,777 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,778 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33199
2024-03-20 07:19:17,778 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33199
2024-03-20 07:19:17,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46435
2024-03-20 07:19:17,778 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,778 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,778 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a3sd63m7
2024-03-20 07:19:17,778 - distributed.worker - INFO - Starting Worker plugin PreImport-cc2c4699-1371-43f3-8b55-7be8cd1d1372
2024-03-20 07:19:17,779 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4d3c604-0477-430f-8b34-86d8850bc06d
2024-03-20 07:19:17,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02466341-7288-4ad6-bf2d-e63b4c85b4e9
2024-03-20 07:19:17,779 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,790 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:17,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37931
2024-03-20 07:19:17,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37931
2024-03-20 07:19:17,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35763
2024-03-20 07:19:17,791 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,791 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,791 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:17,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4n3q6qwt
2024-03-20 07:19:17,792 - distributed.worker - INFO - Starting Worker plugin PreImport-68eacb48-933e-49ff-b4f7-ad45741309ac
2024-03-20 07:19:17,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4833a2fa-98a9-4d7b-b97d-cfb991965018
2024-03-20 07:19:17,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-307a5546-f6b7-4a0d-be8d-c3576ed3a7ab
2024-03-20 07:19:17,792 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,812 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,812 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,862 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,862 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,889 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,890 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:17,896 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40727
2024-03-20 07:19:17,896 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:17,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40727
2024-03-20 07:19:17,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,920 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,920 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-20 07:19:17,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33775. Reason: nanny-close
2024-03-20 07:19:17,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34201. Reason: nanny-close
2024-03-20 07:19:17,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35861. Reason: nanny-close
2024-03-20 07:19:17,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33597. Reason: nanny-close
2024-03-20 07:19:17,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,927 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35719. Reason: nanny-close
2024-03-20 07:19:17,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36613. Reason: nanny-close
2024-03-20 07:19:17,928 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37931. Reason: nanny-close
2024-03-20 07:19:17,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33199. Reason: nanny-close
2024-03-20 07:19:17,929 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,929 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,930 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,931 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:40727 has been closed.
2024-03-20 07:19:17,932 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,932 - distributed.nanny - INFO - Worker closed
2024-03-20 07:19:17,933 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-20 07:19:54,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:54,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:54,227 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:54,228 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44925
2024-03-20 07:19:54,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44925
2024-03-20 07:19:54,228 - distributed.worker - INFO -           Worker name:                          0
2024-03-20 07:19:54,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40617
2024-03-20 07:19:54,228 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33617
2024-03-20 07:19:54,229 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:54,229 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:54,229 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-20 07:19:54,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yh7de_cz
2024-03-20 07:19:54,229 - distributed.worker - INFO - Starting Worker plugin PreImport-b7c10df9-cb8a-4607-871e-ff12999e9aaa
2024-03-20 07:19:54,234 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-20 07:19:54,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c9083d5-7687-44b1-b290-6043ea7bfb21
2024-03-20 07:19:54,234 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de525538-3618-4c62-8156-d5e11b039ddb
2024-03-20 07:19:54,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44925. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-20 07:19:54,235 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-20 07:19:54,237 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-20 07:19:58,797 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:58,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:58,858 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:58,858 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:58,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:58,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:58,975 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:58,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:58,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:58,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:59,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:59,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:59,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:59,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:59,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-20 07:19:59,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-20 07:19:59,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34863
2024-03-20 07:19:59,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34863
2024-03-20 07:19:59,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39517
2024-03-20 07:19:59,506 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,506 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,506 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,506 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p1qnn_e9
2024-03-20 07:19:59,506 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bfb8979-b8dd-4bde-bf13-bf2f701d9f2e
2024-03-20 07:19:59,507 - distributed.worker - INFO - Starting Worker plugin PreImport-f944a92d-154c-414b-a5ea-3e9a69991923
2024-03-20 07:19:59,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c4b60e1-664c-42ee-ba0f-eba22e6948ea
2024-03-20 07:19:59,508 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33653
2024-03-20 07:19:59,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33653
2024-03-20 07:19:59,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39217
2024-03-20 07:19:59,534 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,534 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,534 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ks93svk3
2024-03-20 07:19:59,535 - distributed.worker - INFO - Starting Worker plugin PreImport-a49136a2-20bb-4826-a5a1-fdc4a8ec90d8
2024-03-20 07:19:59,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72583397-17b4-4b1c-872f-f828405e9f98
2024-03-20 07:19:59,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e669fa1-ea66-48be-8e7c-bcc8c4607d46
2024-03-20 07:19:59,535 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,536 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40071
2024-03-20 07:19:59,537 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40071
2024-03-20 07:19:59,537 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42281
2024-03-20 07:19:59,538 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,538 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,538 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,538 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,538 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dh6mswck
2024-03-20 07:19:59,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41cc72df-d6ba-4ce3-af7c-0374ecc144cd
2024-03-20 07:19:59,538 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46f4d5ae-9cf3-41e2-98b9-7b74bf821824
2024-03-20 07:19:59,538 - distributed.worker - INFO - Starting Worker plugin PreImport-477fb6fd-ee67-47d6-90bb-018c613df215
2024-03-20 07:19:59,539 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,620 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33119
2024-03-20 07:19:59,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33119
2024-03-20 07:19:59,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40959
2024-03-20 07:19:59,621 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,621 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,621 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,621 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,621 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p3xd1119
2024-03-20 07:19:59,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2a36979-7db5-4180-bbf3-5e85c03e14ab
2024-03-20 07:19:59,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-deda091e-ffd2-495c-8a52-08c2155cf44c
2024-03-20 07:19:59,621 - distributed.worker - INFO - Starting Worker plugin PreImport-af399552-6f9f-4cf6-9a0d-a853aa66163a
2024-03-20 07:19:59,622 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45131
2024-03-20 07:19:59,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45131
2024-03-20 07:19:59,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44341
2024-03-20 07:19:59,628 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,628 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6sf8ljag
2024-03-20 07:19:59,628 - distributed.worker - INFO - Starting Worker plugin PreImport-90acca85-87ef-4ca9-b1fd-8c23826a8194
2024-03-20 07:19:59,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f2dc238-a967-47c4-86af-7384babb3724
2024-03-20 07:19:59,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ed0eebb-04aa-4690-9cf2-a0cbb756c874
2024-03-20 07:19:59,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,721 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42995
2024-03-20 07:19:59,722 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42995
2024-03-20 07:19:59,722 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39405
2024-03-20 07:19:59,722 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,722 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,722 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,722 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,722 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ac2j3ase
2024-03-20 07:19:59,722 - distributed.worker - INFO - Starting Worker plugin PreImport-ad2156c8-0dba-498e-87f1-e79889315072
2024-03-20 07:19:59,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b501ef0-675b-4749-bcd2-0765963d9ee3
2024-03-20 07:19:59,723 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87fa3421-8175-4ac1-bd45-8772a73eb909
2024-03-20 07:19:59,723 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,812 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38761
2024-03-20 07:19:59,812 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38761
2024-03-20 07:19:59,812 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45799
2024-03-20 07:19:59,812 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,812 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,812 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,812 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ou8kszcq
2024-03-20 07:19:59,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-399a8668-bc3b-4b52-840f-1357b28e20d2
2024-03-20 07:19:59,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2a332ad-9144-44c6-8723-62c2b81d6fed
2024-03-20 07:19:59,813 - distributed.worker - INFO - Starting Worker plugin PreImport-91fbed7c-fbb8-4bb2-9f32-91d8277389dd
2024-03-20 07:19:59,813 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:59,817 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,817 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:19:59,864 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-20 07:19:59,865 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34859
2024-03-20 07:19:59,865 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34859
2024-03-20 07:19:59,866 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35503
2024-03-20 07:19:59,866 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,866 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,866 - distributed.worker - INFO -               Threads:                          1
2024-03-20 07:19:59,866 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-20 07:19:59,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-259b48u4
2024-03-20 07:19:59,866 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f2c9102-51c3-48e3-a89e-a9d3c3e913b9
2024-03-20 07:19:59,866 - distributed.worker - INFO - Starting Worker plugin PreImport-2b672c9f-5a73-4f25-98c3-2ef75dbbc4e7
2024-03-20 07:19:59,866 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f84c2b1d-d004-44a9-bdeb-e25e7bd27acd
2024-03-20 07:19:59,867 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:59,938 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,938 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:19:59,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:19:59,948 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:19:59,948 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:19:59,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:20:00,034 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:20:00,035 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:20:00,035 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:20:00,036 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:20:00,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:20:00,052 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:20:00,052 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:20:00,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:20:00,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:20:00,068 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:20:00,068 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:20:00,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:20:00,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:20:00,138 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:20:00,138 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:20:00,139 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:20:00,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-20 07:20:00,151 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43755
2024-03-20 07:20:00,151 - distributed.worker - INFO - -------------------------------------------------
2024-03-20 07:20:00,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43755
2024-03-20 07:20:00,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34863. Reason: nanny-close
2024-03-20 07:20:00,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40071. Reason: nanny-close
2024-03-20 07:20:00,181 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33653. Reason: nanny-close
2024-03-20 07:20:00,181 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45131. Reason: nanny-close
2024-03-20 07:20:00,182 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33119. Reason: nanny-close
2024-03-20 07:20:00,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,182 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42995. Reason: nanny-close
2024-03-20 07:20:00,183 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38761. Reason: nanny-close
2024-03-20 07:20:00,183 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,183 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,183 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34859. Reason: nanny-close
2024-03-20 07:20:00,184 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,184 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,184 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,184 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,184 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:43755 has been closed.
2024-03-20 07:20:00,186 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,186 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,186 - distributed.nanny - INFO - Worker closed
2024-03-20 07:20:00,186 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-03-20 07:20:50,929 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:20:50,943 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:42989'. Shutting down.
2024-03-20 07:20:50,946 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fcabf0bf0a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:20:52,952 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-03-20 07:21:18,693 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:21:18,711 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff2e3a330a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:21:20,716 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-03-20 07:21:49,488 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:21:49,497 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f08a48700a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:21:51,502 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-03-20 07:22:20,110 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:22:20,121 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f9ae01da0a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:22:22,126 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-03-20 07:22:50,850 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:22:50,860 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe8375210a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-20 07:22:52,865 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-20 07:23:27,081 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-20 07:23:27,088 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
