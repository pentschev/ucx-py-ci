============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.2, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-25 07:01:24,674 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:24,678 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43625 instead
  warnings.warn(
2024-02-25 07:01:24,682 - distributed.scheduler - INFO - State start
2024-02-25 07:01:24,845 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:24,847 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-25 07:01:24,848 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43625/status
2024-02-25 07:01:24,848 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:01:24,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43549'
2024-02-25 07:01:24,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35361'
2024-02-25 07:01:24,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45409'
2024-02-25 07:01:24,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40723'
2024-02-25 07:01:25,382 - distributed.scheduler - INFO - Receive client connection: Client-af3d62de-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:25,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51266
2024-02-25 07:01:26,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:26,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:26,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:26,894 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34073
2024-02-25 07:01:26,895 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34073
2024-02-25 07:01:26,895 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36223
2024-02-25 07:01:26,895 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-25 07:01:26,895 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,895 - distributed.worker - INFO -               Threads:                          4
2024-02-25 07:01:26,895 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-25 07:01:26,895 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8bw6pbs_
2024-02-25 07:01:26,895 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e39ecd7-62cc-4686-a661-9815a7183d5e
2024-02-25 07:01:26,895 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de2c2c1a-94b6-4a02-babc-e8fde77377d7
2024-02-25 07:01:26,896 - distributed.worker - INFO - Starting Worker plugin PreImport-71726c22-5f64-485e-8d9c-ab192f93a02d
2024-02-25 07:01:26,896 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:26,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:26,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:26,938 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42803
2024-02-25 07:01:26,938 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42803
2024-02-25 07:01:26,938 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46597
2024-02-25 07:01:26,939 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-25 07:01:26,939 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,939 - distributed.worker - INFO -               Threads:                          4
2024-02-25 07:01:26,939 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-25 07:01:26,939 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-a3aawerg
2024-02-25 07:01:26,939 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3322fbb-7c1b-45c9-96f9-1939b4ed44d4
2024-02-25 07:01:26,939 - distributed.worker - INFO - Starting Worker plugin PreImport-08976533-ec2f-4034-86a5-5447f860925f
2024-02-25 07:01:26,940 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cca24686-1b4a-4beb-b9d4-87e623cf939f
2024-02-25 07:01:26,940 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:26,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:26,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:26,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:26,944 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:26,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41637
2024-02-25 07:01:26,945 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41637
2024-02-25 07:01:26,945 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33051
2024-02-25 07:01:26,945 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-25 07:01:26,945 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,946 - distributed.worker - INFO -               Threads:                          4
2024-02-25 07:01:26,946 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-25 07:01:26,946 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qayma9nk
2024-02-25 07:01:26,946 - distributed.worker - INFO - Starting Worker plugin PreImport-a909d6a5-3326-44d2-b74a-99cfdc110c29
2024-02-25 07:01:26,946 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51f78b82-2193-4acc-b8fb-3332d618f9b9
2024-02-25 07:01:26,946 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2947c8a6-4782-4de4-8c58-ee25b7d5998a
2024-02-25 07:01:26,946 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:26,949 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41095
2024-02-25 07:01:26,949 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41095
2024-02-25 07:01:26,949 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42035
2024-02-25 07:01:26,949 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-25 07:01:26,949 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:26,949 - distributed.worker - INFO -               Threads:                          4
2024-02-25 07:01:26,949 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-25 07:01:26,949 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-r28lblfz
2024-02-25 07:01:26,950 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-358e6f34-7378-4472-8745-aa0ae39f4d5e
2024-02-25 07:01:26,950 - distributed.worker - INFO - Starting Worker plugin PreImport-984ab66c-e1d7-4a10-8df3-5759eb2d8598
2024-02-25 07:01:26,950 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a84a485-5838-40d9-9db8-5a087a93d253
2024-02-25 07:01:26,950 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:27,722 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34073', status: init, memory: 0, processing: 0>
2024-02-25 07:01:27,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34073
2024-02-25 07:01:27,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51282
2024-02-25 07:01:27,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:27,726 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-25 07:01:27,726 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:27,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-25 07:01:28,118 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42803', status: init, memory: 0, processing: 0>
2024-02-25 07:01:28,119 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42803
2024-02-25 07:01:28,119 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51294
2024-02-25 07:01:28,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:28,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-25 07:01:28,122 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:28,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-25 07:01:28,321 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41095', status: init, memory: 0, processing: 0>
2024-02-25 07:01:28,322 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41095
2024-02-25 07:01:28,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51304
2024-02-25 07:01:28,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:28,324 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-25 07:01:28,324 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:28,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-25 07:01:28,522 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41637', status: init, memory: 0, processing: 0>
2024-02-25 07:01:28,523 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41637
2024-02-25 07:01:28,523 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51302
2024-02-25 07:01:28,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:28,526 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-25 07:01:28,526 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:28,527 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-25 07:01:28,585 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-25 07:01:28,585 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-25 07:01:28,585 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-25 07:01:28,585 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-25 07:01:28,590 - distributed.scheduler - INFO - Remove client Client-af3d62de-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:28,591 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51266; closing.
2024-02-25 07:01:28,591 - distributed.scheduler - INFO - Remove client Client-af3d62de-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:28,591 - distributed.scheduler - INFO - Close client connection: Client-af3d62de-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:28,592 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43549'. Reason: nanny-close
2024-02-25 07:01:28,592 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:28,593 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35361'. Reason: nanny-close
2024-02-25 07:01:28,593 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:28,593 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45409'. Reason: nanny-close
2024-02-25 07:01:28,594 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34073. Reason: nanny-close
2024-02-25 07:01:28,594 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:28,594 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40723'. Reason: nanny-close
2024-02-25 07:01:28,594 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41637. Reason: nanny-close
2024-02-25 07:01:28,594 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:28,595 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42803. Reason: nanny-close
2024-02-25 07:01:28,595 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41095. Reason: nanny-close
2024-02-25 07:01:28,595 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-25 07:01:28,596 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51282; closing.
2024-02-25 07:01:28,596 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844488.5964255')
2024-02-25 07:01:28,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-25 07:01:28,597 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-25 07:01:28,597 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-25 07:01:28,597 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51294; closing.
2024-02-25 07:01:28,597 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:28,598 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844488.5981429')
2024-02-25 07:01:28,598 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51304; closing.
2024-02-25 07:01:28,598 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:28,598 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:28,598 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51302; closing.
2024-02-25 07:01:28,599 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:28,599 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:51294>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:51294>: Stream is closed
2024-02-25 07:01:28,600 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844488.600551')
2024-02-25 07:01:28,601 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41637', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844488.6009605')
2024-02-25 07:01:28,601 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:01:29,207 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:01:29,208 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:01:29,208 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:01:29,209 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-25 07:01:29,210 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-25 07:01:31,281 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:31,286 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33229 instead
  warnings.warn(
2024-02-25 07:01:31,289 - distributed.scheduler - INFO - State start
2024-02-25 07:01:31,312 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:31,313 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:01:31,314 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33229/status
2024-02-25 07:01:31,314 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:01:31,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42503'
2024-02-25 07:01:31,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44129'
2024-02-25 07:01:31,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42513'
2024-02-25 07:01:31,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45613'
2024-02-25 07:01:31,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40131'
2024-02-25 07:01:31,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33517'
2024-02-25 07:01:31,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39109'
2024-02-25 07:01:31,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38367'
2024-02-25 07:01:33,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33287
2024-02-25 07:01:33,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44175
2024-02-25 07:01:33,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33287
2024-02-25 07:01:33,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44175
2024-02-25 07:01:33,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39965
2024-02-25 07:01:33,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39789
2024-02-25 07:01:33,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,413 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,413 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,413 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,413 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xm5_fi_a
2024-02-25 07:01:33,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y8sxaszs
2024-02-25 07:01:33,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d294fc22-16ad-4aaa-8aa8-ec19ef71f612
2024-02-25 07:01:33,414 - distributed.worker - INFO - Starting Worker plugin PreImport-1177ce94-ceb5-47ca-a3e9-e44526cc68f7
2024-02-25 07:01:33,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4039cfe-9e54-46d8-bd6c-d32ce24327ee
2024-02-25 07:01:33,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb6605cf-5d1c-495c-ac47-2a37a38dd228
2024-02-25 07:01:33,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6bf0035-9fe8-46f4-9e76-3bb63e58019f
2024-02-25 07:01:33,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43969
2024-02-25 07:01:33,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43969
2024-02-25 07:01:33,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34685
2024-02-25 07:01:33,434 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,434 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,434 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,434 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,434 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d4buynfr
2024-02-25 07:01:33,434 - distributed.worker - INFO - Starting Worker plugin PreImport-3e34bb42-4fa3-4b59-80f5-cb83af7bed9a
2024-02-25 07:01:33,434 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76296299-31ef-40f8-b6ae-909374f3568e
2024-02-25 07:01:33,436 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,437 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42565
2024-02-25 07:01:33,437 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42565
2024-02-25 07:01:33,437 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44255
2024-02-25 07:01:33,437 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,437 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,437 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,437 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,437 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t09p70qz
2024-02-25 07:01:33,437 - distributed.worker - INFO - Starting Worker plugin PreImport-94592e3f-ba46-4d4e-b58d-2fc0e607eedc
2024-02-25 07:01:33,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9dfc6200-5432-4e2c-9d22-3bd10cfb5f93
2024-02-25 07:01:33,438 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fe1935f-7dd4-49f0-89a0-0acc4e707e3b
2024-02-25 07:01:33,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32769
2024-02-25 07:01:33,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32769
2024-02-25 07:01:33,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36307
2024-02-25 07:01:33,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,458 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a_co6rno
2024-02-25 07:01:33,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e576c87b-fc6a-4f35-96dc-d6ca93792c13
2024-02-25 07:01:33,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f386bd0-26f6-4296-a586-0873d5bd87fb
2024-02-25 07:01:33,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,495 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,499 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,500 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43675
2024-02-25 07:01:33,500 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43675
2024-02-25 07:01:33,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46795
2024-02-25 07:01:33,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,500 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,500 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9r7kjgxb
2024-02-25 07:01:33,501 - distributed.worker - INFO - Starting Worker plugin PreImport-591707cb-933d-4d7d-bc1c-ac37e39d62c0
2024-02-25 07:01:33,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2823c949-ab90-4fd3-8c31-94b3ef2ae162
2024-02-25 07:01:33,501 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44917
2024-02-25 07:01:33,501 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44917
2024-02-25 07:01:33,501 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33491
2024-02-25 07:01:33,501 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,501 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,501 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,501 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-heeymwjv
2024-02-25 07:01:33,502 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-002de374-b3d4-4dbe-a5d8-00f8b19da3b1
2024-02-25 07:01:33,502 - distributed.worker - INFO - Starting Worker plugin PreImport-8c08f74c-b1a8-4e1e-9a94-10e2fd810495
2024-02-25 07:01:33,502 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb483622-a32b-4d8e-87d4-e5f13d6ce217
2024-02-25 07:01:33,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:33,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:33,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:33,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40043
2024-02-25 07:01:33,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40043
2024-02-25 07:01:33,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35759
2024-02-25 07:01:33,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:33,534 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:33,534 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:33,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:33,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2j71doyu
2024-02-25 07:01:33,534 - distributed.worker - INFO - Starting Worker plugin PreImport-f5be1ccd-fb9b-41bc-93ea-f2b3832aff16
2024-02-25 07:01:33,534 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4297a014-3f5c-4cf8-aa3f-6fb118737874
2024-02-25 07:01:35,576 - distributed.worker - INFO - Starting Worker plugin PreImport-a011e901-ec6a-4828-98f5-c722a4f935f0
2024-02-25 07:01:35,577 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,585 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44175', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,610 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44175
2024-02-25 07:01:35,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59164
2024-02-25 07:01:35,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-468e6543-bc63-4366-887a-676875f3030c
2024-02-25 07:01:35,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,612 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,612 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,612 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33287', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,613 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33287
2024-02-25 07:01:35,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59170
2024-02-25 07:01:35,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,614 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,615 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,615 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,616 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,647 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43969', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,648 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43969
2024-02-25 07:01:35,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59184
2024-02-25 07:01:35,649 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,651 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,651 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,714 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,744 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42565', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,745 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42565
2024-02-25 07:01:35,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59188
2024-02-25 07:01:35,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,747 - distributed.worker - INFO - Starting Worker plugin PreImport-6c64d388-070f-4807-ad87-6ebd7e6cf02f
2024-02-25 07:01:35,747 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,747 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,748 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,764 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,771 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32769', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,771 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32769
2024-02-25 07:01:35,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59200
2024-02-25 07:01:35,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,773 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,773 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-442705f5-ae4a-4fdb-ac08-5b47c5a2cbef
2024-02-25 07:01:35,784 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44917', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,785 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44917
2024-02-25 07:01:35,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59202
2024-02-25 07:01:35,785 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,787 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,787 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,801 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1fd5d981-1084-41d3-b8c3-45c9564b4381
2024-02-25 07:01:35,804 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,820 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43675', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,820 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43675
2024-02-25 07:01:35,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59212
2024-02-25 07:01:35,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,824 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,824 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:35,842 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40043', status: init, memory: 0, processing: 0>
2024-02-25 07:01:35,843 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40043
2024-02-25 07:01:35,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59218
2024-02-25 07:01:35,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:35,846 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:35,846 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:35,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:39,879 - distributed.scheduler - INFO - Receive client connection: Client-b345416b-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:39,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59274
2024-02-25 07:01:39,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,891 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,892 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:39,896 - distributed.scheduler - INFO - Remove client Client-b345416b-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:39,897 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59274; closing.
2024-02-25 07:01:39,897 - distributed.scheduler - INFO - Remove client Client-b345416b-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:39,897 - distributed.scheduler - INFO - Close client connection: Client-b345416b-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:39,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42503'. Reason: nanny-close
2024-02-25 07:01:39,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44129'. Reason: nanny-close
2024-02-25 07:01:39,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44175. Reason: nanny-close
2024-02-25 07:01:39,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42513'. Reason: nanny-close
2024-02-25 07:01:39,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45613'. Reason: nanny-close
2024-02-25 07:01:39,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33287. Reason: nanny-close
2024-02-25 07:01:39,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40131'. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43675. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33517'. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42565. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59164; closing.
2024-02-25 07:01:39,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39109'. Reason: nanny-close
2024-02-25 07:01:39,903 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44917. Reason: nanny-close
2024-02-25 07:01:39,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,904 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9041126')
2024-02-25 07:01:39,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38367'. Reason: nanny-close
2024-02-25 07:01:39,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32769. Reason: nanny-close
2024-02-25 07:01:39,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:39,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43969. Reason: nanny-close
2024-02-25 07:01:39,905 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,905 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,905 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,905 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40043. Reason: nanny-close
2024-02-25 07:01:39,905 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,906 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59212; closing.
2024-02-25 07:01:39,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59170; closing.
2024-02-25 07:01:39,907 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,907 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,907 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,907 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,908 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9081345')
2024-02-25 07:01:39,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59188; closing.
2024-02-25 07:01:39,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33287', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9087267')
2024-02-25 07:01:39,908 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:39,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59202; closing.
2024-02-25 07:01:39,909 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42565', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9099154')
2024-02-25 07:01:39,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44917', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9102614')
2024-02-25 07:01:39,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59200; closing.
2024-02-25 07:01:39,910 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:39,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9115586')
2024-02-25 07:01:39,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59184; closing.
2024-02-25 07:01:39,912 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59218; closing.
2024-02-25 07:01:39,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9127579')
2024-02-25 07:01:39,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844499.9131756')
2024-02-25 07:01:39,913 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:01:40,864 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:01:40,864 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:01:40,865 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:01:40,866 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:01:40,866 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-25 07:01:42,973 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:42,978 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43225 instead
  warnings.warn(
2024-02-25 07:01:42,981 - distributed.scheduler - INFO - State start
2024-02-25 07:01:43,002 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:43,003 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:01:43,004 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43225/status
2024-02-25 07:01:43,004 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:01:43,207 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46201'
2024-02-25 07:01:43,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46739'
2024-02-25 07:01:43,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36277'
2024-02-25 07:01:43,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36163'
2024-02-25 07:01:43,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40877'
2024-02-25 07:01:43,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33301'
2024-02-25 07:01:43,269 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45695'
2024-02-25 07:01:43,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35289'
2024-02-25 07:01:44,242 - distributed.scheduler - INFO - Receive client connection: Client-ba30300f-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:44,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42482
2024-02-25 07:01:45,013 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,013 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,018 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,019 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43923
2024-02-25 07:01:45,019 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43923
2024-02-25 07:01:45,019 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36317
2024-02-25 07:01:45,019 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,019 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,019 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,019 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8itx38n
2024-02-25 07:01:45,019 - distributed.worker - INFO - Starting Worker plugin PreImport-7580ecf6-d197-49e2-9f63-251ba86ebdf5
2024-02-25 07:01:45,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d45caf50-dab6-45de-9279-bfb4b57bba42
2024-02-25 07:01:45,237 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,238 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,242 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,243 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43657
2024-02-25 07:01:45,243 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43657
2024-02-25 07:01:45,243 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38271
2024-02-25 07:01:45,244 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,244 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,244 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,244 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,244 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4iw_fk87
2024-02-25 07:01:45,244 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-652f1191-2712-4169-ba41-41e71d677330
2024-02-25 07:01:45,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-93940463-c7f0-4d87-9b16-ef023af84335
2024-02-25 07:01:45,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33387
2024-02-25 07:01:45,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33387
2024-02-25 07:01:45,265 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44505
2024-02-25 07:01:45,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41357
2024-02-25 07:01:45,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38519
2024-02-25 07:01:45,265 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38519
2024-02-25 07:01:45,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41357
2024-02-25 07:01:45,265 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34671
2024-02-25 07:01:45,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32995
2024-02-25 07:01:45,266 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,266 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,266 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6fdq19id
2024-02-25 07:01:45,266 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,266 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,266 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,266 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,266 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,266 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q0246wui
2024-02-25 07:01:45,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o4ety0jy
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin PreImport-f291eaaf-3fce-47dd-9a13-a26373d3a19f
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01f06d88-eb9a-4343-9016-f126b6e6e35c
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin PreImport-959c557e-1d4a-4f01-82af-95cbeccba38d
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin PreImport-794902e6-f8d8-4fac-92df-18201bafa632
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-312c29a8-62e1-4d59-8807-ca76f0af3914
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9b03d39-31e4-4954-b795-1af970ba633c
2024-02-25 07:01:45,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f5ffe89-1486-4d72-a7ee-5f37910e30ef
2024-02-25 07:01:45,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:45,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:45,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36393
2024-02-25 07:01:45,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36393
2024-02-25 07:01:45,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43941
2024-02-25 07:01:45,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,272 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,272 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8solcqkl
2024-02-25 07:01:45,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eac05a1a-dc23-4c9b-81f3-5da4c39c43c9
2024-02-25 07:01:45,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d295a296-e0ab-4517-af6a-d61b4415b216
2024-02-25 07:01:45,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,275 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40635
2024-02-25 07:01:45,275 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40635
2024-02-25 07:01:45,275 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34235
2024-02-25 07:01:45,275 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,275 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,275 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,276 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hzzpwu20
2024-02-25 07:01:45,276 - distributed.worker - INFO - Starting Worker plugin PreImport-cf7e7045-2e11-4705-b46d-7b7ae067c95b
2024-02-25 07:01:45,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6704586-51b9-486f-a227-0bfa67916668
2024-02-25 07:01:45,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:45,277 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40363
2024-02-25 07:01:45,277 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40363
2024-02-25 07:01:45,277 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44391
2024-02-25 07:01:45,277 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,277 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,277 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:45,277 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:45,277 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4721ohb6
2024-02-25 07:01:45,278 - distributed.worker - INFO - Starting Worker plugin PreImport-4218cee0-b45d-44b6-9cf3-bf022f9f5527
2024-02-25 07:01:45,278 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13aee973-7a18-4880-a9bb-c5d212866611
2024-02-25 07:01:45,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf986c6c-6503-4e2e-8f60-1c3729b52ecc
2024-02-25 07:01:45,444 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7decdc1c-580e-4d90-b893-fef89df4b13c
2024-02-25 07:01:45,446 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43923', status: init, memory: 0, processing: 0>
2024-02-25 07:01:45,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43923
2024-02-25 07:01:45,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42512
2024-02-25 07:01:45,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:45,481 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:45,482 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:45,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,144 - distributed.worker - INFO - Starting Worker plugin PreImport-6b0e516d-d738-4972-9604-262c140551ba
2024-02-25 07:01:47,146 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,176 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43657', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,178 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43657
2024-02-25 07:01:47,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42526
2024-02-25 07:01:47,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,180 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,180 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,206 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f91e9a83-95d2-43f9-9cbb-a6b638cf9897
2024-02-25 07:01:47,206 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,212 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6947fed-3a6b-4220-afbf-d89ee31ff877
2024-02-25 07:01:47,214 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,228 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38519', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,229 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38519
2024-02-25 07:01:47,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42530
2024-02-25 07:01:47,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,231 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,231 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,242 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40635', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,243 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40635
2024-02-25 07:01:47,243 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42538
2024-02-25 07:01:47,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,245 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,246 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,284 - distributed.worker - INFO - Starting Worker plugin PreImport-b6c8a5df-e0dd-4257-ba41-6281ebe7ee24
2024-02-25 07:01:47,285 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,292 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,298 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,306 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a11af7f-7b82-4285-a0e7-cf258f7b9a6c
2024-02-25 07:01:47,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36393', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,311 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36393
2024-02-25 07:01:47,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42550
2024-02-25 07:01:47,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,312 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,313 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,322 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41357', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,322 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41357
2024-02-25 07:01:47,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42562
2024-02-25 07:01:47,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40363', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,324 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,324 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40363
2024-02-25 07:01:47,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42556
2024-02-25 07:01:47,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,327 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33387', status: init, memory: 0, processing: 0>
2024-02-25 07:01:47,345 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33387
2024-02-25 07:01:47,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42564
2024-02-25 07:01:47,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:47,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:47,348 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:47,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:47,420 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,422 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,422 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,422 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:47,427 - distributed.scheduler - INFO - Remove client Client-ba30300f-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:47,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42482; closing.
2024-02-25 07:01:47,427 - distributed.scheduler - INFO - Remove client Client-ba30300f-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:47,428 - distributed.scheduler - INFO - Close client connection: Client-ba30300f-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:47,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46201'. Reason: nanny-close
2024-02-25 07:01:47,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46739'. Reason: nanny-close
2024-02-25 07:01:47,430 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36277'. Reason: nanny-close
2024-02-25 07:01:47,431 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43657. Reason: nanny-close
2024-02-25 07:01:47,431 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36163'. Reason: nanny-close
2024-02-25 07:01:47,431 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33387. Reason: nanny-close
2024-02-25 07:01:47,431 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,432 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40877'. Reason: nanny-close
2024-02-25 07:01:47,432 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41357. Reason: nanny-close
2024-02-25 07:01:47,432 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,432 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33301'. Reason: nanny-close
2024-02-25 07:01:47,432 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38519. Reason: nanny-close
2024-02-25 07:01:47,432 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,433 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45695'. Reason: nanny-close
2024-02-25 07:01:47,433 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40363. Reason: nanny-close
2024-02-25 07:01:47,433 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,433 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35289'. Reason: nanny-close
2024-02-25 07:01:47,433 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42526; closing.
2024-02-25 07:01:47,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,433 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:47,434 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40635. Reason: nanny-close
2024-02-25 07:01:47,434 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,434 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43657', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.434137')
2024-02-25 07:01:47,434 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43923. Reason: nanny-close
2024-02-25 07:01:47,434 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,434 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,435 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36393. Reason: nanny-close
2024-02-25 07:01:47,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42530; closing.
2024-02-25 07:01:47,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42562; closing.
2024-02-25 07:01:47,435 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,435 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,436 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,436 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.4368436')
2024-02-25 07:01:47,436 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.4373133')
2024-02-25 07:01:47,437 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,437 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42564; closing.
2024-02-25 07:01:47,437 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,438 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,438 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:47,438 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,439 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,440 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:47,438 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42530>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42530>: Stream is closed
2024-02-25 07:01:47,442 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.441987')
2024-02-25 07:01:47,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42556; closing.
2024-02-25 07:01:47,443 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.4432778')
2024-02-25 07:01:47,443 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42512; closing.
2024-02-25 07:01:47,443 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42538; closing.
2024-02-25 07:01:47,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42550; closing.
2024-02-25 07:01:47,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.4443963')
2024-02-25 07:01:47,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.444912')
2024-02-25 07:01:47,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844507.4453197')
2024-02-25 07:01:47,445 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:01:48,344 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:01:48,345 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:01:48,346 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:01:48,347 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:01:48,348 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-25 07:01:50,514 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:50,519 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44539 instead
  warnings.warn(
2024-02-25 07:01:50,523 - distributed.scheduler - INFO - State start
2024-02-25 07:01:50,546 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:50,547 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:01:50,548 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44539/status
2024-02-25 07:01:50,548 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:01:50,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39709'
2024-02-25 07:01:50,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39453'
2024-02-25 07:01:50,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33527'
2024-02-25 07:01:50,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36133'
2024-02-25 07:01:50,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35005'
2024-02-25 07:01:50,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39523'
2024-02-25 07:01:50,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38299'
2024-02-25 07:01:50,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44525'
2024-02-25 07:01:51,190 - distributed.scheduler - INFO - Receive client connection: Client-bebf7a4c-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:51,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45958
2024-02-25 07:01:52,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,758 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45865
2024-02-25 07:01:52,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45865
2024-02-25 07:01:52,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39975
2024-02-25 07:01:52,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,758 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,758 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,758 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w1i37_3_
2024-02-25 07:01:52,758 - distributed.worker - INFO - Starting Worker plugin PreImport-caa769dc-0db2-4993-a2f7-03326670687d
2024-02-25 07:01:52,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f95820d-227d-454a-ae29-69cacdd6d9e9
2024-02-25 07:01:52,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44781
2024-02-25 07:01:52,793 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44781
2024-02-25 07:01:52,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40501
2024-02-25 07:01:52,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,793 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,793 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,793 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,793 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pexbjgdb
2024-02-25 07:01:52,793 - distributed.worker - INFO - Starting Worker plugin PreImport-afe302df-343e-44a4-814a-cfc63ce81df7
2024-02-25 07:01:52,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33bf86f4-2187-4c07-b316-868db5eb2e04
2024-02-25 07:01:52,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41631
2024-02-25 07:01:52,794 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41631
2024-02-25 07:01:52,794 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38717
2024-02-25 07:01:52,794 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,794 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,794 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46a64027-3a99-457f-8ccc-076f052c741b
2024-02-25 07:01:52,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-notfih4m
2024-02-25 07:01:52,794 - distributed.worker - INFO - Starting Worker plugin PreImport-a0ab5eb4-bcd0-4fef-abdd-c34e4e89ad73
2024-02-25 07:01:52,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f9e75d4-0e9a-4f7d-861f-cde362896899
2024-02-25 07:01:52,795 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61673c8e-05f5-4fb0-a9cb-2af1d0243865
2024-02-25 07:01:52,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35897
2024-02-25 07:01:52,809 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35897
2024-02-25 07:01:52,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40905
2024-02-25 07:01:52,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,809 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,809 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ep1d6z7w
2024-02-25 07:01:52,809 - distributed.worker - INFO - Starting Worker plugin PreImport-39a5cb92-8515-426b-95ee-b6ef547fc398
2024-02-25 07:01:52,809 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7898be4a-04ad-47a6-ae2e-67f590528f9d
2024-02-25 07:01:52,811 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,811 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,816 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,816 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,816 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,817 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35859
2024-02-25 07:01:52,817 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35859
2024-02-25 07:01:52,817 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33219
2024-02-25 07:01:52,817 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,817 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,817 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,817 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jnca3wxq
2024-02-25 07:01:52,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68f93930-79f2-4c71-bb86-1f1613263ece
2024-02-25 07:01:52,819 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3dacbad3-0c03-4014-a955-15e8aadb7125
2024-02-25 07:01:52,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37311
2024-02-25 07:01:52,822 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37311
2024-02-25 07:01:52,822 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35045
2024-02-25 07:01:52,822 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,822 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,822 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,822 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dnqfxn7v
2024-02-25 07:01:52,822 - distributed.worker - INFO - Starting Worker plugin PreImport-4818b27f-b996-4108-97b3-06d364d12931
2024-02-25 07:01:52,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9380a86b-33fa-4111-a3cc-b75883349d8a
2024-02-25 07:01:52,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0cffcf1-46d8-4fd6-9365-a7ad45033111
2024-02-25 07:01:52,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,854 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:01:52,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:01:52,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,859 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41681
2024-02-25 07:01:52,859 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41681
2024-02-25 07:01:52,859 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44585
2024-02-25 07:01:52,859 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,859 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,859 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,859 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:01:52,859 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ty8bju6d
2024-02-25 07:01:52,860 - distributed.worker - INFO - Starting Worker plugin PreImport-2091b62b-ee15-4182-87d5-3cb57900d98a
2024-02-25 07:01:52,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-743130fd-0bdd-4af8-a4f9-c46d1f76659e
2024-02-25 07:01:52,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40499
2024-02-25 07:01:52,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40499
2024-02-25 07:01:52,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39165
2024-02-25 07:01:52,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:01:52,860 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:52,860 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:01:52,861 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:01:52,861 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zhs4c2ti
2024-02-25 07:01:52,861 - distributed.worker - INFO - Starting Worker plugin PreImport-15b07801-5443-4562-87ce-f2049e6dae71
2024-02-25 07:01:52,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8d641cd-f6af-4c64-9597-085854c0dfe2
2024-02-25 07:01:53,946 - distributed.scheduler - INFO - Receive client connection: Client-c1cd7e65-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:01:53,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45970
2024-02-25 07:01:54,995 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b90e1ea5-7071-4ed7-b8fd-2b63ed612688
2024-02-25 07:01:54,996 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45865', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45865
2024-02-25 07:01:55,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45990
2024-02-25 07:01:55,029 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,030 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,030 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,299 - distributed.worker - INFO - Starting Worker plugin PreImport-1352a82e-bcf8-475e-95cf-34be84e5fc22
2024-02-25 07:01:55,301 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,334 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35859', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,335 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35859
2024-02-25 07:01:55,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45994
2024-02-25 07:01:55,334 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,337 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,337 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,346 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,347 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8dea1d39-9a9b-4df4-b9cb-8639057b4f37
2024-02-25 07:01:55,349 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,352 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1dfefaad-d40c-4925-82d2-a78271082ddf
2024-02-25 07:01:55,354 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,365 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14cd2772-b564-4b2e-8e5d-d79c763d9bae
2024-02-25 07:01:55,366 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,371 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44781', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,372 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44781
2024-02-25 07:01:55,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45998
2024-02-25 07:01:55,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,374 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37311', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,374 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,374 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,375 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37311
2024-02-25 07:01:55,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46024
2024-02-25 07:01:55,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,376 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41631', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,379 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41631
2024-02-25 07:01:55,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46006
2024-02-25 07:01:55,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41681', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,380 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41681
2024-02-25 07:01:55,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46032
2024-02-25 07:01:55,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,382 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,382 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,387 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35897', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,387 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35897
2024-02-25 07:01:55,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46012
2024-02-25 07:01:55,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,389 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,390 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,391 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40499', status: init, memory: 0, processing: 0>
2024-02-25 07:01:55,391 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40499
2024-02-25 07:01:55,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46040
2024-02-25 07:01:55,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:01:55,393 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:01:55,393 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:01:55,394 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:01:55,462 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,462 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,463 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,463 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,463 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,463 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,463 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,466 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,466 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,466 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,466 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,466 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,466 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,468 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:01:55,473 - distributed.scheduler - INFO - Remove client Client-c1cd7e65-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:01:55,473 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45970; closing.
2024-02-25 07:01:55,474 - distributed.scheduler - INFO - Remove client Client-c1cd7e65-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:01:55,476 - distributed.scheduler - INFO - Close client connection: Client-c1cd7e65-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,481 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:01:55,490 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:01:55,492 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:01:55,494 - distributed.scheduler - INFO - Remove client Client-bebf7a4c-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:55,494 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45958; closing.
2024-02-25 07:01:55,495 - distributed.scheduler - INFO - Remove client Client-bebf7a4c-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:55,495 - distributed.scheduler - INFO - Close client connection: Client-bebf7a4c-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:01:55,496 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39709'. Reason: nanny-close
2024-02-25 07:01:55,496 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,497 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39453'. Reason: nanny-close
2024-02-25 07:01:55,497 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,497 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33527'. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44781. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36133'. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41631. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35005'. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45865. Reason: nanny-close
2024-02-25 07:01:55,498 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39523'. Reason: nanny-close
2024-02-25 07:01:55,499 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37311. Reason: nanny-close
2024-02-25 07:01:55,499 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38299'. Reason: nanny-close
2024-02-25 07:01:55,499 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,500 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44525'. Reason: nanny-close
2024-02-25 07:01:55,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35897. Reason: nanny-close
2024-02-25 07:01:55,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35859. Reason: nanny-close
2024-02-25 07:01:55,500 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:01:55,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,500 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45998; closing.
2024-02-25 07:01:55,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40499. Reason: nanny-close
2024-02-25 07:01:55,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,500 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.500773')
2024-02-25 07:01:55,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,501 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41681. Reason: nanny-close
2024-02-25 07:01:55,501 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45990; closing.
2024-02-25 07:01:55,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46006; closing.
2024-02-25 07:01:55,502 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,502 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,502 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,502 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,502 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,502 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,502 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45865', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.5031867')
2024-02-25 07:01:55,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:01:55,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41631', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.50358')
2024-02-25 07:01:55,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46024; closing.
2024-02-25 07:01:55,504 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,504 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,504 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,505 - distributed.nanny - INFO - Worker closed
2024-02-25 07:01:55,505 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37311', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.505331')
2024-02-25 07:01:55,505 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45994; closing.
2024-02-25 07:01:55,506 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46040; closing.
2024-02-25 07:01:55,506 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46012; closing.
2024-02-25 07:01:55,506 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.5066538')
2024-02-25 07:01:55,507 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40499', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.507034')
2024-02-25 07:01:55,507 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35897', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.507386')
2024-02-25 07:01:55,507 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46032; closing.
2024-02-25 07:01:55,508 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41681', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844515.5082064')
2024-02-25 07:01:55,508 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:01:56,213 - distributed.scheduler - INFO - Receive client connection: Client-c327918c-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:01:56,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46046
2024-02-25 07:01:56,462 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:01:56,463 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:01:56,464 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:01:56,465 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:01:56,466 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-25 07:01:58,843 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:58,848 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33735 instead
  warnings.warn(
2024-02-25 07:01:58,852 - distributed.scheduler - INFO - State start
2024-02-25 07:01:58,876 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:01:58,877 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-25 07:01:58,877 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:01:58,878 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-25 07:01:59,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38251'
2024-02-25 07:01:59,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40571'
2024-02-25 07:01:59,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41935'
2024-02-25 07:01:59,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33857'
2024-02-25 07:01:59,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46407'
2024-02-25 07:01:59,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44109'
2024-02-25 07:01:59,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39147'
2024-02-25 07:01:59,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43227'
2024-02-25 07:02:01,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,210 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,211 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42443
2024-02-25 07:02:01,211 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42443
2024-02-25 07:02:01,211 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36569
2024-02-25 07:02:01,212 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,212 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,212 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,212 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,212 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zyfh3w0h
2024-02-25 07:02:01,212 - distributed.worker - INFO - Starting Worker plugin PreImport-8bf65c36-cb48-48c7-a4da-debe63407228
2024-02-25 07:02:01,212 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fb45b71-2d92-4ad0-b42d-6b9bfbb92b57
2024-02-25 07:02:01,213 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fecc500-b694-4cd9-aca1-72a3ff05ab07
2024-02-25 07:02:01,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35491
2024-02-25 07:02:01,266 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35491
2024-02-25 07:02:01,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46671
2024-02-25 07:02:01,266 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,266 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,266 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,266 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5s0pyaoa
2024-02-25 07:02:01,266 - distributed.worker - INFO - Starting Worker plugin PreImport-414c38bb-52ab-47a9-91ab-3ca800eaed4d
2024-02-25 07:02:01,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-388bb421-5c16-4039-b0b3-efb46f9dc612
2024-02-25 07:02:01,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,268 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33681
2024-02-25 07:02:01,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33681
2024-02-25 07:02:01,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41355
2024-02-25 07:02:01,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,269 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,269 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,269 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xqab6cwv
2024-02-25 07:02:01,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38189
2024-02-25 07:02:01,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38189
2024-02-25 07:02:01,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34991
2024-02-25 07:02:01,269 - distributed.worker - INFO - Starting Worker plugin PreImport-d6b80742-8095-451f-886c-914c21cfaf76
2024-02-25 07:02:01,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,269 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,269 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44935
2024-02-25 07:02:01,269 - distributed.worker - INFO - Starting Worker plugin RMMSetup-400163f4-fb91-4ed6-9bea-3a200cb4bd6d
2024-02-25 07:02:01,269 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44935
2024-02-25 07:02:01,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oqxx6o_t
2024-02-25 07:02:01,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41221
2024-02-25 07:02:01,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,269 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,269 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,270 - distributed.worker - INFO - Starting Worker plugin PreImport-7e8930dd-1ddf-496b-bd26-1d6ddf95f857
2024-02-25 07:02:01,270 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l0wdp8md
2024-02-25 07:02:01,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e16d15d5-9c63-4676-9bf5-4afeb4439956
2024-02-25 07:02:01,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ba00b21-db32-48ae-9fef-25a5c52b3f38
2024-02-25 07:02:01,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-955e2b2b-2beb-421b-8a90-db2fe755b4d8
2024-02-25 07:02:01,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14c92b41-df4a-4d92-8f29-5aa99157583c
2024-02-25 07:02:01,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,427 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34179
2024-02-25 07:02:01,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34179
2024-02-25 07:02:01,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45135
2024-02-25 07:02:01,427 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,428 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,428 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,428 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c6qzixgh
2024-02-25 07:02:01,428 - distributed.worker - INFO - Starting Worker plugin PreImport-42ba85d3-fdc1-4dc1-911e-21acb0659bfe
2024-02-25 07:02:01,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-162f22ce-6d6e-4e9a-b120-037cd9207a98
2024-02-25 07:02:01,428 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cc6140f-88ae-425d-8c6f-7390ab162fdb
2024-02-25 07:02:01,430 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36605
2024-02-25 07:02:01,430 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36605
2024-02-25 07:02:01,430 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34203
2024-02-25 07:02:01,430 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,430 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,430 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,430 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,430 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ynadi9le
2024-02-25 07:02:01,430 - distributed.worker - INFO - Starting Worker plugin PreImport-706f6966-5b68-4656-af99-1e765f9cd380
2024-02-25 07:02:01,430 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:01,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:01,431 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0c5cbf9-32c1-40de-831b-bc63f1b24ce6
2024-02-25 07:02:01,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:01,439 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43097
2024-02-25 07:02:01,439 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43097
2024-02-25 07:02:01,439 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33399
2024-02-25 07:02:01,439 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:01,439 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:01,439 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:01,439 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:01,439 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-97pxil3e
2024-02-25 07:02:01,439 - distributed.worker - INFO - Starting Worker plugin PreImport-64e597f8-7d63-4b64-a2d5-bcc5868ecf04
2024-02-25 07:02:01,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bcbd9128-6569-469d-86b5-633e58a51de8
2024-02-25 07:02:04,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-927cfa85-979c-461f-8c5a-1675c9f92a7b
2024-02-25 07:02:04,877 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,879 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,884 - distributed.worker - INFO - Starting Worker plugin PreImport-7c3343d3-fa45-4781-a8f3-f67b294ead7f
2024-02-25 07:02:04,886 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30a73917-d69f-490a-81f3-c736a852362a
2024-02-25 07:02:04,889 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,901 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,901 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cec251b-46af-4603-931b-c89229577068
2024-02-25 07:02:04,901 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,915 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,915 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,918 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,918 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,924 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08a2d555-e0d0-41d4-b63a-40c6f5249dc3
2024-02-25 07:02:04,925 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,927 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,927 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,930 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,941 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,952 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,958 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:04,980 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:04,980 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:04,982 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:04,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38251'. Reason: nanny-close
2024-02-25 07:02:04,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:04,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40571'. Reason: nanny-close
2024-02-25 07:02:04,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41935'. Reason: nanny-close
2024-02-25 07:02:04,998 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:04,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42443. Reason: nanny-close
2024-02-25 07:02:04,998 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33857'. Reason: nanny-close
2024-02-25 07:02:04,999 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:04,999 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35491. Reason: nanny-close
2024-02-25 07:02:04,999 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46407'. Reason: nanny-close
2024-02-25 07:02:04,999 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:04,999 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44109'. Reason: nanny-close
2024-02-25 07:02:05,000 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:05,000 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38189. Reason: nanny-close
2024-02-25 07:02:05,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39147'. Reason: nanny-close
2024-02-25 07:02:05,000 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:05,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43227'. Reason: nanny-close
2024-02-25 07:02:05,000 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33681. Reason: nanny-close
2024-02-25 07:02:05,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:05,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,001 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44935. Reason: nanny-close
2024-02-25 07:02:05,001 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36605. Reason: nanny-close
2024-02-25 07:02:05,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43097. Reason: nanny-close
2024-02-25 07:02:05,002 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,003 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,003 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,003 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,003 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,003 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,003 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,004 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,005 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,005 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,005 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:05,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:05,024 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34179. Reason: nanny-close
2024-02-25 07:02:05,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:05,028 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-25 07:02:08,736 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:08,741 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39359 instead
  warnings.warn(
2024-02-25 07:02:08,745 - distributed.scheduler - INFO - State start
2024-02-25 07:02:08,767 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:08,768 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-25 07:02:08,768 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:08,769 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-25 07:02:09,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34355'
2024-02-25 07:02:09,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44047'
2024-02-25 07:02:09,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36599'
2024-02-25 07:02:09,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32797'
2024-02-25 07:02:09,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43699'
2024-02-25 07:02:09,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42013'
2024-02-25 07:02:09,084 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35001'
2024-02-25 07:02:09,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46581'
2024-02-25 07:02:10,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:10,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:10,979 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:10,980 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45003
2024-02-25 07:02:10,980 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45003
2024-02-25 07:02:10,980 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38107
2024-02-25 07:02:10,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:10,980 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:10,980 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:10,980 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:10,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-26_jcfet
2024-02-25 07:02:10,980 - distributed.worker - INFO - Starting Worker plugin PreImport-3cc5985b-7c95-4799-869b-9723e3cc794f
2024-02-25 07:02:10,980 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-125a5502-1748-4bbf-bef1-923c04a666d8
2024-02-25 07:02:10,981 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a13ba32f-cb76-4b82-8c41-c6cc76a0021d
2024-02-25 07:02:10,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:10,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:10,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:10,991 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37047
2024-02-25 07:02:10,991 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37047
2024-02-25 07:02:10,991 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40417
2024-02-25 07:02:10,991 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:10,991 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:10,991 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:10,992 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:10,992 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vcaw3x5k
2024-02-25 07:02:10,992 - distributed.worker - INFO - Starting Worker plugin PreImport-40bbc7df-2dda-4d04-b4d5-d01d5ce6ca41
2024-02-25 07:02:10,992 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcf19633-a494-4733-ad1b-e585cc315d9d
2024-02-25 07:02:10,992 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5b0c3f8-305e-48ce-8edc-6b1cd3431bd7
2024-02-25 07:02:11,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:11,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:11,846 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:11,848 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34249
2024-02-25 07:02:11,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34249
2024-02-25 07:02:11,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40375
2024-02-25 07:02:11,848 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:11,848 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:11,848 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:11,848 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:11,848 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m748tftg
2024-02-25 07:02:11,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38cf190f-da1b-4707-ad25-af760df55367
2024-02-25 07:02:11,848 - distributed.worker - INFO - Starting Worker plugin PreImport-c27996d2-73b4-4e54-880c-234c82c6da33
2024-02-25 07:02:11,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa2be78c-259c-4c09-9809-33206496104a
2024-02-25 07:02:11,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:11,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:11,998 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:11,999 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45095
2024-02-25 07:02:11,999 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45095
2024-02-25 07:02:11,999 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42599
2024-02-25 07:02:11,999 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:11,999 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:11,999 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:11,999 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:11,999 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qdhsoljk
2024-02-25 07:02:12,000 - distributed.worker - INFO - Starting Worker plugin PreImport-b7503689-018b-4bea-8417-a0321e7f3400
2024-02-25 07:02:12,000 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7d5dfed-a57e-47d6-8011-1522aa9dc970
2024-02-25 07:02:12,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:12,042 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:12,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:12,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:12,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:12,051 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46115
2024-02-25 07:02:12,051 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46115
2024-02-25 07:02:12,051 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45467
2024-02-25 07:02:12,051 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:12,051 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:12,051 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:12,051 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:12,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-az8sz6vl
2024-02-25 07:02:12,052 - distributed.worker - INFO - Starting Worker plugin PreImport-9d4af914-698b-4565-938e-d0d3f956e9fb
2024-02-25 07:02:12,052 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a51a6149-a20a-4480-ace7-b6559d71bd04
2024-02-25 07:02:12,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:12,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46675
2024-02-25 07:02:12,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46675
2024-02-25 07:02:12,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33083
2024-02-25 07:02:12,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:12,055 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:12,055 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:12,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:12,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j960vhp7
2024-02-25 07:02:12,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:12,055 - distributed.worker - INFO - Starting Worker plugin PreImport-e2369b59-159a-4a5b-ac1e-107d4d1f3723
2024-02-25 07:02:12,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:12,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba4bade6-ee68-4376-a6b5-24e9b8c73bb2
2024-02-25 07:02:12,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:12,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:12,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:12,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39019
2024-02-25 07:02:12,065 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39019
2024-02-25 07:02:12,065 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40813
2024-02-25 07:02:12,065 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:12,065 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:12,065 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:12,065 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:12,065 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k65foc7l
2024-02-25 07:02:12,065 - distributed.worker - INFO - Starting Worker plugin PreImport-f6798f3c-cbe3-4a50-82cf-b374c8d69e3a
2024-02-25 07:02:12,066 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84fe8959-7de9-430b-8179-9b2b3e6f165c
2024-02-25 07:02:12,066 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73f454df-b59f-4389-8092-077ebd0cfcb1
2024-02-25 07:02:12,067 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:12,068 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39341
2024-02-25 07:02:12,068 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39341
2024-02-25 07:02:12,068 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39987
2024-02-25 07:02:12,068 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:12,068 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:12,068 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:12,068 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:12,069 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gzgbdroy
2024-02-25 07:02:12,069 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bab93518-3cba-4573-b200-ae93411bb3a5
2024-02-25 07:02:12,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c746525-6c24-48f2-827d-7d27dbc8a397
2024-02-25 07:02:13,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34355'. Reason: nanny-close
2024-02-25 07:02:13,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44047'. Reason: nanny-close
2024-02-25 07:02:13,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36599'. Reason: nanny-close
2024-02-25 07:02:13,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32797'. Reason: nanny-close
2024-02-25 07:02:13,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43699'. Reason: nanny-close
2024-02-25 07:02:13,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42013'. Reason: nanny-close
2024-02-25 07:02:13,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35001'. Reason: nanny-close
2024-02-25 07:02:13,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46581'. Reason: nanny-close
2024-02-25 07:02:14,982 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,001 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1496ca19-1fee-44b5-adde-3097ee9183e9
2024-02-25 07:02:15,020 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,074 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8989cf0-b6f8-4924-aeef-4eff8ead7aaf
2024-02-25 07:02:15,076 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54b8961c-1b1b-4674-bae4-2ce6e0aa6ed9
2024-02-25 07:02:15,086 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,092 - distributed.worker - INFO - Starting Worker plugin PreImport-32f0e947-0ed3-4800-a580-6a4a5d2722d0
2024-02-25 07:02:15,093 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,108 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:15,114 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:17,610 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:17,611 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:17,611 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:17,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:17,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:17,649 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34249. Reason: nanny-close
2024-02-25 07:02:17,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:17,654 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:17,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:17,661 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:17,661 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:17,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:17,698 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:17,699 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39019. Reason: nanny-close
2024-02-25 07:02:17,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:17,703 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:17,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:17,774 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:17,774 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:17,776 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:17,805 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:17,806 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37047. Reason: nanny-close
2024-02-25 07:02:17,808 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:17,809 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:42278 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-25 07:02:17,974 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53602 parent=53403 started daemon>
2024-02-25 07:02:17,975 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53598 parent=53403 started daemon>
2024-02-25 07:02:17,975 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53594 parent=53403 started daemon>
2024-02-25 07:02:17,975 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53585 parent=53403 started daemon>
2024-02-25 07:02:17,975 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53580 parent=53403 started daemon>
2024-02-25 07:02:17,975 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53575 parent=53403 started daemon>
2024-02-25 07:02:17,976 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53571 parent=53403 started daemon>
2024-02-25 07:02:18,011 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 53585 exit status was already read will report exitcode 255
2024-02-25 07:02:18,178 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 53598 exit status was already read will report exitcode 255
2024-02-25 07:02:18,244 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 53594 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-25 07:02:20,566 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:20,571 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39819 instead
  warnings.warn(
2024-02-25 07:02:20,575 - distributed.scheduler - INFO - State start
2024-02-25 07:02:20,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gzgbdroy', purging
2024-02-25 07:02:20,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qdhsoljk', purging
2024-02-25 07:02:20,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-26_jcfet', purging
2024-02-25 07:02:20,578 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-j960vhp7', purging
2024-02-25 07:02:20,578 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-az8sz6vl', purging
2024-02-25 07:02:20,951 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:20,953 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-25 07:02:20,953 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:20,954 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-25 07:02:21,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45315'
2024-02-25 07:02:21,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45315'. Reason: nanny-close
2024-02-25 07:02:22,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:22,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:23,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:23,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44313
2024-02-25 07:02:23,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44313
2024-02-25 07:02:23,342 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-25 07:02:23,343 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:23,343 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:23,343 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:23,343 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-25 07:02:23,343 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qwko7x38
2024-02-25 07:02:23,343 - distributed.worker - INFO - Starting Worker plugin PreImport-fb4ad1dc-b06c-4875-8beb-088dd1c68990
2024-02-25 07:02:23,343 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9381e213-e519-4e5d-8b70-963e395b91b1
2024-02-25 07:02:23,343 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1e61209-1a09-47f5-bb8c-6f49e28868e3
2024-02-25 07:02:23,344 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:26,027 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:26,028 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:26,029 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:26,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:26,065 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:26,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44313. Reason: nanny-close
2024-02-25 07:02:26,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:26,070 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-25 07:02:30,887 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:30,891 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-25 07:02:30,894 - distributed.scheduler - INFO - State start
2024-02-25 07:02:30,915 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:30,916 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:02:30,916 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-25 07:02:30,916 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:02:31,016 - distributed.scheduler - INFO - Receive client connection: Client-d7af17d2-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:02:31,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42726
2024-02-25 07:02:31,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36649'
2024-02-25 07:02:31,186 - distributed.scheduler - INFO - Receive client connection: Client-d6bcd348-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:31,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42792
2024-02-25 07:02:32,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:32,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:33,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:33,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42023
2024-02-25 07:02:33,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42023
2024-02-25 07:02:33,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44245
2024-02-25 07:02:33,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:33,158 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:33,158 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:33,158 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-25 07:02:33,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sf1hiq6_
2024-02-25 07:02:33,159 - distributed.worker - INFO - Starting Worker plugin PreImport-0013bb14-dc2a-42b5-a6b1-47d8072edc57
2024-02-25 07:02:33,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76163032-7ddd-4e6e-9bce-62c5c7666602
2024-02-25 07:02:33,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9cf97959-58cb-4497-8659-aa092e6cee06
2024-02-25 07:02:33,161 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:33,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42023', status: init, memory: 0, processing: 0>
2024-02-25 07:02:33,213 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42023
2024-02-25 07:02:33,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42806
2024-02-25 07:02:33,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:33,215 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:33,215 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:33,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:33,269 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:33,271 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:33,272 - distributed.scheduler - INFO - Remove client Client-d6bcd348-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:33,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42792; closing.
2024-02-25 07:02:33,272 - distributed.scheduler - INFO - Remove client Client-d6bcd348-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:33,273 - distributed.scheduler - INFO - Close client connection: Client-d6bcd348-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:33,274 - distributed.scheduler - INFO - Remove client Client-d7af17d2-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:02:33,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36649'. Reason: nanny-close
2024-02-25 07:02:33,274 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42726; closing.
2024-02-25 07:02:33,274 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:33,274 - distributed.scheduler - INFO - Remove client Client-d7af17d2-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:02:33,274 - distributed.scheduler - INFO - Close client connection: Client-d7af17d2-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:02:33,275 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42023. Reason: nanny-close
2024-02-25 07:02:33,277 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42806; closing.
2024-02-25 07:02:33,277 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:33,277 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844553.2779074')
2024-02-25 07:02:33,278 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:02:33,279 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:33,839 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:02:33,839 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:02:33,840 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:33,841 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:02:33,841 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-25 07:02:36,033 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:36,037 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-25 07:02:36,040 - distributed.scheduler - INFO - State start
2024-02-25 07:02:36,062 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:36,063 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:02:36,063 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-25 07:02:36,064 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:02:36,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34229', status: init, memory: 0, processing: 0>
2024-02-25 07:02:36,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34229
2024-02-25 07:02:36,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42914
2024-02-25 07:02:36,619 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42914; closing.
2024-02-25 07:02:36,619 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34229', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844556.6193511')
2024-02-25 07:02:36,619 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:02:38,501 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:42908'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42908>: Stream is closed
2024-02-25 07:02:38,765 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:02:38,765 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:02:38,766 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:38,767 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:02:38,767 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-25 07:02:40,987 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:40,991 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-25 07:02:40,995 - distributed.scheduler - INFO - State start
2024-02-25 07:02:41,015 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:41,016 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-25 07:02:41,017 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-25 07:02:41,017 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:02:41,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37717'
2024-02-25 07:02:41,572 - distributed.scheduler - INFO - Receive client connection: Client-dcc45ccf-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:41,582 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45186
2024-02-25 07:02:42,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:42,686 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:42,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:42,690 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42265
2024-02-25 07:02:42,690 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42265
2024-02-25 07:02:42,691 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42779
2024-02-25 07:02:42,691 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-25 07:02:42,691 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:42,691 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:42,691 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-25 07:02:42,691 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-kmfrgum5
2024-02-25 07:02:42,691 - distributed.worker - INFO - Starting Worker plugin PreImport-5f5f50bf-0246-4c50-8136-4c3ae73caa5f
2024-02-25 07:02:42,691 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9ac3be4-15db-4a87-8bad-cce7fd662319
2024-02-25 07:02:42,691 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0041f5b7-565c-40c4-b150-17a3998d554a
2024-02-25 07:02:42,695 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:42,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42265', status: init, memory: 0, processing: 0>
2024-02-25 07:02:42,743 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42265
2024-02-25 07:02:42,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45198
2024-02-25 07:02:42,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:42,744 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-25 07:02:42,744 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:42,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-25 07:02:42,807 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:42,809 - distributed.scheduler - INFO - Remove client Client-dcc45ccf-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:42,809 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45186; closing.
2024-02-25 07:02:42,810 - distributed.scheduler - INFO - Remove client Client-dcc45ccf-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:42,810 - distributed.scheduler - INFO - Close client connection: Client-dcc45ccf-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:42,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37717'. Reason: nanny-close
2024-02-25 07:02:42,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:42,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42265. Reason: nanny-close
2024-02-25 07:02:42,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-25 07:02:42,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45198; closing.
2024-02-25 07:02:42,814 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844562.8145516')
2024-02-25 07:02:42,814 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:02:42,815 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:43,326 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:02:43,326 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:02:43,327 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:43,328 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-25 07:02:43,328 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-25 07:02:45,444 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:45,449 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-25 07:02:45,452 - distributed.scheduler - INFO - State start
2024-02-25 07:02:45,473 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:45,474 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:02:45,475 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-25 07:02:45,475 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:02:45,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39119'
2024-02-25 07:02:45,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42371'
2024-02-25 07:02:45,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38395'
2024-02-25 07:02:45,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45707'
2024-02-25 07:02:45,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43739'
2024-02-25 07:02:45,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37335'
2024-02-25 07:02:45,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41473'
2024-02-25 07:02:45,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44471'
2024-02-25 07:02:46,273 - distributed.scheduler - INFO - Receive client connection: Client-df6c9aef-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:46,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44792
2024-02-25 07:02:47,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,541 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,542 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41065
2024-02-25 07:02:47,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41065
2024-02-25 07:02:47,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36183
2024-02-25 07:02:47,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,542 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,542 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ogfcz0ax
2024-02-25 07:02:47,542 - distributed.worker - INFO - Starting Worker plugin PreImport-cc58a138-ad87-41d9-ae52-d7d8f759ec28
2024-02-25 07:02:47,543 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62ad4097-93bf-41d4-a0e0-f430dfbc0c9b
2024-02-25 07:02:47,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,549 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39857
2024-02-25 07:02:47,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39857
2024-02-25 07:02:47,550 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35041
2024-02-25 07:02:47,550 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,550 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,550 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,550 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,550 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n9m_79zp
2024-02-25 07:02:47,550 - distributed.worker - INFO - Starting Worker plugin PreImport-733ffd0c-e758-409f-ae8e-7ed89d0fc6d1
2024-02-25 07:02:47,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fc35142-1b35-43ff-b36a-ca03cceec173
2024-02-25 07:02:47,550 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbba55e1-5516-4625-9463-56fe18522ef1
2024-02-25 07:02:47,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43865
2024-02-25 07:02:47,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43989
2024-02-25 07:02:47,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43865
2024-02-25 07:02:47,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43989
2024-02-25 07:02:47,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46311
2024-02-25 07:02:47,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42569
2024-02-25 07:02:47,810 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,810 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,810 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,810 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,811 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,811 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dpgpf58s
2024-02-25 07:02:47,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lx9bo71p
2024-02-25 07:02:47,811 - distributed.worker - INFO - Starting Worker plugin PreImport-d4b74733-c94a-411c-b992-ae2f82a7f2dc
2024-02-25 07:02:47,811 - distributed.worker - INFO - Starting Worker plugin PreImport-98d31fc6-bcb8-4b03-9cc3-caab6e500046
2024-02-25 07:02:47,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b2327b1-9e97-435d-9466-99d9e7b338a9
2024-02-25 07:02:47,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82314f4f-b34a-4781-8912-bfc0eb8da8c5
2024-02-25 07:02:47,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72cc244e-ba50-4b83-8ac3-0d7e9aa75b25
2024-02-25 07:02:47,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7115d98-6716-439b-b2ef-df650083c48e
2024-02-25 07:02:47,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:47,964 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:47,965 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,965 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46115
2024-02-25 07:02:47,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46115
2024-02-25 07:02:47,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40147
2024-02-25 07:02:47,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,967 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33283
2024-02-25 07:02:47,967 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33283
2024-02-25 07:02:47,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7zsle4ww
2024-02-25 07:02:47,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42537
2024-02-25 07:02:47,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,967 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,967 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5lcslpah
2024-02-25 07:02:47,967 - distributed.worker - INFO - Starting Worker plugin PreImport-a5c20cf1-1baf-4311-95d4-44f2834e321b
2024-02-25 07:02:47,967 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72eb0d8c-02e2-466c-aad5-f13e73cb127a
2024-02-25 07:02:47,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34249
2024-02-25 07:02:47,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34249
2024-02-25 07:02:47,967 - distributed.worker - INFO - Starting Worker plugin PreImport-0db9c2f6-39b6-4535-8b08-09557dcd60c6
2024-02-25 07:02:47,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33863
2024-02-25 07:02:47,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,968 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b380ad98-92b1-4873-96e4-1b7b07ff014d
2024-02-25 07:02:47,968 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,968 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,968 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8lw7sacs
2024-02-25 07:02:47,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af640b58-236b-4c8f-b5a7-44e7e0ae7682
2024-02-25 07:02:47,970 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3a19edd-3916-499a-af92-93eb10686e71
2024-02-25 07:02:47,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:47,980 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37969
2024-02-25 07:02:47,980 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37969
2024-02-25 07:02:47,980 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41813
2024-02-25 07:02:47,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:47,980 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:47,980 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:47,980 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-25 07:02:47,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m51d94j3
2024-02-25 07:02:47,981 - distributed.worker - INFO - Starting Worker plugin PreImport-f830b84e-9046-4350-b55d-5ef0cb34118d
2024-02-25 07:02:47,981 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b90ce02-47ee-4397-8841-5d63c7e054d3
2024-02-25 07:02:48,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fc113d1-4176-4a8f-bde1-92d804e73d2b
2024-02-25 07:02:48,174 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:48,204 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41065', status: init, memory: 0, processing: 0>
2024-02-25 07:02:48,205 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41065
2024-02-25 07:02:48,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44810
2024-02-25 07:02:48,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:48,207 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:48,208 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:48,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:48,599 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:48,633 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39857', status: init, memory: 0, processing: 0>
2024-02-25 07:02:48,634 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39857
2024-02-25 07:02:48,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44812
2024-02-25 07:02:48,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:48,636 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:48,636 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:48,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:48,966 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44808'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44808>: Stream is closed
2024-02-25 07:02:49,580 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43989', status: init, memory: 0, processing: 0>
2024-02-25 07:02:49,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43989
2024-02-25 07:02:49,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44820
2024-02-25 07:02:49,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:49,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:49,607 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:49,610 - distributed.worker - INFO - Starting Worker plugin PreImport-acf5823a-98cb-45f3-89a9-e9fa40817470
2024-02-25 07:02:49,610 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e35bdf2-f977-4d79-a564-92b059134b6f
2024-02-25 07:02:49,633 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,635 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34249', status: init, memory: 0, processing: 0>
2024-02-25 07:02:49,635 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13084f65-3a45-4d27-abf6-16e4cf53b275
2024-02-25 07:02:49,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34249
2024-02-25 07:02:49,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44824
2024-02-25 07:02:49,636 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,637 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:49,638 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:49,638 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:49,642 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4afc0b46-0fdb-4206-9056-c73dbf918dbd
2024-02-25 07:02:49,644 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46115', status: init, memory: 0, processing: 0>
2024-02-25 07:02:49,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46115
2024-02-25 07:02:49,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44846
2024-02-25 07:02:49,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:49,659 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:49,659 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:49,665 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33283', status: init, memory: 0, processing: 0>
2024-02-25 07:02:49,665 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33283
2024-02-25 07:02:49,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44840
2024-02-25 07:02:49,667 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37969', status: init, memory: 0, processing: 0>
2024-02-25 07:02:49,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:49,667 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37969
2024-02-25 07:02:49,668 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44860
2024-02-25 07:02:49,668 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:49,668 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:49,669 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:49,669 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,670 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:49,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:49,673 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43865', status: init, memory: 0, processing: 0>
2024-02-25 07:02:49,673 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43865
2024-02-25 07:02:49,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44874
2024-02-25 07:02:49,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:49,675 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:49,676 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:49,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,715 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-25 07:02:49,727 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,727 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,727 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:49,733 - distributed.scheduler - INFO - Remove client Client-df6c9aef-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:49,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44792; closing.
2024-02-25 07:02:49,733 - distributed.scheduler - INFO - Remove client Client-df6c9aef-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:49,733 - distributed.scheduler - INFO - Close client connection: Client-df6c9aef-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:49,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39119'. Reason: nanny-close
2024-02-25 07:02:49,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42371'. Reason: nanny-close
2024-02-25 07:02:49,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38395'. Reason: nanny-close
2024-02-25 07:02:49,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39857. Reason: nanny-close
2024-02-25 07:02:49,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45707'. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43865. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43739'. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46115. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37335'. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43989. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41473'. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33283. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44471'. Reason: nanny-close
2024-02-25 07:02:49,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34249. Reason: nanny-close
2024-02-25 07:02:49,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:49,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44812; closing.
2024-02-25 07:02:49,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41065. Reason: nanny-close
2024-02-25 07:02:49,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37969. Reason: nanny-close
2024-02-25 07:02:49,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.7396388')
2024-02-25 07:02:49,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,740 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44846; closing.
2024-02-25 07:02:49,740 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44874; closing.
2024-02-25 07:02:49,740 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,740 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,741 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.7410238')
2024-02-25 07:02:49,741 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,741 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,741 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43865', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.7414005')
2024-02-25 07:02:49,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:49,742 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44820; closing.
2024-02-25 07:02:49,743 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,743 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,743 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:49,743 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44874>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-25 07:02:49,746 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44846>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-25 07:02:49,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44840; closing.
2024-02-25 07:02:49,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43989', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.7465544')
2024-02-25 07:02:49,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44860; closing.
2024-02-25 07:02:49,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44810; closing.
2024-02-25 07:02:49,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44824; closing.
2024-02-25 07:02:49,747 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33283', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.747614')
2024-02-25 07:02:49,748 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.748021')
2024-02-25 07:02:49,748 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41065', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.7484853')
2024-02-25 07:02:49,748 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844569.7488174')
2024-02-25 07:02:49,749 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:02:50,650 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:02:50,651 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:02:50,652 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:50,653 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:02:50,653 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-25 07:02:52,913 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:52,918 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-25 07:02:52,922 - distributed.scheduler - INFO - State start
2024-02-25 07:02:52,944 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:52,945 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-25 07:02:52,946 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-25 07:02:52,946 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-25 07:02:53,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43603'
2024-02-25 07:02:54,103 - distributed.scheduler - INFO - Receive client connection: Client-e5a8d7b5-d3ab-11ee-8b84-d8c49764f6bb
2024-02-25 07:02:54,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36602
2024-02-25 07:02:54,405 - distributed.scheduler - INFO - Receive client connection: Client-e3da5dc8-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:54,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36622
2024-02-25 07:02:54,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:02:54,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:02:54,731 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:02:54,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37759
2024-02-25 07:02:54,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37759
2024-02-25 07:02:54,732 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43665
2024-02-25 07:02:54,732 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:02:54,732 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:54,732 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:02:54,732 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-25 07:02:54,732 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b61luye9
2024-02-25 07:02:54,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5aac9914-5d10-4508-a51f-fbe9337d1e91
2024-02-25 07:02:54,733 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd23a833-402e-43d8-8b5e-837e4c5e37dd
2024-02-25 07:02:55,013 - distributed.worker - INFO - Starting Worker plugin PreImport-1f000edb-1e04-47a9-a4ee-8245b97d02d4
2024-02-25 07:02:55,013 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:55,056 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37759', status: init, memory: 0, processing: 0>
2024-02-25 07:02:55,058 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37759
2024-02-25 07:02:55,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36626
2024-02-25 07:02:55,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:02:55,059 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:02:55,059 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:02:55,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:02:55,123 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-25 07:02:55,127 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:55,129 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-25 07:02:55,131 - distributed.scheduler - INFO - Remove client Client-e3da5dc8-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:55,131 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36622; closing.
2024-02-25 07:02:55,132 - distributed.scheduler - INFO - Remove client Client-e3da5dc8-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:55,132 - distributed.scheduler - INFO - Close client connection: Client-e3da5dc8-d3ab-11ee-8a5b-d8c49764f6bb
2024-02-25 07:02:55,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43603'. Reason: nanny-close
2024-02-25 07:02:55,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:02:55,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37759. Reason: nanny-close
2024-02-25 07:02:55,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:02:55,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36626; closing.
2024-02-25 07:02:55,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708844575.1373134')
2024-02-25 07:02:55,137 - distributed.scheduler - INFO - Lost all workers
2024-02-25 07:02:55,138 - distributed.nanny - INFO - Worker closed
2024-02-25 07:02:55,799 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-25 07:02:55,799 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-25 07:02:55,800 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:55,801 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-25 07:02:55,801 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-25 07:02:58,037 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:58,041 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38149 instead
  warnings.warn(
2024-02-25 07:02:58,045 - distributed.scheduler - INFO - State start
2024-02-25 07:02:58,241 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-25 07:02:58,242 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-25 07:02:58,243 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-25 07:02:58,244 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-25 07:02:58,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34023'
2024-02-25 07:02:59,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34023'. Reason: nanny-close
2024-02-25 07:03:00,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-25 07:03:00,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-25 07:03:00,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-25 07:03:00,511 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39611
2024-02-25 07:03:00,511 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39611
2024-02-25 07:03:00,511 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41661
2024-02-25 07:03:00,511 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-25 07:03:00,511 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:03:00,511 - distributed.worker - INFO -               Threads:                          1
2024-02-25 07:03:00,511 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-25 07:03:00,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-105j6_yh
2024-02-25 07:03:00,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c04a2f4-8b8c-444d-82fc-c959d90448e7
2024-02-25 07:03:00,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a27bffe-e595-4d27-8507-ec152381ae5a
2024-02-25 07:03:01,199 - distributed.worker - INFO - Starting Worker plugin PreImport-2b507795-f3ae-4eac-be1d-553b1ead7d87
2024-02-25 07:03:01,201 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:03:03,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-25 07:03:03,696 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-25 07:03:03,696 - distributed.worker - INFO - -------------------------------------------------
2024-02-25 07:03:03,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-25 07:03:03,728 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-25 07:03:03,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39611. Reason: nanny-close
2024-02-25 07:03:03,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-25 07:03:03,733 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36625 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37235 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35829 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43695 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45577 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42779 instead
  warnings.warn(
2024-02-25 07:04:56,633 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45055 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42965 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42987 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46669 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46871 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41373 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40785 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45283 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44987 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-5436' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37151 instead
  warnings.warn(
2024-02-25 07:10:37,584 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42301 instead
  warnings.warn(
2024-02-25 07:10:39,461 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33583 instead
  warnings.warn(
2024-02-25 07:10:41,372 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45713 instead
  warnings.warn(
2024-02-25 07:10:44,610 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37321 instead
  warnings.warn(
2024-02-25 07:10:47,656 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38613 instead
  warnings.warn(
2024-02-25 07:10:50,828 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39751 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36473 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32769 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44369 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46691 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34155 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-4467' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42407 instead
  warnings.warn(
[1708845207.758946] [dgx13:65713:0]            sock.c:481  UCX  ERROR bind(fd=135 addr=0.0.0.0:42698) failed: Address already in use
2024-02-25 07:13:40,913 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 443, in ep
    raise CommClosedError("UCX Endpoint is closed")
distributed.comm.core.CommClosedError: UCX Endpoint is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CommClosedError('UCX Endpoint is closed')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45583 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43675 instead
  warnings.warn(
[1708845248.903317] [dgx13:66619:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:40548) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33409 instead
  warnings.warn(
2024-02-25 07:14:47,536 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-430a5392-000c-4dfd-88f0-ebf4348abdd0
Function:  _run_coroutine_on_worker
args:      (142261944445230179479580008156458145337, <function shuffle_task at 0x7fc729cefaf0>, ('explicit-comms-shuffle-b8d442c19b80c9026427f66fce11b819', {0: set(), 1: set(), 2: {('from_pandas-cb393ec4ea697ffa65fe88980b15e9d6', 0)}}, {0: {0}, 1: {1}, 2: set()}, ['key'], 2, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
