============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-26 06:39:10,920 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:10,925 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45671 instead
  warnings.warn(
2023-12-26 06:39:10,929 - distributed.scheduler - INFO - State start
2023-12-26 06:39:10,953 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:10,954 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-26 06:39:10,955 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45671/status
2023-12-26 06:39:10,955 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:39:11,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39201'
2023-12-26 06:39:11,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35479'
2023-12-26 06:39:11,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41289'
2023-12-26 06:39:11,072 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40403'
2023-12-26 06:39:11,547 - distributed.scheduler - INFO - Receive client connection: Client-7924e6aa-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:11,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55712
2023-12-26 06:39:12,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:12,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:12,791 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:12,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:12,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:12,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:12,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:12,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:12,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:12,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:12,872 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:12,876 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-26 06:39:12,877 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35771
2023-12-26 06:39:12,877 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35771
2023-12-26 06:39:12,877 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44257
2023-12-26 06:39:12,878 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-26 06:39:12,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:12,878 - distributed.worker - INFO -               Threads:                          4
2023-12-26 06:39:12,878 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-26 06:39:12,878 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-5gmbgpoa
2023-12-26 06:39:12,878 - distributed.worker - INFO - Starting Worker plugin PreImport-edf6b55d-1793-4f24-b47f-501678dd562b
2023-12-26 06:39:12,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea3b7379-bf25-4d22-85d3-9ca3015bfb10
2023-12-26 06:39:12,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be3649da-61d7-4246-8796-1e7325b04840
2023-12-26 06:39:12,879 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:13,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35771', status: init, memory: 0, processing: 0>
2023-12-26 06:39:13,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35771
2023-12-26 06:39:13,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55728
2023-12-26 06:39:13,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:13,256 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-26 06:39:13,256 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:13,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-26 06:39:14,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45327
2023-12-26 06:39:14,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45327
2023-12-26 06:39:14,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41571
2023-12-26 06:39:14,195 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-26 06:39:14,196 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,196 - distributed.worker - INFO -               Threads:                          4
2023-12-26 06:39:14,196 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-26 06:39:14,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-2zwom80p
2023-12-26 06:39:14,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40424b20-aa4f-4712-bf7d-f7cb47f1cb79
2023-12-26 06:39:14,197 - distributed.worker - INFO - Starting Worker plugin PreImport-f2a27695-2e06-4d71-8f08-6062f885edac
2023-12-26 06:39:14,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d8a07fd-239b-43ec-bc54-76805bf4d3fe
2023-12-26 06:39:14,197 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,196 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41465
2023-12-26 06:39:14,197 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41465
2023-12-26 06:39:14,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45739
2023-12-26 06:39:14,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-26 06:39:14,198 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,196 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46331
2023-12-26 06:39:14,198 - distributed.worker - INFO -               Threads:                          4
2023-12-26 06:39:14,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46331
2023-12-26 06:39:14,198 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-26 06:39:14,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-krgwfqpn
2023-12-26 06:39:14,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40003
2023-12-26 06:39:14,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-26 06:39:14,198 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,198 - distributed.worker - INFO -               Threads:                          4
2023-12-26 06:39:14,199 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-26 06:39:14,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57c554bb-0803-4353-b2ad-b678e1d23e42
2023-12-26 06:39:14,199 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ve15vuki
2023-12-26 06:39:14,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42c02a02-4b8f-4195-bcaa-ddae0bc1373e
2023-12-26 06:39:14,199 - distributed.worker - INFO - Starting Worker plugin PreImport-33622aff-88bf-437d-8aaf-dc95a4d710c8
2023-12-26 06:39:14,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db06c7da-6d5b-4363-9032-6ddb9ecd9a3b
2023-12-26 06:39:14,200 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,200 - distributed.worker - INFO - Starting Worker plugin PreImport-d054de9a-a5f6-4507-9393-7234597dca31
2023-12-26 06:39:14,200 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58f15589-c57b-4bf9-ad14-63e27db5b500
2023-12-26 06:39:14,200 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45327', status: init, memory: 0, processing: 0>
2023-12-26 06:39:14,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45327
2023-12-26 06:39:14,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55746
2023-12-26 06:39:14,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:14,222 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-26 06:39:14,223 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-26 06:39:14,229 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46331', status: init, memory: 0, processing: 0>
2023-12-26 06:39:14,229 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46331
2023-12-26 06:39:14,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55762
2023-12-26 06:39:14,230 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41465', status: init, memory: 0, processing: 0>
2023-12-26 06:39:14,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:14,230 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41465
2023-12-26 06:39:14,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55750
2023-12-26 06:39:14,231 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-26 06:39:14,231 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:14,232 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-26 06:39:14,232 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:14,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-26 06:39:14,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-26 06:39:14,332 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-26 06:39:14,332 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-26 06:39:14,332 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-26 06:39:14,333 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-26 06:39:14,338 - distributed.scheduler - INFO - Remove client Client-7924e6aa-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:14,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55712; closing.
2023-12-26 06:39:14,338 - distributed.scheduler - INFO - Remove client Client-7924e6aa-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:14,339 - distributed.scheduler - INFO - Close client connection: Client-7924e6aa-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:14,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39201'. Reason: nanny-close
2023-12-26 06:39:14,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:14,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35479'. Reason: nanny-close
2023-12-26 06:39:14,341 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:14,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41465. Reason: nanny-close
2023-12-26 06:39:14,342 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45327. Reason: nanny-close
2023-12-26 06:39:14,343 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55750; closing.
2023-12-26 06:39:14,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-26 06:39:14,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-26 06:39:14,344 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41289'. Reason: nanny-close
2023-12-26 06:39:14,344 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:14,345 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:14,345 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40403'. Reason: nanny-close
2023-12-26 06:39:14,345 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:14,345 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:14,345 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46331. Reason: nanny-close
2023-12-26 06:39:14,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41465', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572754.3456864')
2023-12-26 06:39:14,346 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35771. Reason: nanny-close
2023-12-26 06:39:14,347 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55746; closing.
2023-12-26 06:39:14,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572754.3475647')
2023-12-26 06:39:14,347 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-26 06:39:14,348 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-26 06:39:14,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55762; closing.
2023-12-26 06:39:14,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46331', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572754.3489738')
2023-12-26 06:39:14,349 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:14,349 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55728; closing.
2023-12-26 06:39:14,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572754.3496492')
2023-12-26 06:39:14,349 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:14,349 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:39:15,557 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:39:15,557 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:39:15,558 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:39:15,559 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-26 06:39:15,559 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-26 06:39:17,519 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:17,523 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-26 06:39:17,526 - distributed.scheduler - INFO - State start
2023-12-26 06:39:17,547 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:17,548 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:39:17,549 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-26 06:39:17,549 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:39:17,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41021'
2023-12-26 06:39:17,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45239'
2023-12-26 06:39:17,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46149'
2023-12-26 06:39:17,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41307'
2023-12-26 06:39:17,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45111'
2023-12-26 06:39:17,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35141'
2023-12-26 06:39:17,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42625'
2023-12-26 06:39:17,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39303'
2023-12-26 06:39:18,965 - distributed.scheduler - INFO - Receive client connection: Client-7d230f5f-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:18,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60826
2023-12-26 06:39:19,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,587 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,631 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,635 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,660 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,660 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:19,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:19,661 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:19,665 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:22,022 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42373
2023-12-26 06:39:22,023 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42373
2023-12-26 06:39:22,023 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34185
2023-12-26 06:39:22,023 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,023 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,023 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,023 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-33gtt0dv
2023-12-26 06:39:22,024 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76e4e3a8-54f2-4816-9b92-94f8d58c6941
2023-12-26 06:39:22,022 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35271
2023-12-26 06:39:22,025 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35271
2023-12-26 06:39:22,025 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39175
2023-12-26 06:39:22,025 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,025 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,025 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,025 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,025 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e5i3hn7r
2023-12-26 06:39:22,026 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0067256-f5d4-4b47-bbc9-9950889310cb
2023-12-26 06:39:22,027 - distributed.worker - INFO - Starting Worker plugin PreImport-a140441e-41da-435c-bb05-e798f1a939a7
2023-12-26 06:39:22,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-95b882e1-fbe0-42af-87cd-08d62a1994df
2023-12-26 06:39:22,041 - distributed.worker - INFO - Starting Worker plugin PreImport-b66a5a3d-3274-46c7-8404-db28d3a7b26e
2023-12-26 06:39:22,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01707a67-eb7c-4fc4-815a-8ca285e75378
2023-12-26 06:39:22,042 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42373', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,079 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42373
2023-12-26 06:39:22,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34084
2023-12-26 06:39:22,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,081 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,081 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,087 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40927
2023-12-26 06:39:22,089 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40927
2023-12-26 06:39:22,089 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38515
2023-12-26 06:39:22,089 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,089 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,089 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e5r5nopi
2023-12-26 06:39:22,089 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34855
2023-12-26 06:39:22,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2be45f4e-5723-4aff-b608-3d0df8609b2a
2023-12-26 06:39:22,090 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34855
2023-12-26 06:39:22,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35963
2023-12-26 06:39:22,090 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,090 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,090 - distributed.worker - INFO - Starting Worker plugin PreImport-e23f728a-0a8b-4065-afbd-c469e310538d
2023-12-26 06:39:22,090 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,089 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37875
2023-12-26 06:39:22,090 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37875
2023-12-26 06:39:22,090 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,090 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d86e28a1-1c97-4f00-990a-7cc22d6845d7
2023-12-26 06:39:22,090 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2abki4j1
2023-12-26 06:39:22,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34793
2023-12-26 06:39:22,090 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,090 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,090 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,090 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ufl4wzc6
2023-12-26 06:39:22,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3145bbe-db1a-44cf-9475-ccabad3034ad
2023-12-26 06:39:22,091 - distributed.worker - INFO - Starting Worker plugin PreImport-6a89dd55-18a1-40f5-bb98-30bcd938804b
2023-12-26 06:39:22,091 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4603a597-6c7a-4f83-8b3b-91ed81d7568d
2023-12-26 06:39:22,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ecee2d6-9f95-4d3c-8990-ad6fa88fa3ae
2023-12-26 06:39:22,116 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35271', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,117 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35271
2023-12-26 06:39:22,117 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34100
2023-12-26 06:39:22,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,119 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,119 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,148 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,153 - distributed.worker - INFO - Starting Worker plugin PreImport-5a4f297e-d9c4-413d-b639-9c5f94dbcb19
2023-12-26 06:39:22,153 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ee8ee01-f1ed-4aef-8d83-ee0546c9271f
2023-12-26 06:39:22,154 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,165 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,165 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36125
2023-12-26 06:39:22,166 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36125
2023-12-26 06:39:22,166 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38435
2023-12-26 06:39:22,166 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,166 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,166 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zq40v2i0
2023-12-26 06:39:22,166 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36585
2023-12-26 06:39:22,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36585
2023-12-26 06:39:22,167 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40911
2023-12-26 06:39:22,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,167 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ac2d5d1-fdf3-418b-a4e9-1a51da2e19db
2023-12-26 06:39:22,167 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,167 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0iuj0u4e
2023-12-26 06:39:22,167 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40037
2023-12-26 06:39:22,168 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40037
2023-12-26 06:39:22,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4435d6e-6d72-4efe-a932-007940669840
2023-12-26 06:39:22,168 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42627
2023-12-26 06:39:22,168 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,168 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,168 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:22,168 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:22,168 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b9n53om5
2023-12-26 06:39:22,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bd5979e-0558-454c-9640-80f675c94a9c
2023-12-26 06:39:22,169 - distributed.worker - INFO - Starting Worker plugin PreImport-9704266d-044a-451b-8dce-29ca8b56c534
2023-12-26 06:39:22,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aed02be7-2295-43e3-aaa9-241dbfbc6ae9
2023-12-26 06:39:22,177 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34855', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,177 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34855
2023-12-26 06:39:22,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34120
2023-12-26 06:39:22,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,179 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,179 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40927', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,185 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40927
2023-12-26 06:39:22,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34116
2023-12-26 06:39:22,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,188 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,194 - distributed.worker - INFO - Starting Worker plugin PreImport-110d83ec-7d27-4db6-b951-f750f7a1fb1d
2023-12-26 06:39:22,195 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-538c1d43-6e00-4186-8a23-e8734bbb9a9f
2023-12-26 06:39:22,195 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,197 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37875', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,197 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37875
2023-12-26 06:39:22,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34134
2023-12-26 06:39:22,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,199 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,199 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,199 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,202 - distributed.worker - INFO - Starting Worker plugin PreImport-fcb55aed-9f32-4e0e-b4e7-27d9f72d66d6
2023-12-26 06:39:22,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41a7cf67-8d14-478b-94df-72e61cfa08fd
2023-12-26 06:39:22,202 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36585', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36585
2023-12-26 06:39:22,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34150
2023-12-26 06:39:22,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,219 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,223 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36125', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,224 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36125
2023-12-26 06:39:22,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34162
2023-12-26 06:39:22,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,226 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,232 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40037', status: init, memory: 0, processing: 0>
2023-12-26 06:39:22,233 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40037
2023-12-26 06:39:22,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34156
2023-12-26 06:39:22,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:22,235 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:22,235 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:22,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:22,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,284 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,285 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,285 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,285 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,285 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:22,289 - distributed.scheduler - INFO - Remove client Client-7d230f5f-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:22,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60826; closing.
2023-12-26 06:39:22,290 - distributed.scheduler - INFO - Remove client Client-7d230f5f-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:22,290 - distributed.scheduler - INFO - Close client connection: Client-7d230f5f-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:22,291 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41021'. Reason: nanny-close
2023-12-26 06:39:22,291 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45239'. Reason: nanny-close
2023-12-26 06:39:22,293 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,293 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37875. Reason: nanny-close
2023-12-26 06:39:22,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46149'. Reason: nanny-close
2023-12-26 06:39:22,293 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,294 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42373. Reason: nanny-close
2023-12-26 06:39:22,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41307'. Reason: nanny-close
2023-12-26 06:39:22,294 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,294 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35271. Reason: nanny-close
2023-12-26 06:39:22,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45111'. Reason: nanny-close
2023-12-26 06:39:22,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,295 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34855. Reason: nanny-close
2023-12-26 06:39:22,295 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35141'. Reason: nanny-close
2023-12-26 06:39:22,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40927. Reason: nanny-close
2023-12-26 06:39:22,296 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42625'. Reason: nanny-close
2023-12-26 06:39:22,297 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,297 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40037. Reason: nanny-close
2023-12-26 06:39:22,297 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39303'. Reason: nanny-close
2023-12-26 06:39:22,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:22,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36585. Reason: nanny-close
2023-12-26 06:39:22,298 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,298 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,299 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36125. Reason: nanny-close
2023-12-26 06:39:22,299 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,299 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,299 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,300 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:22,301 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,301 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,301 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34084; closing.
2023-12-26 06:39:22,302 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,302 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34134; closing.
2023-12-26 06:39:22,302 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:22,303 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3037033')
2023-12-26 06:39:22,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3052452')
2023-12-26 06:39:22,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34100; closing.
2023-12-26 06:39:22,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34120; closing.
2023-12-26 06:39:22,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34116; closing.
2023-12-26 06:39:22,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34156; closing.
2023-12-26 06:39:22,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34150; closing.
2023-12-26 06:39:22,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34162; closing.
2023-12-26 06:39:22,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.309161')
2023-12-26 06:39:22,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3095036')
2023-12-26 06:39:22,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3098154')
2023-12-26 06:39:22,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3103423')
2023-12-26 06:39:22,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36585', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3106668')
2023-12-26 06:39:22,311 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572762.3110032')
2023-12-26 06:39:22,311 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:39:23,809 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:39:23,809 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:39:23,811 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:39:23,812 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:39:23,813 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-26 06:39:25,970 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:25,974 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-26 06:39:25,977 - distributed.scheduler - INFO - State start
2023-12-26 06:39:26,000 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:26,001 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:39:26,002 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-26 06:39:26,002 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:39:26,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41049'
2023-12-26 06:39:26,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42147'
2023-12-26 06:39:26,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35611'
2023-12-26 06:39:26,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33811'
2023-12-26 06:39:26,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45375'
2023-12-26 06:39:26,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36031'
2023-12-26 06:39:26,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46067'
2023-12-26 06:39:26,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38701'
2023-12-26 06:39:26,635 - distributed.scheduler - INFO - Receive client connection: Client-8224682b-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:26,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34376
2023-12-26 06:39:27,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:27,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:27,864 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,173 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,173 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:28,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,175 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:28,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:28,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:29,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36153
2023-12-26 06:39:29,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36153
2023-12-26 06:39:29,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33787
2023-12-26 06:39:29,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:29,519 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:29,519 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:29,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:29,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uj1_namh
2023-12-26 06:39:29,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d961114-fc5c-4b27-b737-e7b4b030692c
2023-12-26 06:39:29,730 - distributed.worker - INFO - Starting Worker plugin PreImport-ab0f718f-975b-418a-9f6b-dc52c68fa5a6
2023-12-26 06:39:29,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3a3dd8f-300d-4c72-a71a-8d9f015fbfbe
2023-12-26 06:39:29,731 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:29,767 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36153', status: init, memory: 0, processing: 0>
2023-12-26 06:39:29,768 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36153
2023-12-26 06:39:29,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34396
2023-12-26 06:39:29,770 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:29,771 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:29,771 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:29,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35407
2023-12-26 06:39:30,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35407
2023-12-26 06:39:30,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34945
2023-12-26 06:39:30,733 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,733 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,733 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,733 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qgn_aaod
2023-12-26 06:39:30,734 - distributed.worker - INFO - Starting Worker plugin PreImport-19bf0567-d5a6-42ee-bafa-d7e71165792f
2023-12-26 06:39:30,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82b913b4-0eaa-4e7e-867a-334652bef52a
2023-12-26 06:39:30,735 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f16c6d28-b111-4985-8a07-3dbfe05c2873
2023-12-26 06:39:30,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42847
2023-12-26 06:39:30,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42847
2023-12-26 06:39:30,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45663
2023-12-26 06:39:30,741 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,741 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,741 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,741 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_wailbec
2023-12-26 06:39:30,742 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d2f0d84-0ed2-4a07-9e1e-f998a628919a
2023-12-26 06:39:30,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39581
2023-12-26 06:39:30,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39581
2023-12-26 06:39:30,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34787
2023-12-26 06:39:30,748 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,748 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,748 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,748 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,748 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yrrc4egh
2023-12-26 06:39:30,749 - distributed.worker - INFO - Starting Worker plugin PreImport-4f04c881-2400-49c5-99c2-b915404828ff
2023-12-26 06:39:30,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36080e50-bdca-4c2c-80a7-12ecff487aa7
2023-12-26 06:39:30,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46369
2023-12-26 06:39:30,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46369
2023-12-26 06:39:30,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87617cdf-7482-4da3-bd9a-02d8e84cde9e
2023-12-26 06:39:30,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45105
2023-12-26 06:39:30,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,750 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,750 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,750 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kz755pva
2023-12-26 06:39:30,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0d557d6-6a12-4162-9dfb-f53ec78c0fb5
2023-12-26 06:39:30,752 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33679
2023-12-26 06:39:30,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33679
2023-12-26 06:39:30,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35313
2023-12-26 06:39:30,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,753 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46683
2023-12-26 06:39:30,753 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46683
2023-12-26 06:39:30,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39229
2023-12-26 06:39:30,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yk78ko6f
2023-12-26 06:39:30,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,754 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,754 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,754 - distributed.worker - INFO - Starting Worker plugin PreImport-50fb8754-420a-4ce2-802b-bcbab607c028
2023-12-26 06:39:30,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cta8vr3f
2023-12-26 06:39:30,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-caaf857c-2820-48e5-bca7-8696b8c65085
2023-12-26 06:39:30,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b53115a6-c0f3-448a-ac54-8cc86d5ee734
2023-12-26 06:39:30,755 - distributed.worker - INFO - Starting Worker plugin PreImport-dd3a56a7-0747-4b8b-aa18-1ab8791ee393
2023-12-26 06:39:30,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17c7e854-1a5f-42db-bda3-1870a1877614
2023-12-26 06:39:30,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38285
2023-12-26 06:39:30,759 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38285
2023-12-26 06:39:30,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42511
2023-12-26 06:39:30,759 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,759 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,759 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:30,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1a322d2-a97d-4ff1-aaf4-0d00e8928f13
2023-12-26 06:39:30,760 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:30,760 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5aov9y92
2023-12-26 06:39:30,761 - distributed.worker - INFO - Starting Worker plugin PreImport-5228dae1-e9c9-446c-9070-93ef7bc2c91b
2023-12-26 06:39:30,761 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a321308-c918-4407-8ba6-05f19d48148f
2023-12-26 06:39:30,761 - distributed.worker - INFO - Starting Worker plugin RMMSetup-178e9e84-7375-4972-b32a-7412a5f08fa9
2023-12-26 06:39:30,778 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,780 - distributed.worker - INFO - Starting Worker plugin PreImport-9c4399b3-d040-4715-89a8-0c0ab50643a8
2023-12-26 06:39:30,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01296b34-ffa8-497c-bf16-d225aeba2af6
2023-12-26 06:39:30,781 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,784 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,785 - distributed.worker - INFO - Starting Worker plugin PreImport-6114e9b5-9d46-4aea-b2b6-182d70c591cc
2023-12-26 06:39:30,785 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14159267-6b5a-4e15-9dc0-6765dba7f55c
2023-12-26 06:39:30,785 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,785 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,787 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,789 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39581', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,810 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39581
2023-12-26 06:39:30,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50664
2023-12-26 06:39:30,811 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35407', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35407
2023-12-26 06:39:30,812 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50644
2023-12-26 06:39:30,812 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33679', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,813 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33679
2023-12-26 06:39:30,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50668
2023-12-26 06:39:30,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46683', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,815 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,815 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,817 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46683
2023-12-26 06:39:30,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50676
2023-12-26 06:39:30,817 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,817 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42847', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,818 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42847
2023-12-26 06:39:30,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50654
2023-12-26 06:39:30,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,821 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46369', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,822 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46369
2023-12-26 06:39:30,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50672
2023-12-26 06:39:30,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,823 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38285', status: init, memory: 0, processing: 0>
2023-12-26 06:39:30,824 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38285
2023-12-26 06:39:30,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50688
2023-12-26 06:39:30,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,825 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,825 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:30,827 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:30,827 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:30,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:30,916 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,916 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,916 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,916 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,917 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,917 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,917 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,917 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:30,921 - distributed.scheduler - INFO - Remove client Client-8224682b-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:30,921 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34376; closing.
2023-12-26 06:39:30,922 - distributed.scheduler - INFO - Remove client Client-8224682b-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:30,922 - distributed.scheduler - INFO - Close client connection: Client-8224682b-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:30,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41049'. Reason: nanny-close
2023-12-26 06:39:30,923 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42147'. Reason: nanny-close
2023-12-26 06:39:30,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39581. Reason: nanny-close
2023-12-26 06:39:30,924 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,925 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35611'. Reason: nanny-close
2023-12-26 06:39:30,925 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36153. Reason: nanny-close
2023-12-26 06:39:30,926 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33811'. Reason: nanny-close
2023-12-26 06:39:30,926 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42847. Reason: nanny-close
2023-12-26 06:39:30,926 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45375'. Reason: nanny-close
2023-12-26 06:39:30,926 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,926 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,927 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35407. Reason: nanny-close
2023-12-26 06:39:30,927 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50664; closing.
2023-12-26 06:39:30,927 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36031'. Reason: nanny-close
2023-12-26 06:39:30,927 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33679. Reason: nanny-close
2023-12-26 06:39:30,927 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,927 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9276388')
2023-12-26 06:39:30,928 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46067'. Reason: nanny-close
2023-12-26 06:39:30,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,928 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46683. Reason: nanny-close
2023-12-26 06:39:30,928 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,928 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38701'. Reason: nanny-close
2023-12-26 06:39:30,929 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:30,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46369. Reason: nanny-close
2023-12-26 06:39:30,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,929 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34396; closing.
2023-12-26 06:39:30,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,930 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,930 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,930 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50654; closing.
2023-12-26 06:39:30,930 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,930 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38285. Reason: nanny-close
2023-12-26 06:39:30,930 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9308379')
2023-12-26 06:39:30,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50644; closing.
2023-12-26 06:39:30,931 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,931 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,931 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42847', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9318306')
2023-12-26 06:39:30,932 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9321818')
2023-12-26 06:39:30,932 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50668; closing.
2023-12-26 06:39:30,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9331338')
2023-12-26 06:39:30,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:30,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50676; closing.
2023-12-26 06:39:30,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46683', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9340441')
2023-12-26 06:39:30,934 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50672; closing.
2023-12-26 06:39:30,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9349353')
2023-12-26 06:39:30,935 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,935 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50688; closing.
2023-12-26 06:39:30,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572770.9357252')
2023-12-26 06:39:30,935 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:30,935 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:39:30,936 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50688>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:39:32,340 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:39:32,341 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:39:32,341 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:39:32,342 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:39:32,343 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-26 06:39:34,373 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:34,378 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45573 instead
  warnings.warn(
2023-12-26 06:39:34,381 - distributed.scheduler - INFO - State start
2023-12-26 06:39:34,404 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:34,405 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:39:34,406 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45573/status
2023-12-26 06:39:34,406 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:39:34,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34849'
2023-12-26 06:39:34,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33669'
2023-12-26 06:39:34,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42469'
2023-12-26 06:39:34,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41341'
2023-12-26 06:39:34,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42613'
2023-12-26 06:39:34,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42569'
2023-12-26 06:39:34,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46371'
2023-12-26 06:39:34,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39231'
2023-12-26 06:39:35,062 - distributed.scheduler - INFO - Receive client connection: Client-872ab424-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:35,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50852
2023-12-26 06:39:36,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,410 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,438 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:36,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:36,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:36,442 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:38,985 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35709
2023-12-26 06:39:38,986 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35709
2023-12-26 06:39:38,986 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42249
2023-12-26 06:39:38,986 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:38,986 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:38,986 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:38,986 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:38,986 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dwmt7d_p
2023-12-26 06:39:38,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8ab0f4c-a669-4e2d-84b7-1ca60e70703c
2023-12-26 06:39:38,987 - distributed.worker - INFO - Starting Worker plugin PreImport-d9636930-ff14-41e6-abbb-90af2631ddad
2023-12-26 06:39:38,987 - distributed.worker - INFO - Starting Worker plugin RMMSetup-127b42f0-1c7b-4b2a-a349-6e17592cce1d
2023-12-26 06:39:38,989 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38729
2023-12-26 06:39:38,990 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38729
2023-12-26 06:39:38,990 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38373
2023-12-26 06:39:38,990 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:38,990 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:38,990 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:38,990 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:38,990 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5jhpuhhv
2023-12-26 06:39:38,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4baf742f-6d7a-4257-b279-31abc800d486
2023-12-26 06:39:38,990 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45363
2023-12-26 06:39:38,991 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45363
2023-12-26 06:39:38,991 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39681
2023-12-26 06:39:38,991 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:38,991 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:38,991 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:38,991 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:38,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d601_irb
2023-12-26 06:39:38,992 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae3aca25-1752-44dc-8a55-efb499210d58
2023-12-26 06:39:38,992 - distributed.worker - INFO - Starting Worker plugin PreImport-ed741a56-60ab-4e60-8ef1-74bb22d48f6c
2023-12-26 06:39:38,992 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80c0606c-f64a-449f-bf89-f62cd1b70f39
2023-12-26 06:39:39,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36467
2023-12-26 06:39:39,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36467
2023-12-26 06:39:39,057 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33417
2023-12-26 06:39:39,057 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,057 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,057 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:39,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:39,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-59jx_45j
2023-12-26 06:39:39,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd803195-5399-40a8-be81-0a6ff452a3e5
2023-12-26 06:39:39,058 - distributed.worker - INFO - Starting Worker plugin PreImport-810dee68-4624-41cb-8944-5c723e250c61
2023-12-26 06:39:39,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-00f9b3fa-1356-423a-945c-9878082f1b69
2023-12-26 06:39:39,068 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46461
2023-12-26 06:39:39,068 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46461
2023-12-26 06:39:39,069 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38753
2023-12-26 06:39:39,069 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,069 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,069 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:39,069 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:39,069 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4g_g40xs
2023-12-26 06:39:39,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1de1401b-79ac-48c9-82a9-316611396c6b
2023-12-26 06:39:39,070 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42339
2023-12-26 06:39:39,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42339
2023-12-26 06:39:39,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38669
2023-12-26 06:39:39,071 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,071 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,071 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:39,071 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:39,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g0rfwgc_
2023-12-26 06:39:39,069 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39033
2023-12-26 06:39:39,072 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39033
2023-12-26 06:39:39,072 - distributed.worker - INFO - Starting Worker plugin PreImport-0be6eac9-068c-41df-aa1b-f010dfede0dc
2023-12-26 06:39:39,072 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38165
2023-12-26 06:39:39,072 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cea90d68-840d-44a6-8a93-0f13dd490820
2023-12-26 06:39:39,072 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,072 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:39,072 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:39,072 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bgb8a40a
2023-12-26 06:39:39,073 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5fe6c47-68c9-41fb-8ff2-0f175a1f7ffc
2023-12-26 06:39:39,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39799
2023-12-26 06:39:39,075 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39799
2023-12-26 06:39:39,075 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39547
2023-12-26 06:39:39,075 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,075 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,075 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:39,075 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:39,075 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wnr11gvd
2023-12-26 06:39:39,075 - distributed.worker - INFO - Starting Worker plugin RMMSetup-36116eed-d373-4a26-a85b-5e5b95a2a675
2023-12-26 06:39:39,076 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83623f97-5766-4e0c-9f26-922eddbebe47
2023-12-26 06:39:39,231 - distributed.worker - INFO - Starting Worker plugin PreImport-0ded5a0f-65ae-4203-a941-c96eacc07330
2023-12-26 06:39:39,231 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95d599ea-81e4-42fc-adde-26b7f4c52c43
2023-12-26 06:39:39,232 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,232 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,244 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,254 - distributed.worker - INFO - Starting Worker plugin PreImport-13ac15e4-b25c-40a9-8d21-f7b2a39d8d0e
2023-12-26 06:39:39,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-added031-dfae-4bbe-91b1-a9b783b4d043
2023-12-26 06:39:39,255 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,256 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,256 - distributed.worker - INFO - Starting Worker plugin PreImport-9f42be99-6301-484e-b218-0029ca127223
2023-12-26 06:39:39,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e93e8b1-5ce7-440a-93c4-8b7e98a0077e
2023-12-26 06:39:39,258 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,257 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35709', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,259 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35709
2023-12-26 06:39:39,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50874
2023-12-26 06:39:39,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,260 - distributed.worker - INFO - Starting Worker plugin PreImport-f886986e-6a43-49eb-a653-c9bd85c04460
2023-12-26 06:39:39,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-519f6aa0-7526-4bdf-9ab6-40ceadc5c926
2023-12-26 06:39:39,261 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,261 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,261 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,261 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,264 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38729', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38729
2023-12-26 06:39:39,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50878
2023-12-26 06:39:39,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,267 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,267 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,279 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45363', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,279 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45363
2023-12-26 06:39:39,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50890
2023-12-26 06:39:39,281 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,282 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,282 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,284 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39799', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39799
2023-12-26 06:39:39,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50932
2023-12-26 06:39:39,286 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,287 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,287 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39033', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,287 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39033
2023-12-26 06:39:39,287 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50894
2023-12-26 06:39:39,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,290 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,290 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46461', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,292 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46461
2023-12-26 06:39:39,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50920
2023-12-26 06:39:39,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,294 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,294 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,296 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36467', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,296 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36467
2023-12-26 06:39:39,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50906
2023-12-26 06:39:39,297 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42339', status: init, memory: 0, processing: 0>
2023-12-26 06:39:39,298 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42339
2023-12-26 06:39:39,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50938
2023-12-26 06:39:39,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,299 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,299 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:39,300 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:39,300 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:39,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,307 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:39,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:39,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,398 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:39,404 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:39,406 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:39,409 - distributed.scheduler - INFO - Remove client Client-872ab424-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:39,409 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50852; closing.
2023-12-26 06:39:39,409 - distributed.scheduler - INFO - Remove client Client-872ab424-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:39,409 - distributed.scheduler - INFO - Close client connection: Client-872ab424-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:39,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34849'. Reason: nanny-close
2023-12-26 06:39:39,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33669'. Reason: nanny-close
2023-12-26 06:39:39,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42339. Reason: nanny-close
2023-12-26 06:39:39,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42469'. Reason: nanny-close
2023-12-26 06:39:39,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46461. Reason: nanny-close
2023-12-26 06:39:39,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41341'. Reason: nanny-close
2023-12-26 06:39:39,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35709. Reason: nanny-close
2023-12-26 06:39:39,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42613'. Reason: nanny-close
2023-12-26 06:39:39,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39799. Reason: nanny-close
2023-12-26 06:39:39,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42569'. Reason: nanny-close
2023-12-26 06:39:39,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50938; closing.
2023-12-26 06:39:39,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45363. Reason: nanny-close
2023-12-26 06:39:39,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.4156613')
2023-12-26 06:39:39,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46371'. Reason: nanny-close
2023-12-26 06:39:39,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36467. Reason: nanny-close
2023-12-26 06:39:39,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39231'. Reason: nanny-close
2023-12-26 06:39:39,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:39,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39033. Reason: nanny-close
2023-12-26 06:39:39,417 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,417 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,417 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50932; closing.
2023-12-26 06:39:39,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50920; closing.
2023-12-26 06:39:39,418 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50874; closing.
2023-12-26 06:39:39,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38729. Reason: nanny-close
2023-12-26 06:39:39,419 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,419 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.4194531')
2023-12-26 06:39:39,420 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.4201334')
2023-12-26 06:39:39,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35709', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.4206815')
2023-12-26 06:39:39,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:39,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50890; closing.
2023-12-26 06:39:39,422 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.4225676')
2023-12-26 06:39:39,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50906; closing.
2023-12-26 06:39:39,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50894; closing.
2023-12-26 06:39:39,423 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36467', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.4238095')
2023-12-26 06:39:39,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.424217')
2023-12-26 06:39:39,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50878; closing.
2023-12-26 06:39:39,425 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:39,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572779.425328')
2023-12-26 06:39:39,425 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:39:39,425 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50906>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:39:39,426 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50894>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:39:40,978 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:39:40,978 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:39:40,979 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:39:40,980 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:39:40,980 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-26 06:39:43,165 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:43,170 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43621 instead
  warnings.warn(
2023-12-26 06:39:43,174 - distributed.scheduler - INFO - State start
2023-12-26 06:39:43,196 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:43,197 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:39:43,198 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43621/status
2023-12-26 06:39:43,198 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:39:43,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45213'
2023-12-26 06:39:43,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39515'
2023-12-26 06:39:43,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35481'
2023-12-26 06:39:43,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34379'
2023-12-26 06:39:43,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42997'
2023-12-26 06:39:43,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38305'
2023-12-26 06:39:43,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44127'
2023-12-26 06:39:43,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40437'
2023-12-26 06:39:43,442 - distributed.scheduler - INFO - Receive client connection: Client-8c520b00-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:43,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46834
2023-12-26 06:39:45,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,151 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,169 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:45,238 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:45,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:45,243 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:47,620 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34023
2023-12-26 06:39:47,621 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34023
2023-12-26 06:39:47,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37321
2023-12-26 06:39:47,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,621 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,621 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,622 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kcc3qt11
2023-12-26 06:39:47,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4c15808-bbbe-4d56-94ba-da35a7ea2960
2023-12-26 06:39:47,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36905
2023-12-26 06:39:47,638 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36905
2023-12-26 06:39:47,638 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45737
2023-12-26 06:39:47,638 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,638 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,639 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2o0zoqge
2023-12-26 06:39:47,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a8b4ef4-82c8-4d1f-b052-c13ae1dad19c
2023-12-26 06:39:47,639 - distributed.worker - INFO - Starting Worker plugin PreImport-b573d94d-1504-4f8b-addd-aa7268b8d3f7
2023-12-26 06:39:47,638 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43837
2023-12-26 06:39:47,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8322a541-aa13-4cf8-9fa8-129ee2323454
2023-12-26 06:39:47,640 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43837
2023-12-26 06:39:47,640 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42441
2023-12-26 06:39:47,640 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,640 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,640 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,640 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,640 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1gyvdszh
2023-12-26 06:39:47,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d9dea07-3ffd-48e7-a404-c11dfe8c8953
2023-12-26 06:39:47,641 - distributed.worker - INFO - Starting Worker plugin PreImport-b138fedd-4cfa-4f7e-a14d-5b926511d040
2023-12-26 06:39:47,642 - distributed.worker - INFO - Starting Worker plugin RMMSetup-206dd3e2-ff4c-4cb3-aea1-697b271dfb49
2023-12-26 06:39:47,683 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35477
2023-12-26 06:39:47,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35477
2023-12-26 06:39:47,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35545
2023-12-26 06:39:47,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,684 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,684 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,684 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bjd85gjm
2023-12-26 06:39:47,685 - distributed.worker - INFO - Starting Worker plugin PreImport-8a60788a-b734-4198-baae-1929c7934bc2
2023-12-26 06:39:47,685 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4669f91e-90d5-49dd-a7e1-05aa14ba2d34
2023-12-26 06:39:47,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f091b478-328c-4f9d-a639-c616c1fe94c8
2023-12-26 06:39:47,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40081
2023-12-26 06:39:47,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40081
2023-12-26 06:39:47,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33129
2023-12-26 06:39:47,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,698 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,699 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8pbqlob1
2023-12-26 06:39:47,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a02417f-f400-463c-a270-e9da9fe56d5b
2023-12-26 06:39:47,701 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38125
2023-12-26 06:39:47,702 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38125
2023-12-26 06:39:47,702 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34113
2023-12-26 06:39:47,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,702 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,702 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,702 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1e0zj4t0
2023-12-26 06:39:47,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67dffc2d-66cc-4ba5-bed3-9c89e66197da
2023-12-26 06:39:47,703 - distributed.worker - INFO - Starting Worker plugin PreImport-775cecc7-748f-45ee-ba77-52288f0518f2
2023-12-26 06:39:47,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b16f00f-d838-4ba6-8171-da71d5b69010
2023-12-26 06:39:47,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37999
2023-12-26 06:39:47,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37999
2023-12-26 06:39:47,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34483
2023-12-26 06:39:47,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,709 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,709 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z1wn43sy
2023-12-26 06:39:47,709 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35517
2023-12-26 06:39:47,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35517
2023-12-26 06:39:47,710 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37055
2023-12-26 06:39:47,710 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,710 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ba5b9ee-54cd-4351-a59e-da182ee133a9
2023-12-26 06:39:47,710 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:47,710 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:47,710 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rxpzinzz
2023-12-26 06:39:47,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f74d8da-d3db-4cd4-85d6-9a1e45ef73b0
2023-12-26 06:39:47,849 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,858 - distributed.worker - INFO - Starting Worker plugin PreImport-3ea2285f-3b4e-4081-81b7-5b5c584cf926
2023-12-26 06:39:47,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39a17fce-7a39-406d-aaa5-c9f7ae671d3c
2023-12-26 06:39:47,858 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,876 - distributed.worker - INFO - Starting Worker plugin PreImport-c4727f80-7118-40eb-97b9-421cb469ea5c
2023-12-26 06:39:47,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8380a05c-084a-47e1-b358-b6390ebbb147
2023-12-26 06:39:47,877 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,876 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36905', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,878 - distributed.worker - INFO - Starting Worker plugin PreImport-07d7229c-4511-4b53-b41c-f6a7fd05e537
2023-12-26 06:39:47,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11c1fbde-5e76-426d-a01d-da685d154383
2023-12-26 06:39:47,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,879 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36905
2023-12-26 06:39:47,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46860
2023-12-26 06:39:47,880 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,880 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,880 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,881 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,881 - distributed.worker - INFO - Starting Worker plugin PreImport-fe7535f5-e293-4f12-9f5e-22ce21f3011f
2023-12-26 06:39:47,881 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,881 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e80ff03f-f279-4142-8d90-bcd818b793b0
2023-12-26 06:39:47,881 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,883 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34023', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,884 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34023
2023-12-26 06:39:47,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46876
2023-12-26 06:39:47,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,885 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43837', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43837
2023-12-26 06:39:47,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46880
2023-12-26 06:39:47,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,899 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,899 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,901 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40081', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,901 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40081
2023-12-26 06:39:47,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46896
2023-12-26 06:39:47,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,903 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,903 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,904 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37999', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,904 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37999
2023-12-26 06:39:47,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46914
2023-12-26 06:39:47,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,905 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,906 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,906 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,914 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35477', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,914 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35477
2023-12-26 06:39:47,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46938
2023-12-26 06:39:47,915 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35517', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35517
2023-12-26 06:39:47,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46912
2023-12-26 06:39:47,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38125', status: init, memory: 0, processing: 0>
2023-12-26 06:39:47,917 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,917 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,917 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38125
2023-12-26 06:39:47,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46930
2023-12-26 06:39:47,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,918 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,918 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:47,919 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:47,920 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:47,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:47,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,980 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,980 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,980 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,980 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,990 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,990 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,990 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,990 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,990 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,991 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,991 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,991 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:39:47,997 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:47,998 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:39:48,000 - distributed.scheduler - INFO - Remove client Client-8c520b00-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:48,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46834; closing.
2023-12-26 06:39:48,001 - distributed.scheduler - INFO - Remove client Client-8c520b00-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:48,001 - distributed.scheduler - INFO - Close client connection: Client-8c520b00-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:48,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45213'. Reason: nanny-close
2023-12-26 06:39:48,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39515'. Reason: nanny-close
2023-12-26 06:39:48,003 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35477. Reason: nanny-close
2023-12-26 06:39:48,004 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35481'. Reason: nanny-close
2023-12-26 06:39:48,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35517. Reason: nanny-close
2023-12-26 06:39:48,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34379'. Reason: nanny-close
2023-12-26 06:39:48,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36905. Reason: nanny-close
2023-12-26 06:39:48,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42997'. Reason: nanny-close
2023-12-26 06:39:48,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37999. Reason: nanny-close
2023-12-26 06:39:48,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,006 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38305'. Reason: nanny-close
2023-12-26 06:39:48,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46938; closing.
2023-12-26 06:39:48,006 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43837. Reason: nanny-close
2023-12-26 06:39:48,007 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44127'. Reason: nanny-close
2023-12-26 06:39:48,007 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0071454')
2023-12-26 06:39:48,007 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,007 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38125. Reason: nanny-close
2023-12-26 06:39:48,007 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46860; closing.
2023-12-26 06:39:48,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,007 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40437'. Reason: nanny-close
2023-12-26 06:39:48,007 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:48,008 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34023. Reason: nanny-close
2023-12-26 06:39:48,008 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0082703')
2023-12-26 06:39:48,008 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,008 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40081. Reason: nanny-close
2023-12-26 06:39:48,009 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,009 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,009 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,009 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46914; closing.
2023-12-26 06:39:48,009 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46912; closing.
2023-12-26 06:39:48,009 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:48,011 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,010 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46860>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:39:48,011 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0119348')
2023-12-26 06:39:48,012 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,012 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0123208')
2023-12-26 06:39:48,012 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:48,013 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46880; closing.
2023-12-26 06:39:48,013 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46930; closing.
2023-12-26 06:39:48,013 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46876; closing.
2023-12-26 06:39:48,014 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43837', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0143273')
2023-12-26 06:39:48,014 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0146217')
2023-12-26 06:39:48,014 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0149474')
2023-12-26 06:39:48,015 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46896; closing.
2023-12-26 06:39:48,015 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40081', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572788.0157137')
2023-12-26 06:39:48,015 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:39:48,016 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46896>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:39:49,369 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:39:49,369 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:39:49,370 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:39:49,371 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:39:49,371 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-26 06:39:51,406 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:51,410 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37417 instead
  warnings.warn(
2023-12-26 06:39:51,414 - distributed.scheduler - INFO - State start
2023-12-26 06:39:51,435 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:39:51,436 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:39:51,437 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37417/status
2023-12-26 06:39:51,437 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:39:51,652 - distributed.scheduler - INFO - Receive client connection: Client-91506369-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:51,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36994
2023-12-26 06:39:51,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44729'
2023-12-26 06:39:51,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39181'
2023-12-26 06:39:51,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35753'
2023-12-26 06:39:51,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40935'
2023-12-26 06:39:51,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41323'
2023-12-26 06:39:51,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39951'
2023-12-26 06:39:51,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42401'
2023-12-26 06:39:51,866 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39997'
2023-12-26 06:39:53,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,707 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,755 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:53,812 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:39:53,812 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:39:53,815 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:39:56,231 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44885
2023-12-26 06:39:56,233 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44885
2023-12-26 06:39:56,233 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34099
2023-12-26 06:39:56,233 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,234 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,234 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,234 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,234 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ckdlqxb4
2023-12-26 06:39:56,235 - distributed.worker - INFO - Starting Worker plugin PreImport-ea14499d-5d23-4e3c-9969-0fc815aa6e6b
2023-12-26 06:39:56,235 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6034f957-a799-4e41-b44b-9e9bef823216
2023-12-26 06:39:56,236 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9638b9ba-637d-4db8-af3a-f3b8a8233767
2023-12-26 06:39:56,247 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,274 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44885', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,276 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44885
2023-12-26 06:39:56,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37092
2023-12-26 06:39:56,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,278 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,278 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37253
2023-12-26 06:39:56,363 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37253
2023-12-26 06:39:56,363 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37861
2023-12-26 06:39:56,363 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,363 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,363 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,363 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,363 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2iyrc5jq
2023-12-26 06:39:56,364 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1494ec3b-dbd6-4df6-a951-2f3cf526612b
2023-12-26 06:39:56,364 - distributed.worker - INFO - Starting Worker plugin PreImport-b15834c2-63b6-4b0c-a027-d2af6db553cf
2023-12-26 06:39:56,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd0eefea-a685-472f-aca8-61d482d65989
2023-12-26 06:39:56,372 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37595
2023-12-26 06:39:56,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37595
2023-12-26 06:39:56,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39893
2023-12-26 06:39:56,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,373 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,373 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cg1yy2bs
2023-12-26 06:39:56,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-779ea203-0316-4023-b506-d0520dad37b0
2023-12-26 06:39:56,377 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36361
2023-12-26 06:39:56,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36361
2023-12-26 06:39:56,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37065
2023-12-26 06:39:56,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,378 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,378 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fod1v5u0
2023-12-26 06:39:56,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ee81aec-f71e-41d7-89b7-14defaa7f55b
2023-12-26 06:39:56,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37393
2023-12-26 06:39:56,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37393
2023-12-26 06:39:56,387 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33019
2023-12-26 06:39:56,387 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,387 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,387 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,387 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3m4b_y_3
2023-12-26 06:39:56,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9e2e69f-4b7b-444f-bc94-3bede916eb5e
2023-12-26 06:39:56,388 - distributed.worker - INFO - Starting Worker plugin PreImport-967650aa-4bf3-4ed0-a005-3b8f806e4211
2023-12-26 06:39:56,389 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c425671-315f-452e-9f50-989381b858ec
2023-12-26 06:39:56,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34791
2023-12-26 06:39:56,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34791
2023-12-26 06:39:56,389 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37895
2023-12-26 06:39:56,389 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46005
2023-12-26 06:39:56,389 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,389 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46005
2023-12-26 06:39:56,390 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46095
2023-12-26 06:39:56,390 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,390 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,390 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,390 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4d7cjp83
2023-12-26 06:39:56,390 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,390 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cg7inz92
2023-12-26 06:39:56,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad0c90fd-a597-48af-9b0d-03d494f637f9
2023-12-26 06:39:56,390 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b743140c-8183-476c-8a0d-ca5e6ffc16ea
2023-12-26 06:39:56,391 - distributed.worker - INFO - Starting Worker plugin PreImport-26c2a2cb-4809-48e8-8c77-34da18c2c336
2023-12-26 06:39:56,391 - distributed.worker - INFO - Starting Worker plugin RMMSetup-962f4d57-6191-4998-9e29-58a48348c3e4
2023-12-26 06:39:56,390 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39809
2023-12-26 06:39:56,391 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39809
2023-12-26 06:39:56,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38307
2023-12-26 06:39:56,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,392 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,392 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:39:56,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:39:56,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bekj_4ol
2023-12-26 06:39:56,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad4bb6a9-e544-446b-8aeb-fb91e203e2a6
2023-12-26 06:39:56,439 - distributed.worker - INFO - Starting Worker plugin PreImport-12f0433b-0719-4600-922d-bb8d5c01ba72
2023-12-26 06:39:56,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a27aee34-1610-4337-81cf-42e435b1eb46
2023-12-26 06:39:56,440 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,446 - distributed.worker - INFO - Starting Worker plugin PreImport-5b60a6f1-7051-4265-b19c-6fad83ae60b6
2023-12-26 06:39:56,446 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8aa1a78d-4ea6-4845-8185-8073b5bc9fe9
2023-12-26 06:39:56,446 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,454 - distributed.worker - INFO - Starting Worker plugin PreImport-a61bb2a2-466b-41a1-822f-0b773451563e
2023-12-26 06:39:56,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e4d0a7ac-9902-4225-bfc9-c20c360b4d9e
2023-12-26 06:39:56,454 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,458 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,462 - distributed.worker - INFO - Starting Worker plugin PreImport-aac38943-5a97-40e0-ae6f-ecc191ead64c
2023-12-26 06:39:56,462 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5faff8f-f2fc-477c-accf-5455608c8507
2023-12-26 06:39:56,463 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,469 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,469 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37595', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37595
2023-12-26 06:39:56,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37100
2023-12-26 06:39:56,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,472 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39809', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,473 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39809
2023-12-26 06:39:56,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37106
2023-12-26 06:39:56,473 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,473 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,474 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,475 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,475 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,479 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34791', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,480 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34791
2023-12-26 06:39:56,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37122
2023-12-26 06:39:56,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,482 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,482 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,489 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36361', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,489 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36361
2023-12-26 06:39:56,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37152
2023-12-26 06:39:56,490 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37253', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,490 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,490 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37253
2023-12-26 06:39:56,490 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37136
2023-12-26 06:39:56,491 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,491 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,493 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,493 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,494 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46005', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,495 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46005
2023-12-26 06:39:56,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37156
2023-12-26 06:39:56,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,497 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,497 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37393', status: init, memory: 0, processing: 0>
2023-12-26 06:39:56,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37393
2023-12-26 06:39:56,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37158
2023-12-26 06:39:56,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:39:56,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,501 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:39:56,501 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:39:56,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:39:56,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:39:56,531 - distributed.scheduler - INFO - Remove client Client-91506369-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:56,531 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36994; closing.
2023-12-26 06:39:56,531 - distributed.scheduler - INFO - Remove client Client-91506369-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:56,532 - distributed.scheduler - INFO - Close client connection: Client-91506369-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:39:56,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44729'. Reason: nanny-close
2023-12-26 06:39:56,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39181'. Reason: nanny-close
2023-12-26 06:39:56,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44885. Reason: nanny-close
2023-12-26 06:39:56,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35753'. Reason: nanny-close
2023-12-26 06:39:56,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40935'. Reason: nanny-close
2023-12-26 06:39:56,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37595. Reason: nanny-close
2023-12-26 06:39:56,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41323'. Reason: nanny-close
2023-12-26 06:39:56,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39951'. Reason: nanny-close
2023-12-26 06:39:56,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36361. Reason: nanny-close
2023-12-26 06:39:56,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42401'. Reason: nanny-close
2023-12-26 06:39:56,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,537 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39997'. Reason: nanny-close
2023-12-26 06:39:56,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39809. Reason: nanny-close
2023-12-26 06:39:56,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37092; closing.
2023-12-26 06:39:56,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,538 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34791. Reason: nanny-close
2023-12-26 06:39:56,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44885', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.538538')
2023-12-26 06:39:56,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37100; closing.
2023-12-26 06:39:56,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,540 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,540 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,540 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.5405974')
2023-12-26 06:39:56,541 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,541 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,542 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37152; closing.
2023-12-26 06:39:56,543 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,543 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37100>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:39:56,545 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.545655')
2023-12-26 06:39:56,546 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37106; closing.
2023-12-26 06:39:56,546 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37122; closing.
2023-12-26 06:39:56,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39809', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.5466232')
2023-12-26 06:39:56,547 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.5469863')
2023-12-26 06:39:56,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46005. Reason: nanny-close
2023-12-26 06:39:56,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,550 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:39:56,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37393. Reason: nanny-close
2023-12-26 06:39:56,551 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37156; closing.
2023-12-26 06:39:56,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37253. Reason: nanny-close
2023-12-26 06:39:56,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46005', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.551305')
2023-12-26 06:39:56,552 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37158; closing.
2023-12-26 06:39:56,552 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,552 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.5529065')
2023-12-26 06:39:56,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:39:56,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37136; closing.
2023-12-26 06:39:56,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37253', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572796.5542305')
2023-12-26 06:39:56,554 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:56,554 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:39:56,555 - distributed.nanny - INFO - Worker closed
2023-12-26 06:39:58,000 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:39:58,000 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:39:58,001 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:39:58,002 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:39:58,002 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-26 06:40:00,090 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:00,095 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45183 instead
  warnings.warn(
2023-12-26 06:40:00,099 - distributed.scheduler - INFO - State start
2023-12-26 06:40:00,248 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:00,249 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:40:00,250 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45183/status
2023-12-26 06:40:00,250 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:00,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37487'
2023-12-26 06:40:01,140 - distributed.scheduler - INFO - Receive client connection: Client-966e8560-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:01,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58566
2023-12-26 06:40:02,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:02,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:02,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:03,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36575
2023-12-26 06:40:03,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36575
2023-12-26 06:40:03,509 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-26 06:40:03,509 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:03,509 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:03,509 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:03,509 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-26 06:40:03,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g_yy1m_s
2023-12-26 06:40:03,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69f02f76-fc31-419b-93d1-4dc993957ed4
2023-12-26 06:40:03,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e43c8a8-b9f4-4a15-970c-165ecea88153
2023-12-26 06:40:03,510 - distributed.worker - INFO - Starting Worker plugin PreImport-d9a007df-a5a1-4fa5-b76b-702c67a50b0b
2023-12-26 06:40:03,511 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:03,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36575', status: init, memory: 0, processing: 0>
2023-12-26 06:40:03,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36575
2023-12-26 06:40:03,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58584
2023-12-26 06:40:03,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:03,552 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:03,552 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:03,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:03,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:03,644 - distributed.scheduler - INFO - Remove client Client-966e8560-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:03,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58566; closing.
2023-12-26 06:40:03,645 - distributed.scheduler - INFO - Remove client Client-966e8560-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:03,645 - distributed.scheduler - INFO - Close client connection: Client-966e8560-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:03,646 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37487'. Reason: nanny-close
2023-12-26 06:40:03,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:03,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36575. Reason: nanny-close
2023-12-26 06:40:03,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58584; closing.
2023-12-26 06:40:03,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:03,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36575', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572803.6506956')
2023-12-26 06:40:03,651 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:40:03,652 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:04,713 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:04,713 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:04,714 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:04,715 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:40:04,715 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-26 06:40:08,633 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:08,637 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39031 instead
  warnings.warn(
2023-12-26 06:40:08,641 - distributed.scheduler - INFO - State start
2023-12-26 06:40:08,662 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:08,663 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:40:08,664 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39031/status
2023-12-26 06:40:08,664 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:08,691 - distributed.scheduler - INFO - Receive client connection: Client-9b9df24c-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:08,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58678
2023-12-26 06:40:08,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37419'
2023-12-26 06:40:10,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:10,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:10,813 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:11,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42679
2023-12-26 06:40:11,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42679
2023-12-26 06:40:11,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40873
2023-12-26 06:40:11,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:11,787 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:11,787 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:11,787 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-26 06:40:11,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e_2yv91x
2023-12-26 06:40:11,787 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e689e107-ab41-4159-be6b-fd89f25d146b
2023-12-26 06:40:11,788 - distributed.worker - INFO - Starting Worker plugin PreImport-f53e6c76-b73e-4862-912e-8240780f7108
2023-12-26 06:40:11,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1c21d7e-9d62-42a4-b829-91942e84e3e1
2023-12-26 06:40:11,790 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:11,817 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42679', status: init, memory: 0, processing: 0>
2023-12-26 06:40:11,818 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42679
2023-12-26 06:40:11,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54780
2023-12-26 06:40:11,819 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:11,820 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:11,820 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:11,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:11,856 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:11,858 - distributed.scheduler - INFO - Remove client Client-9b9df24c-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:11,859 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58678; closing.
2023-12-26 06:40:11,859 - distributed.scheduler - INFO - Remove client Client-9b9df24c-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:11,859 - distributed.scheduler - INFO - Close client connection: Client-9b9df24c-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:11,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37419'. Reason: nanny-close
2023-12-26 06:40:11,872 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:11,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42679. Reason: nanny-close
2023-12-26 06:40:11,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54780; closing.
2023-12-26 06:40:11,875 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:11,875 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572811.8757246')
2023-12-26 06:40:11,876 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:40:11,877 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:12,927 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:12,927 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:12,928 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:12,928 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:40:12,929 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-26 06:40:15,050 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:15,054 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46045 instead
  warnings.warn(
2023-12-26 06:40:15,058 - distributed.scheduler - INFO - State start
2023-12-26 06:40:15,082 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:15,083 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:40:15,084 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46045/status
2023-12-26 06:40:15,085 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:18,518 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:54812'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54812>: Stream is closed
2023-12-26 06:40:18,742 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:18,742 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:18,743 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:18,744 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:40:18,744 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-26 06:40:20,865 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:20,870 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39811 instead
  warnings.warn(
2023-12-26 06:40:20,874 - distributed.scheduler - INFO - State start
2023-12-26 06:40:20,896 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:20,897 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-26 06:40:20,898 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39811/status
2023-12-26 06:40:20,898 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:20,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39169'
2023-12-26 06:40:22,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:22,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:22,463 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:23,299 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42265
2023-12-26 06:40:23,300 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42265
2023-12-26 06:40:23,300 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37605
2023-12-26 06:40:23,300 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-26 06:40:23,300 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:23,300 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:23,300 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-26 06:40:23,300 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6_h90xi5
2023-12-26 06:40:23,300 - distributed.worker - INFO - Starting Worker plugin PreImport-a3973431-3de8-4bb9-bb83-0d2f8f952331
2023-12-26 06:40:23,301 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f20ebdd5-5657-4466-99fd-ac2f8bd428f3
2023-12-26 06:40:23,301 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0ecaf24-0408-4ee2-ab21-b433e807e3c7
2023-12-26 06:40:23,301 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:23,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42265', status: init, memory: 0, processing: 0>
2023-12-26 06:40:23,345 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42265
2023-12-26 06:40:23,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41986
2023-12-26 06:40:23,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:23,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-26 06:40:23,348 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:23,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-26 06:40:25,391 - distributed.scheduler - INFO - Receive client connection: Client-a2d391b2-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:25,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41998
2023-12-26 06:40:25,398 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:25,405 - distributed.scheduler - INFO - Remove client Client-a2d391b2-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:25,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41998; closing.
2023-12-26 06:40:25,405 - distributed.scheduler - INFO - Remove client Client-a2d391b2-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:25,405 - distributed.scheduler - INFO - Close client connection: Client-a2d391b2-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:25,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39169'. Reason: nanny-close
2023-12-26 06:40:25,407 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:25,408 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42265. Reason: nanny-close
2023-12-26 06:40:25,410 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41986; closing.
2023-12-26 06:40:25,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-26 06:40:25,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572825.4108896')
2023-12-26 06:40:25,411 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:40:25,412 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:26,423 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:26,423 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:26,424 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:26,425 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-26 06:40:26,425 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-26 06:40:28,555 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:28,560 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-26 06:40:28,563 - distributed.scheduler - INFO - State start
2023-12-26 06:40:28,586 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:28,588 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:40:28,588 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-26 06:40:28,589 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:28,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46831'
2023-12-26 06:40:28,636 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43903'
2023-12-26 06:40:28,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45623'
2023-12-26 06:40:28,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33685'
2023-12-26 06:40:28,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45229'
2023-12-26 06:40:28,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46235'
2023-12-26 06:40:28,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43975'
2023-12-26 06:40:28,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32979'
2023-12-26 06:40:29,730 - distributed.scheduler - INFO - Receive client connection: Client-a76ec576-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:29,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54626
2023-12-26 06:40:30,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,537 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,554 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,559 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:30,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:30,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:30,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:32,912 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42227
2023-12-26 06:40:32,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42227
2023-12-26 06:40:32,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43059
2023-12-26 06:40:32,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:32,913 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:32,913 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:32,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:32,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u2h8mrg4
2023-12-26 06:40:32,913 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38925
2023-12-26 06:40:32,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38925
2023-12-26 06:40:32,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43737
2023-12-26 06:40:32,914 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:32,914 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5c1f16d-bff4-4fb0-8148-0e37818098f8
2023-12-26 06:40:32,914 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:32,914 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:32,914 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:32,914 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sfl_bdut
2023-12-26 06:40:32,914 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2e10d07-2b7f-4eee-8520-b2135e5eda3e
2023-12-26 06:40:32,915 - distributed.worker - INFO - Starting Worker plugin PreImport-a0c6ac63-2e47-4b34-92a8-685e4bd617c9
2023-12-26 06:40:32,915 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0144d6ba-61c1-4275-a3ae-4097f46538da
2023-12-26 06:40:32,932 - distributed.worker - INFO - Starting Worker plugin PreImport-ea765314-ce6a-4b36-ad81-c655e0397b31
2023-12-26 06:40:32,932 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b29cb3b0-e325-45cb-90d2-4d2a8f5c4f4e
2023-12-26 06:40:32,933 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:32,935 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:32,959 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38925', status: init, memory: 0, processing: 0>
2023-12-26 06:40:32,963 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38925
2023-12-26 06:40:32,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44602
2023-12-26 06:40:32,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:32,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42227', status: init, memory: 0, processing: 0>
2023-12-26 06:40:32,964 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:32,964 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:32,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42227
2023-12-26 06:40:32,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44592
2023-12-26 06:40:32,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:32,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:32,967 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:32,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:32,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45371
2023-12-26 06:40:33,036 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45371
2023-12-26 06:40:33,036 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35281
2023-12-26 06:40:33,036 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,036 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,036 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:33,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:33,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-24a_zwas
2023-12-26 06:40:33,037 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66fb016c-ad13-4ee8-9d63-be6d5a0fad62
2023-12-26 06:40:33,037 - distributed.worker - INFO - Starting Worker plugin PreImport-040568d5-8045-4b63-a1a4-80c4d0cae2bc
2023-12-26 06:40:33,038 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f726426f-b11b-4b41-98de-a32558b44912
2023-12-26 06:40:33,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34579
2023-12-26 06:40:33,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34579
2023-12-26 06:40:33,041 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39481
2023-12-26 06:40:33,041 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,041 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,041 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:33,041 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:33,041 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kagtm7b4
2023-12-26 06:40:33,041 - distributed.worker - INFO - Starting Worker plugin PreImport-e1101a45-bc01-48f6-8473-ac4d15ea506a
2023-12-26 06:40:33,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93501fdd-74c1-4d9a-8a12-0ef54d9b4f48
2023-12-26 06:40:33,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbfc8deb-8978-450c-845f-a3ebb9e72473
2023-12-26 06:40:33,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38475
2023-12-26 06:40:33,043 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38475
2023-12-26 06:40:33,043 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46613
2023-12-26 06:40:33,043 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,043 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,043 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:33,044 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:33,044 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-clxe2xji
2023-12-26 06:40:33,044 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52c84796-9bd5-4832-aef3-18b4a3c9c3e6
2023-12-26 06:40:33,044 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41319
2023-12-26 06:40:33,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41319
2023-12-26 06:40:33,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38847
2023-12-26 06:40:33,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,045 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,045 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:33,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:33,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9yvg98zw
2023-12-26 06:40:33,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0fd8ebf-5c40-4d74-bcd3-1a558668349e
2023-12-26 06:40:33,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39939
2023-12-26 06:40:33,050 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39939
2023-12-26 06:40:33,050 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44695
2023-12-26 06:40:33,050 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,050 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,050 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:33,050 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:33,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mvcj97fn
2023-12-26 06:40:33,051 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f28b65d-c25c-4f1b-847b-f3f1f50b16c9
2023-12-26 06:40:33,054 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39951
2023-12-26 06:40:33,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39951
2023-12-26 06:40:33,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44029
2023-12-26 06:40:33,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,055 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,055 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:33,056 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-26 06:40:33,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r5eh7l50
2023-12-26 06:40:33,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dfd1d9f-e970-4004-8607-4335a7237b71
2023-12-26 06:40:33,057 - distributed.worker - INFO - Starting Worker plugin PreImport-0280759a-bc21-4753-8d12-7771876c7f63
2023-12-26 06:40:33,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bef735b4-6198-4d14-b652-4d991ae456ea
2023-12-26 06:40:33,105 - distributed.worker - INFO - Starting Worker plugin PreImport-3584ea38-4094-46f1-9eca-03bc204150e0
2023-12-26 06:40:33,105 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,105 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18893709-c8fa-49e0-a294-f9a0a4264b08
2023-12-26 06:40:33,105 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,107 - distributed.worker - INFO - Starting Worker plugin PreImport-1b05eb70-c4cd-4a9b-b91e-97e57a69cb05
2023-12-26 06:40:33,108 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f68bf0e-2b8f-4be1-8cd1-5d9b35ece07c
2023-12-26 06:40:33,108 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,111 - distributed.worker - INFO - Starting Worker plugin PreImport-1bd48acb-7492-49a5-a2e9-4efe8f704569
2023-12-26 06:40:33,111 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c71c006-51d2-4885-900c-24adda201468
2023-12-26 06:40:33,111 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,115 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,117 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,129 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38475', status: init, memory: 0, processing: 0>
2023-12-26 06:40:33,130 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38475
2023-12-26 06:40:33,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44610
2023-12-26 06:40:33,131 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:33,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,132 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41319', status: init, memory: 0, processing: 0>
2023-12-26 06:40:33,132 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,132 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41319
2023-12-26 06:40:33,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44626
2023-12-26 06:40:33,133 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:33,134 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,134 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,135 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39939', status: init, memory: 0, processing: 0>
2023-12-26 06:40:33,135 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39939
2023-12-26 06:40:33,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44640
2023-12-26 06:40:33,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,136 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34579', status: init, memory: 0, processing: 0>
2023-12-26 06:40:33,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:33,137 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34579
2023-12-26 06:40:33,137 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44622
2023-12-26 06:40:33,137 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,137 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,138 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:33,139 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,139 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,144 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45371', status: init, memory: 0, processing: 0>
2023-12-26 06:40:33,145 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45371
2023-12-26 06:40:33,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44652
2023-12-26 06:40:33,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,146 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:33,147 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,147 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,148 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39951', status: init, memory: 0, processing: 0>
2023-12-26 06:40:33,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39951
2023-12-26 06:40:33,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44660
2023-12-26 06:40:33,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:33,150 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:33,150 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:33,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:33,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,191 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,192 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:33,196 - distributed.scheduler - INFO - Remove client Client-a76ec576-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:33,197 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54626; closing.
2023-12-26 06:40:33,197 - distributed.scheduler - INFO - Remove client Client-a76ec576-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:33,197 - distributed.scheduler - INFO - Close client connection: Client-a76ec576-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:33,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46831'. Reason: nanny-close
2023-12-26 06:40:33,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43903'. Reason: nanny-close
2023-12-26 06:40:33,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,200 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34579. Reason: nanny-close
2023-12-26 06:40:33,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45623'. Reason: nanny-close
2023-12-26 06:40:33,200 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,200 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42227. Reason: nanny-close
2023-12-26 06:40:33,201 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33685'. Reason: nanny-close
2023-12-26 06:40:33,201 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38925. Reason: nanny-close
2023-12-26 06:40:33,201 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45229'. Reason: nanny-close
2023-12-26 06:40:33,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,202 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38475. Reason: nanny-close
2023-12-26 06:40:33,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,202 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44622; closing.
2023-12-26 06:40:33,202 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46235'. Reason: nanny-close
2023-12-26 06:40:33,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34579', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2027023')
2023-12-26 06:40:33,202 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45371. Reason: nanny-close
2023-12-26 06:40:33,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43975'. Reason: nanny-close
2023-12-26 06:40:33,203 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,203 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39951. Reason: nanny-close
2023-12-26 06:40:33,204 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44602; closing.
2023-12-26 06:40:33,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,204 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,204 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32979'. Reason: nanny-close
2023-12-26 06:40:33,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:33,204 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39939. Reason: nanny-close
2023-12-26 06:40:33,204 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2048514')
2023-12-26 06:40:33,205 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41319. Reason: nanny-close
2023-12-26 06:40:33,205 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,205 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,205 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44592; closing.
2023-12-26 06:40:33,205 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,205 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:33,208 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,205 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44602>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44602>: Stream is closed
2023-12-26 06:40:33,208 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,208 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,208 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42227', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2087202')
2023-12-26 06:40:33,209 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44610; closing.
2023-12-26 06:40:33,209 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:33,209 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38475', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2099013')
2023-12-26 06:40:33,210 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44652; closing.
2023-12-26 06:40:33,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45371', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2111368')
2023-12-26 06:40:33,211 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44660; closing.
2023-12-26 06:40:33,211 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44640; closing.
2023-12-26 06:40:33,212 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2124314')
2023-12-26 06:40:33,212 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39939', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2128')
2023-12-26 06:40:33,213 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44626; closing.
2023-12-26 06:40:33,213 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41319', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572833.2136157')
2023-12-26 06:40:33,213 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:40:33,213 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44640>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:40:33,214 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44660>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-26 06:40:34,665 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:34,666 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:34,667 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:34,668 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:40:34,668 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-26 06:40:36,844 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:36,848 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-26 06:40:36,851 - distributed.scheduler - INFO - State start
2023-12-26 06:40:36,873 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:36,874 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:40:36,874 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-26 06:40:36,875 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:36,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37533'
2023-12-26 06:40:38,078 - distributed.scheduler - INFO - Receive client connection: Client-ac58c512-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:38,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44742
2023-12-26 06:40:38,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:38,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:38,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:39,149 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39405
2023-12-26 06:40:39,149 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39405
2023-12-26 06:40:39,149 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33893
2023-12-26 06:40:39,149 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:39,149 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:39,149 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:39,149 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-26 06:40:39,149 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rf0h98eq
2023-12-26 06:40:39,150 - distributed.worker - INFO - Starting Worker plugin PreImport-aaa9db59-dc9d-4918-a5a5-c48e2ef8c4ee
2023-12-26 06:40:39,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e30cd68-3979-4013-b810-f4f0f0ba1a30
2023-12-26 06:40:39,150 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4df1cb8f-81e8-4ee8-9cd4-61cb27bfe468
2023-12-26 06:40:39,155 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:39,178 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39405', status: init, memory: 0, processing: 0>
2023-12-26 06:40:39,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39405
2023-12-26 06:40:39,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44770
2023-12-26 06:40:39,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:39,181 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:39,181 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:39,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:39,216 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:40:39,220 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:39,222 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:39,224 - distributed.scheduler - INFO - Remove client Client-ac58c512-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:39,225 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44742; closing.
2023-12-26 06:40:39,225 - distributed.scheduler - INFO - Remove client Client-ac58c512-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:39,225 - distributed.scheduler - INFO - Close client connection: Client-ac58c512-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:39,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37533'. Reason: nanny-close
2023-12-26 06:40:39,227 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:39,228 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39405. Reason: nanny-close
2023-12-26 06:40:39,230 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:39,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44770; closing.
2023-12-26 06:40:39,230 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39405', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572839.2307348')
2023-12-26 06:40:39,231 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:40:39,237 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:40,243 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:40,243 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:40,244 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:40,245 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:40:40,245 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-26 06:40:42,415 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:42,420 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-26 06:40:42,424 - distributed.scheduler - INFO - State start
2023-12-26 06:40:42,448 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-26 06:40:42,450 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-26 06:40:42,450 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-26 06:40:42,451 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-26 06:40:42,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35875'
2023-12-26 06:40:43,969 - distributed.scheduler - INFO - Receive client connection: Client-afb667b6-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:43,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44404
2023-12-26 06:40:44,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-26 06:40:44,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-26 06:40:44,036 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-26 06:40:44,824 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38933
2023-12-26 06:40:44,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38933
2023-12-26 06:40:44,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33591
2023-12-26 06:40:44,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-26 06:40:44,825 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:44,825 - distributed.worker - INFO -               Threads:                          1
2023-12-26 06:40:44,825 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-26 06:40:44,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t7zloak2
2023-12-26 06:40:44,825 - distributed.worker - INFO - Starting Worker plugin PreImport-87f12813-1589-4253-9481-c2e52f2bc023
2023-12-26 06:40:44,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2a5fdcd-b0e5-414b-a3b7-448258849e3b
2023-12-26 06:40:44,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-050e1f09-1121-46f1-ba19-3cbb5627f1ef
2023-12-26 06:40:44,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:44,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38933', status: init, memory: 0, processing: 0>
2023-12-26 06:40:44,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38933
2023-12-26 06:40:44,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44422
2023-12-26 06:40:44,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-26 06:40:44,864 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-26 06:40:44,864 - distributed.worker - INFO - -------------------------------------------------
2023-12-26 06:40:44,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-26 06:40:44,907 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-26 06:40:44,912 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-26 06:40:44,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:44,917 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-26 06:40:44,919 - distributed.scheduler - INFO - Remove client Client-afb667b6-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:44,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44404; closing.
2023-12-26 06:40:44,919 - distributed.scheduler - INFO - Remove client Client-afb667b6-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:44,920 - distributed.scheduler - INFO - Close client connection: Client-afb667b6-a3b9-11ee-b4f3-d8c49764f6bb
2023-12-26 06:40:44,921 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35875'. Reason: nanny-close
2023-12-26 06:40:44,921 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-26 06:40:44,922 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38933. Reason: nanny-close
2023-12-26 06:40:44,924 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-26 06:40:44,924 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44422; closing.
2023-12-26 06:40:44,924 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38933', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703572844.924784')
2023-12-26 06:40:44,925 - distributed.scheduler - INFO - Lost all workers
2023-12-26 06:40:44,926 - distributed.nanny - INFO - Worker closed
2023-12-26 06:40:45,937 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-26 06:40:45,937 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-26 06:40:45,938 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-26 06:40:45,939 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-26 06:40:45,939 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34039 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39147 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34293 instead
  warnings.warn(
2023-12-26 06:41:43,560 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:41:43,566 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:42603', name: 6, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33619 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40541 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33939 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41797 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35397 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39327 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41547 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44425 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33817 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37437 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36445 instead
  warnings.warn(
[1703573118.470818] [dgx13:71741:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:34208) failed: Address already in use
[1703573118.470871] [dgx13:71741:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:54786) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37767 instead
  warnings.warn(
[1703573140.168338] [dgx13:72198:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:44532) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38771 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38795 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32969 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35515 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42741 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44359 instead
  warnings.warn(
2023-12-26 06:49:46,149 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:49:46,152 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:49:46,156 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:55940', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:49:46,163 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:38050', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35623 instead
  warnings.warn(
[1703573391.735903] [dgx13:76822:0]            sock.c:470  UCX  ERROR bind(fd=161 addr=0.0.0.0:33648) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36695 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37275 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41505 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33039 instead
  warnings.warn(
2023-12-26 06:51:20,478 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:51:20,481 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:39817', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:51:20,481 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:51:20,482 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:51:20,483 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:42285', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-26 06:51:20,484 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:38535', name: 2, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40071 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37371 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38285 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43157 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers 2023-12-26 06:53:15,571 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60784 remote=tcp://127.0.0.1:33835>: Stream is closed
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1703573597.149284] [dgx13:62707:0]            sock.c:470  UCX  ERROR bind(fd=186 addr=0.0.0.0:37472) failed: Address already in use
2023-12-26 06:53:21,813 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:53:21,821 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:25,721 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:53:25,727 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:26,271 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:53:26,271 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:53:26,275 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:26,275 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:26,497 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:53:26,500 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:26,910 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 2
2023-12-26 06:53:26,915 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 2', 'time': 1703573606.9098766}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:26,989 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 1
2023-12-26 06:53:26,996 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 1', 'time': 1703573606.988352}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:27,000 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 0
2023-12-26 06:53:27,004 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 0', 'time': 1703573606.9996836}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:53:27,013 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 3
2023-12-26 06:53:27,016 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 3', 'time': 1703573607.0122166}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-12-26 06:53:33,949 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38087 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38257 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-12-26 06:54:08,206 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:54:08,211 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-12-26 06:54:08,267 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-12-26 06:54:08,272 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 63 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
