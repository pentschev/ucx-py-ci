============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-08-07 05:38:38,172 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:38:38,176 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36347 instead
  warnings.warn(
2023-08-07 05:38:38,179 - distributed.scheduler - INFO - State start
2023-08-07 05:38:38,200 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:38:38,201 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-07 05:38:38,201 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36347/status
2023-08-07 05:38:38,319 - distributed.scheduler - INFO - Receive client connection: Client-a7baf8a4-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:38,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60584
2023-08-07 05:38:38,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42951'
2023-08-07 05:38:38,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37145'
2023-08-07 05:38:38,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33987'
2023-08-07 05:38:38,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44287'
2023-08-07 05:38:39,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:39,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:39,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:39,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:39,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:39,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:39,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:39,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:39,998 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:39,999 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:40,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:40,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-08-07 05:38:40,018 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42293
2023-08-07 05:38:40,018 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42293
2023-08-07 05:38:40,018 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34849
2023-08-07 05:38:40,018 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-07 05:38:40,018 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:40,018 - distributed.worker - INFO -               Threads:                          4
2023-08-07 05:38:40,019 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-07 05:38:40,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-90sqjf93
2023-08-07 05:38:40,019 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fe49af8-0ba9-4769-bee5-919e6871a04c
2023-08-07 05:38:40,019 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea46cb84-5f97-46ea-8fc7-6d733309a803
2023-08-07 05:38:40,019 - distributed.worker - INFO - Starting Worker plugin PreImport-45c8de17-44b7-4ac8-910a-5383091087ed
2023-08-07 05:38:40,019 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:40,034 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42293', status: init, memory: 0, processing: 0>
2023-08-07 05:38:40,035 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42293
2023-08-07 05:38:40,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60636
2023-08-07 05:38:40,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-07 05:38:40,036 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:40,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-07 05:38:41,321 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45997
2023-08-07 05:38:41,321 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45997
2023-08-07 05:38:41,321 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44373
2023-08-07 05:38:41,321 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-07 05:38:41,321 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,322 - distributed.worker - INFO -               Threads:                          4
2023-08-07 05:38:41,322 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-07 05:38:41,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nk6i68eu
2023-08-07 05:38:41,322 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65733716-9a7a-45a3-9011-8747702b7e8b
2023-08-07 05:38:41,322 - distributed.worker - INFO - Starting Worker plugin PreImport-d36cda49-60b2-4956-895b-85ae28dd5480
2023-08-07 05:38:41,322 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f09e7c6c-c2b8-4b10-9415-9c188bdece5d
2023-08-07 05:38:41,322 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,335 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44707
2023-08-07 05:38:41,335 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44707
2023-08-07 05:38:41,335 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32883
2023-08-07 05:38:41,335 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-07 05:38:41,336 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,336 - distributed.worker - INFO -               Threads:                          4
2023-08-07 05:38:41,336 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-07 05:38:41,336 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w5wxjj__
2023-08-07 05:38:41,336 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a699383-3967-4c01-ac74-e7b5be2630b2
2023-08-07 05:38:41,336 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-416df32b-ef0a-4e1b-901e-b8ebc40568c4
2023-08-07 05:38:41,338 - distributed.worker - INFO - Starting Worker plugin PreImport-7c60c2e1-16bb-4547-9391-78163ec6435b
2023-08-07 05:38:41,339 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,342 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45997', status: init, memory: 0, processing: 0>
2023-08-07 05:38:41,342 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45997
2023-08-07 05:38:41,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60652
2023-08-07 05:38:41,343 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-07 05:38:41,343 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-07 05:38:41,360 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44707', status: init, memory: 0, processing: 0>
2023-08-07 05:38:41,361 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44707
2023-08-07 05:38:41,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60666
2023-08-07 05:38:41,361 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-07 05:38:41,361 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-07 05:38:41,459 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33061
2023-08-07 05:38:41,459 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33061
2023-08-07 05:38:41,459 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41543
2023-08-07 05:38:41,460 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-07 05:38:41,460 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,460 - distributed.worker - INFO -               Threads:                          4
2023-08-07 05:38:41,460 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-07 05:38:41,460 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-swzc0a2f
2023-08-07 05:38:41,460 - distributed.worker - INFO - Starting Worker plugin PreImport-b40d711e-2cf9-445d-bc00-bd1427c40c9c
2023-08-07 05:38:41,460 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac3c865c-be7a-4c36-948c-2319e844d3f5
2023-08-07 05:38:41,461 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c371a7ff-7f00-46ec-ac40-34c6acc60136
2023-08-07 05:38:41,461 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,483 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33061', status: init, memory: 0, processing: 0>
2023-08-07 05:38:41,483 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33061
2023-08-07 05:38:41,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60670
2023-08-07 05:38:41,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-07 05:38:41,484 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:41,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-07 05:38:41,588 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-07 05:38:41,588 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-07 05:38:41,588 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-07 05:38:41,588 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-07 05:38:41,593 - distributed.scheduler - INFO - Remove client Client-a7baf8a4-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:41,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60584; closing.
2023-08-07 05:38:41,593 - distributed.scheduler - INFO - Remove client Client-a7baf8a4-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:41,593 - distributed.scheduler - INFO - Close client connection: Client-a7baf8a4-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:41,594 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37145'. Reason: nanny-close
2023-08-07 05:38:41,595 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:41,595 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42951'. Reason: nanny-close
2023-08-07 05:38:41,596 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:41,596 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33987'. Reason: nanny-close
2023-08-07 05:38:41,596 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44707. Reason: nanny-close
2023-08-07 05:38:41,596 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:41,597 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45997. Reason: nanny-close
2023-08-07 05:38:41,597 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44287'. Reason: nanny-close
2023-08-07 05:38:41,597 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:41,598 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33061. Reason: nanny-close
2023-08-07 05:38:41,598 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-07 05:38:41,598 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60666; closing.
2023-08-07 05:38:41,598 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42293. Reason: nanny-close
2023-08-07 05:38:41,599 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44707', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:41,599 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-07 05:38:41,599 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44707
2023-08-07 05:38:41,599 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-07 05:38:41,599 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:41,600 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60652; closing.
2023-08-07 05:38:41,600 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:41,600 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-07 05:38:41,600 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45997', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:41,600 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45997
2023-08-07 05:38:41,601 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:41,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60670; closing.
2023-08-07 05:38:41,601 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33061', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:41,601 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33061
2023-08-07 05:38:41,602 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:41,602 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60636; closing.
2023-08-07 05:38:41,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42293', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:41,602 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42293
2023-08-07 05:38:41,602 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:38:42,761 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:38:42,761 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:38:42,762 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:38:42,763 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-07 05:38:42,763 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-08-07 05:38:44,617 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:38:44,622 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42019 instead
  warnings.warn(
2023-08-07 05:38:44,626 - distributed.scheduler - INFO - State start
2023-08-07 05:38:44,645 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:38:44,646 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:38:44,647 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42019/status
2023-08-07 05:38:44,790 - distributed.scheduler - INFO - Receive client connection: Client-ab9362d7-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:44,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35598
2023-08-07 05:38:44,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35037'
2023-08-07 05:38:44,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32789'
2023-08-07 05:38:44,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38295'
2023-08-07 05:38:44,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42433'
2023-08-07 05:38:44,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43321'
2023-08-07 05:38:44,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35039'
2023-08-07 05:38:44,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46127'
2023-08-07 05:38:44,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41001'
2023-08-07 05:38:46,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,669 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,711 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:46,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:46,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:46,773 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:50,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36225
2023-08-07 05:38:50,569 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36225
2023-08-07 05:38:50,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37943
2023-08-07 05:38:50,569 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,569 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,569 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:50,569 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:50,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s1w4n627
2023-08-07 05:38:50,570 - distributed.worker - INFO - Starting Worker plugin PreImport-66f7e33a-4920-4611-83d3-63049cbfe537
2023-08-07 05:38:50,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20a0c445-852d-414f-b5be-6cfa31a4a05c
2023-08-07 05:38:50,570 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1fcf273-f77d-4886-bb1c-cd453a962684
2023-08-07 05:38:50,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41537
2023-08-07 05:38:50,652 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41537
2023-08-07 05:38:50,652 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42737
2023-08-07 05:38:50,652 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,652 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,652 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:50,652 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:50,652 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uuklx5zb
2023-08-07 05:38:50,652 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19593ed4-13a4-49a4-93b3-ea16e8dc3aa7
2023-08-07 05:38:50,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44117
2023-08-07 05:38:50,657 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44117
2023-08-07 05:38:50,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44233
2023-08-07 05:38:50,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,657 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,657 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:50,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:50,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o_vv4ztx
2023-08-07 05:38:50,658 - distributed.worker - INFO - Starting Worker plugin PreImport-433f88bd-da64-4830-a6c8-a6270d39e4f8
2023-08-07 05:38:50,658 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d999fee1-623f-4cab-90f9-58b6408b4845
2023-08-07 05:38:50,658 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39130af1-ef46-40f0-a276-3ec34f38665b
2023-08-07 05:38:50,661 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43815
2023-08-07 05:38:50,661 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43815
2023-08-07 05:38:50,661 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33355
2023-08-07 05:38:50,661 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,661 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,662 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:50,662 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:50,662 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7x6w81hb
2023-08-07 05:38:50,662 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a1b00d4-d5e9-4a78-aee6-e351a73faa33
2023-08-07 05:38:50,665 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37699
2023-08-07 05:38:50,666 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37699
2023-08-07 05:38:50,666 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35635
2023-08-07 05:38:50,666 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,666 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,666 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:50,666 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:50,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_s6gc4ln
2023-08-07 05:38:50,666 - distributed.worker - INFO - Starting Worker plugin PreImport-309d41e5-2b07-4077-84c4-df68fc57d28e
2023-08-07 05:38:50,667 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14683314-1408-4679-80e4-90c6410c9963
2023-08-07 05:38:50,667 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2459130-a199-4ae8-b7c6-6585ce2d2614
2023-08-07 05:38:50,667 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41131
2023-08-07 05:38:50,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41131
2023-08-07 05:38:50,668 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35839
2023-08-07 05:38:50,668 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,668 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,668 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:50,668 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:50,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2sqxge6z
2023-08-07 05:38:50,669 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86b56a5c-3c13-4cf7-bd3e-1bee1eda36c5
2023-08-07 05:38:50,913 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,944 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,948 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36225', status: init, memory: 0, processing: 0>
2023-08-07 05:38:50,950 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36225
2023-08-07 05:38:50,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35678
2023-08-07 05:38:50,950 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,951 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:50,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0aeca2ca-0699-4ad4-9141-15f28be160e8
2023-08-07 05:38:50,975 - distributed.worker - INFO - Starting Worker plugin PreImport-3f023cf6-aeac-4083-abbb-f4d87d7f1eec
2023-08-07 05:38:50,975 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,977 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,981 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa5a4970-e9ec-48e0-98e2-fae4b8b91676
2023-08-07 05:38:50,981 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7a5ed76-049e-48f6-b15d-431050d3797d
2023-08-07 05:38:50,982 - distributed.worker - INFO - Starting Worker plugin PreImport-921a1761-0c60-4425-a599-d6732023a9e8
2023-08-07 05:38:50,982 - distributed.worker - INFO - Starting Worker plugin PreImport-f3733887-76a5-4150-b513-c78d05986161
2023-08-07 05:38:50,982 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,983 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,988 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37699', status: init, memory: 0, processing: 0>
2023-08-07 05:38:50,989 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37699
2023-08-07 05:38:50,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35694
2023-08-07 05:38:50,989 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:50,990 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:50,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43815', status: init, memory: 0, processing: 0>
2023-08-07 05:38:51,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43815
2023-08-07 05:38:51,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35698
2023-08-07 05:38:51,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,008 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,008 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44117', status: init, memory: 0, processing: 0>
2023-08-07 05:38:51,009 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44117
2023-08-07 05:38:51,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35700
2023-08-07 05:38:51,009 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,009 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,023 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41131', status: init, memory: 0, processing: 0>
2023-08-07 05:38:51,024 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41131
2023-08-07 05:38:51,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35720
2023-08-07 05:38:51,024 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,024 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41537', status: init, memory: 0, processing: 0>
2023-08-07 05:38:51,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41537
2023-08-07 05:38:51,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35704
2023-08-07 05:38:51,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,026 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40245
2023-08-07 05:38:51,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40245
2023-08-07 05:38:51,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37863
2023-08-07 05:38:51,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,046 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,046 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:51,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:51,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x51avu60
2023-08-07 05:38:51,047 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ea8d158-6d79-4b03-b405-e91f3c091e2b
2023-08-07 05:38:51,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5952be39-6116-4c89-b6db-746c1ade385a
2023-08-07 05:38:51,335 - distributed.worker - INFO - Starting Worker plugin PreImport-a4695573-df64-443c-93ce-23add97c00ae
2023-08-07 05:38:51,335 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,356 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39899
2023-08-07 05:38:51,357 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39899
2023-08-07 05:38:51,357 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39541
2023-08-07 05:38:51,357 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,357 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,357 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:38:51,357 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:38:51,357 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6tq1bnxd
2023-08-07 05:38:51,358 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59683a28-0e8d-4ed7-9bda-a09d8715b9f9
2023-08-07 05:38:51,369 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40245', status: init, memory: 0, processing: 0>
2023-08-07 05:38:51,370 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40245
2023-08-07 05:38:51,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35732
2023-08-07 05:38:51,370 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,370 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-360d305c-aff3-4e83-9db9-bf63e3a269b1
2023-08-07 05:38:51,651 - distributed.worker - INFO - Starting Worker plugin PreImport-f007a358-fbaf-4a83-b441-da1ea19715a3
2023-08-07 05:38:51,652 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,676 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39899', status: init, memory: 0, processing: 0>
2023-08-07 05:38:51,676 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39899
2023-08-07 05:38:51,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35746
2023-08-07 05:38:51,677 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:38:51,677 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:38:51,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:38:51,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,697 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,697 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,697 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:38:51,701 - distributed.scheduler - INFO - Remove client Client-ab9362d7-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:51,701 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35598; closing.
2023-08-07 05:38:51,701 - distributed.scheduler - INFO - Remove client Client-ab9362d7-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:51,701 - distributed.scheduler - INFO - Close client connection: Client-ab9362d7-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:51,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42433'. Reason: nanny-close
2023-08-07 05:38:51,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35037'. Reason: nanny-close
2023-08-07 05:38:51,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32789'. Reason: nanny-close
2023-08-07 05:38:51,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41537. Reason: nanny-close
2023-08-07 05:38:51,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38295'. Reason: nanny-close
2023-08-07 05:38:51,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40245. Reason: nanny-close
2023-08-07 05:38:51,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43321'. Reason: nanny-close
2023-08-07 05:38:51,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,708 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36225. Reason: nanny-close
2023-08-07 05:38:51,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35039'. Reason: nanny-close
2023-08-07 05:38:51,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46127'. Reason: nanny-close
2023-08-07 05:38:51,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37699. Reason: nanny-close
2023-08-07 05:38:51,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,709 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35704; closing.
2023-08-07 05:38:51,710 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41001'. Reason: nanny-close
2023-08-07 05:38:51,710 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41537', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,710 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41131. Reason: nanny-close
2023-08-07 05:38:51,710 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,710 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41537
2023-08-07 05:38:51,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,710 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,710 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44117. Reason: nanny-close
2023-08-07 05:38:51,711 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35678; closing.
2023-08-07 05:38:51,711 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,711 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,711 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43815. Reason: nanny-close
2023-08-07 05:38:51,711 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,711 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41537
2023-08-07 05:38:51,711 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41537
2023-08-07 05:38:51,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36225', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,712 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36225
2023-08-07 05:38:51,712 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,712 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41537
2023-08-07 05:38:51,712 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35732; closing.
2023-08-07 05:38:51,712 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41537
2023-08-07 05:38:51,712 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,713 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,713 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41537
2023-08-07 05:38:51,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40245', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,713 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40245
2023-08-07 05:38:51,713 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35694; closing.
2023-08-07 05:38:51,713 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35700; closing.
2023-08-07 05:38:51,714 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35720; closing.
2023-08-07 05:38:51,714 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37699', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,714 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37699
2023-08-07 05:38:51,714 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:51,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44117', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,715 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44117
2023-08-07 05:38:51,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41131', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,715 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41131
2023-08-07 05:38:51,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35698; closing.
2023-08-07 05:38:51,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43815', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,716 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43815
2023-08-07 05:38:51,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36225
2023-08-07 05:38:51,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40245
2023-08-07 05:38:51,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37699
2023-08-07 05:38:51,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44117
2023-08-07 05:38:51,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41131
2023-08-07 05:38:51,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43815
2023-08-07 05:38:51,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:38:51,723 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39899. Reason: nanny-close
2023-08-07 05:38:51,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35746; closing.
2023-08-07 05:38:51,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:38:51,725 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39899', status: closing, memory: 0, processing: 0>
2023-08-07 05:38:51,725 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39899
2023-08-07 05:38:51,725 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:38:51,726 - distributed.nanny - INFO - Worker closed
2023-08-07 05:38:53,421 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:38:53,421 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:38:53,422 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:38:53,423 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:38:53,424 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-08-07 05:38:55,311 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:38:55,315 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32971 instead
  warnings.warn(
2023-08-07 05:38:55,319 - distributed.scheduler - INFO - State start
2023-08-07 05:38:55,338 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:38:55,339 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:38:55,339 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32971/status
2023-08-07 05:38:55,538 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40409'
2023-08-07 05:38:55,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39241'
2023-08-07 05:38:55,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34257'
2023-08-07 05:38:55,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37119'
2023-08-07 05:38:55,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35283'
2023-08-07 05:38:55,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36349'
2023-08-07 05:38:55,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40183'
2023-08-07 05:38:55,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43305'
2023-08-07 05:38:55,751 - distributed.scheduler - INFO - Receive client connection: Client-b1f2a55c-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:38:55,764 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47526
2023-08-07 05:38:57,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,318 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,332 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,332 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:38:57,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:38:57,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,371 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:38:57,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:00,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45801
2023-08-07 05:39:00,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45801
2023-08-07 05:39:00,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37275
2023-08-07 05:39:00,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,062 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,062 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,062 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gf8mvzeu
2023-08-07 05:39:00,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41209
2023-08-07 05:39:00,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b8ac5d9-dc0d-48e4-89ef-23594e84fae1
2023-08-07 05:39:00,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41209
2023-08-07 05:39:00,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32829
2023-08-07 05:39:00,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,063 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,063 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-enrjckyp
2023-08-07 05:39:00,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04382194-7f49-47ea-adc3-056c4747eecf
2023-08-07 05:39:00,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70788cab-58ed-44ed-b097-f4dc8099a638
2023-08-07 05:39:00,090 - distributed.worker - INFO - Starting Worker plugin PreImport-c2bec523-8227-4ff5-85dd-46197561b4be
2023-08-07 05:39:00,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52cfaf47-a4f7-49e3-b098-f29ca9d5f8f3
2023-08-07 05:39:00,090 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,090 - distributed.worker - INFO - Starting Worker plugin PreImport-268ee62c-ad0b-47b1-8fad-e041616a830c
2023-08-07 05:39:00,091 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41209', status: init, memory: 0, processing: 0>
2023-08-07 05:39:00,248 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41209
2023-08-07 05:39:00,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47550
2023-08-07 05:39:00,249 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,249 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45801', status: init, memory: 0, processing: 0>
2023-08-07 05:39:00,249 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45801
2023-08-07 05:39:00,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47562
2023-08-07 05:39:00,250 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,250 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:00,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:00,522 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38025
2023-08-07 05:39:00,523 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38025
2023-08-07 05:39:00,523 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44163
2023-08-07 05:39:00,523 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,523 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,523 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,524 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,524 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z7eb5g_t
2023-08-07 05:39:00,524 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f36e0a50-7230-4f40-bec2-7337a2fde7dc
2023-08-07 05:39:00,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40259
2023-08-07 05:39:00,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40259
2023-08-07 05:39:00,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44249
2023-08-07 05:39:00,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,539 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,539 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nilqzot7
2023-08-07 05:39:00,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4575ea7-955a-4c07-90d8-70271cc8f7b0
2023-08-07 05:39:00,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33059
2023-08-07 05:39:00,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33059
2023-08-07 05:39:00,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42011
2023-08-07 05:39:00,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,542 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,542 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-drls0gy2
2023-08-07 05:39:00,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90830efb-9baa-4b02-9844-c6106e942e2a
2023-08-07 05:39:00,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45289
2023-08-07 05:39:00,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45289
2023-08-07 05:39:00,546 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39553
2023-08-07 05:39:00,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,546 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,546 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,546 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kgjy1p4v
2023-08-07 05:39:00,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c99ec75-10e6-4662-b733-500a7e28045a
2023-08-07 05:39:00,559 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33877
2023-08-07 05:39:00,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33877
2023-08-07 05:39:00,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34183
2023-08-07 05:39:00,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,560 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,560 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,560 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m17m7w7w
2023-08-07 05:39:00,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44249019-56cf-47a4-b461-9f4f92326a3f
2023-08-07 05:39:00,562 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37389
2023-08-07 05:39:00,562 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37389
2023-08-07 05:39:00,562 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38601
2023-08-07 05:39:00,563 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:00,563 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,563 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:00,563 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:00,563 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8j8bbsv2
2023-08-07 05:39:00,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df7339a5-ed5d-4125-906a-c4fe7eff62f0
2023-08-07 05:39:00,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b01e2195-e104-41a2-98bd-e9f5266de341
2023-08-07 05:39:00,735 - distributed.worker - INFO - Starting Worker plugin PreImport-b8ce33e9-ed12-4d9d-879f-6bd5c7ecc10f
2023-08-07 05:39:00,736 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-726f7909-a745-4ba4-9f34-7eb74dcb3245
2023-08-07 05:39:00,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac060c1d-e2b1-4d65-b73b-8f4f21c41670
2023-08-07 05:39:00,747 - distributed.worker - INFO - Starting Worker plugin PreImport-db204a2c-7ab9-454d-a49e-3b67e8ec429f
2023-08-07 05:39:00,747 - distributed.worker - INFO - Starting Worker plugin PreImport-ef98f831-93a1-46ae-8f61-298a57cdf5dd
2023-08-07 05:39:00,748 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,748 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89da5cd9-3750-4fbf-b1da-ab3436e11c21
2023-08-07 05:39:00,750 - distributed.worker - INFO - Starting Worker plugin PreImport-b5b40db1-9deb-4d1a-aafb-43f3cb2c8240
2023-08-07 05:39:00,751 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45685c8d-86ec-4b80-99a4-19114135f195
2023-08-07 05:39:00,754 - distributed.worker - INFO - Starting Worker plugin PreImport-7de3e7ec-05ad-46fe-b197-7c126b388133
2023-08-07 05:39:00,755 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:00,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07835cd5-8091-4842-a10e-df4c14904b07
2023-08-07 05:39:00,758 - distributed.worker - INFO - Starting Worker plugin PreImport-78c775b0-7686-46b9-ba77-df9cc6fceab2
2023-08-07 05:39:00,758 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,050 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33059', status: init, memory: 0, processing: 0>
2023-08-07 05:39:01,051 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33059
2023-08-07 05:39:01,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47572
2023-08-07 05:39:01,052 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:01,052 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,052 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33877', status: init, memory: 0, processing: 0>
2023-08-07 05:39:01,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33877
2023-08-07 05:39:01,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47588
2023-08-07 05:39:01,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40259', status: init, memory: 0, processing: 0>
2023-08-07 05:39:01,054 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:01,054 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,054 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40259
2023-08-07 05:39:01,054 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:01,054 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47578
2023-08-07 05:39:01,055 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:01,055 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,055 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45289', status: init, memory: 0, processing: 0>
2023-08-07 05:39:01,056 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45289
2023-08-07 05:39:01,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47564
2023-08-07 05:39:01,056 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:01,057 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:01,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:01,057 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37389', status: init, memory: 0, processing: 0>
2023-08-07 05:39:01,058 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37389
2023-08-07 05:39:01,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47596
2023-08-07 05:39:01,058 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:01,058 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:01,059 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38025', status: init, memory: 0, processing: 0>
2023-08-07 05:39:01,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38025
2023-08-07 05:39:01,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47580
2023-08-07 05:39:01,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:01,061 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:01,061 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:01,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:01,160 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,162 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,162 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:01,166 - distributed.scheduler - INFO - Remove client Client-b1f2a55c-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:01,166 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47526; closing.
2023-08-07 05:39:01,166 - distributed.scheduler - INFO - Remove client Client-b1f2a55c-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:01,167 - distributed.scheduler - INFO - Close client connection: Client-b1f2a55c-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:01,167 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40409'. Reason: nanny-close
2023-08-07 05:39:01,168 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39241'. Reason: nanny-close
2023-08-07 05:39:01,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45801. Reason: nanny-close
2023-08-07 05:39:01,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34257'. Reason: nanny-close
2023-08-07 05:39:01,170 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36349'. Reason: nanny-close
2023-08-07 05:39:01,170 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33877. Reason: nanny-close
2023-08-07 05:39:01,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,171 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41209. Reason: nanny-close
2023-08-07 05:39:01,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40183'. Reason: nanny-close
2023-08-07 05:39:01,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,171 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33059. Reason: nanny-close
2023-08-07 05:39:01,171 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47562; closing.
2023-08-07 05:39:01,171 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,172 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43305'. Reason: nanny-close
2023-08-07 05:39:01,172 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,172 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45801', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,172 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45801
2023-08-07 05:39:01,172 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37119'. Reason: nanny-close
2023-08-07 05:39:01,172 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38025. Reason: nanny-close
2023-08-07 05:39:01,172 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35283'. Reason: nanny-close
2023-08-07 05:39:01,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45289. Reason: nanny-close
2023-08-07 05:39:01,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,173 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:01,173 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40259. Reason: nanny-close
2023-08-07 05:39:01,173 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45801
2023-08-07 05:39:01,173 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47588; closing.
2023-08-07 05:39:01,174 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,174 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,174 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37389. Reason: nanny-close
2023-08-07 05:39:01,174 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,174 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45801
2023-08-07 05:39:01,174 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33877', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,174 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33877
2023-08-07 05:39:01,174 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45801
2023-08-07 05:39:01,174 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45801
2023-08-07 05:39:01,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47550; closing.
2023-08-07 05:39:01,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47572; closing.
2023-08-07 05:39:01,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:01,175 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41209', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,175 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41209
2023-08-07 05:39:01,176 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33059', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,176 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,176 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33059
2023-08-07 05:39:01,176 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,176 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,176 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:01,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47580; closing.
2023-08-07 05:39:01,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47564; closing.
2023-08-07 05:39:01,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47578; closing.
2023-08-07 05:39:01,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47596; closing.
2023-08-07 05:39:01,178 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38025', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,178 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38025
2023-08-07 05:39:01,178 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45289', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,178 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45289
2023-08-07 05:39:01,179 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40259', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,179 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40259
2023-08-07 05:39:01,179 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37389', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:01,179 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37389
2023-08-07 05:39:01,179 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:02,735 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:02,735 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:02,736 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:02,737 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:02,738 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-08-07 05:39:04,814 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:04,818 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41543 instead
  warnings.warn(
2023-08-07 05:39:04,822 - distributed.scheduler - INFO - State start
2023-08-07 05:39:04,841 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:04,842 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:04,843 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41543/status
2023-08-07 05:39:05,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46839'
2023-08-07 05:39:05,089 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44949'
2023-08-07 05:39:05,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37739'
2023-08-07 05:39:05,098 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43639'
2023-08-07 05:39:05,107 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40551'
2023-08-07 05:39:05,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44481'
2023-08-07 05:39:05,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40799'
2023-08-07 05:39:05,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36169'
2023-08-07 05:39:06,698 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,698 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,772 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,772 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,784 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,798 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,805 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,820 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:06,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:06,837 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,882 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:06,902 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:09,551 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42409
2023-08-07 05:39:09,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42409
2023-08-07 05:39:09,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36993
2023-08-07 05:39:09,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,552 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,552 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2h9e6ot3
2023-08-07 05:39:09,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72d50973-2fc0-43b6-8950-cd98ef2d1f59
2023-08-07 05:39:09,553 - distributed.worker - INFO - Starting Worker plugin PreImport-d1dca5f0-ced6-4aaa-9bd2-0fa26bce2558
2023-08-07 05:39:09,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4cc8deb-85ec-4ac2-a2a0-a3ccf6b5b345
2023-08-07 05:39:09,562 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39997
2023-08-07 05:39:09,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39997
2023-08-07 05:39:09,563 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40445
2023-08-07 05:39:09,563 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,563 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,563 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,564 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lvafu3lf
2023-08-07 05:39:09,564 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e73fb99e-7863-4d5b-bc2a-81278642368a
2023-08-07 05:39:09,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40347
2023-08-07 05:39:09,580 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40347
2023-08-07 05:39:09,580 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39153
2023-08-07 05:39:09,580 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,580 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,580 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,580 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,580 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tmthxzsx
2023-08-07 05:39:09,581 - distributed.worker - INFO - Starting Worker plugin PreImport-b3e99cdf-3428-453c-907b-d1196cc1a960
2023-08-07 05:39:09,581 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd8ae752-7924-4eb1-9b98-35550a30d926
2023-08-07 05:39:09,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f19e56b-d7eb-4366-824f-d314497e99ca
2023-08-07 05:39:09,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41635
2023-08-07 05:39:09,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41635
2023-08-07 05:39:09,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34245
2023-08-07 05:39:09,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,585 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,585 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-46riifyg
2023-08-07 05:39:09,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37ba2364-9391-463a-bbce-a3dcc5994f0f
2023-08-07 05:39:09,594 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39403
2023-08-07 05:39:09,595 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39403
2023-08-07 05:39:09,595 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39405
2023-08-07 05:39:09,595 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,595 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,595 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,595 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-883nigsk
2023-08-07 05:39:09,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cff0704-62ba-4f7d-9220-e67d955dc204
2023-08-07 05:39:09,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34613
2023-08-07 05:39:09,603 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34613
2023-08-07 05:39:09,603 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37917
2023-08-07 05:39:09,603 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,603 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,603 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,603 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2moqfimi
2023-08-07 05:39:09,604 - distributed.worker - INFO - Starting Worker plugin PreImport-efab97d8-8982-4cf4-bc30-0be40d86b5d6
2023-08-07 05:39:09,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4d6908c-015d-466d-9459-5859deb600b7
2023-08-07 05:39:09,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6813ca0-c3b3-4d82-90e1-30a49504b52b
2023-08-07 05:39:09,613 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46697
2023-08-07 05:39:09,615 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46697
2023-08-07 05:39:09,615 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44889
2023-08-07 05:39:09,615 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,615 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,615 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,615 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,615 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lb18w_tx
2023-08-07 05:39:09,616 - distributed.worker - INFO - Starting Worker plugin PreImport-1edace38-8978-4c58-a125-82529ce4db87
2023-08-07 05:39:09,616 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c00b02ec-b57d-4191-a163-3fd11c4858f2
2023-08-07 05:39:09,617 - distributed.worker - INFO - Starting Worker plugin RMMSetup-728ccbca-a4e0-49ff-97bf-2c034d465a1d
2023-08-07 05:39:09,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41837
2023-08-07 05:39:09,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41837
2023-08-07 05:39:09,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46229
2023-08-07 05:39:09,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,620 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,620 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:09,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:09,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xe8qhi_e
2023-08-07 05:39:09,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58f4bf9b-3f94-4da1-947a-0ff9ec48ed86
2023-08-07 05:39:09,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa1274c3-c75b-47f8-95eb-2a93ec0eada2
2023-08-07 05:39:09,711 - distributed.scheduler - INFO - Receive client connection: Client-b79399ad-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:09,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33750
2023-08-07 05:39:09,821 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,823 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b781b0a8-e956-4c7a-8779-0fd3da6ac497
2023-08-07 05:39:09,830 - distributed.worker - INFO - Starting Worker plugin PreImport-85ec76bc-57b9-4ff7-9702-93b9cfe66b9d
2023-08-07 05:39:09,830 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,831 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4809db3-9f1b-4f8f-b1c1-088442f390c5
2023-08-07 05:39:09,832 - distributed.worker - INFO - Starting Worker plugin PreImport-ea12892b-fda9-4e6b-a856-4f5f8785d2bd
2023-08-07 05:39:09,833 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3ae2fcf-da2e-43d0-83ce-8719c11c9485
2023-08-07 05:39:09,836 - distributed.worker - INFO - Starting Worker plugin PreImport-128d9649-1102-4bce-960e-101cdc8af0fa
2023-08-07 05:39:09,836 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,844 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,848 - distributed.worker - INFO - Starting Worker plugin PreImport-edcdec6c-501d-4e6a-af5a-06b16dd6a50e
2023-08-07 05:39:09,848 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,849 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42409', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42409
2023-08-07 05:39:09,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33760
2023-08-07 05:39:09,857 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,858 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40347', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40347
2023-08-07 05:39:09,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33764
2023-08-07 05:39:09,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,859 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,859 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39403', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39403
2023-08-07 05:39:09,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33768
2023-08-07 05:39:09,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,860 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39997', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39997
2023-08-07 05:39:09,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33792
2023-08-07 05:39:09,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,864 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,873 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34613', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34613
2023-08-07 05:39:09,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33798
2023-08-07 05:39:09,874 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,874 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,874 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41635', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,875 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41635
2023-08-07 05:39:09,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33780
2023-08-07 05:39:09,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,875 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,879 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46697', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,879 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46697
2023-08-07 05:39:09,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33818
2023-08-07 05:39:09,880 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,880 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41837', status: init, memory: 0, processing: 0>
2023-08-07 05:39:09,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41837
2023-08-07 05:39:09,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33814
2023-08-07 05:39:09,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,882 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:09,882 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:09,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:09,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,954 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,955 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:09,960 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:09,961 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:09,964 - distributed.scheduler - INFO - Remove client Client-b79399ad-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:09,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33750; closing.
2023-08-07 05:39:09,964 - distributed.scheduler - INFO - Remove client Client-b79399ad-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:09,965 - distributed.scheduler - INFO - Close client connection: Client-b79399ad-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:09,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43639'. Reason: nanny-close
2023-08-07 05:39:09,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40551'. Reason: nanny-close
2023-08-07 05:39:09,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42409. Reason: nanny-close
2023-08-07 05:39:09,968 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46839'. Reason: nanny-close
2023-08-07 05:39:09,968 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,968 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44949'. Reason: nanny-close
2023-08-07 05:39:09,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41837. Reason: nanny-close
2023-08-07 05:39:09,969 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,969 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39997. Reason: nanny-close
2023-08-07 05:39:09,969 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37739'. Reason: nanny-close
2023-08-07 05:39:09,970 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,970 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33760; closing.
2023-08-07 05:39:09,970 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46697. Reason: nanny-close
2023-08-07 05:39:09,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,970 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44481'. Reason: nanny-close
2023-08-07 05:39:09,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42409', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,970 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,970 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42409
2023-08-07 05:39:09,971 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40799'. Reason: nanny-close
2023-08-07 05:39:09,971 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40347. Reason: nanny-close
2023-08-07 05:39:09,971 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,971 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,971 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,971 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36169'. Reason: nanny-close
2023-08-07 05:39:09,971 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41635. Reason: nanny-close
2023-08-07 05:39:09,972 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42409
2023-08-07 05:39:09,972 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:09,972 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,972 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,972 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42409
2023-08-07 05:39:09,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33814; closing.
2023-08-07 05:39:09,972 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34613. Reason: nanny-close
2023-08-07 05:39:09,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33792; closing.
2023-08-07 05:39:09,972 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,972 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,973 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39403. Reason: nanny-close
2023-08-07 05:39:09,973 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41837', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,973 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41837
2023-08-07 05:39:09,973 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42409
2023-08-07 05:39:09,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39997', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,973 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39997
2023-08-07 05:39:09,973 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42409
2023-08-07 05:39:09,973 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,974 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,974 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33818; closing.
2023-08-07 05:39:09,974 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,974 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46697', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,974 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:09,974 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46697
2023-08-07 05:39:09,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33764; closing.
2023-08-07 05:39:09,975 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33780; closing.
2023-08-07 05:39:09,975 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33798; closing.
2023-08-07 05:39:09,975 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,975 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:09,975 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40347', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,975 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40347
2023-08-07 05:39:09,976 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41635', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,976 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41635
2023-08-07 05:39:09,976 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34613', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,976 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34613
2023-08-07 05:39:09,977 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33768; closing.
2023-08-07 05:39:09,977 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39403', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:09,977 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39403
2023-08-07 05:39:09,977 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:11,433 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:11,433 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:11,434 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:11,434 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:11,435 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-08-07 05:39:13,285 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:13,289 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43315 instead
  warnings.warn(
2023-08-07 05:39:13,292 - distributed.scheduler - INFO - State start
2023-08-07 05:39:13,311 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:13,312 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:13,313 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43315/status
2023-08-07 05:39:13,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37523'
2023-08-07 05:39:13,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43655'
2023-08-07 05:39:13,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33967'
2023-08-07 05:39:13,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43629'
2023-08-07 05:39:13,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33135'
2023-08-07 05:39:13,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36751'
2023-08-07 05:39:13,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43097'
2023-08-07 05:39:13,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35113'
2023-08-07 05:39:13,684 - distributed.scheduler - INFO - Receive client connection: Client-bca8dc43-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:13,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33978
2023-08-07 05:39:15,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,202 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,202 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:15,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:15,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:15,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:17,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41777
2023-08-07 05:39:17,291 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41777
2023-08-07 05:39:17,291 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35315
2023-08-07 05:39:17,291 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:17,291 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:17,291 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:17,291 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:17,292 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4j23nvfx
2023-08-07 05:39:17,293 - distributed.worker - INFO - Starting Worker plugin PreImport-107a864b-a870-40c8-8380-0d7759d44528
2023-08-07 05:39:17,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83c58a00-3e3b-4faa-ad4f-d22b59ff22aa
2023-08-07 05:39:17,293 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f2fec4c-c0a2-40da-b0fc-a0a6dbe46a6e
2023-08-07 05:39:17,488 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:17,524 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41777', status: init, memory: 0, processing: 0>
2023-08-07 05:39:17,525 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41777
2023-08-07 05:39:17,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34236
2023-08-07 05:39:17,526 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:17,526 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:17,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44359
2023-08-07 05:39:18,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44359
2023-08-07 05:39:18,161 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34585
2023-08-07 05:39:18,161 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,161 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,161 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,162 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b41d68a8
2023-08-07 05:39:18,162 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3478c106-160c-49c7-bc20-b6c98dbf6ddb
2023-08-07 05:39:18,162 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32769
2023-08-07 05:39:18,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32769
2023-08-07 05:39:18,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39867
2023-08-07 05:39:18,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,163 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,163 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wv397h9y
2023-08-07 05:39:18,164 - distributed.worker - INFO - Starting Worker plugin PreImport-5612d81e-2abc-4a4d-b7e6-c9ca3200739e
2023-08-07 05:39:18,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6bc4d39-fe64-473f-8a99-ff1440b34e7a
2023-08-07 05:39:18,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6031615-d240-4c01-9e7a-1b1011c9a10a
2023-08-07 05:39:18,178 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44265
2023-08-07 05:39:18,179 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44265
2023-08-07 05:39:18,179 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42915
2023-08-07 05:39:18,179 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,179 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,179 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,180 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,180 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rl3lj8g5
2023-08-07 05:39:18,180 - distributed.worker - INFO - Starting Worker plugin PreImport-a08d4479-23fd-4305-b833-ef534996c973
2023-08-07 05:39:18,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d57ec44e-cc4f-45fa-9f5b-9c77fa323938
2023-08-07 05:39:18,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2fb5def-ea17-4dda-9195-fd9cc55ecb99
2023-08-07 05:39:18,293 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,301 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aeff0e20-dea3-42e6-8ecc-30c38144a738
2023-08-07 05:39:18,301 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,301 - distributed.worker - INFO - Starting Worker plugin PreImport-44e69869-2512-4644-8df0-f883a4013ce1
2023-08-07 05:39:18,302 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,333 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32769', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,334 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32769
2023-08-07 05:39:18,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34252
2023-08-07 05:39:18,335 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,335 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,336 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44359', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,337 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44359
2023-08-07 05:39:18,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34268
2023-08-07 05:39:18,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,337 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,337 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44265', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44265
2023-08-07 05:39:18,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34248
2023-08-07 05:39:18,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,339 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43265
2023-08-07 05:39:18,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43265
2023-08-07 05:39:18,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45431
2023-08-07 05:39:18,420 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,420 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,420 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,420 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,420 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o_3olcy6
2023-08-07 05:39:18,421 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c36a736-7ec7-4d69-89d7-5700a68081ad
2023-08-07 05:39:18,423 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37611
2023-08-07 05:39:18,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37611
2023-08-07 05:39:18,423 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34899
2023-08-07 05:39:18,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42693
2023-08-07 05:39:18,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34899
2023-08-07 05:39:18,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,424 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36249
2023-08-07 05:39:18,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37343
2023-08-07 05:39:18,424 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36249
2023-08-07 05:39:18,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,424 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44361
2023-08-07 05:39:18,424 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,424 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,424 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,424 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,424 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xkkqhfnt
2023-08-07 05:39:18,424 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,424 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:18,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9fj6bnfw
2023-08-07 05:39:18,425 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:18,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hmmu3oyg
2023-08-07 05:39:18,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c93e967-378e-4ba4-be5c-b45ba18e076f
2023-08-07 05:39:18,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34a0fb78-0ac8-4a1d-8277-b72f36a75bdf
2023-08-07 05:39:18,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b5fa32e-58ba-44fb-99b7-30ae9bf7721e
2023-08-07 05:39:18,425 - distributed.worker - INFO - Starting Worker plugin PreImport-b5054191-0e22-47c1-8b40-cb12bf01a6ea
2023-08-07 05:39:18,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67dcabdb-208f-4df7-9b88-f2a72aa2f557
2023-08-07 05:39:18,431 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2ba9b89-eb34-43e0-b074-3a18678dd272
2023-08-07 05:39:18,549 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-059d0e7d-dc79-41f8-9e90-525f594f7e1f
2023-08-07 05:39:18,550 - distributed.worker - INFO - Starting Worker plugin PreImport-5a70b7d8-b894-4d22-bd6b-f24f7c9372fb
2023-08-07 05:39:18,550 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,556 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14543398-80be-4e6f-93ee-b1d1a23026ba
2023-08-07 05:39:18,556 - distributed.worker - INFO - Starting Worker plugin PreImport-376fbc24-138d-48cf-ab8c-6321c8834624
2023-08-07 05:39:18,556 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,560 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,580 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36249', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,580 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36249
2023-08-07 05:39:18,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34294
2023-08-07 05:39:18,581 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,581 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,585 - distributed.worker - INFO - Starting Worker plugin PreImport-3652f1c7-2078-4bfa-827a-4262bf69cae3
2023-08-07 05:39:18,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43265', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,586 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43265
2023-08-07 05:39:18,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34288
2023-08-07 05:39:18,587 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,587 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37611', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37611
2023-08-07 05:39:18,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34304
2023-08-07 05:39:18,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,599 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,615 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34899', status: init, memory: 0, processing: 0>
2023-08-07 05:39:18,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34899
2023-08-07 05:39:18,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34308
2023-08-07 05:39:18,616 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:18,616 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:18,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:18,650 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,650 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,650 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:18,655 - distributed.scheduler - INFO - Remove client Client-bca8dc43-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:18,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33978; closing.
2023-08-07 05:39:18,656 - distributed.scheduler - INFO - Remove client Client-bca8dc43-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:18,656 - distributed.scheduler - INFO - Close client connection: Client-bca8dc43-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:18,657 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43629'. Reason: nanny-close
2023-08-07 05:39:18,658 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,658 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37523'. Reason: nanny-close
2023-08-07 05:39:18,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43655'. Reason: nanny-close
2023-08-07 05:39:18,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,659 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37611. Reason: nanny-close
2023-08-07 05:39:18,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33967'. Reason: nanny-close
2023-08-07 05:39:18,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,660 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36249. Reason: nanny-close
2023-08-07 05:39:18,660 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33135'. Reason: nanny-close
2023-08-07 05:39:18,660 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36751'. Reason: nanny-close
2023-08-07 05:39:18,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41777. Reason: nanny-close
2023-08-07 05:39:18,661 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43097'. Reason: nanny-close
2023-08-07 05:39:18,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44265. Reason: nanny-close
2023-08-07 05:39:18,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,662 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,662 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34294; closing.
2023-08-07 05:39:18,662 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35113'. Reason: nanny-close
2023-08-07 05:39:18,662 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43265. Reason: nanny-close
2023-08-07 05:39:18,662 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36249', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,662 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36249
2023-08-07 05:39:18,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,663 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32769. Reason: nanny-close
2023-08-07 05:39:18,663 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34304; closing.
2023-08-07 05:39:18,663 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,663 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,663 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,663 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37611', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,664 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44359. Reason: nanny-close
2023-08-07 05:39:18,664 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37611
2023-08-07 05:39:18,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,664 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:18,664 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36249
2023-08-07 05:39:18,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,665 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36249
2023-08-07 05:39:18,665 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34236; closing.
2023-08-07 05:39:18,665 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,665 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36249
2023-08-07 05:39:18,665 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,666 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34899. Reason: nanny-close
2023-08-07 05:39:18,666 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,666 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,666 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,667 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,668 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:18,665 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34304>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-07 05:39:18,668 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41777', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,668 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41777
2023-08-07 05:39:18,669 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34248; closing.
2023-08-07 05:39:18,669 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:18,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44265', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,670 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44265
2023-08-07 05:39:18,670 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34252; closing.
2023-08-07 05:39:18,670 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34288; closing.
2023-08-07 05:39:18,671 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32769', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,671 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32769
2023-08-07 05:39:18,671 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43265', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,672 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43265
2023-08-07 05:39:18,672 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34268; closing.
2023-08-07 05:39:18,672 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34308; closing.
2023-08-07 05:39:18,673 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44359', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,673 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44359
2023-08-07 05:39:18,673 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34899', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:18,673 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34899
2023-08-07 05:39:18,673 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:18,674 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34308>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-07 05:39:18,674 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34268>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-07 05:39:20,074 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:20,075 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:20,075 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:20,076 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:20,077 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-08-07 05:39:22,067 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:22,071 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34585 instead
  warnings.warn(
2023-08-07 05:39:22,075 - distributed.scheduler - INFO - State start
2023-08-07 05:39:22,096 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:22,097 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:22,097 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34585/status
2023-08-07 05:39:22,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41767'
2023-08-07 05:39:22,479 - distributed.scheduler - INFO - Receive client connection: Client-c1df290b-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:22,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34392
2023-08-07 05:39:23,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:23,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-07 05:39:24,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:24,998 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37085
2023-08-07 05:39:24,998 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37085
2023-08-07 05:39:24,999 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-08-07 05:39:24,999 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:24,999 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:24,999 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:24,999 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-07 05:39:24,999 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xhzkp3f5
2023-08-07 05:39:24,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53949cea-661f-4053-b70d-7017d5e46646
2023-08-07 05:39:24,999 - distributed.worker - INFO - Starting Worker plugin PreImport-9ed02e2e-53d3-4ae8-b17b-da7c6640db30
2023-08-07 05:39:25,000 - distributed.worker - INFO - Starting Worker plugin RMMSetup-452a75b9-0d55-464e-8d46-0a3e04f032b1
2023-08-07 05:39:25,000 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:25,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37085', status: init, memory: 0, processing: 0>
2023-08-07 05:39:25,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37085
2023-08-07 05:39:25,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38560
2023-08-07 05:39:25,027 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:25,027 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:25,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:25,042 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:25,045 - distributed.scheduler - INFO - Remove client Client-c1df290b-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:25,045 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34392; closing.
2023-08-07 05:39:25,045 - distributed.scheduler - INFO - Remove client Client-c1df290b-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:25,045 - distributed.scheduler - INFO - Close client connection: Client-c1df290b-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:25,046 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41767'. Reason: nanny-close
2023-08-07 05:39:25,047 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:25,048 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37085. Reason: nanny-close
2023-08-07 05:39:25,049 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38560; closing.
2023-08-07 05:39:25,049 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:25,050 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37085', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:25,050 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37085
2023-08-07 05:39:25,050 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:25,051 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:26,063 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:26,063 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:26,063 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:26,064 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:26,065 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-08-07 05:39:29,682 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:29,686 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33055 instead
  warnings.warn(
2023-08-07 05:39:29,690 - distributed.scheduler - INFO - State start
2023-08-07 05:39:29,709 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:29,710 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:29,711 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33055/status
2023-08-07 05:39:29,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46033'
2023-08-07 05:39:31,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:31,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-07 05:39:31,726 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:32,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42891
2023-08-07 05:39:32,471 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42891
2023-08-07 05:39:32,471 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37779
2023-08-07 05:39:32,471 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:32,471 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:32,471 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:32,471 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-07 05:39:32,471 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n13wcbez
2023-08-07 05:39:32,471 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eba89d20-0ec6-4385-a777-acfa0f7e19a3
2023-08-07 05:39:32,472 - distributed.worker - INFO - Starting Worker plugin PreImport-f3c79ad7-b1ff-4592-8202-7036dfe36d35
2023-08-07 05:39:32,473 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a98f35b7-dd5f-4269-9e5c-dc10c7cc326b
2023-08-07 05:39:32,473 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:32,503 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42891', status: init, memory: 0, processing: 0>
2023-08-07 05:39:32,517 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42891
2023-08-07 05:39:32,517 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38682
2023-08-07 05:39:32,518 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:32,518 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:32,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:34,026 - distributed.scheduler - INFO - Receive client connection: Client-c66fee07-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:34,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38698
2023-08-07 05:39:34,033 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:34,039 - distributed.scheduler - INFO - Remove client Client-c66fee07-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:34,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38698; closing.
2023-08-07 05:39:34,040 - distributed.scheduler - INFO - Remove client Client-c66fee07-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:34,040 - distributed.scheduler - INFO - Close client connection: Client-c66fee07-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:34,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46033'. Reason: nanny-close
2023-08-07 05:39:34,041 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:34,042 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42891. Reason: nanny-close
2023-08-07 05:39:34,044 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38682; closing.
2023-08-07 05:39:34,044 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:34,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42891', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:34,044 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42891
2023-08-07 05:39:34,045 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:34,046 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:35,107 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:35,107 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:35,108 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:35,108 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:35,109 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-08-07 05:39:37,038 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:37,043 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33435 instead
  warnings.warn(
2023-08-07 05:39:37,046 - distributed.scheduler - INFO - State start
2023-08-07 05:39:37,067 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:37,068 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:37,068 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33435/status
2023-08-07 05:39:40,936 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:40,936 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:40,937 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:40,937 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:40,938 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-08-07 05:39:42,756 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:42,760 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33721 instead
  warnings.warn(
2023-08-07 05:39:42,764 - distributed.scheduler - INFO - State start
2023-08-07 05:39:42,783 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:42,784 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-07 05:39:42,784 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33721/status
2023-08-07 05:39:42,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41407'
2023-08-07 05:39:42,962 - distributed.scheduler - INFO - Receive client connection: Client-ce4109a5-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:42,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51436
2023-08-07 05:39:44,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:44,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:44,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:45,161 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36791
2023-08-07 05:39:45,162 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36791
2023-08-07 05:39:45,162 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33471
2023-08-07 05:39:45,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-07 05:39:45,162 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:45,162 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:45,162 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-07 05:39:45,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-boef1x_w
2023-08-07 05:39:45,162 - distributed.worker - INFO - Starting Worker plugin PreImport-8d543ba6-48eb-4adc-bd4c-ca419084fffe
2023-08-07 05:39:45,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbd7ccb4-765c-4429-ad27-59b5db2a8322
2023-08-07 05:39:45,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-415a16a0-a242-41c0-8c0f-5ae2714f2710
2023-08-07 05:39:45,163 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:45,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36791', status: init, memory: 0, processing: 0>
2023-08-07 05:39:45,183 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36791
2023-08-07 05:39:45,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51732
2023-08-07 05:39:45,183 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-07 05:39:45,183 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:45,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-07 05:39:45,220 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:45,223 - distributed.scheduler - INFO - Remove client Client-ce4109a5-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:45,223 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51436; closing.
2023-08-07 05:39:45,223 - distributed.scheduler - INFO - Remove client Client-ce4109a5-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:45,223 - distributed.scheduler - INFO - Close client connection: Client-ce4109a5-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:45,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41407'. Reason: nanny-close
2023-08-07 05:39:45,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:45,226 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36791. Reason: nanny-close
2023-08-07 05:39:45,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51732; closing.
2023-08-07 05:39:45,228 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-07 05:39:45,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36791', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:45,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36791
2023-08-07 05:39:45,229 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:45,229 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:46,090 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:46,091 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:46,091 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:46,092 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-07 05:39:46,092 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-08-07 05:39:47,953 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:47,956 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34187 instead
  warnings.warn(
2023-08-07 05:39:47,960 - distributed.scheduler - INFO - State start
2023-08-07 05:39:47,979 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:47,980 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:47,980 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34187/status
2023-08-07 05:39:48,131 - distributed.scheduler - INFO - Receive client connection: Client-d154b74a-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:48,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52336
2023-08-07 05:39:48,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42603'
2023-08-07 05:39:48,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37781'
2023-08-07 05:39:48,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46459'
2023-08-07 05:39:48,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43477'
2023-08-07 05:39:48,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39533'
2023-08-07 05:39:48,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35419'
2023-08-07 05:39:48,315 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40407'
2023-08-07 05:39:48,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33921'
2023-08-07 05:39:49,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:49,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:49,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:49,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:49,981 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:49,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:49,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:49,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:49,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:49,992 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:49,992 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:50,010 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:50,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:50,015 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:50,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:50,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:50,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:50,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:50,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:50,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:50,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:50,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:50,100 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:50,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:53,011 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45427
2023-08-07 05:39:53,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45427
2023-08-07 05:39:53,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46081
2023-08-07 05:39:53,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,012 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,012 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,012 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,012 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-db3jgcu5
2023-08-07 05:39:53,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cffeb287-3777-4db4-b605-97dd263f3847
2023-08-07 05:39:53,013 - distributed.worker - INFO - Starting Worker plugin PreImport-2ed02753-4915-4b26-adbc-edb15ae2d746
2023-08-07 05:39:53,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3d36257-6c9d-4bf9-ab9d-dd604cb4729d
2023-08-07 05:39:53,019 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41067
2023-08-07 05:39:53,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41067
2023-08-07 05:39:53,021 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34781
2023-08-07 05:39:53,021 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,021 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,021 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,021 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s0ng5hgo
2023-08-07 05:39:53,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce59ff47-bef8-4679-b6af-93c0b8dc121a
2023-08-07 05:39:53,022 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48440695-a386-414b-9c6a-c59f7679e1c9
2023-08-07 05:39:53,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33565
2023-08-07 05:39:53,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33565
2023-08-07 05:39:53,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40027
2023-08-07 05:39:53,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,028 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,028 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ipbzzif4
2023-08-07 05:39:53,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38755
2023-08-07 05:39:53,029 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38755
2023-08-07 05:39:53,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0976a078-6e16-4352-8f0a-33574a5656fe
2023-08-07 05:39:53,029 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39553
2023-08-07 05:39:53,029 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,029 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,029 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,029 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,029 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iux_it3z
2023-08-07 05:39:53,030 - distributed.worker - INFO - Starting Worker plugin PreImport-51b0feba-dddc-48d9-9429-6d07274f8e2c
2023-08-07 05:39:53,030 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfbf9932-209b-4310-af89-e0a1a79a9be3
2023-08-07 05:39:53,030 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07de6c71-951d-4166-97ee-3c590f345365
2023-08-07 05:39:53,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42021
2023-08-07 05:39:53,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42021
2023-08-07 05:39:53,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38907
2023-08-07 05:39:53,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,062 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,062 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,062 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sb0nnwom
2023-08-07 05:39:53,063 - distributed.worker - INFO - Starting Worker plugin PreImport-b58f0292-e401-40a0-bd89-3553ab4a811a
2023-08-07 05:39:53,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2aff8d1-4b2c-43dd-afad-430d38b39290
2023-08-07 05:39:53,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f888f77a-e739-4500-aaee-72c3f6a63014
2023-08-07 05:39:53,071 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35119
2023-08-07 05:39:53,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35119
2023-08-07 05:39:53,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43543
2023-08-07 05:39:53,072 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,072 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,072 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,072 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,072 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uuqm3u3b
2023-08-07 05:39:53,072 - distributed.worker - INFO - Starting Worker plugin PreImport-236a96b8-f8fd-4ba1-ac70-d769f3c6a4d1
2023-08-07 05:39:53,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f82489b-1ad3-457e-af06-6d9aece7c825
2023-08-07 05:39:53,073 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08812b8b-2386-42d1-a01e-99c59b4ab12c
2023-08-07 05:39:53,076 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45131
2023-08-07 05:39:53,077 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45131
2023-08-07 05:39:53,078 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46373
2023-08-07 05:39:53,078 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,078 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,078 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,078 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p2ijyscf
2023-08-07 05:39:53,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ea97164-0883-451e-9719-8c24c32812e2
2023-08-07 05:39:53,081 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36025
2023-08-07 05:39:53,082 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36025
2023-08-07 05:39:53,082 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35955
2023-08-07 05:39:53,082 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,082 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,082 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:53,082 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-07 05:39:53,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nqzkdufw
2023-08-07 05:39:53,083 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e05d8fa-d10a-4a0b-9ae6-2065ba3c2bb7
2023-08-07 05:39:53,370 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb8957b6-533c-400d-9744-75dcb3fdd14c
2023-08-07 05:39:53,370 - distributed.worker - INFO - Starting Worker plugin PreImport-b8b8c536-c28e-409a-a7d7-91d8e6439771
2023-08-07 05:39:53,370 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,375 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8444eb2-f942-4516-a15f-af8a81885678
2023-08-07 05:39:53,375 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4c5c5e5-67dd-4bbb-8db6-3ba15b129f8c
2023-08-07 05:39:53,375 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,375 - distributed.worker - INFO - Starting Worker plugin PreImport-292a11f9-9290-4c8c-a22c-5920235abbb4
2023-08-07 05:39:53,376 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,376 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,376 - distributed.worker - INFO - Starting Worker plugin PreImport-34d31a28-3499-4346-b7fb-61b1370b73a6
2023-08-07 05:39:53,376 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,376 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,376 - distributed.worker - INFO - Starting Worker plugin PreImport-0da0a3f3-fe9f-4692-b805-17adbc400a2f
2023-08-07 05:39:53,376 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,376 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,399 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33565', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,400 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33565
2023-08-07 05:39:53,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52416
2023-08-07 05:39:53,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,401 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,402 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42021', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42021
2023-08-07 05:39:53,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52418
2023-08-07 05:39:53,403 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,403 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,405 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35119', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,406 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35119
2023-08-07 05:39:53,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52428
2023-08-07 05:39:53,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,407 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45131', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,413 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45131
2023-08-07 05:39:53,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52468
2023-08-07 05:39:53,414 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,414 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41067', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,416 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41067
2023-08-07 05:39:53,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52444
2023-08-07 05:39:53,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,417 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,417 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38755', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,418 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38755
2023-08-07 05:39:53,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52452
2023-08-07 05:39:53,418 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,419 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,419 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36025', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,420 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36025
2023-08-07 05:39:53,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52470
2023-08-07 05:39:53,421 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,421 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45427', status: init, memory: 0, processing: 0>
2023-08-07 05:39:53,421 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,421 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45427
2023-08-07 05:39:53,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52424
2023-08-07 05:39:53,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,422 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:53,422 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:53,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:53,441 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,443 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-07 05:39:53,454 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,454 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,455 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,455 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,455 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,455 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,455 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,455 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:53,459 - distributed.scheduler - INFO - Remove client Client-d154b74a-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:53,459 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52336; closing.
2023-08-07 05:39:53,460 - distributed.scheduler - INFO - Remove client Client-d154b74a-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:53,460 - distributed.scheduler - INFO - Close client connection: Client-d154b74a-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:53,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43477'. Reason: nanny-close
2023-08-07 05:39:53,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42603'. Reason: nanny-close
2023-08-07 05:39:53,461 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,462 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37781'. Reason: nanny-close
2023-08-07 05:39:53,462 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46459'. Reason: nanny-close
2023-08-07 05:39:53,463 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41067. Reason: nanny-close
2023-08-07 05:39:53,463 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,463 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33565. Reason: nanny-close
2023-08-07 05:39:53,464 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39533'. Reason: nanny-close
2023-08-07 05:39:53,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,464 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35419'. Reason: nanny-close
2023-08-07 05:39:53,464 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42021. Reason: nanny-close
2023-08-07 05:39:53,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,465 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38755. Reason: nanny-close
2023-08-07 05:39:53,465 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52416; closing.
2023-08-07 05:39:53,465 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,466 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40407'. Reason: nanny-close
2023-08-07 05:39:53,466 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,466 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33565', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,466 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33565
2023-08-07 05:39:53,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,466 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,466 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33921'. Reason: nanny-close
2023-08-07 05:39:53,466 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36025. Reason: nanny-close
2023-08-07 05:39:53,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,467 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,467 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52444; closing.
2023-08-07 05:39:53,467 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35119. Reason: nanny-close
2023-08-07 05:39:53,467 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,467 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,467 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33565
2023-08-07 05:39:53,467 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41067', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,468 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41067
2023-08-07 05:39:53,468 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45131. Reason: nanny-close
2023-08-07 05:39:53,468 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33565
2023-08-07 05:39:53,468 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,468 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52418; closing.
2023-08-07 05:39:53,468 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33565
2023-08-07 05:39:53,468 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33565
2023-08-07 05:39:53,469 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42021', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,469 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,469 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42021
2023-08-07 05:39:53,469 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33565
2023-08-07 05:39:53,469 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,469 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52452; closing.
2023-08-07 05:39:53,470 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52428; closing.
2023-08-07 05:39:53,470 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38755', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,470 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,470 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38755
2023-08-07 05:39:53,470 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35119', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,470 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35119
2023-08-07 05:39:53,471 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52470; closing.
2023-08-07 05:39:53,471 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:53,471 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36025', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,471 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36025
2023-08-07 05:39:53,472 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52468; closing.
2023-08-07 05:39:53,472 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45131', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,472 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45131
2023-08-07 05:39:53,473 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41067
2023-08-07 05:39:53,473 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42021
2023-08-07 05:39:53,473 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38755
2023-08-07 05:39:53,473 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35119
2023-08-07 05:39:53,474 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36025
2023-08-07 05:39:53,474 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45131
2023-08-07 05:39:53,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:53,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45427. Reason: nanny-close
2023-08-07 05:39:53,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52424; closing.
2023-08-07 05:39:53,477 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:53,477 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45427', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:53,477 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45427
2023-08-07 05:39:53,478 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:53,479 - distributed.nanny - INFO - Worker closed
2023-08-07 05:39:54,979 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:39:54,979 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:39:54,980 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:39:54,981 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:39:54,981 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-08-07 05:39:56,894 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:56,898 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43761 instead
  warnings.warn(
2023-08-07 05:39:56,902 - distributed.scheduler - INFO - State start
2023-08-07 05:39:56,921 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:39:56,922 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:39:56,922 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43761/status
2023-08-07 05:39:57,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36603'
2023-08-07 05:39:58,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:39:58,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:39:58,515 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:39:58,905 - distributed.scheduler - INFO - Receive client connection: Client-d6a6d154-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:58,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54332
2023-08-07 05:39:59,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39329
2023-08-07 05:39:59,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39329
2023-08-07 05:39:59,370 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37803
2023-08-07 05:39:59,370 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:39:59,370 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:59,370 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:39:59,370 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-07 05:39:59,370 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jlkj06t9
2023-08-07 05:39:59,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cffc91ea-77d5-4f95-8836-685efede8485
2023-08-07 05:39:59,608 - distributed.worker - INFO - Starting Worker plugin PreImport-b0c1af55-1371-4de7-a4b9-b63b6617397e
2023-08-07 05:39:59,608 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7839b1b-e315-4a31-b341-0f5340b65afd
2023-08-07 05:39:59,609 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:59,636 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39329', status: init, memory: 0, processing: 0>
2023-08-07 05:39:59,638 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39329
2023-08-07 05:39:59,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54354
2023-08-07 05:39:59,638 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:39:59,638 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:39:59,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:39:59,739 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:39:59,742 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:59,744 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:39:59,746 - distributed.scheduler - INFO - Remove client Client-d6a6d154-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:59,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54332; closing.
2023-08-07 05:39:59,746 - distributed.scheduler - INFO - Remove client Client-d6a6d154-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:59,747 - distributed.scheduler - INFO - Close client connection: Client-d6a6d154-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:39:59,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36603'. Reason: nanny-close
2023-08-07 05:39:59,748 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:39:59,749 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39329. Reason: nanny-close
2023-08-07 05:39:59,751 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:39:59,751 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54354; closing.
2023-08-07 05:39:59,751 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39329', status: closing, memory: 0, processing: 0>
2023-08-07 05:39:59,751 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39329
2023-08-07 05:39:59,751 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:39:59,752 - distributed.nanny - INFO - Worker closed
2023-08-07 05:40:00,764 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:40:00,764 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:40:00,765 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:40:00,765 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:40:00,766 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-08-07 05:40:02,630 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:40:02,635 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41799 instead
  warnings.warn(
2023-08-07 05:40:02,639 - distributed.scheduler - INFO - State start
2023-08-07 05:40:02,659 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-07 05:40:02,660 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-07 05:40:02,660 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41799/status
2023-08-07 05:40:02,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46531'
2023-08-07 05:40:04,254 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:04,254 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:04,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-07 05:40:04,310 - distributed.scheduler - INFO - Receive client connection: Client-da1198b8-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:40:04,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54444
2023-08-07 05:40:05,249 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36219
2023-08-07 05:40:05,250 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36219
2023-08-07 05:40:05,250 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44945
2023-08-07 05:40:05,250 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-07 05:40:05,250 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:40:05,250 - distributed.worker - INFO -               Threads:                          1
2023-08-07 05:40:05,250 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-07 05:40:05,250 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7mcl8lws
2023-08-07 05:40:05,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81c243b7-df30-4539-b93a-830aac9f1338
2023-08-07 05:40:05,354 - distributed.worker - INFO - Starting Worker plugin PreImport-c57737e8-ec51-4c5e-b515-9881bd6c7585
2023-08-07 05:40:05,355 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26593a56-a095-44ce-92fa-02dc35f38d43
2023-08-07 05:40:05,355 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:40:05,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36219', status: init, memory: 0, processing: 0>
2023-08-07 05:40:05,379 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36219
2023-08-07 05:40:05,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41808
2023-08-07 05:40:05,380 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-07 05:40:05,380 - distributed.worker - INFO - -------------------------------------------------
2023-08-07 05:40:05,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-07 05:40:05,460 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-08-07 05:40:05,465 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-07 05:40:05,468 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:40:05,470 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-07 05:40:05,472 - distributed.scheduler - INFO - Remove client Client-da1198b8-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:40:05,472 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54444; closing.
2023-08-07 05:40:05,473 - distributed.scheduler - INFO - Remove client Client-da1198b8-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:40:05,473 - distributed.scheduler - INFO - Close client connection: Client-da1198b8-34e4-11ee-8a36-d8c49764f6bb
2023-08-07 05:40:05,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46531'. Reason: nanny-close
2023-08-07 05:40:05,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-07 05:40:05,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36219. Reason: nanny-close
2023-08-07 05:40:05,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41808; closing.
2023-08-07 05:40:05,477 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-07 05:40:05,477 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36219', status: closing, memory: 0, processing: 0>
2023-08-07 05:40:05,477 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36219
2023-08-07 05:40:05,477 - distributed.scheduler - INFO - Lost all workers
2023-08-07 05:40:05,478 - distributed.nanny - INFO - Worker closed
2023-08-07 05:40:06,440 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-07 05:40:06,440 - distributed.scheduler - INFO - Scheduler closing...
2023-08-07 05:40:06,441 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-07 05:40:06,441 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-07 05:40:06,442 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45975 instead
  warnings.warn(
2023-08-07 05:40:15,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:15,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:15,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-07 05:40:24,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:24,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:24,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:24,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:24,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:24,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:24,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:24,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:24,975 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:24,975 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:24,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:24,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:25,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:25,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:25,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:25,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35903 instead
  warnings.warn(
2023-08-07 05:40:33,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:33,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:33,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-07 05:40:43,518 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,531 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,531 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,538 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:43,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:43,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43177 instead
  warnings.warn(
2023-08-07 05:40:54,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:54,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:54,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:54,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:54,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:54,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:54,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:54,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:54,995 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:54,995 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:54,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:54,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:55,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:55,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:40:55,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:40:55,040 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44555 instead
  warnings.warn(
2023-08-07 05:41:07,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,535 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,535 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:07,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:07,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39893 instead
  warnings.warn(
2023-08-07 05:41:20,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,782 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,782 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,816 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,816 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,839 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,870 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,870 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:20,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:20,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36193 instead
  warnings.warn(
2023-08-07 05:41:33,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,571 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,594 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,594 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,667 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,710 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-07 05:41:33,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-07 05:41:33,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44893 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35833 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40021 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46833 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] 2023-08-07 05:48:14,588 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-64582c73-9639-4ad9-aa7b-1da9fa6a92e4
Function:  _run_coroutine_on_worker
args:      (315747957562812073511809311501066565531, <function shuffle_task at 0x7fa468c92790>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4)", "('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)"}, 1: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)"}, 2: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)"}, 3: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)"}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-08-07 05:48:14,600 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-841af1fd-f24e-457c-9477-d89b2735bd01
Function:  _run_coroutine_on_worker
args:      (315747957562812073511809311501066565531, <function shuffle_task at 0x7f4445aea9d0>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4)", "('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)"}, 1: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)"}, 2: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)"}, 3: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)"}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-08-07 05:48:14,603 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-d3e83798-8ac3-4a00-a663-fda62afb2d93
Function:  _run_coroutine_on_worker
args:      (315747957562812073511809311501066565531, <function shuffle_task at 0x7f34840eed30>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4)", "('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)"}, 1: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)"}, 2: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)"}, 3: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)"}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-08-07 05:48:14,626 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-74cca980-7b00-407b-b1b9-a506af602ffc
Function:  _run_coroutine_on_worker
args:      (315747957562812073511809311501066565531, <function shuffle_task at 0x7f2bcb34f670>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)", "('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4)"}, 1: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)"}, 2: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)"}, 3: {"('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)"}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-08-07 05:48:14,797 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-07 05:48:14,821 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f451a6c3f40>>, <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-07 05:48:16,824 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
