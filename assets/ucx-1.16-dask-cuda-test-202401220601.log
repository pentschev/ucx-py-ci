============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-22 06:30:08,674 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:30:08,678 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36269 instead
  warnings.warn(
2024-01-22 06:30:08,682 - distributed.scheduler - INFO - State start
2024-01-22 06:30:08,704 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:30:08,705 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-22 06:30:08,706 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:30:08,707 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-22 06:30:08,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39261'
2024-01-22 06:30:08,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33521'
2024-01-22 06:30:08,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41639'
2024-01-22 06:30:08,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34243'
2024-01-22 06:30:10,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:30:10,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:30:10,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:30:10,550 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39341
2024-01-22 06:30:10,550 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39341
2024-01-22 06:30:10,550 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38029
2024-01-22 06:30:10,550 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:10,551 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,551 - distributed.worker - INFO -               Threads:                          4
2024-01-22 06:30:10,551 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-22 06:30:10,551 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-iwo7m0ox
2024-01-22 06:30:10,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee856638-481f-4c9d-b244-c3be5c2d6957
2024-01-22 06:30:10,552 - distributed.worker - INFO - Starting Worker plugin PreImport-f193eab2-9d5c-4cbd-8f33-11787298301a
2024-01-22 06:30:10,552 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afbbda50-95cf-4ef4-ade0-e3eea4b06e25
2024-01-22 06:30:10,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:30:10,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:30:10,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:30:10,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:30:10,612 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:30:10,612 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:30:10,613 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38281
2024-01-22 06:30:10,613 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38471
2024-01-22 06:30:10,613 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38471
2024-01-22 06:30:10,613 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38281
2024-01-22 06:30:10,613 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38083
2024-01-22 06:30:10,613 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36521
2024-01-22 06:30:10,613 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:10,613 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:10,613 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,613 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,613 - distributed.worker - INFO -               Threads:                          4
2024-01-22 06:30:10,613 - distributed.worker - INFO -               Threads:                          4
2024-01-22 06:30:10,613 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-22 06:30:10,613 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-22 06:30:10,613 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-y403j3ah
2024-01-22 06:30:10,613 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-no5_09xm
2024-01-22 06:30:10,613 - distributed.worker - INFO - Starting Worker plugin PreImport-c8f5b9a3-1b80-4309-b80e-ac6644eefbda
2024-01-22 06:30:10,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb886d78-409a-41cb-a215-e511f0890cf8
2024-01-22 06:30:10,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09758303-42ae-4c5c-8af6-572e37e02a2c
2024-01-22 06:30:10,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6d72ef7-f1b4-4c7a-a2f0-e4035cdb8118
2024-01-22 06:30:10,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bba22fa-2c93-4104-ac11-70c55379aafb
2024-01-22 06:30:10,613 - distributed.worker - INFO - Starting Worker plugin PreImport-00dad5a3-655d-44fd-a989-10cc57098687
2024-01-22 06:30:10,614 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,614 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,652 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:30:10,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:30:10,656 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:30:10,657 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33495
2024-01-22 06:30:10,657 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33495
2024-01-22 06:30:10,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40643
2024-01-22 06:30:10,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:10,657 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:10,657 - distributed.worker - INFO -               Threads:                          4
2024-01-22 06:30:10,657 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-22 06:30:10,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8lylbrbi
2024-01-22 06:30:10,657 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-029e9fe5-c459-416e-8416-1784f4d80e27
2024-01-22 06:30:10,657 - distributed.worker - INFO - Starting Worker plugin PreImport-f39049fa-fa03-492d-ba18-39e5c7840f3c
2024-01-22 06:30:10,658 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65d3adc3-4d6f-4c3a-a7f7-4e2f497fae67
2024-01-22 06:30:10,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:30:39,601 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39261'. Reason: nanny-close
2024-01-22 06:30:39,601 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33521'. Reason: nanny-close
2024-01-22 06:30:39,602 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41639'. Reason: nanny-close
2024-01-22 06:30:39,602 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34243'. Reason: nanny-close
2024-01-22 06:30:40,553 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:40,614 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:40,615 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:30:40,659 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-22 06:31:11,973 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:11,977 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:31:11,980 - distributed.scheduler - INFO - State start
2024-01-22 06:31:11,982 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/mockworker-y403j3ah', purging
2024-01-22 06:31:11,982 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/mockworker-8lylbrbi', purging
2024-01-22 06:31:11,982 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/mockworker-no5_09xm', purging
2024-01-22 06:31:11,983 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/mockworker-iwo7m0ox', purging
2024-01-22 06:31:12,003 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:12,003 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:12,004 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:31:12,004 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:12,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42433'
2024-01-22 06:31:12,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33177'
2024-01-22 06:31:12,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43921'
2024-01-22 06:31:12,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41281'
2024-01-22 06:31:12,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37723'
2024-01-22 06:31:12,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40043'
2024-01-22 06:31:12,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41561'
2024-01-22 06:31:12,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41991'
2024-01-22 06:31:12,601 - distributed.scheduler - INFO - Receive client connection: Client-d4c7a49a-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:12,617 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54660
2024-01-22 06:31:13,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:13,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:13,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:13,970 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38121
2024-01-22 06:31:13,970 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38121
2024-01-22 06:31:13,971 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37737
2024-01-22 06:31:13,971 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:13,971 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:13,971 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:13,971 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:13,971 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kmysgxdp
2024-01-22 06:31:13,971 - distributed.worker - INFO - Starting Worker plugin PreImport-50869ff7-d7d8-4fbd-b723-62ff5650615f
2024-01-22 06:31:13,971 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d501f5ef-77b7-440d-bda4-7c0d37dc6a85
2024-01-22 06:31:13,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:13,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:13,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-643aa2b0-9f74-4005-9101-19e603c3b617
2024-01-22 06:31:13,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:13,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:13,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:13,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:13,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:13,978 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43917
2024-01-22 06:31:13,978 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43917
2024-01-22 06:31:13,978 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35953
2024-01-22 06:31:13,979 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:13,979 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:13,979 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:13,979 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:13,979 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rty7p_53
2024-01-22 06:31:13,979 - distributed.worker - INFO - Starting Worker plugin PreImport-ef5f13bb-c287-4822-8c39-4a7d8c9dcf54
2024-01-22 06:31:13,979 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f66ee6a-7626-4219-8081-89163e850265
2024-01-22 06:31:13,979 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0dadeecd-9c57-4275-8007-a3df755032ef
2024-01-22 06:31:13,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:13,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:13,982 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33373
2024-01-22 06:31:13,982 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33373
2024-01-22 06:31:13,983 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36559
2024-01-22 06:31:13,983 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:13,983 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:13,983 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:13,983 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:13,983 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-icnl8xig
2024-01-22 06:31:13,983 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04f9da8e-9a2b-4912-9deb-c0cefe3d3123
2024-01-22 06:31:13,983 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39803
2024-01-22 06:31:13,983 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39803
2024-01-22 06:31:13,983 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40349
2024-01-22 06:31:13,983 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:13,983 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:13,983 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:13,983 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:13,984 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2140_ygl
2024-01-22 06:31:13,984 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21542414-6881-4bef-967b-f1c979236408
2024-01-22 06:31:13,984 - distributed.worker - INFO - Starting Worker plugin PreImport-b23918f3-7904-4a30-a1ca-898284851689
2024-01-22 06:31:13,984 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d12e556e-cdae-4b2d-8e9c-cd3a07cfe5c1
2024-01-22 06:31:14,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:14,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:14,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:14,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:14,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:14,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34357
2024-01-22 06:31:14,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34357
2024-01-22 06:31:14,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34335
2024-01-22 06:31:14,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:14,033 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:14,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:14,033 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:14,033 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:14,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t0zddra0
2024-01-22 06:31:14,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fa928ce-99c5-46b6-95d2-d6e342dd4c83
2024-01-22 06:31:14,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dbcb4cbc-a5f9-47d2-8e62-dacc2d1a328d
2024-01-22 06:31:14,034 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42465
2024-01-22 06:31:14,034 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42465
2024-01-22 06:31:14,034 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39035
2024-01-22 06:31:14,034 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:14,034 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:14,034 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:14,034 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:14,034 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gtnlgv9i
2024-01-22 06:31:14,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-439348d5-7587-4ec3-8ea7-4e35ec640ef3
2024-01-22 06:31:14,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:14,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:14,042 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:14,042 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:14,042 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:14,043 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46703
2024-01-22 06:31:14,043 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46703
2024-01-22 06:31:14,044 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42057
2024-01-22 06:31:14,044 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:14,044 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:14,044 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:14,044 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:14,044 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7l4s1ahm
2024-01-22 06:31:14,044 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56818a79-a98a-48f5-b840-d6427299ea81
2024-01-22 06:31:14,044 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e237eac-7d47-4c4d-b036-6a20378ac91b
2024-01-22 06:31:14,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:14,047 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39935
2024-01-22 06:31:14,047 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39935
2024-01-22 06:31:14,048 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36387
2024-01-22 06:31:14,048 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:14,048 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:14,048 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:14,048 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:14,048 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ei0gnl14
2024-01-22 06:31:14,048 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff5b333c-c6db-4508-aa2c-4aeafa918a57
2024-01-22 06:31:16,160 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ceb00a02-d8bb-43db-8a62-a438af7f70cf
2024-01-22 06:31:16,177 - distributed.worker - INFO - Starting Worker plugin PreImport-3dea4a43-53a6-4518-bc63-a95c5262965d
2024-01-22 06:31:16,176 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,177 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39803', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,186 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39803
2024-01-22 06:31:16,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54684
2024-01-22 06:31:16,187 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,187 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,202 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33373', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,203 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33373
2024-01-22 06:31:16,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54698
2024-01-22 06:31:16,204 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,205 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,205 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38121', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38121
2024-01-22 06:31:16,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54708
2024-01-22 06:31:16,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,217 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,226 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43917', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,227 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43917
2024-01-22 06:31:16,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54720
2024-01-22 06:31:16,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,229 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,229 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-059d1869-2db6-4ace-b81c-e5caf0d3ac62
2024-01-22 06:31:16,275 - distributed.worker - INFO - Starting Worker plugin PreImport-e4b007a1-7cab-4c84-bc34-fc27a5f3530b
2024-01-22 06:31:16,277 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,283 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d43ed62-736a-4f4c-ad21-8a0fb48194e4
2024-01-22 06:31:16,284 - distributed.worker - INFO - Starting Worker plugin PreImport-a63d11bd-9263-4909-b7e4-d5104a2043bb
2024-01-22 06:31:16,285 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,290 - distributed.worker - INFO - Starting Worker plugin PreImport-835e4456-0f3e-473c-b6d1-fb06e81206fe
2024-01-22 06:31:16,292 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,309 - distributed.worker - INFO - Starting Worker plugin PreImport-41690524-786f-4dff-8af6-d4d3807b287b
2024-01-22 06:31:16,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,317 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42465', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,317 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42465
2024-01-22 06:31:16,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54730
2024-01-22 06:31:16,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,321 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,321 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,327 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34357', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,328 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34357
2024-01-22 06:31:16,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54744
2024-01-22 06:31:16,329 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39935', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,329 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39935
2024-01-22 06:31:16,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54736
2024-01-22 06:31:16,329 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,330 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,331 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,333 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,333 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46703', status: init, memory: 0, processing: 0>
2024-01-22 06:31:16,348 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46703
2024-01-22 06:31:16,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54758
2024-01-22 06:31:16,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:16,350 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:16,350 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:16,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:16,416 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,421 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,422 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:16,426 - distributed.scheduler - INFO - Remove client Client-d4c7a49a-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:16,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54660; closing.
2024-01-22 06:31:16,426 - distributed.scheduler - INFO - Remove client Client-d4c7a49a-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:16,426 - distributed.scheduler - INFO - Close client connection: Client-d4c7a49a-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:16,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42433'. Reason: nanny-close
2024-01-22 06:31:16,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33177'. Reason: nanny-close
2024-01-22 06:31:16,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43921'. Reason: nanny-close
2024-01-22 06:31:16,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38121. Reason: nanny-close
2024-01-22 06:31:16,430 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41281'. Reason: nanny-close
2024-01-22 06:31:16,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46703. Reason: nanny-close
2024-01-22 06:31:16,430 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37723'. Reason: nanny-close
2024-01-22 06:31:16,431 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33373. Reason: nanny-close
2024-01-22 06:31:16,431 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,431 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40043'. Reason: nanny-close
2024-01-22 06:31:16,431 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39803. Reason: nanny-close
2024-01-22 06:31:16,431 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,432 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41561'. Reason: nanny-close
2024-01-22 06:31:16,432 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43917. Reason: nanny-close
2024-01-22 06:31:16,432 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,432 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41991'. Reason: nanny-close
2024-01-22 06:31:16,432 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,432 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:16,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54708; closing.
2024-01-22 06:31:16,432 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39935. Reason: nanny-close
2024-01-22 06:31:16,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,433 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.433173')
2024-01-22 06:31:16,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,433 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34357. Reason: nanny-close
2024-01-22 06:31:16,433 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42465. Reason: nanny-close
2024-01-22 06:31:16,434 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54698; closing.
2024-01-22 06:31:16,434 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,434 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,434 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,434 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.4350936')
2024-01-22 06:31:16,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54758; closing.
2024-01-22 06:31:16,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54684; closing.
2024-01-22 06:31:16,436 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.4365113')
2024-01-22 06:31:16,436 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,436 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,436 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54720; closing.
2024-01-22 06:31:16,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.4371274')
2024-01-22 06:31:16,437 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43917', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.4377096')
2024-01-22 06:31:16,437 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:16,438 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54736; closing.
2024-01-22 06:31:16,438 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,438 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54744; closing.
2024-01-22 06:31:16,439 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54730; closing.
2024-01-22 06:31:16,439 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,439 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.4393897')
2024-01-22 06:31:16,439 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.4398108')
2024-01-22 06:31:16,440 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42465', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905076.440174')
2024-01-22 06:31:16,440 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:16,440 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:31:17,494 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:31:17,494 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:31:17,495 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:31:17,496 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:31:17,496 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-22 06:31:19,690 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:19,695 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:31:19,698 - distributed.scheduler - INFO - State start
2024-01-22 06:31:19,720 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:19,721 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:19,722 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:31:19,722 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:20,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39173'
2024-01-22 06:31:20,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43613'
2024-01-22 06:31:20,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38115'
2024-01-22 06:31:20,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44707'
2024-01-22 06:31:20,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37295'
2024-01-22 06:31:20,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46709'
2024-01-22 06:31:20,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40217'
2024-01-22 06:31:20,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40417'
2024-01-22 06:31:20,148 - distributed.scheduler - INFO - Receive client connection: Client-d963e4df-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:20,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45674
2024-01-22 06:31:21,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:21,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:21,906 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:21,907 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35845
2024-01-22 06:31:21,907 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35845
2024-01-22 06:31:21,907 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43139
2024-01-22 06:31:21,907 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:21,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:21,908 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:21,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:21,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ivdiofhe
2024-01-22 06:31:21,908 - distributed.worker - INFO - Starting Worker plugin PreImport-99148e54-71b2-4633-b90a-ee5a4bc62dd4
2024-01-22 06:31:21,908 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed783ecd-9d81-4f35-a510-c6e3abee35ab
2024-01-22 06:31:21,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6cd7f4f-8c5d-4bb9-bbf5-30b339aea164
2024-01-22 06:31:21,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:21,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:21,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:21,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41841
2024-01-22 06:31:21,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41841
2024-01-22 06:31:21,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33145
2024-01-22 06:31:21,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:21,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:21,930 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:21,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:21,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o2j5_y36
2024-01-22 06:31:21,930 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b57e6c75-2dd1-4fa9-9e86-1ff0c60eb77f
2024-01-22 06:31:21,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b1f00d0-5171-4558-9c5b-12238521e8d2
2024-01-22 06:31:21,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:21,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:21,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:21,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:21,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:21,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:21,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:21,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:21,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43801
2024-01-22 06:31:21,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43801
2024-01-22 06:31:21,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39701
2024-01-22 06:31:21,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:21,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:21,953 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:21,953 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:21,953 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:21,953 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gxynvx7c
2024-01-22 06:31:21,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34673
2024-01-22 06:31:21,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34673
2024-01-22 06:31:21,954 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37033
2024-01-22 06:31:21,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f8080ed-cb08-4491-a72e-2963ec510db9
2024-01-22 06:31:21,954 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:21,954 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:21,954 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:21,954 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34221
2024-01-22 06:31:21,954 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:21,954 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34221
2024-01-22 06:31:21,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-axh4ny3m
2024-01-22 06:31:21,954 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44319
2024-01-22 06:31:21,954 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:21,954 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:21,954 - distributed.worker - INFO - Starting Worker plugin PreImport-ea59c5fb-e0f0-4de9-ada8-6d31099aab0a
2024-01-22 06:31:21,954 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:21,954 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:21,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7f8b79d-2e70-4ea8-a74f-7facb7149e4d
2024-01-22 06:31:21,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_z7ieuo1
2024-01-22 06:31:21,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2468b89-b7a3-48af-b8b6-82107cd49f31
2024-01-22 06:31:22,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:22,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:22,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:22,008 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43503
2024-01-22 06:31:22,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43503
2024-01-22 06:31:22,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45893
2024-01-22 06:31:22,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:22,008 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:22,008 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:22,008 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:22,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dbq6e7gl
2024-01-22 06:31:22,008 - distributed.worker - INFO - Starting Worker plugin PreImport-680e0b86-ae27-4fdf-ab81-2c24fda52481
2024-01-22 06:31:22,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfb98f42-c14c-4cc2-bd1f-ea5e70e45b07
2024-01-22 06:31:22,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d162673a-e3ff-473b-b6ea-4cdd84fcdbbe
2024-01-22 06:31:22,018 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:22,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:22,022 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:22,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46563
2024-01-22 06:31:22,023 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46563
2024-01-22 06:31:22,023 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46113
2024-01-22 06:31:22,023 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:22,023 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:22,023 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:22,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:22,023 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-na9so6mu
2024-01-22 06:31:22,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8271062-bd20-4f67-8717-4b44194fc60c
2024-01-22 06:31:22,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:22,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:22,034 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:22,034 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40961
2024-01-22 06:31:22,034 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40961
2024-01-22 06:31:22,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39699
2024-01-22 06:31:22,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:22,035 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:22,035 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:22,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:22,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nzt80z5u
2024-01-22 06:31:22,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ebf3acb-8acf-4098-ba10-dac3ecbc2b21
2024-01-22 06:31:24,260 - distributed.worker - INFO - Starting Worker plugin PreImport-7b370ec8-1d5d-43c3-abbf-9110e011654f
2024-01-22 06:31:24,261 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,269 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ecfed5de-57e4-4c80-a851-bcfd7d62ff85
2024-01-22 06:31:24,269 - distributed.worker - INFO - Starting Worker plugin PreImport-7c9b409d-8b68-4ed8-a034-76e5e8c99b30
2024-01-22 06:31:24,270 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41841', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41841
2024-01-22 06:31:24,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45704
2024-01-22 06:31:24,286 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,286 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34221', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34221
2024-01-22 06:31:24,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45718
2024-01-22 06:31:24,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,296 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,303 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1b3d2aa-b9c6-4dcb-b74e-9ee70f66d814
2024-01-22 06:31:24,324 - distributed.worker - INFO - Starting Worker plugin PreImport-6bb44e39-3b4c-4658-b450-cc22e4ea0007
2024-01-22 06:31:24,325 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,339 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35845', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,340 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35845
2024-01-22 06:31:24,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45720
2024-01-22 06:31:24,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b064af7c-7b43-450c-b730-443f8962e4bc
2024-01-22 06:31:24,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,364 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43801', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43801
2024-01-22 06:31:24,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45734
2024-01-22 06:31:24,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,369 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,369 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,374 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34673', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,374 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34673
2024-01-22 06:31:24,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45746
2024-01-22 06:31:24,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,376 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34b5119f-075c-46d3-a689-095d764e08da
2024-01-22 06:31:24,380 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70a7fa75-8b6d-4192-8f9d-1ae9bfaea9f0
2024-01-22 06:31:24,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,380 - distributed.worker - INFO - Starting Worker plugin PreImport-7beeb01f-c001-4a69-9ccb-0368c9d01c6c
2024-01-22 06:31:24,381 - distributed.worker - INFO - Starting Worker plugin PreImport-42e00dea-d51f-4a37-958a-25de13b089b8
2024-01-22 06:31:24,381 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,382 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,394 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,401 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46563', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46563
2024-01-22 06:31:24,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45762
2024-01-22 06:31:24,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,404 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,404 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,414 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40961', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40961
2024-01-22 06:31:24,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45772
2024-01-22 06:31:24,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,417 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,427 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43503', status: init, memory: 0, processing: 0>
2024-01-22 06:31:24,427 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43503
2024-01-22 06:31:24,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45784
2024-01-22 06:31:24,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:24,430 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:24,430 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:24,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:24,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,443 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,443 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,443 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,447 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,447 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,447 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:24,451 - distributed.scheduler - INFO - Remove client Client-d963e4df-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:24,452 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45674; closing.
2024-01-22 06:31:24,452 - distributed.scheduler - INFO - Remove client Client-d963e4df-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:24,452 - distributed.scheduler - INFO - Close client connection: Client-d963e4df-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:24,453 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39173'. Reason: nanny-close
2024-01-22 06:31:24,453 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43613'. Reason: nanny-close
2024-01-22 06:31:24,454 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38115'. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35845. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44707'. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43801. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37295'. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41841. Reason: nanny-close
2024-01-22 06:31:24,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46709'. Reason: nanny-close
2024-01-22 06:31:24,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40217'. Reason: nanny-close
2024-01-22 06:31:24,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34673. Reason: nanny-close
2024-01-22 06:31:24,456 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40417'. Reason: nanny-close
2024-01-22 06:31:24,456 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34221. Reason: nanny-close
2024-01-22 06:31:24,457 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,457 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46563. Reason: nanny-close
2024-01-22 06:31:24,457 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,457 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45720; closing.
2024-01-22 06:31:24,457 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35845', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.4578764')
2024-01-22 06:31:24,457 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,458 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45704; closing.
2024-01-22 06:31:24,458 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,458 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,458 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,458 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,459 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41841', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.4589524')
2024-01-22 06:31:24,459 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,459 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,460 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,460 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45718; closing.
2024-01-22 06:31:24,460 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45746; closing.
2024-01-22 06:31:24,460 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,460 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45734; closing.
2024-01-22 06:31:24,461 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,460 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45704>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-22 06:31:24,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.462515')
2024-01-22 06:31:24,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.4628758')
2024-01-22 06:31:24,463 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45762; closing.
2024-01-22 06:31:24,463 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.463406')
2024-01-22 06:31:24,463 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46563', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.4638734')
2024-01-22 06:31:24,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:24,466 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40961. Reason: nanny-close
2024-01-22 06:31:24,466 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43503. Reason: nanny-close
2024-01-22 06:31:24,468 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45772; closing.
2024-01-22 06:31:24,468 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,468 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.4685063')
2024-01-22 06:31:24,468 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:24,468 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45784; closing.
2024-01-22 06:31:24,469 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905084.4692137')
2024-01-22 06:31:24,469 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:31:24,470 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:24,470 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:25,369 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:31:25,370 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:31:25,370 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:31:25,371 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:31:25,372 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-22 06:31:27,540 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:27,544 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:31:27,547 - distributed.scheduler - INFO - State start
2024-01-22 06:31:27,567 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:27,568 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:27,568 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:31:27,569 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:27,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40317'
2024-01-22 06:31:27,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42315'
2024-01-22 06:31:27,769 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44101'
2024-01-22 06:31:27,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44525'
2024-01-22 06:31:27,787 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40349'
2024-01-22 06:31:27,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39609'
2024-01-22 06:31:27,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35381'
2024-01-22 06:31:27,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42107'
2024-01-22 06:31:27,987 - distributed.scheduler - INFO - Receive client connection: Client-de1e05f9-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:28,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45964
2024-01-22 06:31:29,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,631 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,631 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,631 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,631 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38767
2024-01-22 06:31:29,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38455
2024-01-22 06:31:29,632 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38767
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35357
2024-01-22 06:31:29,632 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38455
2024-01-22 06:31:29,632 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40117
2024-01-22 06:31:29,632 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,632 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35357
2024-01-22 06:31:29,632 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39361
2024-01-22 06:31:29,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,632 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,632 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42525
2024-01-22 06:31:29,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,632 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,632 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,632 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,632 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,632 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,632 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nzccz_kh
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j4qysp0i
2024-01-22 06:31:29,632 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-almsyjku
2024-01-22 06:31:29,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46675
2024-01-22 06:31:29,632 - distributed.worker - INFO - Starting Worker plugin PreImport-0caa599a-11cf-44a1-a7f1-e3b8acf6b656
2024-01-22 06:31:29,632 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46675
2024-01-22 06:31:29,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-569e2b24-178c-43ff-80a7-b1cae5fb885d
2024-01-22 06:31:29,632 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45653
2024-01-22 06:31:29,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9823947-3bf8-4dbf-b988-c9e0a6d2b399
2024-01-22 06:31:29,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b2264fc-caa7-44a5-be54-071bf8497dec
2024-01-22 06:31:29,632 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5efc1319-cf40-4519-8661-11c6111e8163
2024-01-22 06:31:29,632 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,633 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,633 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ypu_uke
2024-01-22 06:31:29,633 - distributed.worker - INFO - Starting Worker plugin PreImport-8d1780ed-f50f-4d4b-a131-4c63526938d8
2024-01-22 06:31:29,633 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cefd2442-7c3f-4729-aaaf-0ae5bb8b9584
2024-01-22 06:31:29,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a76a1ac-0c83-4340-8153-938c129500e9
2024-01-22 06:31:29,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34249
2024-01-22 06:31:29,637 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34249
2024-01-22 06:31:29,637 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34305
2024-01-22 06:31:29,637 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,637 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,638 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,638 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,638 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nnczi4et
2024-01-22 06:31:29,638 - distributed.worker - INFO - Starting Worker plugin PreImport-c5909c97-522c-43ba-9518-b00ee2e297d2
2024-01-22 06:31:29,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ad3fa012-31bd-4263-ae76-9d4b20ec0104
2024-01-22 06:31:29,638 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47e9028c-f365-41ae-a970-ff352e7d9533
2024-01-22 06:31:29,638 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,641 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,641 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:29,641 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,641 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:29,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46447
2024-01-22 06:31:29,644 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46447
2024-01-22 06:31:29,644 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38709
2024-01-22 06:31:29,644 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,644 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,644 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,644 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a34p564b
2024-01-22 06:31:29,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f82082d5-4eb9-4721-ab22-e8fc3e0fca45
2024-01-22 06:31:29,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:29,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40621
2024-01-22 06:31:29,646 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40621
2024-01-22 06:31:29,646 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40713
2024-01-22 06:31:29,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37729
2024-01-22 06:31:29,646 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,646 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37729
2024-01-22 06:31:29,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,646 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36329
2024-01-22 06:31:29,646 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,646 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:29,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:29,646 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y540yf38
2024-01-22 06:31:29,646 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:29,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:29,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bua7384g
2024-01-22 06:31:29,647 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6623d004-70a0-4d67-9ea6-403726191df6
2024-01-22 06:31:29,647 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae32a02a-1ab0-436f-a58b-6e33a3a6c60c
2024-01-22 06:31:32,100 - distributed.worker - INFO - Starting Worker plugin PreImport-5a305bed-7100-4b45-9a3a-eb35d5eb7687
2024-01-22 06:31:32,100 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,115 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1090a611-4295-417e-a625-b2727d78061e
2024-01-22 06:31:32,115 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,125 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35357', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,127 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35357
2024-01-22 06:31:32,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60158
2024-01-22 06:31:32,127 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c79c3262-7a10-4a3d-8880-5b983ec87eb4
2024-01-22 06:31:32,127 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,128 - distributed.worker - INFO - Starting Worker plugin PreImport-d8f9690d-04b3-4474-8345-02cec749ddbf
2024-01-22 06:31:32,128 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,128 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,128 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22318931-b9d4-4664-9d00-3e4399f80b9e
2024-01-22 06:31:32,133 - distributed.worker - INFO - Starting Worker plugin PreImport-04fced30-c0e1-4c52-8952-ed9fc09ffb82
2024-01-22 06:31:32,134 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,134 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,138 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f71b140d-b4ed-46e4-80b3-19e77883aa61
2024-01-22 06:31:32,139 - distributed.worker - INFO - Starting Worker plugin PreImport-c7270a9c-204b-4804-8dbd-340f77571dc4
2024-01-22 06:31:32,140 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38767', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38767
2024-01-22 06:31:32,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60170
2024-01-22 06:31:32,140 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,140 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,142 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,144 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d63ff0d6-1b46-44ee-8279-c3b40715dc19
2024-01-22 06:31:32,145 - distributed.worker - INFO - Starting Worker plugin PreImport-0c1de7e6-6219-4156-8066-b674f8aa05db
2024-01-22 06:31:32,145 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,154 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40621', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,155 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40621
2024-01-22 06:31:32,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60176
2024-01-22 06:31:32,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,157 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,171 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37729', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37729
2024-01-22 06:31:32,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60216
2024-01-22 06:31:32,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,173 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,173 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,174 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38455', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,175 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38455
2024-01-22 06:31:32,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60188
2024-01-22 06:31:32,175 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46675', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,176 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46675
2024-01-22 06:31:32,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60196
2024-01-22 06:31:32,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,177 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34249', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,178 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,178 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,178 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34249
2024-01-22 06:31:32,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60206
2024-01-22 06:31:32,179 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,179 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,180 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46447', status: init, memory: 0, processing: 0>
2024-01-22 06:31:32,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46447
2024-01-22 06:31:32,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60210
2024-01-22 06:31:32,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:32,186 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:32,187 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:32,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:32,263 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,264 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,264 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,264 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,264 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,265 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,266 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,267 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,280 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:32,288 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:32,290 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:32,292 - distributed.scheduler - INFO - Remove client Client-de1e05f9-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:32,292 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45964; closing.
2024-01-22 06:31:32,292 - distributed.scheduler - INFO - Remove client Client-de1e05f9-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:32,293 - distributed.scheduler - INFO - Close client connection: Client-de1e05f9-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:32,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40317'. Reason: nanny-close
2024-01-22 06:31:32,294 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42315'. Reason: nanny-close
2024-01-22 06:31:32,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44101'. Reason: nanny-close
2024-01-22 06:31:32,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,295 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46675. Reason: nanny-close
2024-01-22 06:31:32,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44525'. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38455. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40349'. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35357. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39609'. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38767. Reason: nanny-close
2024-01-22 06:31:32,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35381'. Reason: nanny-close
2024-01-22 06:31:32,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,297 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46447. Reason: nanny-close
2024-01-22 06:31:32,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42107'. Reason: nanny-close
2024-01-22 06:31:32,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:32,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34249. Reason: nanny-close
2024-01-22 06:31:32,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37729. Reason: nanny-close
2024-01-22 06:31:32,298 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,298 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60196; closing.
2024-01-22 06:31:32,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60158; closing.
2024-01-22 06:31:32,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40621. Reason: nanny-close
2024-01-22 06:31:32,298 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,299 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,299 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.2989852')
2024-01-22 06:31:32,299 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.299574')
2024-01-22 06:31:32,299 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,300 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,300 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60170; closing.
2024-01-22 06:31:32,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,300 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60188; closing.
2024-01-22 06:31:32,300 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:32,300 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,301 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,301 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.301722')
2024-01-22 06:31:32,302 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,302 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38455', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.3021207')
2024-01-22 06:31:32,302 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,303 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:32,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60216; closing.
2024-01-22 06:31:32,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60210; closing.
2024-01-22 06:31:32,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60206; closing.
2024-01-22 06:31:32,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60176; closing.
2024-01-22 06:31:32,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.3042157')
2024-01-22 06:31:32,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.3045704')
2024-01-22 06:31:32,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.3049304')
2024-01-22 06:31:32,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905092.305362')
2024-01-22 06:31:32,305 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:31:33,210 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:31:33,210 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:31:33,211 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:31:33,212 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:31:33,212 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-22 06:31:35,509 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:35,513 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:31:35,517 - distributed.scheduler - INFO - State start
2024-01-22 06:31:35,617 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:35,618 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:35,618 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:31:35,619 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:35,703 - distributed.scheduler - INFO - Receive client connection: Client-e2c845e7-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:35,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60286
2024-01-22 06:31:36,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33701'
2024-01-22 06:31:36,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40931'
2024-01-22 06:31:36,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41169'
2024-01-22 06:31:36,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40707'
2024-01-22 06:31:36,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41223'
2024-01-22 06:31:36,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40387'
2024-01-22 06:31:36,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40031'
2024-01-22 06:31:36,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43091'
2024-01-22 06:31:38,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,551 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,557 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39621
2024-01-22 06:31:38,557 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39621
2024-01-22 06:31:38,557 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41023
2024-01-22 06:31:38,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,557 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,557 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xyy5dh5d
2024-01-22 06:31:38,557 - distributed.worker - INFO - Starting Worker plugin PreImport-8fe812ec-15b3-4346-9ee0-86978b2e55a9
2024-01-22 06:31:38,557 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07d8298d-3112-443f-8ff4-417d29094594
2024-01-22 06:31:38,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,587 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,587 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40799
2024-01-22 06:31:38,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40799
2024-01-22 06:31:38,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40171
2024-01-22 06:31:38,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,590 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,590 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dbkzljkf
2024-01-22 06:31:38,590 - distributed.worker - INFO - Starting Worker plugin PreImport-9cff290f-8493-40de-b005-3bababb47341
2024-01-22 06:31:38,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c5deb2d-e9bb-4e52-94fd-41e4a62d2412
2024-01-22 06:31:38,590 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d8614b0-befb-48c6-b9ea-fd28b54ab469
2024-01-22 06:31:38,592 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37625
2024-01-22 06:31:38,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37625
2024-01-22 06:31:38,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46801
2024-01-22 06:31:38,593 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,593 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,593 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g9a5u7x9
2024-01-22 06:31:38,593 - distributed.worker - INFO - Starting Worker plugin RMMSetup-126d5a25-87b6-4cc5-b0ee-b80428c6ce89
2024-01-22 06:31:38,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,631 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39391
2024-01-22 06:31:38,631 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39391
2024-01-22 06:31:38,632 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35665
2024-01-22 06:31:38,632 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,632 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,632 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l211zq_8
2024-01-22 06:31:38,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eafe4af5-5669-470d-ab4e-e6185af51702
2024-01-22 06:31:38,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-782edea0-7456-4fb1-9fba-60745c7cfeec
2024-01-22 06:31:38,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,810 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,810 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,814 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,815 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,815 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33885
2024-01-22 06:31:38,816 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33885
2024-01-22 06:31:38,816 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37733
2024-01-22 06:31:38,816 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,816 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,816 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,816 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,816 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u0txqj83
2024-01-22 06:31:38,816 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37619
2024-01-22 06:31:38,816 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37619
2024-01-22 06:31:38,816 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f34cc70-ef4d-4047-a5ff-7286123b34bf
2024-01-22 06:31:38,816 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45109
2024-01-22 06:31:38,816 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,816 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,816 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,816 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hyj9fdk7
2024-01-22 06:31:38,817 - distributed.worker - INFO - Starting Worker plugin PreImport-495509a2-7c12-4919-b69d-c137a7d0d125
2024-01-22 06:31:38,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-014e10c3-235b-41e8-8455-11890a691167
2024-01-22 06:31:38,817 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9562907a-b507-4101-94e9-12bbbb34af05
2024-01-22 06:31:38,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,830 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,831 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37267
2024-01-22 06:31:38,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37267
2024-01-22 06:31:38,831 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41097
2024-01-22 06:31:38,831 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,831 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,831 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,831 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m8tnfpva
2024-01-22 06:31:38,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a910c8d-87d8-45bd-bc44-cf967f5980d0
2024-01-22 06:31:38,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:38,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:38,844 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:38,845 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33169
2024-01-22 06:31:38,845 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33169
2024-01-22 06:31:38,845 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39531
2024-01-22 06:31:38,846 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:38,846 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:38,846 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:38,846 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:38,846 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n4j1uymb
2024-01-22 06:31:38,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13899549-80bd-4cbf-857a-fd844cc51e0e
2024-01-22 06:31:39,513 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-836a2617-c690-4db2-a5b0-017544b5a8c8
2024-01-22 06:31:39,513 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:39,534 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39621', status: init, memory: 0, processing: 0>
2024-01-22 06:31:39,536 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39621
2024-01-22 06:31:39,536 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60360
2024-01-22 06:31:39,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:39,538 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:39,538 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:39,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,797 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,803 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-418a858d-8cff-4572-a662-a2d699d51696
2024-01-22 06:31:40,804 - distributed.worker - INFO - Starting Worker plugin PreImport-0844949a-751f-42fc-af81-73d36e82f16b
2024-01-22 06:31:40,805 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40799', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,822 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40799
2024-01-22 06:31:40,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53944
2024-01-22 06:31:40,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,824 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,824 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37625', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37625
2024-01-22 06:31:40,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53952
2024-01-22 06:31:40,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,868 - distributed.worker - INFO - Starting Worker plugin PreImport-a02c8a24-79cc-429f-8d7c-00da6f5307fd
2024-01-22 06:31:40,870 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02ba7393-4c0c-4b32-8e83-3bbe9ec6e1a4
2024-01-22 06:31:40,874 - distributed.worker - INFO - Starting Worker plugin PreImport-8eef8b87-7014-487e-bf6b-1084a5968853
2024-01-22 06:31:40,875 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,880 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b751921-a431-4dc4-95ba-7d801c0be6dd
2024-01-22 06:31:40,881 - distributed.worker - INFO - Starting Worker plugin PreImport-a748a0c8-2575-461a-a62d-e5427885da38
2024-01-22 06:31:40,882 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61f8d519-a132-4ab6-b056-7bfa7b6b3d1f
2024-01-22 06:31:40,882 - distributed.worker - INFO - Starting Worker plugin PreImport-5d7cb472-a0bc-4c6e-a097-612d533246f5
2024-01-22 06:31:40,882 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,883 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,903 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37619', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,903 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37619
2024-01-22 06:31:40,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53980
2024-01-22 06:31:40,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,905 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,905 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39391', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,908 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39391
2024-01-22 06:31:40,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53966
2024-01-22 06:31:40,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37267', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37267
2024-01-22 06:31:40,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54004
2024-01-22 06:31:40,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,910 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,911 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,911 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33885', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,917 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33885
2024-01-22 06:31:40,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53992
2024-01-22 06:31:40,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,920 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,920 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,920 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33169', status: init, memory: 0, processing: 0>
2024-01-22 06:31:40,921 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33169
2024-01-22 06:31:40,921 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54006
2024-01-22 06:31:40,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:40,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:40,924 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:40,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:40,968 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,968 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,968 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,968 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,969 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,970 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,971 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,973 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,985 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:31:40,994 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,996 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:40,999 - distributed.scheduler - INFO - Remove client Client-e2c845e7-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:40,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60286; closing.
2024-01-22 06:31:40,999 - distributed.scheduler - INFO - Remove client Client-e2c845e7-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:41,000 - distributed.scheduler - INFO - Close client connection: Client-e2c845e7-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:41,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33701'. Reason: nanny-close
2024-01-22 06:31:41,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40931'. Reason: nanny-close
2024-01-22 06:31:41,003 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41169'. Reason: nanny-close
2024-01-22 06:31:41,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40799. Reason: nanny-close
2024-01-22 06:31:41,003 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40707'. Reason: nanny-close
2024-01-22 06:31:41,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37625. Reason: nanny-close
2024-01-22 06:31:41,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,004 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41223'. Reason: nanny-close
2024-01-22 06:31:41,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39391. Reason: nanny-close
2024-01-22 06:31:41,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40387'. Reason: nanny-close
2024-01-22 06:31:41,005 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53944; closing.
2024-01-22 06:31:41,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39621. Reason: nanny-close
2024-01-22 06:31:41,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40031'. Reason: nanny-close
2024-01-22 06:31:41,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37267. Reason: nanny-close
2024-01-22 06:31:41,005 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.0055962')
2024-01-22 06:31:41,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,006 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43091'. Reason: nanny-close
2024-01-22 06:31:41,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37619. Reason: nanny-close
2024-01-22 06:31:41,006 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:41,006 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,007 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53952; closing.
2024-01-22 06:31:41,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,007 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,008 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33169. Reason: nanny-close
2024-01-22 06:31:41,008 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.008132')
2024-01-22 06:31:41,008 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33885. Reason: nanny-close
2024-01-22 06:31:41,008 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,008 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60360; closing.
2024-01-22 06:31:41,008 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,008 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,009 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.0092263')
2024-01-22 06:31:41,009 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53966; closing.
2024-01-22 06:31:41,009 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54004; closing.
2024-01-22 06:31:41,010 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,010 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.010321')
2024-01-22 06:31:41,010 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:41,010 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.0106206')
2024-01-22 06:31:41,010 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53980; closing.
2024-01-22 06:31:41,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.0115063')
2024-01-22 06:31:41,011 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54006; closing.
2024-01-22 06:31:41,012 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,012 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:41,012 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.0123796')
2024-01-22 06:31:41,012 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53992; closing.
2024-01-22 06:31:41,013 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33885', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905101.0131216')
2024-01-22 06:31:41,013 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:31:41,013 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53992>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-22 06:31:42,217 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:31:42,218 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:31:42,219 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:31:42,220 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:31:42,221 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-22 06:31:44,226 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:44,230 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:31:44,233 - distributed.scheduler - INFO - State start
2024-01-22 06:31:44,253 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:44,254 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:44,255 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:31:44,255 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:44,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36657'
2024-01-22 06:31:44,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40309'
2024-01-22 06:31:44,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41171'
2024-01-22 06:31:44,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39885'
2024-01-22 06:31:44,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39295'
2024-01-22 06:31:44,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34139'
2024-01-22 06:31:44,787 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38111'
2024-01-22 06:31:44,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42307'
2024-01-22 06:31:46,028 - distributed.scheduler - INFO - Receive client connection: Client-e8223c67-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:46,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54136
2024-01-22 06:31:46,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,615 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,616 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33165
2024-01-22 06:31:46,616 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33165
2024-01-22 06:31:46,616 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45039
2024-01-22 06:31:46,616 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,616 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,616 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,616 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,616 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hvwbt_yj
2024-01-22 06:31:46,616 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9eadf101-ec7b-485c-a1af-a934d53698bc
2024-01-22 06:31:46,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,618 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b09f3dd2-636c-4a44-a614-f02db45aa306
2024-01-22 06:31:46,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42255
2024-01-22 06:31:46,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42255
2024-01-22 06:31:46,619 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33703
2024-01-22 06:31:46,619 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,619 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,619 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,619 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,619 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dpi7u_x9
2024-01-22 06:31:46,619 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-922f060d-066f-44ef-98b6-82126f03b27a
2024-01-22 06:31:46,619 - distributed.worker - INFO - Starting Worker plugin PreImport-56a455f2-ab22-4d9c-91ab-07d67287a772
2024-01-22 06:31:46,620 - distributed.worker - INFO - Starting Worker plugin RMMSetup-98da44e3-bc86-4c2e-95dc-bd31327b50d1
2024-01-22 06:31:46,622 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,623 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37335
2024-01-22 06:31:46,623 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37335
2024-01-22 06:31:46,623 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34529
2024-01-22 06:31:46,623 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,623 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,623 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,623 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,623 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f8yy14sx
2024-01-22 06:31:46,624 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bdb2e026-8fe3-4532-bbf5-94d9f98d5c75
2024-01-22 06:31:46,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,635 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,636 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42723
2024-01-22 06:31:46,636 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42723
2024-01-22 06:31:46,636 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41753
2024-01-22 06:31:46,636 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,636 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,636 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,636 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,636 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z9o6_c2v
2024-01-22 06:31:46,636 - distributed.worker - INFO - Starting Worker plugin PreImport-f368e710-3644-4704-8bd3-518d89fe7a1f
2024-01-22 06:31:46,636 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d39c9232-dd46-4fc1-84e1-a26b4ce7b2e2
2024-01-22 06:31:46,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cee63ac-f16f-48b6-b718-0dd882ea3421
2024-01-22 06:31:46,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,660 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,665 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38169
2024-01-22 06:31:46,665 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38169
2024-01-22 06:31:46,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33675
2024-01-22 06:31:46,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,665 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,665 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,665 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,665 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k6sevmi9
2024-01-22 06:31:46,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09a23b66-c03e-4bbe-bfc9-7221eb144e2d
2024-01-22 06:31:46,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,692 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,693 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34661
2024-01-22 06:31:46,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,693 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34661
2024-01-22 06:31:46,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38497
2024-01-22 06:31:46,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,693 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,693 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,693 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vqv0wguw
2024-01-22 06:31:46,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e46f32b-f661-4b5b-a11a-8003661908e7
2024-01-22 06:31:46,698 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,699 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43241
2024-01-22 06:31:46,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43241
2024-01-22 06:31:46,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39039
2024-01-22 06:31:46,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,699 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hpzhnd94
2024-01-22 06:31:46,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02f28e53-33ec-41da-9b75-9ee1386b8b9b
2024-01-22 06:31:46,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:46,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:46,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:46,706 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32801
2024-01-22 06:31:46,706 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32801
2024-01-22 06:31:46,706 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45767
2024-01-22 06:31:46,706 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:46,706 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:46,706 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:46,706 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:31:46,706 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ps_ekc6
2024-01-22 06:31:46,707 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff0b20ff-1bb3-4814-bbca-299be2f2da94
2024-01-22 06:31:46,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6bb8c144-cf45-4768-936a-6347e2f488f8
2024-01-22 06:31:48,706 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,729 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6516c529-fa96-4f0d-981f-0a509b9a7ab9
2024-01-22 06:31:48,730 - distributed.worker - INFO - Starting Worker plugin PreImport-a9182376-2ff1-401c-828d-1204fe71c0aa
2024-01-22 06:31:48,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,732 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,732 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42255', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42255
2024-01-22 06:31:48,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54160
2024-01-22 06:31:48,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,736 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,736 - distributed.worker - INFO - Starting Worker plugin PreImport-81bb5162-e4e2-430c-944c-be37bc62eab5
2024-01-22 06:31:48,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,738 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,772 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33165', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,773 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33165
2024-01-22 06:31:48,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54176
2024-01-22 06:31:48,773 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42723', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,774 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42723
2024-01-22 06:31:48,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54172
2024-01-22 06:31:48,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,776 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,777 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37335', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,777 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37335
2024-01-22 06:31:48,777 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54174
2024-01-22 06:31:48,777 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8aa96d76-1098-426b-947f-14766ee51bd2
2024-01-22 06:31:48,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,779 - distributed.worker - INFO - Starting Worker plugin PreImport-b13cfd0b-d933-4fc7-bf65-b2a557a5c413
2024-01-22 06:31:48,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,781 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,781 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33b0d46d-5ff4-477a-8c67-a092d88b89c5
2024-01-22 06:31:48,787 - distributed.worker - INFO - Starting Worker plugin PreImport-463fd24c-5600-4d62-b03f-e2c63fbce4e2
2024-01-22 06:31:48,787 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,806 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38169', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,807 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38169
2024-01-22 06:31:48,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54192
2024-01-22 06:31:48,808 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,809 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,809 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34661', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,810 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34661
2024-01-22 06:31:48,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54198
2024-01-22 06:31:48,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,812 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b291e17f-3a40-4eac-b60c-fc903c61c4fb
2024-01-22 06:31:48,813 - distributed.worker - INFO - Starting Worker plugin PreImport-7f264d74-43b0-49f5-9b0c-fef4d9fc12bb
2024-01-22 06:31:48,814 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,814 - distributed.worker - INFO - Starting Worker plugin PreImport-d328fdb2-2ca0-4d4c-8641-3d142be5eeec
2024-01-22 06:31:48,815 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,835 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32801', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32801
2024-01-22 06:31:48,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54204
2024-01-22 06:31:48,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,838 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,850 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43241', status: init, memory: 0, processing: 0>
2024-01-22 06:31:48,851 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43241
2024-01-22 06:31:48,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54218
2024-01-22 06:31:48,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:48,853 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:48,853 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:48,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:48,874 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,875 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,875 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,875 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,878 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,881 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:31:48,885 - distributed.scheduler - INFO - Remove client Client-e8223c67-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:48,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54136; closing.
2024-01-22 06:31:48,885 - distributed.scheduler - INFO - Remove client Client-e8223c67-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:48,885 - distributed.scheduler - INFO - Close client connection: Client-e8223c67-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:48,886 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36657'. Reason: nanny-close
2024-01-22 06:31:48,887 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,887 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40309'. Reason: nanny-close
2024-01-22 06:31:48,887 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41171'. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37335. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39885'. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33165. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39295'. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38169. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34139'. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42255. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38111'. Reason: nanny-close
2024-01-22 06:31:48,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42723. Reason: nanny-close
2024-01-22 06:31:48,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42307'. Reason: nanny-close
2024-01-22 06:31:48,890 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:48,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43241. Reason: nanny-close
2024-01-22 06:31:48,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32801. Reason: nanny-close
2024-01-22 06:31:48,890 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,890 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,891 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,891 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54192; closing.
2024-01-22 06:31:48,891 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,891 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34661. Reason: nanny-close
2024-01-22 06:31:48,891 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.891556')
2024-01-22 06:31:48,892 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,892 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54174; closing.
2024-01-22 06:31:48,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,892 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54176; closing.
2024-01-22 06:31:48,892 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,892 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,893 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,893 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:48,893 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,893 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.8932831')
2024-01-22 06:31:48,893 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,893 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.8937955')
2024-01-22 06:31:48,894 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54160; closing.
2024-01-22 06:31:48,894 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,894 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,895 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:48,895 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.8958194')
2024-01-22 06:31:48,896 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54204; closing.
2024-01-22 06:31:48,896 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54172; closing.
2024-01-22 06:31:48,897 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54176>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-22 06:31:48,899 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54174>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-22 06:31:48,900 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.8999188')
2024-01-22 06:31:48,900 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42723', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.9003448')
2024-01-22 06:31:48,900 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54218; closing.
2024-01-22 06:31:48,901 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54198; closing.
2024-01-22 06:31:48,901 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.9014976')
2024-01-22 06:31:48,902 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34661', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905108.9019663')
2024-01-22 06:31:48,902 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:31:50,003 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:31:50,003 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:31:50,004 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:31:50,005 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:31:50,006 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-22 06:31:52,102 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:52,106 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:31:52,110 - distributed.scheduler - INFO - State start
2024-01-22 06:31:52,131 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:52,132 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:52,133 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:31:52,133 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:52,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34201'
2024-01-22 06:31:53,147 - distributed.scheduler - INFO - Receive client connection: Client-ecc2a8b2-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:53,162 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58764
2024-01-22 06:31:54,272 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:31:54,272 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:31:54,789 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:31:54,790 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36033
2024-01-22 06:31:54,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36033
2024-01-22 06:31:54,790 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-22 06:31:54,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:31:54,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:54,790 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:31:54,790 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-22 06:31:54,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-avapv24y
2024-01-22 06:31:54,791 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3929ee9c-6eb8-4e2e-9ea9-5ef90028c8b6
2024-01-22 06:31:54,791 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e168db2-6d41-4c39-88bb-2c544f0de583
2024-01-22 06:31:54,791 - distributed.worker - INFO - Starting Worker plugin PreImport-ff66866c-4eef-406d-bf64-dfe9c56fcf0f
2024-01-22 06:31:54,791 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:54,838 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36033', status: init, memory: 0, processing: 0>
2024-01-22 06:31:54,839 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36033
2024-01-22 06:31:54,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58790
2024-01-22 06:31:54,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:31:54,844 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:31:54,844 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:31:54,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:31:54,847 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:31:54,850 - distributed.scheduler - INFO - Remove client Client-ecc2a8b2-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:54,850 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58764; closing.
2024-01-22 06:31:54,850 - distributed.scheduler - INFO - Remove client Client-ecc2a8b2-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:54,851 - distributed.scheduler - INFO - Close client connection: Client-ecc2a8b2-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:54,852 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34201'. Reason: nanny-close
2024-01-22 06:31:54,885 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:31:54,886 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36033. Reason: nanny-close
2024-01-22 06:31:54,888 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:31:54,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58790; closing.
2024-01-22 06:31:54,889 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905114.8891132')
2024-01-22 06:31:54,889 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:31:54,890 - distributed.nanny - INFO - Worker closed
2024-01-22 06:31:55,517 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:31:55,517 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:31:55,517 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:31:55,518 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:31:55,519 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-22 06:31:59,537 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:59,542 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36735 instead
  warnings.warn(
2024-01-22 06:31:59,546 - distributed.scheduler - INFO - State start
2024-01-22 06:31:59,568 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:31:59,569 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:31:59,570 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36735/status
2024-01-22 06:31:59,570 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:31:59,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33703'
2024-01-22 06:31:59,697 - distributed.scheduler - INFO - Receive client connection: Client-f12b5732-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:31:59,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58890
2024-01-22 06:32:01,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:01,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:02,471 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:02,472 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37967
2024-01-22 06:32:02,472 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37967
2024-01-22 06:32:02,472 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33935
2024-01-22 06:32:02,472 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:02,472 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:02,472 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:02,472 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-22 06:32:02,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0rdo3ug8
2024-01-22 06:32:02,473 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74b0a386-2fda-4e80-b7af-9fdb926fbd57
2024-01-22 06:32:02,473 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28fbf497-0b54-4415-834b-411c574cdd49
2024-01-22 06:32:02,473 - distributed.worker - INFO - Starting Worker plugin PreImport-6649e5e7-6f66-4678-9416-65bdca962381
2024-01-22 06:32:02,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:02,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37967', status: init, memory: 0, processing: 0>
2024-01-22 06:32:02,887 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37967
2024-01-22 06:32:02,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45914
2024-01-22 06:32:02,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:02,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:02,889 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:02,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:02,990 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:02,993 - distributed.scheduler - INFO - Remove client Client-f12b5732-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:02,994 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58890; closing.
2024-01-22 06:32:02,994 - distributed.scheduler - INFO - Remove client Client-f12b5732-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:02,994 - distributed.scheduler - INFO - Close client connection: Client-f12b5732-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:02,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33703'. Reason: nanny-close
2024-01-22 06:32:02,995 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:02,997 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37967. Reason: nanny-close
2024-01-22 06:32:02,998 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45914; closing.
2024-01-22 06:32:02,999 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:02,999 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905122.999254')
2024-01-22 06:32:02,999 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:32:03,000 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:03,660 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:32:03,661 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:32:03,661 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:32:03,662 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:32:03,663 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-22 06:32:05,920 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:05,925 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:32:05,929 - distributed.scheduler - INFO - State start
2024-01-22 06:32:05,951 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:05,952 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:32:05,953 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:32:05,953 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:32:08,243 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45930'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45930>: Stream is closed
2024-01-22 06:32:08,513 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:32:08,514 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:32:08,515 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:32:08,516 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:32:08,516 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-22 06:32:10,735 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:10,739 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45325 instead
  warnings.warn(
2024-01-22 06:32:10,743 - distributed.scheduler - INFO - State start
2024-01-22 06:32:10,764 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:10,765 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-22 06:32:10,766 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45325/status
2024-01-22 06:32:10,766 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:32:10,773 - distributed.scheduler - INFO - Receive client connection: Client-f7d8b643-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:10,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57416
2024-01-22 06:32:10,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39555'
2024-01-22 06:32:12,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:12,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:12,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:12,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39651
2024-01-22 06:32:12,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39651
2024-01-22 06:32:12,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34529
2024-01-22 06:32:12,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-22 06:32:12,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:12,547 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:12,547 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-22 06:32:12,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8l674gue
2024-01-22 06:32:12,547 - distributed.worker - INFO - Starting Worker plugin PreImport-e4456ca0-851f-4036-ab7f-9a855687f4ad
2024-01-22 06:32:12,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a80b3d4c-9778-4c5e-944a-bf6acdbd7de2
2024-01-22 06:32:12,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7be05b9c-d116-4ff6-9a8c-8b0617734c18
2024-01-22 06:32:12,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:12,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39651', status: init, memory: 0, processing: 0>
2024-01-22 06:32:12,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39651
2024-01-22 06:32:12,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57428
2024-01-22 06:32:12,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:12,599 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-22 06:32:12,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:12,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-22 06:32:12,625 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:12,629 - distributed.scheduler - INFO - Remove client Client-f7d8b643-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:12,629 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57416; closing.
2024-01-22 06:32:12,630 - distributed.scheduler - INFO - Remove client Client-f7d8b643-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:12,630 - distributed.scheduler - INFO - Close client connection: Client-f7d8b643-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:12,631 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39555'. Reason: nanny-close
2024-01-22 06:32:12,633 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:12,634 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39651. Reason: nanny-close
2024-01-22 06:32:12,636 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-22 06:32:12,636 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57428; closing.
2024-01-22 06:32:12,636 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905132.6364362')
2024-01-22 06:32:12,636 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:32:12,637 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:13,096 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:32:13,096 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:32:13,097 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:32:13,098 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-22 06:32:13,098 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-22 06:32:15,162 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:15,167 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46701 instead
  warnings.warn(
2024-01-22 06:32:15,172 - distributed.scheduler - INFO - State start
2024-01-22 06:32:15,251 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:15,252 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:32:15,253 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46701/status
2024-01-22 06:32:15,253 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:32:15,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33489'
2024-01-22 06:32:15,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38961'
2024-01-22 06:32:15,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46385'
2024-01-22 06:32:15,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33905'
2024-01-22 06:32:15,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39439'
2024-01-22 06:32:15,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45275'
2024-01-22 06:32:15,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46215'
2024-01-22 06:32:15,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34823'
2024-01-22 06:32:15,776 - distributed.scheduler - INFO - Receive client connection: Client-fa7f8f74-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:15,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60368
2024-01-22 06:32:17,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,351 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33831
2024-01-22 06:32:17,351 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33831
2024-01-22 06:32:17,351 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44915
2024-01-22 06:32:17,351 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,351 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,351 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,351 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0cik7m0a
2024-01-22 06:32:17,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab2c8a13-a225-4e13-8e42-db9c325dca89
2024-01-22 06:32:17,351 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9cc5cde-8e44-4a28-9ed8-9e11e9ccdc08
2024-01-22 06:32:17,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39333
2024-01-22 06:32:17,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39333
2024-01-22 06:32:17,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42281
2024-01-22 06:32:17,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,401 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,401 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q33gjyff
2024-01-22 06:32:17,401 - distributed.worker - INFO - Starting Worker plugin PreImport-d0ddf8c9-d674-42ea-a199-6b8e143fb6d0
2024-01-22 06:32:17,402 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99c4ceb5-6801-4e66-9ceb-710dfcc5534c
2024-01-22 06:32:17,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48c39021-2c87-41c3-a2b3-f1f960263f02
2024-01-22 06:32:17,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,404 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,404 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,407 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,408 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36001
2024-01-22 06:32:17,408 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36001
2024-01-22 06:32:17,408 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41049
2024-01-22 06:32:17,408 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,408 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,408 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,409 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-06zoe3bg
2024-01-22 06:32:17,409 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,409 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b8113a9-5703-4eb6-8366-4ba574dc224c
2024-01-22 06:32:17,410 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38769
2024-01-22 06:32:17,410 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38769
2024-01-22 06:32:17,410 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38861
2024-01-22 06:32:17,410 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,410 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,410 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,410 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,410 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ayozpmk
2024-01-22 06:32:17,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e66fc80e-6e37-44fe-951f-fcbf8807ae14
2024-01-22 06:32:17,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40097
2024-01-22 06:32:17,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40097
2024-01-22 06:32:17,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34103
2024-01-22 06:32:17,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,413 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,413 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p06ogly1
2024-01-22 06:32:17,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa66c41d-3ed5-4d17-8207-f49a6e4c0e47
2024-01-22 06:32:17,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,415 - distributed.worker - INFO - Starting Worker plugin PreImport-d7d96666-59ac-4e1e-8b80-0c20ca18a99f
2024-01-22 06:32:17,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61e1c8c3-7ec3-4a5a-aa51-166124dc71d8
2024-01-22 06:32:17,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39225
2024-01-22 06:32:17,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39225
2024-01-22 06:32:17,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46283
2024-01-22 06:32:17,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,416 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,416 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,416 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-03svr6ik
2024-01-22 06:32:17,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e95bb7a7-ac31-4200-966c-d9ad0afe542d
2024-01-22 06:32:17,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,582 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33263
2024-01-22 06:32:17,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33263
2024-01-22 06:32:17,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46117
2024-01-22 06:32:17,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,583 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,583 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y5j1xfmp
2024-01-22 06:32:17,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-008fc1fa-c79c-4509-a259-07336de8acd3
2024-01-22 06:32:17,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d41709e5-d804-4877-8cfa-ac25c5f83f9e
2024-01-22 06:32:17,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:17,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:17,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:17,622 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39091
2024-01-22 06:32:17,622 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39091
2024-01-22 06:32:17,622 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44597
2024-01-22 06:32:17,622 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:17,622 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:17,622 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:17,622 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-22 06:32:17,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lxk0flxp
2024-01-22 06:32:17,623 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6c23a82-8e49-4874-b9f3-ed90ee7afd05
2024-01-22 06:32:19,622 - distributed.worker - INFO - Starting Worker plugin PreImport-ebe7a6c9-df8c-4c99-97f5-f1301e85f6cd
2024-01-22 06:32:19,622 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33831', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,646 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33831
2024-01-22 06:32:19,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60384
2024-01-22 06:32:19,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,648 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,765 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,787 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39333', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,788 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39333
2024-01-22 06:32:19,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60400
2024-01-22 06:32:19,789 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,790 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,818 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40097', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40097
2024-01-22 06:32:19,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60408
2024-01-22 06:32:19,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,876 - distributed.worker - INFO - Starting Worker plugin PreImport-7851a3fa-b3d2-4fe9-9d80-a38d1ca157ea
2024-01-22 06:32:19,879 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,881 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83c1ef97-f480-4154-a2cd-cd171f27a2bd
2024-01-22 06:32:19,882 - distributed.worker - INFO - Starting Worker plugin PreImport-92471a1d-d678-41f8-9c19-278ed20aa633
2024-01-22 06:32:19,883 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,896 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5df637be-4a89-4697-95ee-46c680a24d98
2024-01-22 06:32:19,897 - distributed.worker - INFO - Starting Worker plugin PreImport-a0685490-7715-4b83-8cdd-1d9df819a057
2024-01-22 06:32:19,899 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c4c353a-4fd0-41a8-a072-9cabbed94f9e
2024-01-22 06:32:19,907 - distributed.worker - INFO - Starting Worker plugin PreImport-b370fdf1-06f9-43f5-abb0-9ad56eefa680
2024-01-22 06:32:19,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-225719dc-450f-4efb-a9bb-fec3a2be8ca2
2024-01-22 06:32:19,919 - distributed.worker - INFO - Starting Worker plugin PreImport-1381184c-7576-4f17-8758-d88178b71f55
2024-01-22 06:32:19,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39225', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39225
2024-01-22 06:32:19,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60430
2024-01-22 06:32:19,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,927 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,927 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33263', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,927 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33263
2024-01-22 06:32:19,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60416
2024-01-22 06:32:19,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,930 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,931 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38769', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,939 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38769
2024-01-22 06:32:19,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60436
2024-01-22 06:32:19,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39091', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,940 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39091
2024-01-22 06:32:19,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60432
2024-01-22 06:32:19,941 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,941 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,943 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,943 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,944 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36001', status: init, memory: 0, processing: 0>
2024-01-22 06:32:19,945 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36001
2024-01-22 06:32:19,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60448
2024-01-22 06:32:19,946 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:19,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:19,947 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:19,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:19,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:20,009 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,009 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,009 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,009 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,015 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-22 06:32:20,026 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,026 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:20,031 - distributed.scheduler - INFO - Remove client Client-fa7f8f74-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:20,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60368; closing.
2024-01-22 06:32:20,032 - distributed.scheduler - INFO - Remove client Client-fa7f8f74-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:20,032 - distributed.scheduler - INFO - Close client connection: Client-fa7f8f74-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:20,033 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33489'. Reason: nanny-close
2024-01-22 06:32:20,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38961'. Reason: nanny-close
2024-01-22 06:32:20,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46385'. Reason: nanny-close
2024-01-22 06:32:20,035 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36001. Reason: nanny-close
2024-01-22 06:32:20,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33905'. Reason: nanny-close
2024-01-22 06:32:20,035 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33831. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39439'. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39225. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45275'. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40097. Reason: nanny-close
2024-01-22 06:32:20,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46215'. Reason: nanny-close
2024-01-22 06:32:20,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39333. Reason: nanny-close
2024-01-22 06:32:20,037 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60448; closing.
2024-01-22 06:32:20,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34823'. Reason: nanny-close
2024-01-22 06:32:20,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36001', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0374887')
2024-01-22 06:32:20,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38769. Reason: nanny-close
2024-01-22 06:32:20,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:20,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33263. Reason: nanny-close
2024-01-22 06:32:20,038 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39091. Reason: nanny-close
2024-01-22 06:32:20,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,038 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60384; closing.
2024-01-22 06:32:20,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,040 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,040 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0403516')
2024-01-22 06:32:20,040 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,040 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,040 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60430; closing.
2024-01-22 06:32:20,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60408; closing.
2024-01-22 06:32:20,041 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60400; closing.
2024-01-22 06:32:20,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:20,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39225', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0418732')
2024-01-22 06:32:20,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0421748')
2024-01-22 06:32:20,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0424516')
2024-01-22 06:32:20,042 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,042 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60436; closing.
2024-01-22 06:32:20,043 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:20,043 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0434394')
2024-01-22 06:32:20,043 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60416; closing.
2024-01-22 06:32:20,044 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60432; closing.
2024-01-22 06:32:20,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33263', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0444827')
2024-01-22 06:32:20,045 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39091', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905140.0449429')
2024-01-22 06:32:20,045 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:32:20,899 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:32:20,899 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:32:20,900 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:32:20,901 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:32:20,901 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-22 06:32:23,069 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:23,073 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:32:23,076 - distributed.scheduler - INFO - State start
2024-01-22 06:32:23,099 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:23,100 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:32:23,100 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:32:23,101 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:32:23,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40851'
2024-01-22 06:32:23,274 - distributed.scheduler - INFO - Receive client connection: Client-ff2cb645-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:23,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60510
2024-01-22 06:32:24,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:24,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:24,970 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:24,971 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34227
2024-01-22 06:32:24,971 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34227
2024-01-22 06:32:24,971 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43903
2024-01-22 06:32:24,971 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:24,971 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:24,971 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:24,971 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-22 06:32:24,971 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-04f4zilp
2024-01-22 06:32:24,971 - distributed.worker - INFO - Starting Worker plugin PreImport-64ad2f73-5a41-46f3-ae20-5d5ecd05901b
2024-01-22 06:32:24,971 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a221f2d-7afc-42f3-a5c7-be4be2f5681e
2024-01-22 06:32:25,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee7f489d-eb94-4466-95f9-d332f4a7f13e
2024-01-22 06:32:25,254 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:25,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34227', status: init, memory: 0, processing: 0>
2024-01-22 06:32:25,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34227
2024-01-22 06:32:25,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60554
2024-01-22 06:32:25,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:25,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:25,348 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:25,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:25,376 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:32:25,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:25,383 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:25,386 - distributed.scheduler - INFO - Remove client Client-ff2cb645-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:25,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60510; closing.
2024-01-22 06:32:25,386 - distributed.scheduler - INFO - Remove client Client-ff2cb645-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:25,386 - distributed.scheduler - INFO - Close client connection: Client-ff2cb645-b8ef-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:25,388 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40851'. Reason: nanny-close
2024-01-22 06:32:25,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:25,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34227. Reason: nanny-close
2024-01-22 06:32:25,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60554; closing.
2024-01-22 06:32:25,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:25,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34227', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905145.4038897')
2024-01-22 06:32:25,404 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:32:25,405 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:25,952 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:32:25,953 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:32:25,953 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:32:25,954 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:32:25,955 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-22 06:32:28,248 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:28,253 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-22 06:32:28,257 - distributed.scheduler - INFO - State start
2024-01-22 06:32:28,280 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-22 06:32:28,281 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-22 06:32:28,282 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-22 06:32:28,282 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-22 06:32:28,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33147'
2024-01-22 06:32:29,562 - distributed.scheduler - INFO - Receive client connection: Client-0233e183-b8f0-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:29,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60668
2024-01-22 06:32:30,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-22 06:32:30,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-22 06:32:30,114 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-22 06:32:30,115 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43905
2024-01-22 06:32:30,115 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43905
2024-01-22 06:32:30,115 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39523
2024-01-22 06:32:30,115 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-22 06:32:30,115 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:30,115 - distributed.worker - INFO -               Threads:                          1
2024-01-22 06:32:30,115 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-22 06:32:30,115 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v2m_h_55
2024-01-22 06:32:30,115 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eca2e191-aea2-4808-8421-545128b71dfe
2024-01-22 06:32:30,115 - distributed.worker - INFO - Starting Worker plugin PreImport-9fccd8e0-e3e5-412c-9c0b-0ef2f1d21d4c
2024-01-22 06:32:30,116 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ccfe5cf8-7b92-4138-b600-def06e10f708
2024-01-22 06:32:30,425 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:30,492 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43905', status: init, memory: 0, processing: 0>
2024-01-22 06:32:30,494 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43905
2024-01-22 06:32:30,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48248
2024-01-22 06:32:30,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-22 06:32:30,496 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-22 06:32:30,496 - distributed.worker - INFO - -------------------------------------------------
2024-01-22 06:32:30,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-22 06:32:30,536 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-22 06:32:30,562 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-22 06:32:30,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:30,568 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-22 06:32:30,570 - distributed.scheduler - INFO - Remove client Client-0233e183-b8f0-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:30,571 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60668; closing.
2024-01-22 06:32:30,571 - distributed.scheduler - INFO - Remove client Client-0233e183-b8f0-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:30,571 - distributed.scheduler - INFO - Close client connection: Client-0233e183-b8f0-11ee-ba25-d8c49764f6bb
2024-01-22 06:32:30,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33147'. Reason: nanny-close
2024-01-22 06:32:30,572 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-22 06:32:30,573 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43905. Reason: nanny-close
2024-01-22 06:32:30,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-22 06:32:30,575 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48248; closing.
2024-01-22 06:32:30,575 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705905150.5756788')
2024-01-22 06:32:30,576 - distributed.scheduler - INFO - Lost all workers
2024-01-22 06:32:30,576 - distributed.nanny - INFO - Worker closed
2024-01-22 06:32:31,137 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-22 06:32:31,138 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-22 06:32:31,138 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-22 06:32:31,139 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-22 06:32:31,140 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43651 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40733 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43667 instead
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 51 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
