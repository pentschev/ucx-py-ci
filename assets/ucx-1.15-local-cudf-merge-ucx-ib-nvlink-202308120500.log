2023-08-12 06:33:04,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,420 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,420 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,502 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,502 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,526 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,526 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:04,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:04,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1691821993.446850] [dgx13:64574:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:64574:0:64574]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  64574) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f4850cd00cd]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f4850ccdc71]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x27e0c) [0x7f4850ccde0c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x739b8) [0x7f4850d789b8]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f4850d4fd4f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f4850d8baad]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x6da) [0x7f4850d9099a]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f4850d916df]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7f4850e3fec2]
 9  /opt/conda/envs/gdf/bin/python(+0x14e546) [0x55c10b6f5546]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x15f) [0x55c10b6f40af]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x55c10b6db6e4]
12  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55c10b6d4f94]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10b6e63f9]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55c10b6d7022]
15  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55c10b6d4f94]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10b6e63f9]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55c10b6d7022]
18  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55c10b789612]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55c10b6dc24d]
20  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55c10b789612]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55c10b6dc24d]
22  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55c10b789612]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55c10b6dc24d]
24  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55c10b789612]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55c10b6dc24d]
26  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55c10b789612]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55c10b6dc24d]
28  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55c10b789612]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f48710b51e9]
30  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55c10b6de67c]
31  /opt/conda/envs/gdf/bin/python(+0xf248b) [0x55c10b69948b]
32  /opt/conda/envs/gdf/bin/python(+0x1366f3) [0x55c10b6dd6f3]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x55c10b6db6e4]
34  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
35  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10b6d64c6]
36  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
37  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10b6d64c6]
38  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10b6d64c6]
40  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10b6d64c6]
42  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55c10b6d4f94]
43  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10b6e63f9]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4abe) [0x55c10b6da8ee]
45  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55c10b6d4f94]
46  /opt/conda/envs/gdf/bin/python(+0x14c88b) [0x55c10b6f388b]
47  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55c10b6f400c]
48  /opt/conda/envs/gdf/bin/python(+0x21073e) [0x55c10b7b773e]
49  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55c10b6de67c]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55c10b6da3d6]
51  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
52  /opt/conda/envs/gdf/bin/python(+0x14c96c) [0x55c10b6f396c]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55c10b6da3d6]
54  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10b6d64c6]
56  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55c10b6d4f94]
57  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10b6e63f9]
58  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10b6d64c6]
59  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55c10b6e66a2]
60  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55c10b6d6212]
61  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55c10b6d4f94]
=================================
2023-08-12 06:33:13,674 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34947
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7fc8c0312100, tag: 0x71b1e4d5cf052f0a, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7fc8c0312100, tag: 0x71b1e4d5cf052f0a, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-12 06:33:14,125 - distributed.nanny - WARNING - Restarting worker
2023-08-12 06:33:15,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:15,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1691821999.755299] [dgx13:64579:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_0: LRU push returned Unsupported operation
[dgx13:64579:0:64579]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  64579) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fa6d39400cd]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7fa6d393dc71]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x27e0c) [0x7fa6d393de0c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x739b8) [0x7fa6d39e89b8]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7fa6d39bfd4f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7fa6d39fbaad]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x6da) [0x7fa6d3a0099a]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7fa6d3a016df]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7fa6d3aafec2]
 9  /opt/conda/envs/gdf/bin/python(+0x14e546) [0x55b6e8490546]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x15f) [0x55b6e848f0af]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x55b6e84766e4]
12  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55b6e846ff94]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b6e84813f9]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55b6e8472022]
15  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55b6e846ff94]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b6e84813f9]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55b6e8472022]
18  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55b6e8524612]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55b6e847724d]
20  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55b6e8524612]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55b6e847724d]
22  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55b6e8524612]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55b6e847724d]
24  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55b6e8524612]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55b6e847724d]
26  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55b6e8524612]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x641d) [0x55b6e847724d]
28  /opt/conda/envs/gdf/bin/python(+0x1e2612) [0x55b6e8524612]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fa6f3d3a1e9]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8aa6) [0x7fa6f3d3aaa6]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55b6e847967c]
32  /opt/conda/envs/gdf/bin/python(+0xf248b) [0x55b6e843448b]
33  /opt/conda/envs/gdf/bin/python(+0x1366f3) [0x55b6e84786f3]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x55b6e84766e4]
35  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b6e84714c6]
37  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b6e84714c6]
39  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b6e84714c6]
41  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b6e84714c6]
43  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55b6e846ff94]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b6e84813f9]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4abe) [0x55b6e84758ee]
46  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55b6e846ff94]
47  /opt/conda/envs/gdf/bin/python(+0x14c88b) [0x55b6e848e88b]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55b6e848f00c]
49  /opt/conda/envs/gdf/bin/python(+0x21073e) [0x55b6e855273e]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55b6e847967c]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55b6e84753d6]
52  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
53  /opt/conda/envs/gdf/bin/python(+0x14c96c) [0x55b6e848e96c]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55b6e84753d6]
55  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b6e84714c6]
57  /opt/conda/envs/gdf/bin/python(+0x12df94) [0x55b6e846ff94]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55b6e84813f9]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55b6e84714c6]
60  /opt/conda/envs/gdf/bin/python(+0x13f6a2) [0x55b6e84816a2]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55b6e8471212]
=================================
2023-08-12 06:33:20,067 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #032] ep: 0x7fb5b0107300, tag: 0xb9203877db5146b6, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #032] ep: 0x7fb5b0107300, tag: 0xb9203877db5146b6, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-12 06:33:20,068 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #041] ep: 0x7fb874e29340, tag: 0x51f3e90edc0579d7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #041] ep: 0x7fb874e29340, tag: 0x51f3e90edc0579d7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-12 06:33:20,068 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #045] ep: 0x7f04305ca180, tag: 0x410429860f8e123a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #045] ep: 0x7f04305ca180, tag: 0x410429860f8e123a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-12 06:33:20,068 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7fc8c0312240, tag: 0xa98dc9bf98349ee3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7fc8c0312240, tag: 0xa98dc9bf98349ee3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-12 06:33:20,068 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f04ed20d200, tag: 0x32850090d34d759a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f04ed20d200, tag: 0x32850090d34d759a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-12 06:33:20,068 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f2e80ae61c0, tag: 0x3030f6ab62aac27e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f2e80ae61c0, tag: 0x3030f6ab62aac27e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-12 06:33:20,074 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51776
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-08-12 06:33:20,465 - distributed.nanny - WARNING - Restarting worker
2023-08-12 06:33:22,015 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 06:33:22,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 06:33:30,216 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,216 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,230 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,231 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,241 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,242 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,274 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 06:33:30,274 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
