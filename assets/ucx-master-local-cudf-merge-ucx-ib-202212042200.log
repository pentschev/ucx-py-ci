2022-12-05 00:59:10,520 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fn098mk8', purging
2022-12-05 00:59:10,521 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3t8unwkl', purging
2022-12-05 00:59:10,521 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-619migdk', purging
2022-12-05 00:59:10,521 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dm93s3t0', purging
2022-12-05 00:59:10,521 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c6tqn64f', purging
2022-12-05 00:59:10,522 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yqy6dgrb', purging
2022-12-05 00:59:10,522 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wtjr2dl8', purging
2022-12-05 00:59:10,522 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7ro2vqpw', purging
2022-12-05 00:59:10,523 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,523 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,563 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,571 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:10,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:10,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
terminate called after throwing an instance of 'rmm::out_of_memory'
  what():  std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2022-12-05 00:59:20,345 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:57849 -> ucx://127.0.0.1:34945
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #025] ep: 0x7f9fc25b2100, tag: 0xab01da3c8557a13, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 1755, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2022-12-05 00:59:20,355 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34945
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f9fc25b2140, tag: 0x6fdc84e0e663ef1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2049, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2854, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils_comm.py", line 400, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils_comm.py", line 385, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2834, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/core.py", line 975, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f9fc25b2140, tag: 0x6fdc84e0e663ef1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2022-12-05 00:59:20,494 - distributed.nanny - WARNING - Restarting worker
2022-12-05 00:59:22,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-12-05 00:59:22,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-12-05 00:59:23,047 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2022-12-05 00:59:23,047 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2049, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2854, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils_comm.py", line 400, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils_comm.py", line 385, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2834, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/core.py", line 975, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2022-12-05 00:59:23,206 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7f9b34
args:      ([                key   payload
52360     838568715  14853471
72870     843304517  79550890
52368     805466532  40226167
157731    815657471  63752814
72883     807961745  90179757
...             ...       ...
99994056    3800759  16699501
99994061  202272888  96397845
99994063  849664787  58539223
99993999  603915352  71254684
99994008  830711875   7017648

[12500893 rows x 2 columns],                 key   payload
20132     614121363  85323902
20137     969379033  88188010
2762      966037359  93232985
20147     918792376  98612377
2766      908631592  83035471
...             ...       ...
99965368  900766701  85923919
99965372  122949703  21071130
99965412  921661581  76077829
99965427  920415307  23684622
99965439  935941571  18812643

[12495890 rows x 2 columns],                  key   payload
20485      136437678  29097533
20490     1068975075  35197522
20491     1009562285  11983805
20492       35316219  90745115
20494      531921021  86433483
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
