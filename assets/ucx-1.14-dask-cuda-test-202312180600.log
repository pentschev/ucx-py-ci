============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-18 06:40:36,145 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:36,150 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43483 instead
  warnings.warn(
2023-12-18 06:40:36,154 - distributed.scheduler - INFO - State start
2023-12-18 06:40:36,615 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:36,616 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-18 06:40:36,617 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43483/status
2023-12-18 06:40:36,617 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:40:36,996 - distributed.scheduler - INFO - Receive client connection: Client-588b8105-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:37,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40498
2023-12-18 06:40:37,110 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45705'
2023-12-18 06:40:37,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33247'
2023-12-18 06:40:37,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44077'
2023-12-18 06:40:37,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42723'
2023-12-18 06:40:39,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:39,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:39,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:39,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:39,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:39,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:39,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:39,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:39,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-18 06:40:39,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38293
2023-12-18 06:40:39,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38293
2023-12-18 06:40:39,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43073
2023-12-18 06:40:39,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:39,046 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:39,046 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:39,046 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:39,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-paazyvac
2023-12-18 06:40:39,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21568e9c-be07-4e7a-954d-2ff7b63652ca
2023-12-18 06:40:39,047 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c93d31c4-3dbd-43b2-bedf-ec275150d832
2023-12-18 06:40:39,048 - distributed.worker - INFO - Starting Worker plugin PreImport-a2c354fc-ac30-4cbc-9ae8-488f6b8c518c
2023-12-18 06:40:39,048 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:39,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:39,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:39,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:39,684 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38293', status: init, memory: 0, processing: 0>
2023-12-18 06:40:39,685 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38293
2023-12-18 06:40:39,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40542
2023-12-18 06:40:39,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:39,687 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:39,687 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:39,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:40,754 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43923
2023-12-18 06:40:40,755 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43923
2023-12-18 06:40:40,755 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41621
2023-12-18 06:40:40,755 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:40,755 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,756 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:40,756 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:40,756 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ccw0zyep
2023-12-18 06:40:40,756 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c2dca68-5f4c-48b9-8516-d82f6f1b9621
2023-12-18 06:40:40,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc4af58a-1c51-4f2b-bd76-552ff33629f7
2023-12-18 06:40:40,756 - distributed.worker - INFO - Starting Worker plugin PreImport-8f8376bb-29ab-41f5-b771-14d9343a6a46
2023-12-18 06:40:40,757 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,784 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43923', status: init, memory: 0, processing: 0>
2023-12-18 06:40:40,785 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43923
2023-12-18 06:40:40,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42270
2023-12-18 06:40:40,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:40,787 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:40,787 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:40,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46611
2023-12-18 06:40:40,812 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46611
2023-12-18 06:40:40,812 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44647
2023-12-18 06:40:40,812 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:40,812 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,812 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:40,813 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:40,813 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qlz35vgi
2023-12-18 06:40:40,813 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d733517-7069-4d84-80d4-11276c975be1
2023-12-18 06:40:40,813 - distributed.worker - INFO - Starting Worker plugin PreImport-efc87d1c-ae68-4447-9844-90cb28b1716b
2023-12-18 06:40:40,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2283332-e6e0-4c1f-86bc-e0862cc9368e
2023-12-18 06:40:40,814 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,833 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36963
2023-12-18 06:40:40,834 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36963
2023-12-18 06:40:40,834 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37665
2023-12-18 06:40:40,834 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:40:40,834 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,834 - distributed.worker - INFO -               Threads:                          4
2023-12-18 06:40:40,834 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-18 06:40:40,834 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-tz1b9ycg
2023-12-18 06:40:40,835 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-362afab3-00f2-4df1-b050-096f43d123fd
2023-12-18 06:40:40,835 - distributed.worker - INFO - Starting Worker plugin PreImport-7a7e2f73-5840-4b04-b1d0-e73414fca178
2023-12-18 06:40:40,836 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26b3a661-e446-495c-9028-e7f192a2d939
2023-12-18 06:40:40,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,850 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46611', status: init, memory: 0, processing: 0>
2023-12-18 06:40:40,850 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46611
2023-12-18 06:40:40,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42276
2023-12-18 06:40:40,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:40,853 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:40,853 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:40,869 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36963', status: init, memory: 0, processing: 0>
2023-12-18 06:40:40,870 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36963
2023-12-18 06:40:40,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42278
2023-12-18 06:40:40,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:40,873 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:40:40,873 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:40,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:40:40,971 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:40,972 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:40,972 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:40,972 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-18 06:40:40,977 - distributed.scheduler - INFO - Remove client Client-588b8105-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:40,978 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40498; closing.
2023-12-18 06:40:40,978 - distributed.scheduler - INFO - Remove client Client-588b8105-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:40,979 - distributed.scheduler - INFO - Close client connection: Client-588b8105-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:40:40,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45705'. Reason: nanny-close
2023-12-18 06:40:40,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:40,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33247'. Reason: nanny-close
2023-12-18 06:40:40,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:40,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43923. Reason: nanny-close
2023-12-18 06:40:40,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36963. Reason: nanny-close
2023-12-18 06:40:40,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:40,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42270; closing.
2023-12-18 06:40:40,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881640.983329')
2023-12-18 06:40:40,984 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:40,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:40,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42278; closing.
2023-12-18 06:40:40,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44077'. Reason: nanny-close
2023-12-18 06:40:40,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881640.9855173')
2023-12-18 06:40:40,985 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:40,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42723'. Reason: nanny-close
2023-12-18 06:40:40,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:40,986 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46611. Reason: nanny-close
2023-12-18 06:40:40,986 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:40,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38293. Reason: nanny-close
2023-12-18 06:40:40,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:40,988 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42276; closing.
2023-12-18 06:40:40,989 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:40:40,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881640.9891772')
2023-12-18 06:40:40,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40542; closing.
2023-12-18 06:40:40,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881640.9903748')
2023-12-18 06:40:40,990 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:40,990 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:40:40,990 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:40,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:40542>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-18 06:40:42,296 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:40:42,296 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:40:42,297 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:40:42,298 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-18 06:40:42,299 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-18 06:40:44,599 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:44,603 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41361 instead
  warnings.warn(
2023-12-18 06:40:44,610 - distributed.scheduler - INFO - State start
2023-12-18 06:40:44,633 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:44,634 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:40:44,635 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:40:44,635 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:40:44,847 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45577'
2023-12-18 06:40:44,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43371'
2023-12-18 06:40:44,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34285'
2023-12-18 06:40:44,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37057'
2023-12-18 06:40:44,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38573'
2023-12-18 06:40:44,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43301'
2023-12-18 06:40:44,913 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46639'
2023-12-18 06:40:44,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33431'
2023-12-18 06:40:47,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,886 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,891 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,900 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,915 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:47,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:40:47,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:40:47,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:40:50,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45577'. Reason: nanny-close
2023-12-18 06:40:50,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43371'. Reason: nanny-close
2023-12-18 06:40:50,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34285'. Reason: nanny-close
2023-12-18 06:40:50,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37057'. Reason: nanny-close
2023-12-18 06:40:50,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38573'. Reason: nanny-close
2023-12-18 06:40:50,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43301'. Reason: nanny-close
2023-12-18 06:40:50,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46639'. Reason: nanny-close
2023-12-18 06:40:50,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33431'. Reason: nanny-close
2023-12-18 06:40:54,446 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44175
2023-12-18 06:40:54,447 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44175
2023-12-18 06:40:54,447 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46529
2023-12-18 06:40:54,447 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,447 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,447 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,447 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,447 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mq73rca2
2023-12-18 06:40:54,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6918bbb-8300-4e10-ad09-6c6dbdb4b3fe
2023-12-18 06:40:54,449 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41049
2023-12-18 06:40:54,450 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41049
2023-12-18 06:40:54,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34619
2023-12-18 06:40:54,451 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,451 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,451 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,451 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,451 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ervw3osx
2023-12-18 06:40:54,452 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db1c9e9d-b5ce-4255-b400-7ae74c75ff43
2023-12-18 06:40:54,473 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43949
2023-12-18 06:40:54,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43949
2023-12-18 06:40:54,474 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33855
2023-12-18 06:40:54,474 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,474 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,474 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,474 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ar3m_b_5
2023-12-18 06:40:54,475 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b00f68fb-786f-44a4-a5b2-198b307fd9b5
2023-12-18 06:40:54,475 - distributed.worker - INFO - Starting Worker plugin PreImport-ade42792-ff69-4ca5-b1de-47e9082f4344
2023-12-18 06:40:54,475 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63641c23-3000-430f-a44f-eb6c13ccda3d
2023-12-18 06:40:54,475 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40131
2023-12-18 06:40:54,476 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40131
2023-12-18 06:40:54,476 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45207
2023-12-18 06:40:54,476 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,476 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,476 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,476 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mka638y0
2023-12-18 06:40:54,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b6a6849-a52d-456d-8132-8a527391e1d4
2023-12-18 06:40:54,482 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46771
2023-12-18 06:40:54,483 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46771
2023-12-18 06:40:54,483 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37771
2023-12-18 06:40:54,483 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,483 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,483 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,483 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,483 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ubdluo1o
2023-12-18 06:40:54,484 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d493d68-9da6-49fa-8f4d-aff8733fb739
2023-12-18 06:40:54,485 - distributed.worker - INFO - Starting Worker plugin PreImport-fb24571d-0bc4-483c-81e8-a8f1075773c4
2023-12-18 06:40:54,485 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c429ff3-bef4-4906-acaf-f1c411c64bbf
2023-12-18 06:40:54,515 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34355
2023-12-18 06:40:54,516 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34355
2023-12-18 06:40:54,516 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33639
2023-12-18 06:40:54,516 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,516 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,516 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,516 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,516 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6euos8lx
2023-12-18 06:40:54,517 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a187556-c705-4241-89c4-9d05a1e615c6
2023-12-18 06:40:54,517 - distributed.worker - INFO - Starting Worker plugin PreImport-96b4bfde-4f30-4252-a25b-66c9b7dda147
2023-12-18 06:40:54,517 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5304567d-4f18-486c-9be1-ed3e3047b9ca
2023-12-18 06:40:54,666 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39343
2023-12-18 06:40:54,667 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39343
2023-12-18 06:40:54,667 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43147
2023-12-18 06:40:54,667 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,667 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40801
2023-12-18 06:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,667 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40801
2023-12-18 06:40:54,667 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39271
2023-12-18 06:40:54,667 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,668 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:40:54,668 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,668 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c9qgxzb8
2023-12-18 06:40:54,668 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:40:54,668 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:40:54,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xzhgvod9
2023-12-18 06:40:54,668 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cc5d0a0-669b-4ee3-b5cd-09f64fd5ef5b
2023-12-18 06:40:54,668 - distributed.worker - INFO - Starting Worker plugin PreImport-717f9395-e154-4b24-9a3d-f46140c8bace
2023-12-18 06:40:54,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20df3ade-2a99-4e95-a7f9-b7394c7f6381
2023-12-18 06:40:54,669 - distributed.worker - INFO - Starting Worker plugin PreImport-e029abac-de29-4021-8d44-783f3c34e8dd
2023-12-18 06:40:54,669 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90d72908-ce78-4ffa-be20-1193a2a86af5
2023-12-18 06:40:54,750 - distributed.worker - INFO - Starting Worker plugin PreImport-30682d0b-0593-4f78-87ca-ac972e43be36
2023-12-18 06:40:54,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4be5f6ec-744e-43f3-9435-3cf2a6e3a34b
2023-12-18 06:40:54,751 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,755 - distributed.worker - INFO - Starting Worker plugin PreImport-b06ae65f-f96a-4557-9593-feb1834b5ab4
2023-12-18 06:40:54,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-591d6323-342a-442e-b65b-eba4f0b181a7
2023-12-18 06:40:54,755 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,766 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,767 - distributed.worker - INFO - Starting Worker plugin PreImport-a67bc686-2991-485b-903f-54d53d5975c0
2023-12-18 06:40:54,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3c8834a-8279-4704-a23c-60b3de621025
2023-12-18 06:40:54,768 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,774 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,791 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,807 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:54,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a649e7e-56f4-4aae-8f33-692d1f543531
2023-12-18 06:40:54,814 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:55,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:55,494 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:55,494 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:55,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:55,501 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:55,503 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41049. Reason: nanny-close
2023-12-18 06:40:55,505 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:55,507 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:55,515 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:55,516 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:55,516 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:55,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:55,552 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:55,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39343. Reason: nanny-close
2023-12-18 06:40:55,556 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:55,558 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:55,719 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:55,720 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:55,721 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:55,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:55,747 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:55,748 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:55,748 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:55,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:55,755 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:55,755 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:55,756 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44175. Reason: nanny-close
2023-12-18 06:40:55,756 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43949. Reason: nanny-close
2023-12-18 06:40:55,758 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:55,759 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:55,759 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:55,760 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:55,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:55,846 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:55,846 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:55,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:55,857 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:55,858 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40131. Reason: nanny-close
2023-12-18 06:40:55,860 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:55,861 - distributed.nanny - INFO - Worker closed
2023-12-18 06:40:56,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:40:56,099 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:40:56,099 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:40:56,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:40:56,109 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:40:56,110 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34355. Reason: nanny-close
2023-12-18 06:40:56,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:40:56,113 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 376, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:43252 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-12-18 06:40:56,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63368 parent=63179 started daemon>
2023-12-18 06:40:56,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63365 parent=63179 started daemon>
2023-12-18 06:40:56,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63363 parent=63179 started daemon>
2023-12-18 06:40:56,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63360 parent=63179 started daemon>
2023-12-18 06:40:56,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63355 parent=63179 started daemon>
2023-12-18 06:40:56,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63347 parent=63179 started daemon>
2023-12-18 06:40:56,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63344 parent=63179 started daemon>
2023-12-18 06:40:56,920 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63368 exit status was already read will report exitcode 255
2023-12-18 06:40:56,965 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63344 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-18 06:40:59,612 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:40:59,617 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34259 instead
  warnings.warn(
2023-12-18 06:40:59,621 - distributed.scheduler - INFO - State start
2023-12-18 06:40:59,623 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xzhgvod9', purging
2023-12-18 06:40:59,623 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ubdluo1o', purging
2023-12-18 06:40:59,999 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:00,000 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:41:00,001 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:00,002 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:41:01,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33729'
2023-12-18 06:41:01,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40937'
2023-12-18 06:41:01,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45599'
2023-12-18 06:41:01,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41791'
2023-12-18 06:41:01,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35279'
2023-12-18 06:41:01,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38993'
2023-12-18 06:41:01,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41299'
2023-12-18 06:41:01,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34675'
2023-12-18 06:41:03,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,272 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:03,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:03,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:03,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33729'. Reason: nanny-close
2023-12-18 06:41:03,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40937'. Reason: nanny-close
2023-12-18 06:41:03,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45599'. Reason: nanny-close
2023-12-18 06:41:03,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41791'. Reason: nanny-close
2023-12-18 06:41:03,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35279'. Reason: nanny-close
2023-12-18 06:41:03,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38993'. Reason: nanny-close
2023-12-18 06:41:03,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41299'. Reason: nanny-close
2023-12-18 06:41:03,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34675'. Reason: nanny-close
2023-12-18 06:41:07,016 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37547
2023-12-18 06:41:07,017 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37547
2023-12-18 06:41:07,017 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34751
2023-12-18 06:41:07,017 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,017 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,017 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,018 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,018 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bv_3_y_r
2023-12-18 06:41:07,018 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b34eded3-e7b8-44ab-85fc-2f4b2ccb9adb
2023-12-18 06:41:07,018 - distributed.worker - INFO - Starting Worker plugin PreImport-c127719f-c9a7-406d-8ee8-5effd99e4adf
2023-12-18 06:41:07,019 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c2c58d6-f65f-4498-af29-9048a8dcaac0
2023-12-18 06:41:07,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33491
2023-12-18 06:41:07,023 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33491
2023-12-18 06:41:07,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44073
2023-12-18 06:41:07,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,024 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,024 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-957o_4sv
2023-12-18 06:41:07,024 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-794f8d43-55e8-4271-997b-15eb4532897f
2023-12-18 06:41:07,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40763
2023-12-18 06:41:07,025 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40763
2023-12-18 06:41:07,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46607
2023-12-18 06:41:07,026 - distributed.worker - INFO - Starting Worker plugin PreImport-f25e862d-003b-44d0-896b-962148bd1c85
2023-12-18 06:41:07,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,026 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,026 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8986b5a8-7d39-45ec-b6cc-501b6be0dc88
2023-12-18 06:41:07,026 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,026 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0qw25gep
2023-12-18 06:41:07,027 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-064a95e0-2366-4c74-985f-53d00747eb29
2023-12-18 06:41:07,028 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d482030-e1e6-42c3-b938-38353a418c7d
2023-12-18 06:41:07,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36909
2023-12-18 06:41:07,029 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36909
2023-12-18 06:41:07,029 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39973
2023-12-18 06:41:07,030 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,030 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,030 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,030 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,030 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e8v20rlu
2023-12-18 06:41:07,031 - distributed.worker - INFO - Starting Worker plugin PreImport-24c7ecda-2cd4-4d41-87fe-a68adb0ea4b8
2023-12-18 06:41:07,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-719b2359-a672-4788-874d-4c9d5d667a6a
2023-12-18 06:41:07,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-420e5948-e30a-4764-8e4f-dd06d303b603
2023-12-18 06:41:07,032 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40033
2023-12-18 06:41:07,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40033
2023-12-18 06:41:07,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43257
2023-12-18 06:41:07,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,033 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46155
2023-12-18 06:41:07,034 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46155
2023-12-18 06:41:07,034 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35529
2023-12-18 06:41:07,034 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,034 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,034 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,034 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,034 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-igzpf8i3
2023-12-18 06:41:07,034 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,034 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,034 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-74apwm1d
2023-12-18 06:41:07,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-189a8ecd-0075-42c8-975c-bde10f811c14
2023-12-18 06:41:07,035 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6306b30-296a-47d7-9c3a-39c63a7854ed
2023-12-18 06:41:07,035 - distributed.worker - INFO - Starting Worker plugin PreImport-12f075fa-cd5f-4172-a6c0-310753013542
2023-12-18 06:41:07,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f5f430c6-eb99-49a9-b5eb-5842c4eb7c91
2023-12-18 06:41:07,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39819
2023-12-18 06:41:07,050 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39819
2023-12-18 06:41:07,050 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45675
2023-12-18 06:41:07,050 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,050 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,050 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,050 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ak03tsv
2023-12-18 06:41:07,051 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0eb3504a-f97e-4e21-b0a9-13112ae03085
2023-12-18 06:41:07,051 - distributed.worker - INFO - Starting Worker plugin PreImport-0b525f14-769f-4c3b-8b3b-d792f27eddd8
2023-12-18 06:41:07,051 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9752c034-2778-428c-9f6b-6050c8a6b6aa
2023-12-18 06:41:07,061 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c10df60b-6d8f-47f1-b8c1-1bafc98e538b
2023-12-18 06:41:07,061 - distributed.worker - INFO - Starting Worker plugin PreImport-c168ead7-0ed9-416c-a1a5-cb67937594b7
2023-12-18 06:41:07,061 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,061 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39435
2023-12-18 06:41:07,075 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39435
2023-12-18 06:41:07,075 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,075 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43457
2023-12-18 06:41:07,075 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:07,075 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,075 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:07,076 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:07,076 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cw3hcv38
2023-12-18 06:41:07,076 - distributed.worker - INFO - Starting Worker plugin PreImport-ed32aa67-235e-4190-a7a1-bd05dfbd36c0
2023-12-18 06:41:07,076 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b338b04-b30a-4d36-954c-cffcda5a72ae
2023-12-18 06:41:07,079 - distributed.worker - INFO - Starting Worker plugin PreImport-5236ea31-13f6-4e8f-8e54-d9dbcfd156ff
2023-12-18 06:41:07,079 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,081 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,085 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,085 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:07,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f875fd10-6ce0-43e0-aaae-4809ba8e6957
2023-12-18 06:41:07,088 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:08,787 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:08,788 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:08,788 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:08,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:08,843 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:08,845 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40763. Reason: nanny-close
2023-12-18 06:41:08,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:08,849 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:09,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:09,295 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:09,295 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:09,296 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:09,297 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:09,297 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:09,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:09,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:09,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:09,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:09,300 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39435. Reason: nanny-close
2023-12-18 06:41:09,300 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:09,300 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:09,300 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:09,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33491. Reason: nanny-close
2023-12-18 06:41:09,302 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:09,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:09,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:09,304 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:09,305 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:09,350 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:09,351 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39819. Reason: nanny-close
2023-12-18 06:41:09,354 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:09,355 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 376, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:44162 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-12-18 06:41:09,987 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63653 parent=63464 started daemon>
2023-12-18 06:41:09,987 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63650 parent=63464 started daemon>
2023-12-18 06:41:09,987 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63647 parent=63464 started daemon>
2023-12-18 06:41:09,988 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63644 parent=63464 started daemon>
2023-12-18 06:41:09,988 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63640 parent=63464 started daemon>
2023-12-18 06:41:09,988 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63636 parent=63464 started daemon>
2023-12-18 06:41:09,988 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63629 parent=63464 started daemon>
2023-12-18 06:41:10,179 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63636 exit status was already read will report exitcode 255
2023-12-18 06:41:10,222 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63629 exit status was already read will report exitcode 255
2023-12-18 06:41:10,507 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63647 exit status was already read will report exitcode 255
2023-12-18 06:41:10,572 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63650 exit status was already read will report exitcode 255
2023-12-18 06:41:10,655 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63644 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-18 06:41:13,013 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:13,018 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42583 instead
  warnings.warn(
2023-12-18 06:41:13,022 - distributed.scheduler - INFO - State start
2023-12-18 06:41:13,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-e8v20rlu', purging
2023-12-18 06:41:13,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-bv_3_y_r', purging
2023-12-18 06:41:13,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-igzpf8i3', purging
2023-12-18 06:41:13,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-74apwm1d', purging
2023-12-18 06:41:13,276 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:13,277 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:41:13,277 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:13,278 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:41:13,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36857'
2023-12-18 06:41:13,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33831'
2023-12-18 06:41:13,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45659'
2023-12-18 06:41:13,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45629'
2023-12-18 06:41:13,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40417'
2023-12-18 06:41:13,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39965'
2023-12-18 06:41:13,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41045'
2023-12-18 06:41:13,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35313'
2023-12-18 06:41:14,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36857'. Reason: nanny-close
2023-12-18 06:41:14,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33831'. Reason: nanny-close
2023-12-18 06:41:14,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45659'. Reason: nanny-close
2023-12-18 06:41:14,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45629'. Reason: nanny-close
2023-12-18 06:41:14,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40417'. Reason: nanny-close
2023-12-18 06:41:14,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39965'. Reason: nanny-close
2023-12-18 06:41:14,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41045'. Reason: nanny-close
2023-12-18 06:41:14,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35313'. Reason: nanny-close
2023-12-18 06:41:15,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,753 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,753 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,781 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,781 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,786 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:15,866 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:15,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:15,873 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:20,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34567
2023-12-18 06:41:20,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34567
2023-12-18 06:41:20,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45557
2023-12-18 06:41:20,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,590 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,590 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9gy2xbkv
2023-12-18 06:41:20,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5cad27aa-f581-450d-a6f9-531544822388
2023-12-18 06:41:20,591 - distributed.worker - INFO - Starting Worker plugin PreImport-0425195c-02f0-4bbd-9d88-6e94c1f4ab2d
2023-12-18 06:41:20,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46f91ad2-affc-485c-bd89-0a865acec502
2023-12-18 06:41:20,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45625
2023-12-18 06:41:20,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45625
2023-12-18 06:41:20,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35901
2023-12-18 06:41:20,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,627 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,627 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,627 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,627 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ab0eh1_
2023-12-18 06:41:20,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5984c1fe-51fd-449d-a7a6-5dd7f8544f34
2023-12-18 06:41:20,629 - distributed.worker - INFO - Starting Worker plugin PreImport-a1fc9d7c-d629-46f2-86dd-af4cbdaa4cb8
2023-12-18 06:41:20,629 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c433825a-e5bc-41b0-942c-a52b84ba5b87
2023-12-18 06:41:20,920 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40219
2023-12-18 06:41:20,920 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40219
2023-12-18 06:41:20,921 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39237
2023-12-18 06:41:20,921 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,921 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,921 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,921 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eevqh1e_
2023-12-18 06:41:20,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24de8a29-6dce-48a7-aa0f-d9ae3a5fa6b3
2023-12-18 06:41:20,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46283
2023-12-18 06:41:20,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46283
2023-12-18 06:41:20,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34689
2023-12-18 06:41:20,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,961 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,961 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41193
2023-12-18 06:41:20,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41193
2023-12-18 06:41:20,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pa_e05sd
2023-12-18 06:41:20,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43929
2023-12-18 06:41:20,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,961 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,962 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,962 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,962 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3e23754-e2fb-46a4-bf67-a06b589d10fa
2023-12-18 06:41:20,962 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o9zgam7f
2023-12-18 06:41:20,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d18a6dde-0468-45b8-80b0-d96200caf35e
2023-12-18 06:41:20,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-134475f8-d8f6-4d3e-a884-80d8c54ecf9e
2023-12-18 06:41:20,963 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37611
2023-12-18 06:41:20,964 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37611
2023-12-18 06:41:20,964 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41863
2023-12-18 06:41:20,964 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,964 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,964 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,964 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,964 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o6pqbvc4
2023-12-18 06:41:20,965 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88da36f3-dc3b-467c-846e-c36dc6eaf7cc
2023-12-18 06:41:20,972 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45697
2023-12-18 06:41:20,973 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45697
2023-12-18 06:41:20,973 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38569
2023-12-18 06:41:20,973 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,973 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,973 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,973 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,973 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_3o46s0s
2023-12-18 06:41:20,974 - distributed.worker - INFO - Starting Worker plugin PreImport-5e246632-1074-4b2b-99c7-b914b30f26f1
2023-12-18 06:41:20,974 - distributed.worker - INFO - Starting Worker plugin RMMSetup-791e37c3-f01b-491e-83e6-9a6ded77215c
2023-12-18 06:41:20,984 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34247
2023-12-18 06:41:20,986 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34247
2023-12-18 06:41:20,986 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39031
2023-12-18 06:41:20,986 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:20,986 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:20,986 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:20,986 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:20,986 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-24kocvi0
2023-12-18 06:41:20,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c11b645-3130-4772-886c-dab56c28bce4
2023-12-18 06:41:20,988 - distributed.worker - INFO - Starting Worker plugin PreImport-4102330b-edba-4469-988e-07d726bc52c2
2023-12-18 06:41:20,988 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e0a42c8-9e8f-40df-8a6f-508b5556ebcb
2023-12-18 06:41:20,994 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,030 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,047 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,047 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,063 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,081 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,082 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34567. Reason: nanny-close
2023-12-18 06:41:21,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,087 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,104 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,104 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,120 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,121 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45625. Reason: nanny-close
2023-12-18 06:41:21,123 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,125 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,174 - distributed.worker - INFO - Starting Worker plugin PreImport-f88ce4e4-fff5-479e-868c-97a3e53fe834
2023-12-18 06:41:21,174 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ef6987a-a35d-4ed0-83de-968a459432e2
2023-12-18 06:41:21,174 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,184 - distributed.worker - INFO - Starting Worker plugin PreImport-7f5bf3fa-d5d5-41f2-b330-86b3d6aeb05a
2023-12-18 06:41:21,184 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,187 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-145b883f-8219-4eb3-9d5f-5543cc0b0eab
2023-12-18 06:41:21,187 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,188 - distributed.worker - INFO - Starting Worker plugin PreImport-ce1caf29-dbd4-43c0-90c1-58cfa069c8c2
2023-12-18 06:41:21,188 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,191 - distributed.worker - INFO - Starting Worker plugin PreImport-16ec990c-f165-4442-85a0-aa4b000b7665
2023-12-18 06:41:21,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0356f3be-f8b7-4319-9d49-fee2ee96b9e7
2023-12-18 06:41:21,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08aa416d-6dbf-4d97-91b5-f0ff26ff261d
2023-12-18 06:41:21,191 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,191 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,210 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,215 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,215 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,217 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,220 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,222 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,222 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:21,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46283. Reason: nanny-close
2023-12-18 06:41:21,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,224 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:21,224 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45697. Reason: nanny-close
2023-12-18 06:41:21,224 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:21,224 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41193. Reason: nanny-close
2023-12-18 06:41:21,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,227 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,227 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,227 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:21,273 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,273 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,274 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:21,274 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40219. Reason: nanny-close
2023-12-18 06:41:21,274 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34247. Reason: nanny-close
2023-12-18 06:41:21,275 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37611. Reason: nanny-close
2023-12-18 06:41:21,276 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,276 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,278 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,278 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:21,278 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:21,280 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 376, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:45220 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-12-18 06:41:22,291 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63926 parent=63737 started daemon>
2023-12-18 06:41:22,291 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63923 parent=63737 started daemon>
2023-12-18 06:41:22,291 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63920 parent=63737 started daemon>
2023-12-18 06:41:22,292 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63917 parent=63737 started daemon>
2023-12-18 06:41:22,292 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63913 parent=63737 started daemon>
2023-12-18 06:41:22,292 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63909 parent=63737 started daemon>
2023-12-18 06:41:22,292 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63902 parent=63737 started daemon>
2023-12-18 06:41:22,534 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63909 exit status was already read will report exitcode 255
2023-12-18 06:41:22,575 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63902 exit status was already read will report exitcode 255
2023-12-18 06:41:23,457 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63913 exit status was already read will report exitcode 255
2023-12-18 06:41:23,668 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63926 exit status was already read will report exitcode 255
2023-12-18 06:41:24,124 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63917 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 183, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    import dask.dataframe._pyarrow_compat
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py", line 5, in <module>
    import pandas as pd
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/compat/__init__.py", line 22, in <module>
    from pandas.compat.pyarrow import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
  File "<frozen importlib._bootstrap>", line 398, in parent
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 183, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    import dask.dataframe._pyarrow_compat
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py", line 5, in <module>
    import pandas as pd
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/__init__.py", line 48, in <module>
    from pandas.core.api import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/api.py", line 47, in <module>
    from pandas.core.groupby import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/groupby/__init__.py", line 1, in <module>
    from pandas.core.groupby.generic import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/groupby/generic.py", line 76, in <module>
    from pandas.core.frame import DataFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/frame.py", line 172, in <module>
    from pandas.core.generic import NDFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/generic.py", line 131, in <module>
    from pandas.core import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/indexing.py", line 68, in <module>
    from pandas.core.indexes.api import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/indexes/api.py", line 25, in <module>
    from pandas.core.indexes.category import CategoricalIndex
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/indexes/category.py", line 84, in <module>
    class CategoricalIndex(NDArrayBackedExtensionIndex):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/indexes/category.py", line 374, in CategoricalIndex
    def __contains__(self, key: Any) -> bool:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/util/_decorators.py", line 411, in decorator
    [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/util/_decorators.py", line 414, in <listcomp>
    else dedent(component.__doc__ or "")
  File "/opt/conda/envs/gdf/lib/python3.9/textwrap.py", line 431, in dedent
    indents = _leading_whitespace_re.findall(text)
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-18 06:41:27,909 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:27,914 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40359 instead
  warnings.warn(
2023-12-18 06:41:27,918 - distributed.scheduler - INFO - State start
2023-12-18 06:41:28,168 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:28,170 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:41:28,171 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40359/status
2023-12-18 06:41:28,171 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:41:28,198 - distributed.scheduler - INFO - Receive client connection: Client-77648204-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:28,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40884
2023-12-18 06:41:28,239 - distributed.scheduler - INFO - Receive client connection: Client-78a81adb-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:28,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40918
2023-12-18 06:41:28,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43241'
2023-12-18 06:41:28,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33501'
2023-12-18 06:41:28,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43613'
2023-12-18 06:41:28,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46029'
2023-12-18 06:41:28,875 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32943'
2023-12-18 06:41:28,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38735'
2023-12-18 06:41:28,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41781'
2023-12-18 06:41:28,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36741'
2023-12-18 06:41:30,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:30,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:30,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:30,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:34,615 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32895
2023-12-18 06:41:34,616 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32895
2023-12-18 06:41:34,616 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44833
2023-12-18 06:41:34,616 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,616 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,616 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:34,616 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:34,616 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_bxqs3yp
2023-12-18 06:41:34,617 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ad0cd9b-e736-4267-bbb1-42d283861ade
2023-12-18 06:41:34,617 - distributed.worker - INFO - Starting Worker plugin PreImport-f7a085b7-03b2-41a5-a206-57d90ad64a6e
2023-12-18 06:41:34,617 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3071bf9d-b31c-4d1b-b1c7-930cb2b3371e
2023-12-18 06:41:34,623 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44623
2023-12-18 06:41:34,623 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44623
2023-12-18 06:41:34,623 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38165
2023-12-18 06:41:34,624 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,624 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,624 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:34,624 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:34,624 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fbxsf9pb
2023-12-18 06:41:34,624 - distributed.worker - INFO - Starting Worker plugin PreImport-b3f63596-3311-453a-b85f-32d39ca1b87b
2023-12-18 06:41:34,625 - distributed.worker - INFO - Starting Worker plugin RMMSetup-377e5760-5c3b-4c0a-adcb-813587290ed0
2023-12-18 06:41:34,630 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38305
2023-12-18 06:41:34,630 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38305
2023-12-18 06:41:34,630 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37093
2023-12-18 06:41:34,631 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,631 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,631 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:34,631 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:34,631 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9pxzorwq
2023-12-18 06:41:34,631 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b709e72f-0462-4ea4-b253-6c36475c10a0
2023-12-18 06:41:34,810 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eca8090e-7e12-4275-94ec-227490d545a5
2023-12-18 06:41:34,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,845 - distributed.worker - INFO - Starting Worker plugin PreImport-ec70d0c4-aaaa-43b6-ae86-43e841a85fb7
2023-12-18 06:41:34,846 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b3e72f5-e5b9-421a-9429-02cb60d87769
2023-12-18 06:41:34,846 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,849 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32895', status: init, memory: 0, processing: 0>
2023-12-18 06:41:34,850 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32895
2023-12-18 06:41:34,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52242
2023-12-18 06:41:34,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:34,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,858 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:34,870 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44623', status: init, memory: 0, processing: 0>
2023-12-18 06:41:34,870 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44623
2023-12-18 06:41:34,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52256
2023-12-18 06:41:34,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:34,872 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,873 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,873 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38305', status: init, memory: 0, processing: 0>
2023-12-18 06:41:34,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38305
2023-12-18 06:41:34,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52260
2023-12-18 06:41:34,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:34,878 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:34,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:34,983 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41191
2023-12-18 06:41:34,985 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41191
2023-12-18 06:41:34,985 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40685
2023-12-18 06:41:34,985 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:34,985 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:34,985 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:34,985 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:34,985 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cd3pztxc
2023-12-18 06:41:34,986 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-113b655f-7fc2-480a-9f4c-b822b1ded6fc
2023-12-18 06:41:34,987 - distributed.worker - INFO - Starting Worker plugin PreImport-ebf19032-c046-4dd8-b55a-ba6292631e70
2023-12-18 06:41:34,987 - distributed.worker - INFO - Starting Worker plugin RMMSetup-321afe2c-3d24-4422-b49f-c4bc17309426
2023-12-18 06:41:35,006 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37877
2023-12-18 06:41:35,007 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37877
2023-12-18 06:41:35,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41597
2023-12-18 06:41:35,007 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36819
2023-12-18 06:41:35,007 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41597
2023-12-18 06:41:35,007 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,007 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,007 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40031
2023-12-18 06:41:35,007 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:35,007 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,007 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:35,007 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lusf8b1m
2023-12-18 06:41:35,007 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:35,007 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:35,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bubhpf3a
2023-12-18 06:41:35,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4f1fbbb-d118-48d9-b9dc-d649eb149818
2023-12-18 06:41:35,008 - distributed.worker - INFO - Starting Worker plugin PreImport-62e3d275-448a-4205-850b-afd27d3c225e
2023-12-18 06:41:35,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-475f6372-7af8-40c6-b15e-08a71b2a780b
2023-12-18 06:41:35,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26735369-8ce6-4e6f-95c4-bdade996a948
2023-12-18 06:41:35,054 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45419
2023-12-18 06:41:35,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45419
2023-12-18 06:41:35,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38027
2023-12-18 06:41:35,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,055 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,055 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:35,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:35,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zjo4w328
2023-12-18 06:41:35,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5da1f2c9-671a-4467-8199-442da5e05b86
2023-12-18 06:41:35,056 - distributed.worker - INFO - Starting Worker plugin PreImport-bfcab70b-3ba6-489e-ae3a-7b73936def1d
2023-12-18 06:41:35,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d855ac31-61b3-4b42-8c36-7adb1fe0f82a
2023-12-18 06:41:35,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34579
2023-12-18 06:41:35,066 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34579
2023-12-18 06:41:35,066 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41537
2023-12-18 06:41:35,066 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,066 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,066 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:35,066 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:41:35,066 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-32gbv0p8
2023-12-18 06:41:35,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6582c70-612d-45f8-8065-d150d667e6c0
2023-12-18 06:41:35,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54511223-e18d-4ddc-84d1-77acf1626fac
2023-12-18 06:41:35,282 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,289 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,295 - distributed.worker - INFO - Starting Worker plugin PreImport-444440f6-4761-4e68-aa27-e80f2c7f5c33
2023-12-18 06:41:35,295 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-97a323e3-8b93-4549-b96e-c7e1f95b85a5
2023-12-18 06:41:35,295 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,307 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,312 - distributed.worker - INFO - Starting Worker plugin PreImport-ea3e586d-40b2-4072-a2eb-90a5f3b38c74
2023-12-18 06:41:35,312 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,313 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41191', status: init, memory: 0, processing: 0>
2023-12-18 06:41:35,314 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41191
2023-12-18 06:41:35,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52266
2023-12-18 06:41:35,315 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37877', status: init, memory: 0, processing: 0>
2023-12-18 06:41:35,315 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:35,316 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37877
2023-12-18 06:41:35,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52280
2023-12-18 06:41:35,316 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,316 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:35,320 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,320 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:35,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:35,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41597', status: init, memory: 0, processing: 0>
2023-12-18 06:41:35,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41597
2023-12-18 06:41:35,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52294
2023-12-18 06:41:35,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:35,326 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,326 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:35,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45419', status: init, memory: 0, processing: 0>
2023-12-18 06:41:35,470 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45419
2023-12-18 06:41:35,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52310
2023-12-18 06:41:35,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:35,472 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34579', status: init, memory: 0, processing: 0>
2023-12-18 06:41:35,472 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,472 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,472 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34579
2023-12-18 06:41:35,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52322
2023-12-18 06:41:35,474 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:35,475 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:35,475 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:35,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:35,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:35,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:41:35,573 - distributed.scheduler - INFO - Remove client Client-77648204-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:35,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40884; closing.
2023-12-18 06:41:35,574 - distributed.scheduler - INFO - Remove client Client-77648204-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:35,574 - distributed.scheduler - INFO - Close client connection: Client-77648204-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:35,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43241'. Reason: nanny-close
2023-12-18 06:41:35,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33501'. Reason: nanny-close
2023-12-18 06:41:35,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44623. Reason: nanny-close
2023-12-18 06:41:35,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43613'. Reason: nanny-close
2023-12-18 06:41:35,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32895. Reason: nanny-close
2023-12-18 06:41:35,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46029'. Reason: nanny-close
2023-12-18 06:41:35,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41597. Reason: nanny-close
2023-12-18 06:41:35,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32943'. Reason: nanny-close
2023-12-18 06:41:35,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38305. Reason: nanny-close
2023-12-18 06:41:35,579 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38735'. Reason: nanny-close
2023-12-18 06:41:35,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,579 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52256; closing.
2023-12-18 06:41:35,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41191. Reason: nanny-close
2023-12-18 06:41:35,579 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41781'. Reason: nanny-close
2023-12-18 06:41:35,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44623', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.5797687')
2023-12-18 06:41:35,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34579. Reason: nanny-close
2023-12-18 06:41:35,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36741'. Reason: nanny-close
2023-12-18 06:41:35,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:35,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45419. Reason: nanny-close
2023-12-18 06:41:35,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37877. Reason: nanny-close
2023-12-18 06:41:35,581 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52260; closing.
2023-12-18 06:41:35,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,581 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,581 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52294; closing.
2023-12-18 06:41:35,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52242; closing.
2023-12-18 06:41:35,582 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,582 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,582 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.5826213')
2023-12-18 06:41:35,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41597', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.583012')
2023-12-18 06:41:35,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32895', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.5833771')
2023-12-18 06:41:35,583 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:35,584 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52266; closing.
2023-12-18 06:41:35,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.5848963')
2023-12-18 06:41:35,585 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52310; closing.
2023-12-18 06:41:35,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52322; closing.
2023-12-18 06:41:35,585 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:35,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52280; closing.
2023-12-18 06:41:35,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45419', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.5859828')
2023-12-18 06:41:35,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34579', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.586352')
2023-12-18 06:41:35,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881695.5867138')
2023-12-18 06:41:35,586 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:37,594 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:41:37,594 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:41:37,595 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:37,597 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:41:37,597 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-18 06:41:39,890 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:39,894 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43661 instead
  warnings.warn(
2023-12-18 06:41:39,898 - distributed.scheduler - INFO - State start
2023-12-18 06:41:39,921 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:39,922 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:41:39,923 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43661/status
2023-12-18 06:41:39,923 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:41:39,974 - distributed.scheduler - INFO - Receive client connection: Client-7ea516bc-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:39,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55214
2023-12-18 06:41:40,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46561'
2023-12-18 06:41:41,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:41,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 9370 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34319 instead
  warnings.warn(
2023-12-18 06:41:42,503 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:43,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39125
2023-12-18 06:41:43,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39125
2023-12-18 06:41:43,538 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34319
2023-12-18 06:41:43,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:43,539 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:43,539 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:43,539 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:41:43,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fyt4hcum
2023-12-18 06:41:43,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7807f64-3200-4316-a1a6-dd6e3bac76ad
2023-12-18 06:41:43,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77c85320-aa0e-491f-a898-58489df709c6
2023-12-18 06:41:43,539 - distributed.worker - INFO - Starting Worker plugin PreImport-2a0ae787-3ce4-416d-be46-9098e226263c
2023-12-18 06:41:43,540 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:43,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39125', status: init, memory: 0, processing: 0>
2023-12-18 06:41:43,565 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39125
2023-12-18 06:41:43,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55256
2023-12-18 06:41:43,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:43,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:43,567 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:43,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:43,570 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:43,573 - distributed.scheduler - INFO - Remove client Client-7ea516bc-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:43,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55214; closing.
2023-12-18 06:41:43,573 - distributed.scheduler - INFO - Remove client Client-7ea516bc-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:43,574 - distributed.scheduler - INFO - Close client connection: Client-7ea516bc-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:43,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46561'. Reason: nanny-close
2023-12-18 06:41:43,593 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:43,594 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39125. Reason: nanny-close
2023-12-18 06:41:43,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:43,596 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55256; closing.
2023-12-18 06:41:43,596 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881703.5967662')
2023-12-18 06:41:43,597 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:43,597 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:44,151 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42521', status: init, memory: 0, processing: 0>
2023-12-18 06:41:44,152 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42521
2023-12-18 06:41:44,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55270
2023-12-18 06:41:44,192 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55270; closing.
2023-12-18 06:41:44,193 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42521', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881704.1933277')
2023-12-18 06:41:44,193 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:44,741 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:41:44,742 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:41:44,742 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:44,743 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:41:44,743 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-18 06:41:49,241 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:49,245 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37931 instead
  warnings.warn(
2023-12-18 06:41:49,249 - distributed.scheduler - INFO - State start
2023-12-18 06:41:49,273 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:49,275 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:41:49,275 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37931/status
2023-12-18 06:41:49,276 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:41:49,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42983'
2023-12-18 06:41:49,612 - distributed.scheduler - INFO - Receive client connection: Client-844b5dca-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:49,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55472
2023-12-18 06:41:50,043 - distributed.scheduler - INFO - Receive client connection: Client-84266e0a-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:50,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33176
2023-12-18 06:41:51,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:41:51,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:41:51,692 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:41:52,737 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41859
2023-12-18 06:41:52,737 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41859
2023-12-18 06:41:52,737 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35263
2023-12-18 06:41:52,737 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:41:52,737 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:52,737 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:41:52,738 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:41:52,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-93fgqe7i
2023-12-18 06:41:52,738 - distributed.worker - INFO - Starting Worker plugin PreImport-77f68d31-46b7-496d-8b33-e157b5255842
2023-12-18 06:41:52,739 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31bf093e-dde5-4b66-9c4a-8a1a9505f310
2023-12-18 06:41:52,739 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-810d6ef9-93f3-422a-b458-c77b1bb8f3e3
2023-12-18 06:41:52,740 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:52,764 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41859', status: init, memory: 0, processing: 0>
2023-12-18 06:41:52,766 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41859
2023-12-18 06:41:52,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33204
2023-12-18 06:41:52,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:41:52,767 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:41:52,768 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:41:52,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:41:52,808 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:52,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:41:52,811 - distributed.scheduler - INFO - Remove client Client-844b5dca-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:52,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55472; closing.
2023-12-18 06:41:52,812 - distributed.scheduler - INFO - Remove client Client-844b5dca-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:52,812 - distributed.scheduler - INFO - Remove client Client-84266e0a-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:52,812 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33176; closing.
2023-12-18 06:41:52,813 - distributed.scheduler - INFO - Close client connection: Client-844b5dca-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:41:52,813 - distributed.scheduler - INFO - Remove client Client-84266e0a-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:52,813 - distributed.scheduler - INFO - Close client connection: Client-84266e0a-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:41:52,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42983'. Reason: nanny-close
2023-12-18 06:41:52,815 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:41:52,816 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41859. Reason: nanny-close
2023-12-18 06:41:52,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:41:52,818 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33204; closing.
2023-12-18 06:41:52,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881712.8182104')
2023-12-18 06:41:52,818 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:52,819 - distributed.nanny - INFO - Worker closed
2023-12-18 06:41:53,113 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35681', status: init, memory: 0, processing: 0>
2023-12-18 06:41:53,114 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35681
2023-12-18 06:41:53,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33220
2023-12-18 06:41:53,123 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33220; closing.
2023-12-18 06:41:53,123 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35681', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881713.1237864')
2023-12-18 06:41:53,124 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:41:53,981 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:41:53,981 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:41:53,982 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:53,983 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:41:53,983 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-18 06:41:56,492 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:56,496 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39973 instead
  warnings.warn(
2023-12-18 06:41:56,501 - distributed.scheduler - INFO - State start
2023-12-18 06:41:56,552 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:41:56,553 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:41:56,554 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:41:56,555 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-18 06:42:03,404 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:03,409 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43649 instead
  warnings.warn(
2023-12-18 06:42:03,414 - distributed.scheduler - INFO - State start
2023-12-18 06:42:03,437 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:03,438 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:42:03,439 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:03,440 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:42:03,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44731'
2023-12-18 06:42:05,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:05,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:05,342 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:06,123 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39617
2023-12-18 06:42:06,124 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39617
2023-12-18 06:42:06,124 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46811
2023-12-18 06:42:06,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-18 06:42:06,124 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:06,124 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:06,124 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:42:06,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qmtvglgl
2023-12-18 06:42:06,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-388c8f44-a432-4a39-b934-e52572d73462
2023-12-18 06:42:06,125 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ef6d909-55d8-4015-b80d-3ecaf32128d4
2023-12-18 06:42:06,125 - distributed.worker - INFO - Starting Worker plugin PreImport-5d5a7d9b-4f0b-47f6-8419-0043e5be72f9
2023-12-18 06:42:06,125 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:06,151 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:06,152 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-18 06:42:06,152 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:06,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-18 06:42:06,237 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:06,242 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44731'. Reason: nanny-close
2023-12-18 06:42:06,242 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:06,243 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39617. Reason: nanny-close
2023-12-18 06:42:06,245 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-18 06:42:06,246 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-18 06:42:09,286 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:09,291 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35903 instead
  warnings.warn(
2023-12-18 06:42:09,295 - distributed.scheduler - INFO - State start
2023-12-18 06:42:09,375 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:09,376 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-18 06:42:09,377 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35903/status
2023-12-18 06:42:09,378 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-18 06:42:09,462 - distributed.scheduler - INFO - Receive client connection: Client-903fc5c6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:09,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59004
2023-12-18 06:42:09,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40881'
2023-12-18 06:42:09,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39953'
2023-12-18 06:42:09,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39069'
2023-12-18 06:42:09,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32813'
2023-12-18 06:42:09,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41033'
2023-12-18 06:42:09,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38511'
2023-12-18 06:42:09,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35743'
2023-12-18 06:42:09,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42039'
2023-12-18 06:42:10,304 - distributed.scheduler - INFO - Receive client connection: Client-90177579-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:10,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50648
2023-12-18 06:42:11,713 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,718 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,797 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,798 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,798 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:11,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:11,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:11,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:17,488 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36815
2023-12-18 06:42:17,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36815
2023-12-18 06:42:17,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34005
2023-12-18 06:42:17,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,489 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,489 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,489 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-343vr4k1
2023-12-18 06:42:17,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0f59781e-8413-4cd8-b3c7-9612a229aa0f
2023-12-18 06:42:17,497 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33507
2023-12-18 06:42:17,498 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33507
2023-12-18 06:42:17,498 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34777
2023-12-18 06:42:17,498 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,499 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,499 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,499 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_240_du0
2023-12-18 06:42:17,499 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47b01544-2f31-4725-b43e-11705b35ab19
2023-12-18 06:42:17,500 - distributed.worker - INFO - Starting Worker plugin PreImport-a702d509-9409-4b7e-99fc-f7b8245fb82a
2023-12-18 06:42:17,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7c98a33-c19c-4b84-a406-07168e9428b5
2023-12-18 06:42:17,502 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37601
2023-12-18 06:42:17,503 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37601
2023-12-18 06:42:17,503 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38119
2023-12-18 06:42:17,503 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,503 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,503 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,503 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,503 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-usx24lwn
2023-12-18 06:42:17,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca804d32-e828-49de-be56-9751720202f9
2023-12-18 06:42:17,507 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a55735f2-ebd0-47c3-9071-f88bb6983745
2023-12-18 06:42:17,507 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36003
2023-12-18 06:42:17,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36003
2023-12-18 06:42:17,508 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34755
2023-12-18 06:42:17,508 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,508 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,508 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bl1m5yla
2023-12-18 06:42:17,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54ad6f46-193f-4b40-81ae-b134ec21e568
2023-12-18 06:42:17,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32837
2023-12-18 06:42:17,689 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32837
2023-12-18 06:42:17,689 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38641
2023-12-18 06:42:17,689 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,689 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,689 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,689 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yyvk_lcj
2023-12-18 06:42:17,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c3205b7-fc64-4d59-ae46-6d856c5b5bb4
2023-12-18 06:42:17,690 - distributed.worker - INFO - Starting Worker plugin PreImport-5898f5ae-0d97-4890-b6ba-ae3baee4ded3
2023-12-18 06:42:17,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3f933d0-d450-4f82-91e7-18935fcee3f9
2023-12-18 06:42:17,689 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44245
2023-12-18 06:42:17,690 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44245
2023-12-18 06:42:17,690 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33433
2023-12-18 06:42:17,690 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38057
2023-12-18 06:42:17,690 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38057
2023-12-18 06:42:17,690 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,690 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,690 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37461
2023-12-18 06:42:17,690 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,691 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,690 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,691 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,691 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,691 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4bqz6u7t
2023-12-18 06:42:17,691 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,691 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uaesg6ru
2023-12-18 06:42:17,691 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42c81295-89fd-4691-aa6a-f231a01ff966
2023-12-18 06:42:17,691 - distributed.worker - INFO - Starting Worker plugin PreImport-b0bae0ca-e770-44b8-a4bd-28fa30530525
2023-12-18 06:42:17,691 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4d0b321-d33e-48c9-a646-3583e840a357
2023-12-18 06:42:17,695 - distributed.worker - INFO - Starting Worker plugin PreImport-d15d6aae-87e6-4180-8b98-a4829ff616e8
2023-12-18 06:42:17,696 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce44d0af-76c8-401b-9acb-b8926ec81de4
2023-12-18 06:42:17,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38993
2023-12-18 06:42:17,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38993
2023-12-18 06:42:17,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33175
2023-12-18 06:42:17,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,811 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,811 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:17,812 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-18 06:42:17,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eayyudm7
2023-12-18 06:42:17,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd9ab6a5-6a86-4899-9e60-06456fb52f65
2023-12-18 06:42:17,812 - distributed.worker - INFO - Starting Worker plugin PreImport-7b6e53f4-6047-40f3-9388-2e895438792f
2023-12-18 06:42:17,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e2144bb-ce8e-4adc-ace2-8cc6524a0918
2023-12-18 06:42:17,906 - distributed.worker - INFO - Starting Worker plugin PreImport-de75df73-e9e7-4fc7-8b9f-de8536ccd0c2
2023-12-18 06:42:17,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f63bffa-f781-49e4-88ee-3be743d6394a
2023-12-18 06:42:17,907 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,916 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,918 - distributed.worker - INFO - Starting Worker plugin PreImport-8fddeec5-647c-4bac-a0f5-f8c520a80460
2023-12-18 06:42:17,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e94d2d0-0ef5-42eb-a3de-3c5dd05754c7
2023-12-18 06:42:17,919 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,927 - distributed.worker - INFO - Starting Worker plugin PreImport-fce78485-b793-49b8-829b-15ea52200ecd
2023-12-18 06:42:17,929 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,932 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36815', status: init, memory: 0, processing: 0>
2023-12-18 06:42:17,935 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36815
2023-12-18 06:42:17,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50676
2023-12-18 06:42:17,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:17,940 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,940 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:17,943 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36003', status: init, memory: 0, processing: 0>
2023-12-18 06:42:17,943 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36003
2023-12-18 06:42:17,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50696
2023-12-18 06:42:17,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:17,946 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33507', status: init, memory: 0, processing: 0>
2023-12-18 06:42:17,946 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33507
2023-12-18 06:42:17,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50692
2023-12-18 06:42:17,948 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,948 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,948 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:17,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:17,955 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,956 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:17,964 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93be4d82-bb16-4e7c-99e4-c3c86d759240
2023-12-18 06:42:17,965 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,967 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37601', status: init, memory: 0, processing: 0>
2023-12-18 06:42:17,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37601
2023-12-18 06:42:17,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50702
2023-12-18 06:42:17,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:17,971 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32837', status: init, memory: 0, processing: 0>
2023-12-18 06:42:17,971 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,972 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32837
2023-12-18 06:42:17,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50710
2023-12-18 06:42:17,973 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:17,974 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,974 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,977 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:17,979 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:17,979 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:17,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,000 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34173', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34173
2023-12-18 06:42:18,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50726
2023-12-18 06:42:18,003 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38993', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,003 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38993
2023-12-18 06:42:18,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50736
2023-12-18 06:42:18,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38057', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,008 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38057
2023-12-18 06:42:18,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50712
2023-12-18 06:42:18,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,008 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,009 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44245', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,009 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44245
2023-12-18 06:42:18,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50734
2023-12-18 06:42:18,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,010 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41027', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,011 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41027
2023-12-18 06:42:18,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50746
2023-12-18 06:42:18,011 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,013 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45651', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,014 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45651
2023-12-18 06:42:18,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50760
2023-12-18 06:42:18,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37841', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37841
2023-12-18 06:42:18,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50744
2023-12-18 06:42:18,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42851', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,017 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42851
2023-12-18 06:42:18,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50772
2023-12-18 06:42:18,022 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:18,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,022 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,023 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:18,023 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:18,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:18,029 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38867', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,029 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38867
2023-12-18 06:42:18,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50792
2023-12-18 06:42:18,030 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40333', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,030 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40333
2023-12-18 06:42:18,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50784
2023-12-18 06:42:18,034 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33729', status: init, memory: 0, processing: 0>
2023-12-18 06:42:18,034 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33729
2023-12-18 06:42:18,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50808
2023-12-18 06:42:25,575 - distributed.scheduler - INFO - Remove client Client-903fc5c6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:25,575 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59004; closing.
2023-12-18 06:42:25,575 - distributed.scheduler - INFO - Remove client Client-903fc5c6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:25,576 - distributed.scheduler - INFO - Close client connection: Client-903fc5c6-9d70-11ee-b65b-d8c49764f6bb
2023-12-18 06:42:25,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50744; closing.
2023-12-18 06:42:25,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37841', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7422109')
2023-12-18 06:42:25,743 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50784; closing.
2023-12-18 06:42:25,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.745137')
2023-12-18 06:42:25,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50760; closing.
2023-12-18 06:42:25,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50746; closing.
2023-12-18 06:42:25,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50726; closing.
2023-12-18 06:42:25,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7492537')
2023-12-18 06:42:25,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41027', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7497106')
2023-12-18 06:42:25,751 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50784>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-18 06:42:25,753 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7536101')
2023-12-18 06:42:25,754 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50792; closing.
2023-12-18 06:42:25,754 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50772; closing.
2023-12-18 06:42:25,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7550297')
2023-12-18 06:42:25,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7555072')
2023-12-18 06:42:25,755 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50808; closing.
2023-12-18 06:42:25,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.7565534')
2023-12-18 06:42:25,790 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,790 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,790 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,790 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,792 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-18 06:42:25,808 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,808 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,809 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-18 06:42:25,813 - distributed.scheduler - INFO - Remove client Client-90177579-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:25,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50648; closing.
2023-12-18 06:42:25,814 - distributed.scheduler - INFO - Remove client Client-90177579-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:25,814 - distributed.scheduler - INFO - Close client connection: Client-90177579-9d70-11ee-b53b-d8c49764f6bb
2023-12-18 06:42:25,815 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40881'. Reason: nanny-close
2023-12-18 06:42:25,816 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,816 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39953'. Reason: nanny-close
2023-12-18 06:42:25,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,817 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38057. Reason: nanny-close
2023-12-18 06:42:25,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39069'. Reason: nanny-close
2023-12-18 06:42:25,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33507. Reason: nanny-close
2023-12-18 06:42:25,818 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32813'. Reason: nanny-close
2023-12-18 06:42:25,818 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36815. Reason: nanny-close
2023-12-18 06:42:25,818 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41033'. Reason: nanny-close
2023-12-18 06:42:25,818 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,819 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36003. Reason: nanny-close
2023-12-18 06:42:25,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38511'. Reason: nanny-close
2023-12-18 06:42:25,819 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,819 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44245. Reason: nanny-close
2023-12-18 06:42:25,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35743'. Reason: nanny-close
2023-12-18 06:42:25,819 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,820 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37601. Reason: nanny-close
2023-12-18 06:42:25,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42039'. Reason: nanny-close
2023-12-18 06:42:25,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50676; closing.
2023-12-18 06:42:25,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,820 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:25,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,820 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32837. Reason: nanny-close
2023-12-18 06:42:25,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50696; closing.
2023-12-18 06:42:25,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,821 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.821013')
2023-12-18 06:42:25,821 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38993. Reason: nanny-close
2023-12-18 06:42:25,821 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50712; closing.
2023-12-18 06:42:25,821 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,821 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,822 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.8219478')
2023-12-18 06:42:25,822 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,822 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,822 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,822 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,822 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.8226094')
2023-12-18 06:42:25,822 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,822 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:25,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50692; closing.
2023-12-18 06:42:25,823 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,823 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,824 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,824 - distributed.nanny - INFO - Worker closed
2023-12-18 06:42:25,824 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33507', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.8244429')
2023-12-18 06:42:25,824 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50734; closing.
2023-12-18 06:42:25,825 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50712>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-18 06:42:25,826 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.826489')
2023-12-18 06:42:25,826 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50702; closing.
2023-12-18 06:42:25,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50710; closing.
2023-12-18 06:42:25,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50736; closing.
2023-12-18 06:42:25,827 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.8277214')
2023-12-18 06:42:25,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32837', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.8280873')
2023-12-18 06:42:25,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702881745.828436')
2023-12-18 06:42:25,828 - distributed.scheduler - INFO - Lost all workers
2023-12-18 06:42:27,784 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-18 06:42:27,785 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-18 06:42:27,785 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:27,787 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-18 06:42:27,788 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-18 06:42:30,150 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:30,155 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45889 instead
  warnings.warn(
2023-12-18 06:42:30,158 - distributed.scheduler - INFO - State start
2023-12-18 06:42:30,180 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:30,181 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:42:30,182 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:30,182 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:42:30,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42723'
2023-12-18 06:42:32,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:32,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:32,120 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:32,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42723'. Reason: nanny-close
2023-12-18 06:42:33,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35091
2023-12-18 06:42:33,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35091
2023-12-18 06:42:33,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41437
2023-12-18 06:42:33,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:33,113 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:33,113 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:33,113 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:42:33,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uyqr61yc
2023-12-18 06:42:33,113 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f9defe4-865f-4a14-b449-5473844a158a
2023-12-18 06:42:33,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b49d8ca-a4c8-4f63-8d1e-d1cd7120b182
2023-12-18 06:42:33,119 - distributed.worker - INFO - Starting Worker plugin PreImport-740b633a-1dee-4a99-89a6-8b9eaaa1cea7
2023-12-18 06:42:33,119 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:33,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:33,153 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:33,153 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:33,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:33,166 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:33,167 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35091. Reason: nanny-close
2023-12-18 06:42:33,169 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:33,171 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-18 06:42:36,443 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:36,448 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45241 instead
  warnings.warn(
2023-12-18 06:42:36,452 - distributed.scheduler - INFO - State start
2023-12-18 06:42:36,710 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-18 06:42:36,712 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-18 06:42:36,713 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-18 06:42:36,715 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-18 06:42:36,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32907'
2023-12-18 06:42:38,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-18 06:42:38,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-18 06:42:38,616 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-18 06:42:38,694 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32907'. Reason: nanny-close
2023-12-18 06:42:39,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41119
2023-12-18 06:42:39,507 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41119
2023-12-18 06:42:39,507 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45265
2023-12-18 06:42:39,507 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-18 06:42:39,507 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:39,507 - distributed.worker - INFO -               Threads:                          1
2023-12-18 06:42:39,507 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-18 06:42:39,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zcmmlqi6
2023-12-18 06:42:39,507 - distributed.worker - INFO - Starting Worker plugin PreImport-63a0ba56-f2af-4705-8d76-2232b4eea30d
2023-12-18 06:42:39,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3694c48c-c11b-4def-bdad-518faa7973f5
2023-12-18 06:42:39,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9343440d-dc31-4ebc-a98b-d559e32852e4
2023-12-18 06:42:39,642 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:39,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-18 06:42:39,668 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-18 06:42:39,668 - distributed.worker - INFO - -------------------------------------------------
2023-12-18 06:42:39,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-18 06:42:39,685 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-18 06:42:39,686 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41119. Reason: nanny-close
2023-12-18 06:42:39,688 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-18 06:42:39,690 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44531 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43807 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41027 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36969 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38491 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36083 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46363 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35081 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40055 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34975 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43969 instead
  warnings.warn(
2023-12-18 06:46:58,980 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-18 06:46:58,991 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:33593', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41255 instead
  warnings.warn(
[1702882023.870061] [dgx13:70863:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:33664) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39937 instead
  warnings.warn(
[1702882044.681082] [dgx13:71188:0]            sock.c:470  UCX  ERROR bind(fd=137 addr=0.0.0.0:45229) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44243 instead
  warnings.warn(
2023-12-18 06:48:08,090 - distributed.core - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
[1702882088.124713] [dgx13:71800:0]           mpool.c:54   UCX  WARN  object 0x55be3dc0da80 {flags:0x44040 send length 10224 ucp_proto_progress_tag_rndv_rts() comp:???()host memory} was not returned to mpool ucp_requests
[1702882088.552202] [dgx13:71800:0]         ptr_map.c:18   UCX  WARN  ptr hash 0x55be3d87b5f0 contains 1 elements on destroy
[1702882088.555524] [dgx13:71800:0]          rcache.c:624  UCX  WARN  mlx5_0: destroying inuse region 0x55be4ed01ce0 [0x55be4ed00680..0x55be4ed045f0] g- rw ref 1 lkey 0x76cc97 rkey 0x76cc97 atomic_rkey 0xffffffff
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33025 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34003 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45961 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44401 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32883 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37197 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34273 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] 2023-12-18 06:53:21,822 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:44010 remote=tcp://127.0.0.1:42925>: Stream is closed
2023-12-18 06:53:21,822 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:44012 remote=tcp://127.0.0.1:42925>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41559 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39395 instead
  warnings.warn(
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5255, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5256, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2103, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 75, in asarray
    return _core.array(a, dtype, False, order)
  File "cupy/_core/core.pyx", line 2376, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2400, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2531, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41265 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39429 instead
  warnings.warn(
2023-12-18 06:54:58,052 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 56, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/__init__.py", line 2, in <module>
    from . import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_lowering.py", line 11, in <module>
    from cudf.core.udf.groupby_typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_typing.py", line 19, in <module>
    from cudf.core.udf.utils import Row, UDFError
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-12-18 06:54:58,243 - distributed.core - ERROR - [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 56, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/__init__.py", line 2, in <module>
    from . import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_lowering.py", line 11, in <module>
    from cudf.core.udf.groupby_typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_typing.py", line 19, in <module>
    from cudf.core.udf.utils import Row, UDFError
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-12-18 06:54:58,428 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:46720'.
2023-12-18 06:54:58,429 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:46720'. Shutting down.
2023-12-18 06:54:58,437 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f0dead4fd90>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 56, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/__init__.py", line 2, in <module>
    from . import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_lowering.py", line 11, in <module>
    from cudf.core.udf.groupby_typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_typing.py", line 19, in <module>
    from cudf.core.udf.utils import Row, UDFError
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 56, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/__init__.py", line 2, in <module>
    from . import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_lowering.py", line 11, in <module>
    from cudf.core.udf.groupby_typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_typing.py", line 19, in <module>
    from cudf.core.udf.utils import Row, UDFError
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-12-18 06:55:00,441 - distributed.nanny - ERROR - Worker process died unexpectedly
