============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-11-23 06:38:00,370 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:00,375 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41663 instead
  warnings.warn(
2023-11-23 06:38:00,379 - distributed.scheduler - INFO - State start
2023-11-23 06:38:00,403 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:00,404 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-23 06:38:00,404 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41663/status
2023-11-23 06:38:00,405 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:00,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37493'
2023-11-23 06:38:00,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33941'
2023-11-23 06:38:00,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41875'
2023-11-23 06:38:00,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39549'
2023-11-23 06:38:02,018 - distributed.scheduler - INFO - Receive client connection: Client-d770704c-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:02,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46586
2023-11-23 06:38:02,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:02,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:02,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:02,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:02,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:02,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-11-23 06:38:02,261 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43313
2023-11-23 06:38:02,261 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43313
2023-11-23 06:38:02,261 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37459
2023-11-23 06:38:02,262 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-23 06:38:02,262 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:02,262 - distributed.worker - INFO -               Threads:                          4
2023-11-23 06:38:02,262 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-23 06:38:02,262 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nsvnq09d
2023-11-23 06:38:02,262 - distributed.worker - INFO - Starting Worker plugin PreImport-b0bc05df-312a-4101-a3a5-c85124044713
2023-11-23 06:38:02,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b7ec819-b6b0-4121-92da-f9b9966d963e
2023-11-23 06:38:02,262 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32248adc-b3a3-4bfc-b991-1183c1d5d180
2023-11-23 06:38:02,263 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:02,267 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:02,267 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:02,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:02,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:02,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:02,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:02,668 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43313', status: init, memory: 0, processing: 0>
2023-11-23 06:38:02,670 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43313
2023-11-23 06:38:02,670 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46602
2023-11-23 06:38:02,672 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:02,673 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-23 06:38:02,673 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:02,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-23 06:38:03,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39483
2023-11-23 06:38:03,751 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39483
2023-11-23 06:38:03,751 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36283
2023-11-23 06:38:03,751 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-23 06:38:03,751 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:03,751 - distributed.worker - INFO -               Threads:                          4
2023-11-23 06:38:03,751 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-23 06:38:03,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-uzy7ogmf
2023-11-23 06:38:03,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1991344b-9ce0-45f7-beef-76f66da10e74
2023-11-23 06:38:03,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5193ad43-a8ea-44c5-8c72-c6fc40cf4604
2023-11-23 06:38:03,752 - distributed.worker - INFO - Starting Worker plugin PreImport-8ab3b833-39b4-482d-896e-5d3e883f401c
2023-11-23 06:38:03,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39087
2023-11-23 06:38:04,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39087
2023-11-23 06:38:04,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36755
2023-11-23 06:38:04,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37049
2023-11-23 06:38:04,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-23 06:38:04,258 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36755
2023-11-23 06:38:04,258 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,258 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33303
2023-11-23 06:38:04,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-23 06:38:04,258 - distributed.worker - INFO -               Threads:                          4
2023-11-23 06:38:04,258 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,258 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-23 06:38:04,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-sggraupv
2023-11-23 06:38:04,258 - distributed.worker - INFO -               Threads:                          4
2023-11-23 06:38:04,258 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-23 06:38:04,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d9j0xult
2023-11-23 06:38:04,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a43fa07-ac4c-4ceb-a681-b7d4c931abd4
2023-11-23 06:38:04,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-435cd32c-76c7-4745-8c41-67536cc55f71
2023-11-23 06:38:04,259 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac2640f5-4837-4a0c-a1b3-14e44ae47d28
2023-11-23 06:38:04,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fb13493-6723-4263-883d-ba2e8dc10b5f
2023-11-23 06:38:04,259 - distributed.worker - INFO - Starting Worker plugin PreImport-71c8b45c-4ba6-481e-9bf9-3f0df6d1a53f
2023-11-23 06:38:04,259 - distributed.worker - INFO - Starting Worker plugin PreImport-025e18f0-43f9-46b5-a967-7ab58f319fc7
2023-11-23 06:38:04,259 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,259 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,281 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39483', status: init, memory: 0, processing: 0>
2023-11-23 06:38:04,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39483
2023-11-23 06:38:04,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46604
2023-11-23 06:38:04,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:04,284 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-23 06:38:04,284 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,284 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36755', status: init, memory: 0, processing: 0>
2023-11-23 06:38:04,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36755
2023-11-23 06:38:04,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46620
2023-11-23 06:38:04,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-23 06:38:04,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:04,289 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-23 06:38:04,289 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-23 06:38:04,308 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39087', status: init, memory: 0, processing: 0>
2023-11-23 06:38:04,309 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39087
2023-11-23 06:38:04,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46628
2023-11-23 06:38:04,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:04,317 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-23 06:38:04,317 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:04,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-23 06:38:04,416 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-23 06:38:04,417 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-23 06:38:04,417 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-23 06:38:04,535 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-23 06:38:04,540 - distributed.scheduler - INFO - Remove client Client-d770704c-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:04,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46586; closing.
2023-11-23 06:38:04,541 - distributed.scheduler - INFO - Remove client Client-d770704c-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:04,541 - distributed.scheduler - INFO - Close client connection: Client-d770704c-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:04,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37493'. Reason: nanny-close
2023-11-23 06:38:04,543 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:04,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41875'. Reason: nanny-close
2023-11-23 06:38:04,544 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:04,544 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39483. Reason: nanny-close
2023-11-23 06:38:04,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39549'. Reason: nanny-close
2023-11-23 06:38:04,545 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:04,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33941'. Reason: nanny-close
2023-11-23 06:38:04,545 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39087. Reason: nanny-close
2023-11-23 06:38:04,545 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:04,545 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36755. Reason: nanny-close
2023-11-23 06:38:04,546 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46604; closing.
2023-11-23 06:38:04,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-23 06:38:04,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43313. Reason: nanny-close
2023-11-23 06:38:04,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39483', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721484.54679')
2023-11-23 06:38:04,547 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:04,547 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-23 06:38:04,548 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-23 06:38:04,548 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-23 06:38:04,548 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46628; closing.
2023-11-23 06:38:04,549 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46620; closing.
2023-11-23 06:38:04,549 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46602; closing.
2023-11-23 06:38:04,549 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:04,549 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721484.549665')
2023-11-23 06:38:04,550 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36755', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721484.5499463')
2023-11-23 06:38:04,550 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:04,550 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:04,550 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721484.5502112')
2023-11-23 06:38:04,550 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:38:05,760 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:38:05,760 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:38:05,761 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:38:05,762 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-23 06:38:05,762 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-11-23 06:38:08,239 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:08,244 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38825 instead
  warnings.warn(
2023-11-23 06:38:08,248 - distributed.scheduler - INFO - State start
2023-11-23 06:38:08,272 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:08,273 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:38:08,274 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38825/status
2023-11-23 06:38:08,274 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:08,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37463'
2023-11-23 06:38:08,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37237'
2023-11-23 06:38:08,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41475'
2023-11-23 06:38:08,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35537'
2023-11-23 06:38:08,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34535'
2023-11-23 06:38:08,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35833'
2023-11-23 06:38:08,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41407'
2023-11-23 06:38:08,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44401'
2023-11-23 06:38:09,877 - distributed.scheduler - INFO - Receive client connection: Client-dc086af4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:09,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50150
2023-11-23 06:38:10,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,200 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:10,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:10,484 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:10,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:11,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33427
2023-11-23 06:38:11,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33427
2023-11-23 06:38:11,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39927
2023-11-23 06:38:11,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:11,709 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:11,709 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:11,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:11,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-snpw4whb
2023-11-23 06:38:11,710 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e4eff44-9b3c-455a-88cb-3600ef03365e
2023-11-23 06:38:11,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6bd73488-75e4-43c6-937e-d373d1ebe438
2023-11-23 06:38:12,204 - distributed.worker - INFO - Starting Worker plugin PreImport-e671e17e-1c62-413a-a16a-3ed50df1d9ec
2023-11-23 06:38:12,204 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:12,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33427', status: init, memory: 0, processing: 0>
2023-11-23 06:38:12,249 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33427
2023-11-23 06:38:12,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34222
2023-11-23 06:38:12,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:12,252 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:12,252 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:12,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:12,891 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38715
2023-11-23 06:38:12,892 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38715
2023-11-23 06:38:12,892 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35095
2023-11-23 06:38:12,892 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:12,892 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:12,892 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:12,892 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:12,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hvr97ya9
2023-11-23 06:38:12,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf3c42ee-7a94-40c6-8cd7-49d6a2d416d9
2023-11-23 06:38:12,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff882af7-28f5-4ed9-977f-89c1eca8a2fb
2023-11-23 06:38:12,983 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42245
2023-11-23 06:38:12,983 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42245
2023-11-23 06:38:12,983 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38333
2023-11-23 06:38:12,983 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:12,984 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:12,984 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:12,984 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:12,984 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vfh86g_j
2023-11-23 06:38:12,984 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f7b8e85-a304-40f4-ace4-06ba311f2f8a
2023-11-23 06:38:12,987 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33135
2023-11-23 06:38:12,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33135
2023-11-23 06:38:12,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34581
2023-11-23 06:38:12,988 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:12,988 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:12,988 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:12,989 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:12,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g0yjyift
2023-11-23 06:38:12,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d25b6427-642a-442c-aed7-d6f49a2eadfa
2023-11-23 06:38:12,991 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32825
2023-11-23 06:38:12,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32825
2023-11-23 06:38:12,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39485
2023-11-23 06:38:12,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:12,992 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:12,992 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:12,992 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:12,992 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-141c31a3
2023-11-23 06:38:12,993 - distributed.worker - INFO - Starting Worker plugin PreImport-4defdfcf-1f05-4b41-a45b-f00226d4f276
2023-11-23 06:38:12,993 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-281d1efb-8d96-4e5c-a569-ba0fcd244b96
2023-11-23 06:38:12,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0841b01e-c7a9-4a0f-bb24-b9e6ca49ebbd
2023-11-23 06:38:13,169 - distributed.worker - INFO - Starting Worker plugin PreImport-f76a0a73-f11d-42df-9070-5b76baead848
2023-11-23 06:38:13,169 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39427
2023-11-23 06:38:13,175 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39427
2023-11-23 06:38:13,175 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35627
2023-11-23 06:38:13,175 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,175 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,175 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:13,176 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:13,176 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-64hgecbt
2023-11-23 06:38:13,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6de58a5-265c-4098-b751-5eaa7e28f866
2023-11-23 06:38:13,177 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33057
2023-11-23 06:38:13,177 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33057
2023-11-23 06:38:13,177 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41559
2023-11-23 06:38:13,177 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,178 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,178 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:13,178 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:13,178 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cq45llsi
2023-11-23 06:38:13,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f5449d4-5b2f-4ebf-8bb8-368280477bf8
2023-11-23 06:38:13,184 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40033
2023-11-23 06:38:13,184 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40033
2023-11-23 06:38:13,184 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40839
2023-11-23 06:38:13,184 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,184 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,184 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:13,185 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:13,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qy92pd0g
2023-11-23 06:38:13,185 - distributed.worker - INFO - Starting Worker plugin PreImport-b7332cc6-b6b8-46c4-82f2-3ec58c4f92fc
2023-11-23 06:38:13,185 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94ed0d5e-833f-4fe2-a29f-a0c83d3aab16
2023-11-23 06:38:13,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e2ed991-6353-4a22-91df-c55e9ad46187
2023-11-23 06:38:13,367 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38715', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,368 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38715
2023-11-23 06:38:13,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34228
2023-11-23 06:38:13,369 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,370 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,370 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,488 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ff47a95-8910-419f-87ea-ee68104c6c9e
2023-11-23 06:38:13,488 - distributed.worker - INFO - Starting Worker plugin PreImport-26915165-069f-4b7e-941a-15bd02544d61
2023-11-23 06:38:13,489 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e0ab3c4-dc54-49ef-bab7-d0c1cfafe72c
2023-11-23 06:38:13,498 - distributed.worker - INFO - Starting Worker plugin PreImport-d668688e-6aaa-476a-ac47-a7dbf47307cc
2023-11-23 06:38:13,498 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,498 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,509 - distributed.worker - INFO - Starting Worker plugin PreImport-167de4c6-6b76-4d9c-b4cd-c5732b8e295e
2023-11-23 06:38:13,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0f4f051-2802-40e8-b607-4478430b4bbe
2023-11-23 06:38:13,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f6b1eb9-8c43-409a-bf65-d04bf755af2e
2023-11-23 06:38:13,510 - distributed.worker - INFO - Starting Worker plugin PreImport-093eb3cc-29dc-4b5f-b1f9-77a754495d9d
2023-11-23 06:38:13,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,522 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,525 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42245', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,526 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42245
2023-11-23 06:38:13,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34240
2023-11-23 06:38:13,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,529 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,529 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,530 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32825', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,531 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32825
2023-11-23 06:38:13,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34248
2023-11-23 06:38:13,532 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,532 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33135', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,533 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33135
2023-11-23 06:38:13,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34246
2023-11-23 06:38:13,534 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,535 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,535 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,536 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,536 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39427', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39427
2023-11-23 06:38:13,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34256
2023-11-23 06:38:13,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33057', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33057
2023-11-23 06:38:13,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34268
2023-11-23 06:38:13,550 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40033', status: init, memory: 0, processing: 0>
2023-11-23 06:38:13,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,551 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40033
2023-11-23 06:38:13,551 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34276
2023-11-23 06:38:13,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:13,554 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,554 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,555 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,555 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:13,556 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:13,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:13,580 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,580 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,580 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,581 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,581 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,581 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,581 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,581 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:13,586 - distributed.scheduler - INFO - Remove client Client-dc086af4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:13,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50150; closing.
2023-11-23 06:38:13,586 - distributed.scheduler - INFO - Remove client Client-dc086af4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:13,587 - distributed.scheduler - INFO - Close client connection: Client-dc086af4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:13,588 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37463'. Reason: nanny-close
2023-11-23 06:38:13,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37237'. Reason: nanny-close
2023-11-23 06:38:13,590 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39427. Reason: nanny-close
2023-11-23 06:38:13,590 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41475'. Reason: nanny-close
2023-11-23 06:38:13,590 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33057. Reason: nanny-close
2023-11-23 06:38:13,590 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35537'. Reason: nanny-close
2023-11-23 06:38:13,591 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,591 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32825. Reason: nanny-close
2023-11-23 06:38:13,591 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34535'. Reason: nanny-close
2023-11-23 06:38:13,591 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,591 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40033. Reason: nanny-close
2023-11-23 06:38:13,592 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35833'. Reason: nanny-close
2023-11-23 06:38:13,592 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,592 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,592 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38715. Reason: nanny-close
2023-11-23 06:38:13,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34256; closing.
2023-11-23 06:38:13,592 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41407'. Reason: nanny-close
2023-11-23 06:38:13,592 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,592 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.5928543')
2023-11-23 06:38:13,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,593 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42245. Reason: nanny-close
2023-11-23 06:38:13,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,593 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44401'. Reason: nanny-close
2023-11-23 06:38:13,593 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:13,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,593 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33427. Reason: nanny-close
2023-11-23 06:38:13,594 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33135. Reason: nanny-close
2023-11-23 06:38:13,594 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,594 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,594 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,594 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,595 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,595 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34276; closing.
2023-11-23 06:38:13,596 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34268; closing.
2023-11-23 06:38:13,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,596 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34248; closing.
2023-11-23 06:38:13,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:13,596 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,597 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.5978208')
2023-11-23 06:38:13,598 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,598 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,598 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.5984468')
2023-11-23 06:38:13,599 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.5989945')
2023-11-23 06:38:13,599 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34228; closing.
2023-11-23 06:38:13,599 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:13,600 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.6007943')
2023-11-23 06:38:13,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34222; closing.
2023-11-23 06:38:13,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34240; closing.
2023-11-23 06:38:13,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.60278')
2023-11-23 06:38:13,603 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.603493')
2023-11-23 06:38:13,604 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34246; closing.
2023-11-23 06:38:13,604 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33135', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721493.604636')
2023-11-23 06:38:13,604 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:38:13,605 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34246>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-23 06:38:15,106 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:38:15,106 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:38:15,107 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:38:15,108 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:38:15,108 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-11-23 06:38:17,242 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:17,247 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36139 instead
  warnings.warn(
2023-11-23 06:38:17,251 - distributed.scheduler - INFO - State start
2023-11-23 06:38:17,273 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:17,274 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:38:17,274 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36139/status
2023-11-23 06:38:17,275 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:17,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43783'
2023-11-23 06:38:17,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44787'
2023-11-23 06:38:17,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46875'
2023-11-23 06:38:17,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40415'
2023-11-23 06:38:17,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37683'
2023-11-23 06:38:17,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40183'
2023-11-23 06:38:17,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40921'
2023-11-23 06:38:17,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35933'
2023-11-23 06:38:18,034 - distributed.scheduler - INFO - Receive client connection: Client-e18d16f4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:18,047 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34452
2023-11-23 06:38:19,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,939 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,975 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:19,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:19,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:19,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:23,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34015
2023-11-23 06:38:23,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34015
2023-11-23 06:38:23,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37505
2023-11-23 06:38:23,452 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,452 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,452 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,452 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vxiolup4
2023-11-23 06:38:23,452 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70add610-08f8-4a1b-b442-10b8c9218b49
2023-11-23 06:38:23,458 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36085
2023-11-23 06:38:23,458 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36085
2023-11-23 06:38:23,458 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44909
2023-11-23 06:38:23,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,458 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,458 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,459 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,459 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wylgv1gb
2023-11-23 06:38:23,459 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d2b1d04-cd33-44cc-9270-0633d0b5c6d0
2023-11-23 06:38:23,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33345
2023-11-23 06:38:23,470 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33345
2023-11-23 06:38:23,471 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39761
2023-11-23 06:38:23,471 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,471 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,471 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,471 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,471 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-76mdzzh4
2023-11-23 06:38:23,471 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c52dd6a6-e9bd-4777-90ab-d0884c3060ce
2023-11-23 06:38:23,472 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8676fd67-3e67-4017-94f5-680d229f691c
2023-11-23 06:38:23,476 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40259
2023-11-23 06:38:23,477 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40259
2023-11-23 06:38:23,477 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42947
2023-11-23 06:38:23,477 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,477 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,477 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,477 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,477 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yztmp67k
2023-11-23 06:38:23,478 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e84974d9-e0c8-46bd-bdad-91075b2043ce
2023-11-23 06:38:23,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38801
2023-11-23 06:38:23,501 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38801
2023-11-23 06:38:23,501 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35005
2023-11-23 06:38:23,501 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,501 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,501 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,501 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-my0esz1a
2023-11-23 06:38:23,501 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40827
2023-11-23 06:38:23,502 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40827
2023-11-23 06:38:23,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35959
2023-11-23 06:38:23,502 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,502 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,502 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,502 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g0nzqydz
2023-11-23 06:38:23,503 - distributed.worker - INFO - Starting Worker plugin PreImport-41fda05e-cb19-4156-b790-50b794d46b1a
2023-11-23 06:38:23,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afa32634-e521-406f-afc7-25b4e15acdb2
2023-11-23 06:38:23,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15116e86-18b7-47ea-b46a-cbe7d6b2040b
2023-11-23 06:38:23,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3004cb7-2c34-40e2-b919-57f217527ca4
2023-11-23 06:38:23,510 - distributed.worker - INFO - Starting Worker plugin PreImport-3e5e1479-df37-4957-9f79-2498f77d53e2
2023-11-23 06:38:23,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4aa846ee-f150-4bd9-831d-dd9b58df81f8
2023-11-23 06:38:23,512 - distributed.worker - INFO - Starting Worker plugin PreImport-6df144d6-5de5-4b00-97f7-d021a64a60c3
2023-11-23 06:38:23,513 - distributed.worker - INFO - Starting Worker plugin PreImport-5e3109ba-4b1c-462f-968e-f30d36f74a88
2023-11-23 06:38:23,513 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,512 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99a5f7fd-658c-443d-a9ed-391b7f0543d7
2023-11-23 06:38:23,513 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,514 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15c9d540-eec1-4a72-8405-509382284246
2023-11-23 06:38:23,514 - distributed.worker - INFO - Starting Worker plugin PreImport-d9575143-d012-47d6-8244-fd18020fa789
2023-11-23 06:38:23,515 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2edca486-a3d1-4e69-9c59-aeb51871a47c
2023-11-23 06:38:23,517 - distributed.worker - INFO - Starting Worker plugin PreImport-47ef3493-4ff6-4476-b7b8-8d7756536156
2023-11-23 06:38:23,517 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,526 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,535 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33345', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33345
2023-11-23 06:38:23,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50928
2023-11-23 06:38:23,538 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36085', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36085
2023-11-23 06:38:23,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50944
2023-11-23 06:38:23,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,539 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,539 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40259', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,540 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40259
2023-11-23 06:38:23,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50942
2023-11-23 06:38:23,540 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,540 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,541 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,541 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,544 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34015', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,545 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34015
2023-11-23 06:38:23,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50918
2023-11-23 06:38:23,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,547 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,549 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,549 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,553 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38801', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38801
2023-11-23 06:38:23,554 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50968
2023-11-23 06:38:23,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40827', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40827
2023-11-23 06:38:23,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50958
2023-11-23 06:38:23,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,556 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,558 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41417
2023-11-23 06:38:23,643 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41417
2023-11-23 06:38:23,643 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37379
2023-11-23 06:38:23,643 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,643 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,642 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34017
2023-11-23 06:38:23,643 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34017
2023-11-23 06:38:23,643 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,643 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43535
2023-11-23 06:38:23,643 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,643 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5xgh53xc
2023-11-23 06:38:23,644 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,644 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:23,644 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:23,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bmdvl9bi
2023-11-23 06:38:23,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81ad8012-fb90-4b5a-b4e6-d4ea44b6c575
2023-11-23 06:38:23,645 - distributed.worker - INFO - Starting Worker plugin PreImport-1b0edb46-a48a-419e-9366-4d2fe35eaa53
2023-11-23 06:38:23,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3e42572-d02e-46a0-a6ba-324f7a0c6ffd
2023-11-23 06:38:23,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c81a0f2d-098f-40fe-ab47-908867e5b29b
2023-11-23 06:38:23,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c32f44a-fd23-4190-b38c-153cc4ab755e
2023-11-23 06:38:23,654 - distributed.worker - INFO - Starting Worker plugin PreImport-9f84906e-f2c0-4a5d-8322-291cd7f6a65e
2023-11-23 06:38:23,654 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,654 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,681 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34017', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,681 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34017
2023-11-23 06:38:23,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50982
2023-11-23 06:38:23,682 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41417', status: init, memory: 0, processing: 0>
2023-11-23 06:38:23,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41417
2023-11-23 06:38:23,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50994
2023-11-23 06:38:23,683 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,683 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:23,685 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:23,685 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:23,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,694 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:23,731 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,731 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,731 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,732 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,732 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,732 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,732 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,732 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:23,736 - distributed.scheduler - INFO - Remove client Client-e18d16f4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:23,736 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34452; closing.
2023-11-23 06:38:23,737 - distributed.scheduler - INFO - Remove client Client-e18d16f4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:23,737 - distributed.scheduler - INFO - Close client connection: Client-e18d16f4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:23,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43783'. Reason: nanny-close
2023-11-23 06:38:23,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44787'. Reason: nanny-close
2023-11-23 06:38:23,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46875'. Reason: nanny-close
2023-11-23 06:38:23,740 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34017. Reason: nanny-close
2023-11-23 06:38:23,740 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40415'. Reason: nanny-close
2023-11-23 06:38:23,740 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40259. Reason: nanny-close
2023-11-23 06:38:23,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37683'. Reason: nanny-close
2023-11-23 06:38:23,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,741 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36085. Reason: nanny-close
2023-11-23 06:38:23,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40183'. Reason: nanny-close
2023-11-23 06:38:23,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50982; closing.
2023-11-23 06:38:23,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,742 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40827. Reason: nanny-close
2023-11-23 06:38:23,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34017', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7424443')
2023-11-23 06:38:23,742 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40921'. Reason: nanny-close
2023-11-23 06:38:23,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34015. Reason: nanny-close
2023-11-23 06:38:23,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35933'. Reason: nanny-close
2023-11-23 06:38:23,743 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,743 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38801. Reason: nanny-close
2023-11-23 06:38:23,744 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50942; closing.
2023-11-23 06:38:23,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33345. Reason: nanny-close
2023-11-23 06:38:23,744 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,744 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:23,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50944; closing.
2023-11-23 06:38:23,745 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.745268')
2023-11-23 06:38:23,745 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41417. Reason: nanny-close
2023-11-23 06:38:23,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36085', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7457802')
2023-11-23 06:38:23,746 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,746 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,746 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50958; closing.
2023-11-23 06:38:23,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:23,747 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7477431')
2023-11-23 06:38:23,747 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,748 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50918; closing.
2023-11-23 06:38:23,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50968; closing.
2023-11-23 06:38:23,748 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50928; closing.
2023-11-23 06:38:23,748 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34015', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7489316')
2023-11-23 06:38:23,749 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7493308')
2023-11-23 06:38:23,749 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:23,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7497098')
2023-11-23 06:38:23,750 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50994; closing.
2023-11-23 06:38:23,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41417', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721503.7505062')
2023-11-23 06:38:23,750 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:38:25,406 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:38:25,406 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:38:25,407 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:38:25,408 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:38:25,409 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-11-23 06:38:27,642 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:27,647 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35047 instead
  warnings.warn(
2023-11-23 06:38:27,651 - distributed.scheduler - INFO - State start
2023-11-23 06:38:27,701 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:27,702 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:38:27,702 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35047/status
2023-11-23 06:38:27,703 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:27,913 - distributed.scheduler - INFO - Receive client connection: Client-e7ae4fa4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:27,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51054
2023-11-23 06:38:28,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36501'
2023-11-23 06:38:28,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34433'
2023-11-23 06:38:28,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36319'
2023-11-23 06:38:28,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34167'
2023-11-23 06:38:28,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41755'
2023-11-23 06:38:28,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43977'
2023-11-23 06:38:28,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33179'
2023-11-23 06:38:28,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32883'
2023-11-23 06:38:30,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,421 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,421 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,444 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:30,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:30,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:30,496 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:33,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33971
2023-11-23 06:38:33,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33971
2023-11-23 06:38:33,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46599
2023-11-23 06:38:33,420 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,420 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,420 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,420 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,420 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bcery1et
2023-11-23 06:38:33,421 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4885e99d-c548-4c7b-8c35-d63a3f492221
2023-11-23 06:38:33,459 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36735
2023-11-23 06:38:33,459 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36735
2023-11-23 06:38:33,460 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46141
2023-11-23 06:38:33,460 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,460 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,460 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,460 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,460 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-abq6905w
2023-11-23 06:38:33,460 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e3573d4-1ead-4bbd-87bc-bc6c665d9b9d
2023-11-23 06:38:33,465 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37653
2023-11-23 06:38:33,465 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37653
2023-11-23 06:38:33,466 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33149
2023-11-23 06:38:33,466 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,466 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,466 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,466 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,466 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qxiyonz9
2023-11-23 06:38:33,466 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-751160d0-bbf6-487f-8113-9159843545ec
2023-11-23 06:38:33,467 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2cadf3f-7f26-4b4b-bab9-762e50e0238c
2023-11-23 06:38:33,466 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44107
2023-11-23 06:38:33,467 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44107
2023-11-23 06:38:33,467 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38341
2023-11-23 06:38:33,467 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,467 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,467 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,467 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iv1rr1h9
2023-11-23 06:38:33,468 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ef808a9-b662-40a9-b102-e7a8201a2354
2023-11-23 06:38:33,474 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33761
2023-11-23 06:38:33,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33761
2023-11-23 06:38:33,475 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43189
2023-11-23 06:38:33,475 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,475 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,475 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,475 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,475 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vlj4wg7e
2023-11-23 06:38:33,475 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a772d06-a4cf-48f6-a6b2-23bfdcae05a5
2023-11-23 06:38:33,475 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45907
2023-11-23 06:38:33,476 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45907
2023-11-23 06:38:33,476 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33689
2023-11-23 06:38:33,476 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,476 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,476 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,476 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8kv8lcks
2023-11-23 06:38:33,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-89614fe1-e407-42fb-8254-fba4a033c5a0
2023-11-23 06:38:33,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42049
2023-11-23 06:38:33,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42049
2023-11-23 06:38:33,478 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37231
2023-11-23 06:38:33,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,479 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,479 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,479 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-16hvyv5u
2023-11-23 06:38:33,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f6729ff-2cd0-4be6-8b37-da41b8051975
2023-11-23 06:38:33,479 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36583
2023-11-23 06:38:33,480 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36583
2023-11-23 06:38:33,480 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41327
2023-11-23 06:38:33,480 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,480 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,480 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:33,480 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:33,480 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w3go54d4
2023-11-23 06:38:33,481 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dbfee22e-bd58-4f3a-890b-e1fcd01a1db7
2023-11-23 06:38:33,629 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6223e070-5678-422c-9084-0e6ed4aafd4d
2023-11-23 06:38:33,630 - distributed.worker - INFO - Starting Worker plugin PreImport-85709c03-af87-4d48-a1d4-16dfbb2de0a9
2023-11-23 06:38:33,630 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33971', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33971
2023-11-23 06:38:33,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56924
2023-11-23 06:38:33,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,660 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,661 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,679 - distributed.worker - INFO - Starting Worker plugin PreImport-9eca5e80-6b53-4879-931c-715b07e922e9
2023-11-23 06:38:33,679 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,684 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7c4b1ac-08d0-4e85-ae39-e7134cd65806
2023-11-23 06:38:33,684 - distributed.worker - INFO - Starting Worker plugin PreImport-56538402-076c-45e9-b1e4-ca092b99bd03
2023-11-23 06:38:33,684 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,685 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f8a04c5-dc48-4b7b-97ab-c8a5ca65fbb8
2023-11-23 06:38:33,686 - distributed.worker - INFO - Starting Worker plugin PreImport-4fa6aac0-4fa7-4526-a861-ebf4993872e7
2023-11-23 06:38:33,687 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,691 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c94805e-2800-46eb-9a21-161beb00d200
2023-11-23 06:38:33,691 - distributed.worker - INFO - Starting Worker plugin PreImport-368b2cfb-3174-48a5-877b-86ef252a8365
2023-11-23 06:38:33,692 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-048f3ecc-cf71-4f3f-b0ec-975ad8df1850
2023-11-23 06:38:33,696 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b9bcf28-7e16-43b4-965e-7e9f45918593
2023-11-23 06:38:33,696 - distributed.worker - INFO - Starting Worker plugin PreImport-1e73b7d3-50d9-40e4-bb61-3762d44259c7
2023-11-23 06:38:33,696 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b27a8ae-ff91-465b-8965-be941387f2cf
2023-11-23 06:38:33,697 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,697 - distributed.worker - INFO - Starting Worker plugin PreImport-b51eeabe-31d8-40c9-9632-d4f2d5f7c6a8
2023-11-23 06:38:33,697 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,700 - distributed.worker - INFO - Starting Worker plugin PreImport-6c140f28-4718-4bb9-891a-438c823807aa
2023-11-23 06:38:33,700 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,704 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37653', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,705 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37653
2023-11-23 06:38:33,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56936
2023-11-23 06:38:33,706 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,707 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,710 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36735', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,710 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36735
2023-11-23 06:38:33,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56938
2023-11-23 06:38:33,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,713 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,713 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,722 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33761', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,723 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33761
2023-11-23 06:38:33,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56948
2023-11-23 06:38:33,724 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36583', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36583
2023-11-23 06:38:33,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56964
2023-11-23 06:38:33,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,726 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,726 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,730 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,731 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,731 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,731 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45907', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45907
2023-11-23 06:38:33,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56980
2023-11-23 06:38:33,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,734 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44107', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44107
2023-11-23 06:38:33,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56962
2023-11-23 06:38:33,736 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42049', status: init, memory: 0, processing: 0>
2023-11-23 06:38:33,737 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,738 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42049
2023-11-23 06:38:33,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56994
2023-11-23 06:38:33,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:33,741 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,741 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,741 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:33,741 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:33,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:33,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:33,779 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,779 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,780 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,780 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,780 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,780 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,780 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,780 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:33,786 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:33,788 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:33,791 - distributed.scheduler - INFO - Remove client Client-e7ae4fa4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:33,791 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51054; closing.
2023-11-23 06:38:33,791 - distributed.scheduler - INFO - Remove client Client-e7ae4fa4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:33,792 - distributed.scheduler - INFO - Close client connection: Client-e7ae4fa4-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:33,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36501'. Reason: nanny-close
2023-11-23 06:38:33,793 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34433'. Reason: nanny-close
2023-11-23 06:38:33,795 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,795 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36583. Reason: nanny-close
2023-11-23 06:38:33,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36319'. Reason: nanny-close
2023-11-23 06:38:33,795 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36735. Reason: nanny-close
2023-11-23 06:38:33,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34167'. Reason: nanny-close
2023-11-23 06:38:33,796 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41755'. Reason: nanny-close
2023-11-23 06:38:33,797 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44107. Reason: nanny-close
2023-11-23 06:38:33,797 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43977'. Reason: nanny-close
2023-11-23 06:38:33,797 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56964; closing.
2023-11-23 06:38:33,797 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,797 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,797 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,798 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33971. Reason: nanny-close
2023-11-23 06:38:33,798 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36583', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.798002')
2023-11-23 06:38:33,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33179'. Reason: nanny-close
2023-11-23 06:38:33,798 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,798 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37653. Reason: nanny-close
2023-11-23 06:38:33,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56938; closing.
2023-11-23 06:38:33,799 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,799 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32883'. Reason: nanny-close
2023-11-23 06:38:33,799 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45907. Reason: nanny-close
2023-11-23 06:38:33,799 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,799 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,800 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,800 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.800082')
2023-11-23 06:38:33,800 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33761. Reason: nanny-close
2023-11-23 06:38:33,801 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,801 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56962; closing.
2023-11-23 06:38:33,801 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,801 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:33,801 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.8016443')
2023-11-23 06:38:33,802 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56924; closing.
2023-11-23 06:38:33,802 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,802 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,802 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,802 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42049. Reason: nanny-close
2023-11-23 06:38:33,802 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,802 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33971', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.8027117')
2023-11-23 06:38:33,803 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56936; closing.
2023-11-23 06:38:33,803 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56980; closing.
2023-11-23 06:38:33,804 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,804 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.8041406')
2023-11-23 06:38:33,804 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45907', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.8044593')
2023-11-23 06:38:33,804 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:33,804 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56948; closing.
2023-11-23 06:38:33,805 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.805318')
2023-11-23 06:38:33,805 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:33,806 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56994; closing.
2023-11-23 06:38:33,806 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721513.806564')
2023-11-23 06:38:33,806 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:38:33,807 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:35,611 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:38:35,611 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:38:35,612 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:38:35,613 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:38:35,614 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-11-23 06:38:37,810 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:37,814 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32991 instead
  warnings.warn(
2023-11-23 06:38:37,818 - distributed.scheduler - INFO - State start
2023-11-23 06:38:37,841 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:37,842 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:38:37,842 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32991/status
2023-11-23 06:38:37,843 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:37,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41803'
2023-11-23 06:38:37,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45733'
2023-11-23 06:38:37,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34811'
2023-11-23 06:38:37,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42527'
2023-11-23 06:38:37,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36539'
2023-11-23 06:38:37,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33605'
2023-11-23 06:38:38,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37883'
2023-11-23 06:38:38,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42107'
2023-11-23 06:38:39,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,847 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,850 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,851 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,893 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:39,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:39,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:39,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:42,076 - distributed.scheduler - INFO - Receive client connection: Client-edc44953-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:42,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43338
2023-11-23 06:38:42,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42495
2023-11-23 06:38:42,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42495
2023-11-23 06:38:42,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43889
2023-11-23 06:38:42,733 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,733 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,733 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bi5ajyof
2023-11-23 06:38:42,733 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45265
2023-11-23 06:38:42,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45265
2023-11-23 06:38:42,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43501
2023-11-23 06:38:42,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a9d4e2d-55c4-4f92-8e7f-961056636d33
2023-11-23 06:38:42,734 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,734 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4eb2w2kl
2023-11-23 06:38:42,734 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2117ab0-2be4-46f2-afc3-fd590957a65b
2023-11-23 06:38:42,735 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7a0fdf8-7326-4d9f-8ec8-2814d9038748
2023-11-23 06:38:42,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43777
2023-11-23 06:38:42,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43777
2023-11-23 06:38:42,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39423
2023-11-23 06:38:42,741 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,741 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,741 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,741 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,741 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nhw9pu94
2023-11-23 06:38:42,741 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34109
2023-11-23 06:38:42,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34109
2023-11-23 06:38:42,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35111
2023-11-23 06:38:42,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,742 - distributed.worker - INFO - Starting Worker plugin PreImport-fabde539-1f49-4719-851b-5f63bc246a18
2023-11-23 06:38:42,742 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,742 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7aafa0a-2ee6-47a4-b5f7-4ea222b4baa6
2023-11-23 06:38:42,742 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,742 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tmtvifc_
2023-11-23 06:38:42,742 - distributed.worker - INFO - Starting Worker plugin PreImport-2aee92f6-8c51-4951-8912-1fff0caa2926
2023-11-23 06:38:42,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-803c354c-b8a3-4c7e-8a40-7e449e62e322
2023-11-23 06:38:42,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40059
2023-11-23 06:38:42,743 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b822148d-9222-4d8b-ac0c-7c5d7ddee894
2023-11-23 06:38:42,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40059
2023-11-23 06:38:42,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37889
2023-11-23 06:38:42,743 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,743 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,743 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,743 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dfdinlyn
2023-11-23 06:38:42,744 - distributed.worker - INFO - Starting Worker plugin PreImport-4f42cdbc-0535-4244-9a8d-d70c1d20a081
2023-11-23 06:38:42,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-adb1ba36-7ad2-48ff-a95f-d3839cffef0f
2023-11-23 06:38:42,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76529af0-eb1c-40a6-868c-c4f397bef8c6
2023-11-23 06:38:42,964 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36983
2023-11-23 06:38:42,964 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36983
2023-11-23 06:38:42,965 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37203
2023-11-23 06:38:42,965 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,965 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,965 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,965 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,965 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j685oc0g
2023-11-23 06:38:42,964 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36487
2023-11-23 06:38:42,965 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36487
2023-11-23 06:38:42,965 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46503
2023-11-23 06:38:42,965 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,965 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2c3c365-a857-4c1f-8d4b-daf00b2c8869
2023-11-23 06:38:42,965 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,966 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d1i709zf
2023-11-23 06:38:42,966 - distributed.worker - INFO - Starting Worker plugin RMMSetup-deadb8d8-d2c3-4525-b5cf-3ff69d99eaf1
2023-11-23 06:38:42,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45427
2023-11-23 06:38:42,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45427
2023-11-23 06:38:42,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43125
2023-11-23 06:38:42,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:42,967 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:42,967 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:42,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:42,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ercnnpcj
2023-11-23 06:38:42,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-774d9ea2-33eb-48b3-b15f-3b96336faee5
2023-11-23 06:38:42,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acca9805-dc80-4f35-af8f-44056e6fc02b
2023-11-23 06:38:43,132 - distributed.worker - INFO - Starting Worker plugin PreImport-9ed3716d-185f-4228-8c41-a79bf4e4e1d3
2023-11-23 06:38:43,133 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,135 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dd9a05a-2ed3-465d-b770-a162ecb0fb97
2023-11-23 06:38:43,135 - distributed.worker - INFO - Starting Worker plugin PreImport-a821bb72-8658-47b5-9043-e47ec941d87e
2023-11-23 06:38:43,136 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,145 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,156 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6008cf4a-1519-4e03-a9ba-f61de6f00634
2023-11-23 06:38:43,162 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,165 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45265', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,166 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45265
2023-11-23 06:38:43,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43366
2023-11-23 06:38:43,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,168 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42495', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,168 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42495
2023-11-23 06:38:43,168 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43360
2023-11-23 06:38:43,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31ddf541-211f-45ad-bccc-7d002b1e332c
2023-11-23 06:38:43,170 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,170 - distributed.worker - INFO - Starting Worker plugin PreImport-83f174af-b456-489b-a631-a0c7e05cba73
2023-11-23 06:38:43,171 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,171 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,171 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,171 - distributed.worker - INFO - Starting Worker plugin PreImport-909c66eb-e11d-41c2-bf21-571c0ea95203
2023-11-23 06:38:43,171 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,172 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8297d2ab-d4c5-497f-ae05-20486b713d5f
2023-11-23 06:38:43,174 - distributed.worker - INFO - Starting Worker plugin PreImport-b9ea2664-ac78-4d85-a309-58e3301cc46a
2023-11-23 06:38:43,174 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,176 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40059', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,176 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,176 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40059
2023-11-23 06:38:43,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43378
2023-11-23 06:38:43,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,181 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,181 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,189 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34109', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,190 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34109
2023-11-23 06:38:43,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43386
2023-11-23 06:38:43,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,194 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45427', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45427
2023-11-23 06:38:43,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43400
2023-11-23 06:38:43,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,200 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,200 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,201 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43777', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,202 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43777
2023-11-23 06:38:43,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43398
2023-11-23 06:38:43,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,210 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,210 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36487', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36487
2023-11-23 06:38:43,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43414
2023-11-23 06:38:43,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36983', status: init, memory: 0, processing: 0>
2023-11-23 06:38:43,214 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,214 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36983
2023-11-23 06:38:43,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43426
2023-11-23 06:38:43,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:43,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:43,216 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:43,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:43,326 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,327 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,338 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:38:43,345 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,347 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:38:43,349 - distributed.scheduler - INFO - Remove client Client-edc44953-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:43,350 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43338; closing.
2023-11-23 06:38:43,350 - distributed.scheduler - INFO - Remove client Client-edc44953-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:43,350 - distributed.scheduler - INFO - Close client connection: Client-edc44953-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:43,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41803'. Reason: nanny-close
2023-11-23 06:38:43,351 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,352 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45733'. Reason: nanny-close
2023-11-23 06:38:43,353 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,353 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43777. Reason: nanny-close
2023-11-23 06:38:43,353 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34811'. Reason: nanny-close
2023-11-23 06:38:43,353 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,353 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36487. Reason: nanny-close
2023-11-23 06:38:43,354 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42527'. Reason: nanny-close
2023-11-23 06:38:43,354 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,354 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40059. Reason: nanny-close
2023-11-23 06:38:43,354 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36539'. Reason: nanny-close
2023-11-23 06:38:43,354 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,355 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34109. Reason: nanny-close
2023-11-23 06:38:43,355 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,355 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43398; closing.
2023-11-23 06:38:43,355 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33605'. Reason: nanny-close
2023-11-23 06:38:43,355 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,355 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42495. Reason: nanny-close
2023-11-23 06:38:43,355 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.3559148')
2023-11-23 06:38:43,356 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37883'. Reason: nanny-close
2023-11-23 06:38:43,356 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,356 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,356 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,356 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43378; closing.
2023-11-23 06:38:43,356 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36983. Reason: nanny-close
2023-11-23 06:38:43,356 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42107'. Reason: nanny-close
2023-11-23 06:38:43,356 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:43,356 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,356 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45427. Reason: nanny-close
2023-11-23 06:38:43,357 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40059', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.356948')
2023-11-23 06:38:43,357 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,357 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45265. Reason: nanny-close
2023-11-23 06:38:43,357 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,357 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,358 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,358 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43414; closing.
2023-11-23 06:38:43,358 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,358 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,358 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,359 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:43,359 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,358 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43378>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-23 06:38:43,360 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43386; closing.
2023-11-23 06:38:43,360 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,360 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36487', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.3605137')
2023-11-23 06:38:43,360 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,360 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:43,361 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34109', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.3613796')
2023-11-23 06:38:43,361 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43360; closing.
2023-11-23 06:38:43,362 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43426; closing.
2023-11-23 06:38:43,362 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.362705')
2023-11-23 06:38:43,363 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.3630679')
2023-11-23 06:38:43,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43400; closing.
2023-11-23 06:38:43,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43366; closing.
2023-11-23 06:38:43,364 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.364033')
2023-11-23 06:38:43,364 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721523.3645031')
2023-11-23 06:38:43,364 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:38:45,120 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:38:45,120 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:38:45,121 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:38:45,122 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:38:45,122 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-11-23 06:38:47,399 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:47,403 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45871 instead
  warnings.warn(
2023-11-23 06:38:47,407 - distributed.scheduler - INFO - State start
2023-11-23 06:38:47,428 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:47,429 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:38:47,430 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45871/status
2023-11-23 06:38:47,430 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:47,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37723'
2023-11-23 06:38:47,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45665'
2023-11-23 06:38:47,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36863'
2023-11-23 06:38:47,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40999'
2023-11-23 06:38:47,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37729'
2023-11-23 06:38:47,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44265'
2023-11-23 06:38:47,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41041'
2023-11-23 06:38:47,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45161'
2023-11-23 06:38:48,197 - distributed.scheduler - INFO - Receive client connection: Client-f376db01-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:48,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43568
2023-11-23 06:38:49,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,525 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,526 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,531 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,538 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,543 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,545 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:49,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:49,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:49,580 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:38:52,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43329
2023-11-23 06:38:52,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43329
2023-11-23 06:38:52,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36707
2023-11-23 06:38:52,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:52,758 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:52,758 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:52,758 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:52,759 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gy1trzrn
2023-11-23 06:38:52,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5f67171-ed27-4d43-8e42-06f78122faa8
2023-11-23 06:38:52,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1f5d61b-145d-4242-a7e4-dcdc80407af5
2023-11-23 06:38:52,778 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33643
2023-11-23 06:38:52,779 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33643
2023-11-23 06:38:52,779 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35473
2023-11-23 06:38:52,779 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:52,779 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:52,779 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:52,779 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:52,779 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j8zoet6w
2023-11-23 06:38:52,780 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a785c6a6-bd7f-4a52-be49-03381dda7b27
2023-11-23 06:38:53,166 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38735
2023-11-23 06:38:53,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38735
2023-11-23 06:38:53,167 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45415
2023-11-23 06:38:53,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,167 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,167 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:53,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:53,168 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-27xlmw_l
2023-11-23 06:38:53,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eece5824-7797-45d1-9bc1-8e5100a2ea30
2023-11-23 06:38:53,176 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38259
2023-11-23 06:38:53,177 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38259
2023-11-23 06:38:53,177 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34571
2023-11-23 06:38:53,177 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,177 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,177 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:53,178 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:53,178 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s_0pe6sl
2023-11-23 06:38:53,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15d98f60-4c4e-478d-958c-c8618dd775f7
2023-11-23 06:38:53,180 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34573
2023-11-23 06:38:53,180 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34573
2023-11-23 06:38:53,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39489
2023-11-23 06:38:53,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,181 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,181 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:53,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:53,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dbz24o9k
2023-11-23 06:38:53,181 - distributed.worker - INFO - Starting Worker plugin PreImport-c8f5935d-b287-4e91-9525-ea8e0beea02a
2023-11-23 06:38:53,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fff92a4-2167-4a89-aee2-1c84000824a8
2023-11-23 06:38:53,183 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41825
2023-11-23 06:38:53,184 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41825
2023-11-23 06:38:53,184 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46351
2023-11-23 06:38:53,184 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,184 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,184 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:53,184 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:53,184 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iu95mmip
2023-11-23 06:38:53,184 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34817
2023-11-23 06:38:53,185 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34817
2023-11-23 06:38:53,185 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37593
2023-11-23 06:38:53,185 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06adaeac-fa85-4cd9-a5b9-8bbda63ba606
2023-11-23 06:38:53,185 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,185 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:53,185 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:53,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ntzv2zi_
2023-11-23 06:38:53,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a104b3b-53b3-4dd1-b6b0-574f129acecb
2023-11-23 06:38:53,188 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37491
2023-11-23 06:38:53,189 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37491
2023-11-23 06:38:53,189 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45297
2023-11-23 06:38:53,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,189 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,189 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:38:53,190 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:38:53,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-13qwi6qi
2023-11-23 06:38:53,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bde8a1a-5c30-4e8c-bb8d-074b9bbee38c
2023-11-23 06:38:53,345 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-771ff448-ec68-4666-867d-669c25d696e7
2023-11-23 06:38:53,345 - distributed.worker - INFO - Starting Worker plugin PreImport-d4a5b822-d84c-4808-96f3-c47d714073a0
2023-11-23 06:38:53,346 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,354 - distributed.worker - INFO - Starting Worker plugin PreImport-ac0649b3-6b6a-4f73-8e27-d8574bab7c20
2023-11-23 06:38:53,354 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,387 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43329', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,389 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43329
2023-11-23 06:38:53,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48958
2023-11-23 06:38:53,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,391 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,391 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,396 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,401 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40144e3d-0efe-4307-ae6a-917b4c465c4d
2023-11-23 06:38:53,402 - distributed.worker - INFO - Starting Worker plugin PreImport-fae284ad-196a-4d51-95f1-4d47c2995e4c
2023-11-23 06:38:53,402 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6a3150b-3b5f-4624-9d3f-6a5abe7a0f72
2023-11-23 06:38:53,407 - distributed.worker - INFO - Starting Worker plugin PreImport-baf6ee1e-2408-4252-831b-481e3ef09e44
2023-11-23 06:38:53,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b96f9bc-06c4-409a-a4a9-61c4b651fd16
2023-11-23 06:38:53,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdbe397b-ecfe-4818-977f-3623cbfc78c6
2023-11-23 06:38:53,407 - distributed.worker - INFO - Starting Worker plugin PreImport-510a7c78-a511-4215-9b79-3017804a3267
2023-11-23 06:38:53,407 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,407 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,408 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2968c92-e0cd-4d86-8483-8a47dbb01b5c
2023-11-23 06:38:53,409 - distributed.worker - INFO - Starting Worker plugin PreImport-9070c9df-a308-4b9e-aa9c-87290254566c
2023-11-23 06:38:53,409 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33643', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,412 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33643
2023-11-23 06:38:53,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48970
2023-11-23 06:38:53,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d948c6e-f80c-4de4-8598-c54b98257a40
2023-11-23 06:38:53,415 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,415 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,415 - distributed.worker - INFO - Starting Worker plugin PreImport-ae9bc4a6-732f-4683-9f81-d3e58df4dd4a
2023-11-23 06:38:53,416 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41825', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,441 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41825
2023-11-23 06:38:53,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49000
2023-11-23 06:38:53,442 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38259', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,442 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38259
2023-11-23 06:38:53,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48998
2023-11-23 06:38:53,443 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,443 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,443 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34817', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,444 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34817
2023-11-23 06:38:53,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49012
2023-11-23 06:38:53,444 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,444 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,445 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,445 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,447 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,449 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38735', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,450 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38735
2023-11-23 06:38:53,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48986
2023-11-23 06:38:53,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,452 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,452 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37491', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,458 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37491
2023-11-23 06:38:53,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49014
2023-11-23 06:38:53,459 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34573', status: init, memory: 0, processing: 0>
2023-11-23 06:38:53,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34573
2023-11-23 06:38:53,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49002
2023-11-23 06:38:53,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,461 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:38:53,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,463 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:38:53,463 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:38:53,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,542 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:38:53,547 - distributed.scheduler - INFO - Remove client Client-f376db01-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:53,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43568; closing.
2023-11-23 06:38:53,548 - distributed.scheduler - INFO - Remove client Client-f376db01-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:53,548 - distributed.scheduler - INFO - Close client connection: Client-f376db01-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:53,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37723'. Reason: nanny-close
2023-11-23 06:38:53,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,550 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45665'. Reason: nanny-close
2023-11-23 06:38:53,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38259. Reason: nanny-close
2023-11-23 06:38:53,550 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36863'. Reason: nanny-close
2023-11-23 06:38:53,551 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41825. Reason: nanny-close
2023-11-23 06:38:53,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40999'. Reason: nanny-close
2023-11-23 06:38:53,552 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,552 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33643. Reason: nanny-close
2023-11-23 06:38:53,552 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37729'. Reason: nanny-close
2023-11-23 06:38:53,552 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,552 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48998; closing.
2023-11-23 06:38:53,552 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34573. Reason: nanny-close
2023-11-23 06:38:53,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.5529227')
2023-11-23 06:38:53,553 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44265'. Reason: nanny-close
2023-11-23 06:38:53,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34817. Reason: nanny-close
2023-11-23 06:38:53,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,553 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,553 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41041'. Reason: nanny-close
2023-11-23 06:38:53,554 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,554 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43329. Reason: nanny-close
2023-11-23 06:38:53,554 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,554 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45161'. Reason: nanny-close
2023-11-23 06:38:53,554 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:38:53,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49000; closing.
2023-11-23 06:38:53,554 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,555 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,555 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38735. Reason: nanny-close
2023-11-23 06:38:53,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.5553484')
2023-11-23 06:38:53,555 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,555 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48970; closing.
2023-11-23 06:38:53,555 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37491. Reason: nanny-close
2023-11-23 06:38:53,556 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.5562081')
2023-11-23 06:38:53,556 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,556 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,557 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,557 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,557 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49002; closing.
2023-11-23 06:38:53,557 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49012; closing.
2023-11-23 06:38:53,557 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,557 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.5579286')
2023-11-23 06:38:53,558 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34817', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.558306')
2023-11-23 06:38:53,558 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:38:53,558 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48958; closing.
2023-11-23 06:38:53,559 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43329', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.5593607')
2023-11-23 06:38:53,559 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,559 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48986; closing.
2023-11-23 06:38:53,560 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49014; closing.
2023-11-23 06:38:53,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.560527')
2023-11-23 06:38:53,560 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,560 - distributed.nanny - INFO - Worker closed
2023-11-23 06:38:53,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721533.5608609')
2023-11-23 06:38:53,561 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:38:55,169 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:38:55,170 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:38:55,171 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:38:55,172 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:38:55,172 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-11-23 06:38:57,500 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:57,504 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35699 instead
  warnings.warn(
2023-11-23 06:38:57,508 - distributed.scheduler - INFO - State start
2023-11-23 06:38:57,532 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:38:57,533 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:38:57,534 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35699/status
2023-11-23 06:38:57,534 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:38:57,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39251'
2023-11-23 06:38:58,582 - distributed.scheduler - INFO - Receive client connection: Client-f97fb6c8-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:38:58,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49134
2023-11-23 06:38:59,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:38:59,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:38:59,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:01,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35133
2023-11-23 06:39:01,064 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35133
2023-11-23 06:39:01,064 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-11-23 06:39:01,064 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:01,064 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:01,065 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:01,065 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-23 06:39:01,065 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ssphaac0
2023-11-23 06:39:01,065 - distributed.worker - INFO - Starting Worker plugin PreImport-68e79aeb-87f7-469c-ba2a-f0d78d8bd1a7
2023-11-23 06:39:01,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6040a86-7a9d-4cf1-b68e-2846773da4be
2023-11-23 06:39:01,065 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b446096b-636d-49e4-900d-ea4c4f139c94
2023-11-23 06:39:01,066 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:01,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35133', status: init, memory: 0, processing: 0>
2023-11-23 06:39:01,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35133
2023-11-23 06:39:01,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57670
2023-11-23 06:39:01,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:01,101 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:01,101 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:01,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:01,118 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:01,121 - distributed.scheduler - INFO - Remove client Client-f97fb6c8-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:01,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49134; closing.
2023-11-23 06:39:01,121 - distributed.scheduler - INFO - Remove client Client-f97fb6c8-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:01,121 - distributed.scheduler - INFO - Close client connection: Client-f97fb6c8-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:01,122 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39251'. Reason: nanny-close
2023-11-23 06:39:01,123 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:01,124 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35133. Reason: nanny-close
2023-11-23 06:39:01,126 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:01,126 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57670; closing.
2023-11-23 06:39:01,126 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35133', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721541.126687')
2023-11-23 06:39:01,127 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:39:01,127 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:02,439 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:02,440 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:02,440 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:02,441 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:39:02,442 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-11-23 06:39:07,055 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:07,060 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35393 instead
  warnings.warn(
2023-11-23 06:39:07,064 - distributed.scheduler - INFO - State start
2023-11-23 06:39:07,153 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:07,154 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:39:07,155 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35393/status
2023-11-23 06:39:07,155 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:39:07,368 - distributed.scheduler - INFO - Receive client connection: Client-ff3d3e94-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:07,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57748
2023-11-23 06:39:07,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40295'
2023-11-23 06:39:09,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:09,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:10,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:11,608 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44219
2023-11-23 06:39:11,609 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44219
2023-11-23 06:39:11,609 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39555
2023-11-23 06:39:11,609 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:11,609 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:11,609 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:11,609 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-23 06:39:11,609 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2bi4v6b2
2023-11-23 06:39:11,610 - distributed.worker - INFO - Starting Worker plugin PreImport-62be5003-6887-47bf-b08a-ee3f6c2d80a5
2023-11-23 06:39:11,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de41d2a8-d15a-4299-acb0-6fce103933b4
2023-11-23 06:39:11,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1568466-34ad-4ac8-99c6-6176849b17d6
2023-11-23 06:39:11,612 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:11,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44219', status: init, memory: 0, processing: 0>
2023-11-23 06:39:11,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44219
2023-11-23 06:39:11,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34636
2023-11-23 06:39:11,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:11,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:11,658 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:11,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:11,673 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:11,676 - distributed.scheduler - INFO - Remove client Client-ff3d3e94-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:11,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57748; closing.
2023-11-23 06:39:11,677 - distributed.scheduler - INFO - Remove client Client-ff3d3e94-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:11,677 - distributed.scheduler - INFO - Close client connection: Client-ff3d3e94-89ca-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:11,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40295'. Reason: nanny-close
2023-11-23 06:39:11,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:11,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44219. Reason: nanny-close
2023-11-23 06:39:11,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:11,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34636; closing.
2023-11-23 06:39:11,686 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721551.6860855')
2023-11-23 06:39:11,686 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:39:11,687 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:13,096 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:13,097 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:13,097 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:13,098 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:39:13,099 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-11-23 06:39:15,301 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:15,305 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38073 instead
  warnings.warn(
2023-11-23 06:39:15,309 - distributed.scheduler - INFO - State start
2023-11-23 06:39:15,331 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:15,332 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:39:15,332 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38073/status
2023-11-23 06:39:15,332 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:39:21,436 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:34640'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34640>: Stream is closed
2023-11-23 06:39:21,795 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:21,796 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:21,796 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:21,797 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:39:21,797 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-11-23 06:39:24,216 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:24,220 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38705 instead
  warnings.warn(
2023-11-23 06:39:24,225 - distributed.scheduler - INFO - State start
2023-11-23 06:39:24,598 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:24,600 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-23 06:39:24,602 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38705/status
2023-11-23 06:39:24,602 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:39:24,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34703'
2023-11-23 06:39:25,133 - distributed.scheduler - INFO - Receive client connection: Client-0957390d-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:25,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35264
2023-11-23 06:39:26,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:26,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:26,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:28,155 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46875
2023-11-23 06:39:28,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46875
2023-11-23 06:39:28,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35253
2023-11-23 06:39:28,155 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-23 06:39:28,155 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:28,155 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:28,156 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-23 06:39:28,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-vk0p1h_m
2023-11-23 06:39:28,156 - distributed.worker - INFO - Starting Worker plugin RMMSetup-290fe9e8-0d70-41ed-884c-4854468490ef
2023-11-23 06:39:28,156 - distributed.worker - INFO - Starting Worker plugin PreImport-dc67c988-8253-4eef-a163-05bc5a8b4ef9
2023-11-23 06:39:28,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f20eb53-60d8-4c8b-b780-3d3fcf79d73f
2023-11-23 06:39:28,156 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:28,200 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46875', status: init, memory: 0, processing: 0>
2023-11-23 06:39:28,202 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46875
2023-11-23 06:39:28,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35286
2023-11-23 06:39:28,204 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:28,206 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-23 06:39:28,206 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:28,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-23 06:39:28,219 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:28,224 - distributed.scheduler - INFO - Remove client Client-0957390d-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:28,225 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35264; closing.
2023-11-23 06:39:28,225 - distributed.scheduler - INFO - Remove client Client-0957390d-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:28,226 - distributed.scheduler - INFO - Close client connection: Client-0957390d-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:28,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34703'. Reason: nanny-close
2023-11-23 06:39:28,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:28,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46875. Reason: nanny-close
2023-11-23 06:39:28,238 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-23 06:39:28,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35286; closing.
2023-11-23 06:39:28,239 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721568.239283')
2023-11-23 06:39:28,239 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:39:28,240 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:29,293 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:29,294 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:29,294 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:29,295 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-23 06:39:29,295 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-11-23 06:39:31,536 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:31,541 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40049 instead
  warnings.warn(
2023-11-23 06:39:31,545 - distributed.scheduler - INFO - State start
2023-11-23 06:39:31,622 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:31,624 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:39:31,624 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40049/status
2023-11-23 06:39:31,625 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:39:31,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41227'
2023-11-23 06:39:31,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40641'
2023-11-23 06:39:31,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33937'
2023-11-23 06:39:31,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43735'
2023-11-23 06:39:31,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42375'
2023-11-23 06:39:31,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41789'
2023-11-23 06:39:31,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34519'
2023-11-23 06:39:31,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45003'
2023-11-23 06:39:32,715 - distributed.scheduler - INFO - Receive client connection: Client-0dca6687-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:32,730 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49438
2023-11-23 06:39:33,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,818 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,856 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,856 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,860 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,871 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,871 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,874 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,874 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,891 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:33,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:33,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:33,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:37,412 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34229
2023-11-23 06:39:37,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34229
2023-11-23 06:39:37,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39471
2023-11-23 06:39:37,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:37,413 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:37,413 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:37,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:37,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wi2dzlv0
2023-11-23 06:39:37,414 - distributed.worker - INFO - Starting Worker plugin PreImport-ec82d0d6-e8f0-4e60-b6a3-73844b0e6118
2023-11-23 06:39:37,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f2540c6-9d33-4c3f-ab81-38ae56067f61
2023-11-23 06:39:37,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32969
2023-11-23 06:39:37,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32969
2023-11-23 06:39:37,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40531
2023-11-23 06:39:37,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:37,419 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:37,419 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:37,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:37,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kjcfzaso
2023-11-23 06:39:37,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60927cd4-532f-4b22-895b-01bc6a9098cc
2023-11-23 06:39:37,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35627
2023-11-23 06:39:37,426 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35627
2023-11-23 06:39:37,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43793
2023-11-23 06:39:37,427 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:37,427 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:37,427 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:37,427 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:37,427 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ih3hehf
2023-11-23 06:39:37,427 - distributed.worker - INFO - Starting Worker plugin PreImport-bff03a1a-1481-4ffc-833d-a7b756b1290a
2023-11-23 06:39:37,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3707f0d-cebd-4fcf-9b2f-ba820409172d
2023-11-23 06:39:37,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e903fb48-0003-416f-b958-88bbbced5e79
2023-11-23 06:39:38,009 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39823
2023-11-23 06:39:38,010 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39823
2023-11-23 06:39:38,010 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43073
2023-11-23 06:39:38,010 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,010 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,010 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:38,010 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:38,010 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pl89_eaf
2023-11-23 06:39:38,011 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f81f0f7c-eb90-49aa-af42-68231ec263fc
2023-11-23 06:39:38,015 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33071
2023-11-23 06:39:38,016 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33071
2023-11-23 06:39:38,016 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35035
2023-11-23 06:39:38,016 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,016 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,016 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:38,017 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:38,017 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l2jt9ys9
2023-11-23 06:39:38,017 - distributed.worker - INFO - Starting Worker plugin RMMSetup-679075d5-288f-4f55-8465-22d81f43af40
2023-11-23 06:39:38,018 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35515
2023-11-23 06:39:38,019 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35515
2023-11-23 06:39:38,019 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41153
2023-11-23 06:39:38,019 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,019 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,019 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:38,019 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:38,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5qredrvu
2023-11-23 06:39:38,020 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6975f4b-28ac-4bd1-887d-d88c40cce432
2023-11-23 06:39:38,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2794804c-9fee-4575-97a6-72f058b18949
2023-11-23 06:39:38,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37039
2023-11-23 06:39:38,021 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37039
2023-11-23 06:39:38,021 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35065
2023-11-23 06:39:38,021 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,021 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,021 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:38,021 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:38,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rtb9tj62
2023-11-23 06:39:38,021 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42957
2023-11-23 06:39:38,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42957
2023-11-23 06:39:38,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9716fecc-72d8-47be-99aa-4d80ebf7a5c6
2023-11-23 06:39:38,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35407
2023-11-23 06:39:38,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,022 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,022 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bccd2aa-6768-48de-a304-655c7a2a4f86
2023-11-23 06:39:38,022 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:38,022 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-23 06:39:38,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t1y1nr6c
2023-11-23 06:39:38,022 - distributed.worker - INFO - Starting Worker plugin PreImport-d88b19ae-c396-4a01-993c-e9bfd61334de
2023-11-23 06:39:38,023 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5889c561-3df2-492e-8351-f303247fd9db
2023-11-23 06:39:38,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6f6db52-2358-4e79-a65f-caa97a98ed4e
2023-11-23 06:39:38,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58778b5d-d2d7-41d5-b6b7-c31d78ad84a8
2023-11-23 06:39:38,204 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94ac87e1-6717-49f4-ad5a-1bf38e529c61
2023-11-23 06:39:38,206 - distributed.worker - INFO - Starting Worker plugin PreImport-1af880d8-1105-4cd4-a987-12922c069368
2023-11-23 06:39:38,207 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,226 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32969', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32969
2023-11-23 06:39:38,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49460
2023-11-23 06:39:38,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,257 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34229', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,258 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34229
2023-11-23 06:39:38,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49452
2023-11-23 06:39:38,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,261 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,261 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,263 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,267 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,267 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35627', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35627
2023-11-23 06:39:38,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49476
2023-11-23 06:39:38,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,276 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,276 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,317 - distributed.worker - INFO - Starting Worker plugin PreImport-3b10ccaa-2265-4cfb-9fdd-e80a54196864
2023-11-23 06:39:38,318 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,318 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6db5c3d-188d-4c7c-9c8f-a1a15b074a92
2023-11-23 06:39:38,329 - distributed.worker - INFO - Starting Worker plugin PreImport-e43355f0-8b81-4979-a566-11e4b080db20
2023-11-23 06:39:38,329 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,334 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07dd8d91-38a2-401d-9064-3a7b42207342
2023-11-23 06:39:38,334 - distributed.worker - INFO - Starting Worker plugin PreImport-09e7a0a4-8abc-4687-a86c-92882bf6f7c0
2023-11-23 06:39:38,335 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,336 - distributed.worker - INFO - Starting Worker plugin PreImport-66673757-1754-484f-bc59-90a7afd7590d
2023-11-23 06:39:38,336 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42957', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42957
2023-11-23 06:39:38,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49484
2023-11-23 06:39:38,352 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35515', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,352 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35515
2023-11-23 06:39:38,352 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,352 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49496
2023-11-23 06:39:38,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,355 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,355 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,365 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33071', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,366 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33071
2023-11-23 06:39:38,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49512
2023-11-23 06:39:38,370 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37039', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,370 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,371 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,371 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,371 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37039
2023-11-23 06:39:38,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49522
2023-11-23 06:39:38,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,373 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39823', status: init, memory: 0, processing: 0>
2023-11-23 06:39:38,374 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39823
2023-11-23 06:39:38,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49528
2023-11-23 06:39:38,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,376 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:38,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:38,382 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:38,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:38,475 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,475 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,476 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,476 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,476 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,476 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,476 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,476 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-23 06:39:38,495 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,495 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,496 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:38,502 - distributed.scheduler - INFO - Remove client Client-0dca6687-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:38,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49438; closing.
2023-11-23 06:39:38,503 - distributed.scheduler - INFO - Remove client Client-0dca6687-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:38,504 - distributed.scheduler - INFO - Close client connection: Client-0dca6687-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:38,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41227'. Reason: nanny-close
2023-11-23 06:39:38,505 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40641'. Reason: nanny-close
2023-11-23 06:39:38,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,506 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34229. Reason: nanny-close
2023-11-23 06:39:38,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33937'. Reason: nanny-close
2023-11-23 06:39:38,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32969. Reason: nanny-close
2023-11-23 06:39:38,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43735'. Reason: nanny-close
2023-11-23 06:39:38,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35627. Reason: nanny-close
2023-11-23 06:39:38,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42375'. Reason: nanny-close
2023-11-23 06:39:38,508 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42957. Reason: nanny-close
2023-11-23 06:39:38,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41789'. Reason: nanny-close
2023-11-23 06:39:38,509 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49452; closing.
2023-11-23 06:39:38,509 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35515. Reason: nanny-close
2023-11-23 06:39:38,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49476; closing.
2023-11-23 06:39:38,509 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34519'. Reason: nanny-close
2023-11-23 06:39:38,509 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,509 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39823. Reason: nanny-close
2023-11-23 06:39:38,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34229', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5098987')
2023-11-23 06:39:38,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,510 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45003'. Reason: nanny-close
2023-11-23 06:39:38,510 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:38,510 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37039. Reason: nanny-close
2023-11-23 06:39:38,510 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,511 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,511 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35627', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5109818')
2023-11-23 06:39:38,511 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33071. Reason: nanny-close
2023-11-23 06:39:38,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,511 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,511 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,512 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49460; closing.
2023-11-23 06:39:38,513 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:38,513 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,513 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,513 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49484; closing.
2023-11-23 06:39:38,514 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,514 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5141294')
2023-11-23 06:39:38,514 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:38,515 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42957', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5158138')
2023-11-23 06:39:38,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49496; closing.
2023-11-23 06:39:38,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49528; closing.
2023-11-23 06:39:38,517 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35515', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5177643')
2023-11-23 06:39:38,518 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39823', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5183747')
2023-11-23 06:39:38,519 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49522; closing.
2023-11-23 06:39:38,519 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49512; closing.
2023-11-23 06:39:38,519 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5198264')
2023-11-23 06:39:38,520 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721578.5204217')
2023-11-23 06:39:38,520 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:39:40,322 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:40,323 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:40,323 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:40,325 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:39:40,325 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-11-23 06:39:42,838 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:42,843 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33737 instead
  warnings.warn(
2023-11-23 06:39:42,847 - distributed.scheduler - INFO - State start
2023-11-23 06:39:42,870 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:42,871 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:39:42,872 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33737/status
2023-11-23 06:39:42,872 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:39:42,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40445'
2023-11-23 06:39:43,533 - distributed.scheduler - INFO - Receive client connection: Client-1472c065-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:43,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60718
2023-11-23 06:39:44,907 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:44,907 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:44,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:46,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46659
2023-11-23 06:39:46,317 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46659
2023-11-23 06:39:46,317 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45139
2023-11-23 06:39:46,317 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:46,317 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:46,317 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:46,317 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-23 06:39:46,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-40qwuyyk
2023-11-23 06:39:46,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77cc9662-aa8f-41d8-8ab7-579d05954e81
2023-11-23 06:39:46,413 - distributed.worker - INFO - Starting Worker plugin PreImport-6375dda1-f1dd-4e63-b99d-969b4fc1ffee
2023-11-23 06:39:46,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b59feca-16d8-4d05-bf58-6b1a0feabbc8
2023-11-23 06:39:46,413 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:46,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46659', status: init, memory: 0, processing: 0>
2023-11-23 06:39:46,441 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46659
2023-11-23 06:39:46,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60740
2023-11-23 06:39:46,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:46,443 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:46,443 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:46,447 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:46,516 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:39:46,520 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:46,521 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:46,524 - distributed.scheduler - INFO - Remove client Client-1472c065-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:46,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60718; closing.
2023-11-23 06:39:46,524 - distributed.scheduler - INFO - Remove client Client-1472c065-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:46,524 - distributed.scheduler - INFO - Close client connection: Client-1472c065-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:46,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40445'. Reason: nanny-close
2023-11-23 06:39:46,526 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:46,527 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46659. Reason: nanny-close
2023-11-23 06:39:46,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:46,529 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60740; closing.
2023-11-23 06:39:46,529 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721586.5296729')
2023-11-23 06:39:46,529 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:39:46,530 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:48,144 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:48,144 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:48,145 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:48,146 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:39:48,146 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-11-23 06:39:50,548 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:50,552 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42915 instead
  warnings.warn(
2023-11-23 06:39:50,557 - distributed.scheduler - INFO - State start
2023-11-23 06:39:50,578 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-23 06:39:50,579 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-23 06:39:50,580 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42915/status
2023-11-23 06:39:50,580 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-23 06:39:50,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35255'
2023-11-23 06:39:50,915 - distributed.scheduler - INFO - Receive client connection: Client-1915f9b6-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:50,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46326
2023-11-23 06:39:52,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-23 06:39:52,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-23 06:39:52,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-23 06:39:53,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43079
2023-11-23 06:39:53,379 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43079
2023-11-23 06:39:53,379 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35257
2023-11-23 06:39:53,379 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-23 06:39:53,379 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:53,379 - distributed.worker - INFO -               Threads:                          1
2023-11-23 06:39:53,380 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-23 06:39:53,380 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_i3tultj
2023-11-23 06:39:53,380 - distributed.worker - INFO - Starting Worker plugin RMMSetup-110c6df0-dd24-4188-bd24-101ad8ecf6aa
2023-11-23 06:39:53,481 - distributed.worker - INFO - Starting Worker plugin PreImport-4a944d43-b3d3-4148-9217-8994e263ae0e
2023-11-23 06:39:53,482 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8066b4a-f549-4274-b065-bce7e115529e
2023-11-23 06:39:53,482 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:53,506 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43079', status: init, memory: 0, processing: 0>
2023-11-23 06:39:53,507 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43079
2023-11-23 06:39:53,507 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46350
2023-11-23 06:39:53,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-23 06:39:53,509 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-23 06:39:53,509 - distributed.worker - INFO - -------------------------------------------------
2023-11-23 06:39:53,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-23 06:39:53,590 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-11-23 06:39:53,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-23 06:39:53,599 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:53,601 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-23 06:39:53,603 - distributed.scheduler - INFO - Remove client Client-1915f9b6-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:53,604 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46326; closing.
2023-11-23 06:39:53,604 - distributed.scheduler - INFO - Remove client Client-1915f9b6-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:53,604 - distributed.scheduler - INFO - Close client connection: Client-1915f9b6-89cb-11ee-b3e0-d8c49764f6bb
2023-11-23 06:39:53,605 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35255'. Reason: nanny-close
2023-11-23 06:39:53,606 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-23 06:39:53,607 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43079. Reason: nanny-close
2023-11-23 06:39:53,608 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-23 06:39:53,609 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46350; closing.
2023-11-23 06:39:53,609 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43079', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700721593.6092436')
2023-11-23 06:39:53,609 - distributed.scheduler - INFO - Lost all workers
2023-11-23 06:39:53,610 - distributed.nanny - INFO - Worker closed
2023-11-23 06:39:54,621 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-23 06:39:54,622 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-23 06:39:54,622 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-23 06:39:54,623 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-23 06:39:54,623 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35505 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38261 instead
  warnings.warn(
2023-11-23 06:40:25,092 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-11-23 06:40:25,094 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-11-23 06:40:25,170 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:58293', name: 4, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-11-23 06:40:25,171 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:41899', name: 7, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35351 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36415 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42987 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36833 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44791 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44195 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45159 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44599 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38545 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39661 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46677 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37515 instead
  warnings.warn(
[1700721893.984227] [dgx13:71023:0]            sock.c:470  UCX  ERROR bind(fd=137 addr=0.0.0.0:44680) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40661 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36223 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41545 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39947 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33047 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35169 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40713 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35063 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42403 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35873 instead
  warnings.warn(
[1700722170.116380] [dgx13:76138:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:56973) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34969 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45347 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34907 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45893 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41653 instead
  warnings.warn(
2023-11-23 06:51:15,279 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-11-23 06:51:15,283 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:35435', name: 3, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34195 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42807 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43377 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40759 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36045 instead
  warnings.warn(
[1700722316.782858] [dgx13:77729:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:39578) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36201 instead
  warnings.warn(
2023-11-23 06:52:21,154 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-23 06:52:21,355 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-23 06:52:21,381 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:46267'.
2023-11-23 06:52:21,382 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:46267'. Shutting down.
2023-11-23 06:52:21,409 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff74ec8beb0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-23 06:52:23,412 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
