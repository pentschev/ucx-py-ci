[dgx13:77969:0:77969] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  77969) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fcc55efd07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fcc55efd274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fcc55efd43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fccfd281420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fcc55f7c6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fcc55fa5839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fcc55eb73df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fcc55eba838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fcc55f064a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fcc55eb95dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fcc55f798da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fcc5c06306a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x556c2a0116fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556c2a00d094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556c2a01e519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x556c2a0c1162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fccf329f1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x556c2a01677c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x556c29fc8d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x556c2a0157f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x556c2a013929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556c2a00d094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556c2a01e519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x556c2a00f128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556c2a00d094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x556c2a02bccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x556c2a02c44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x556c2a0ef10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x556c2a01677c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x556c2a0116fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x556c2a02bdac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x556c2a0116fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556c2a00d094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556c2a01e519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556c2a00e5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x556c2a01e7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x556c2a00e312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556c2a00d094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556c2a01e519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x556c2a00f128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556c2a00d094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x556c2a00cd68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556c2a00cd19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x556c2a0ba07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x556c2a0e6fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x556c2a0e3353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x556c2a0db16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x556c2a0db05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x556c2a0da297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x556c2a0adf07]
=================================
2023-11-21 07:40:24,012 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35231 -> ucx://127.0.0.1:42681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fe2c8417100, tag: 0x7622726e57cd9364, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
[dgx13:77975:0:77975] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  77975) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fb1b38a607d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fb1b38a6274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fb1b38a643a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fb258bd3420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fb1b39256b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fb1b394e839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fb1b38603df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fb1b3863838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fb1b38af4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fb1b38625dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fb1b39228da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fb1b39db06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x559f16c8a6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559f16c86094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559f16c97519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x559f16ca4e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x559f16dafb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x559f16c41d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x559f16c8e7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x559f16c8c929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559f16c86094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559f16c97519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x559f16c88128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559f16c86094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x559f16ca4ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x559f16ca544c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x559f16d6810e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x559f16c8f77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x559f16c8a6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x559f16ca4dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x559f16c8a6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559f16c86094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559f16c97519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x559f16c875c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x559f16c977c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x559f16c87312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559f16c86094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559f16c97519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x559f16c88128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559f16c86094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x559f16c85d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x559f16c85d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x559f16d3307b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x559f16d5ffca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x559f16d5c353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x559f16d5416a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x559f16d5405c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x559f16d53297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x559f16d26f07]
=================================
[dgx13:77978:0:77978] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  77978) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fe2c86ef07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fe2c86ef274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fe2c86ef43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fe369a42420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fe2c876e6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fe2c8797839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fe2c86a93df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fe2c86ac838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe2c86f84a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fe2c86ab5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fe2c876b8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fe2c882406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55a62ce376fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a62ce33094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a62ce44519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55a62cee7162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fe35fa5f1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55a62ce3c77c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55a62cdeed05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55a62ce3b7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55a62ce39929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a62ce33094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a62ce44519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55a62ce35128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a62ce33094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55a62ce51ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55a62ce5244c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55a62cf1510e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55a62ce3c77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55a62ce376fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55a62ce51dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55a62ce376fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a62ce33094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a62ce44519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a62ce345c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a62ce447c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55a62ce34312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a62ce33094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a62ce44519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55a62ce35128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a62ce33094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55a62ce32d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55a62ce32d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55a62cee007b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55a62cf0cfca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55a62cf09353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55a62cf0116a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55a62cf0105c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55a62cf00297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55a62ced3f07]
=================================
[dgx13:77981:0:77981] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  77981) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fe12772f07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fe12772f274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fe12772f43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fe1cca68420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fe1277ae6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fe1277d7839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fe1276e93df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fe1276ec838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe1277384a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fe1276eb5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fe1277ab8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fe12786406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55a55d35c6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a55d358094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a55d369519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x55a55d40c162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fe1c2a9f1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55a55d36177c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55a55d313d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55a55d3607f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55a55d35e929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a55d358094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a55d369519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55a55d35a128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a55d358094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55a55d376ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55a55d37744c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55a55d43a10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55a55d36177c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55a55d35c6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55a55d376dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55a55d35c6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a55d358094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a55d369519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55a55d3595c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55a55d3697c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55a55d359312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a55d358094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55a55d369519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55a55d35a128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55a55d358094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55a55d357d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55a55d357d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55a55d40507b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55a55d431fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55a55d42e353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55a55d42616a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55a55d42605c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55a55d425297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55a55d3f8f07]
=================================
Task exception was never retrieved
future: <Task finished name='Task-1199' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1328' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-11-21 07:40:26,734 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:50687 -> ucx://127.0.0.1:56959
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f028d15a180, tag: 0x37b0e72af48b9afd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-21 07:40:26,738 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56959
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-11-21 07:40:26,738 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56959
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-11-21 07:40:26,814 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53359 -> ucx://127.0.0.1:35231
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7ff2fd323100, tag: 0x5206358d1dbc5196, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-21 07:40:26,792 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35231
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-11-21 07:40:26,793 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35231
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-11-21 07:40:26,837 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53359 -> ucx://127.0.0.1:34947
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7ff2fd323180, tag: 0x2bda059cc1ed082a, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-21 07:40:26,870 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34947
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f4a21576140, tag: 0x5d15ddac7ad95953, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f4a21576140, tag: 0x5d15ddac7ad95953, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-21 07:40:26,870 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:50687 -> ucx://127.0.0.1:34947
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f028d15a1c0, tag: 0x49b441e9fc479779, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-21 07:40:26,871 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34947
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f028d15a340, tag: 0x284c192b43a3051c, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f028d15a340, tag: 0x284c192b43a3051c, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
[dgx13:77973:0:77973] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  77973) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f9f94cc207d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f9f94cc2274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f9f94cc243a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fa035ff9420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f9f94d416b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9f94d6a839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f9f94c7c3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f9f94c7f838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f9f94ccb4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f9f94c7e5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f9f94d3e8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f9f94df706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5645f66c66fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5645f66c2094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5645f66d3519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x5645f6776162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fa027f041e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5645f66cb77c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5645f667dd05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x5645f66ca7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x5645f66c8929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5645f66c2094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5645f66d3519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5645f66c4128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5645f66c2094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x5645f66e0ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5645f66e144c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x5645f67a410e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5645f66cb77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5645f66c66fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x5645f66e0dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5645f66c66fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5645f66c2094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5645f66d3519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5645f66c35c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5645f66d37c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5645f66c3312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5645f66c2094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5645f66d3519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5645f66c4128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5645f66c2094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x5645f66c1d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5645f66c1d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5645f676f07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x5645f679bfca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x5645f6798353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5645f679016a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5645f679005c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5645f678f297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5645f6762f07]
=================================
2023-11-21 07:40:29,314 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:50687 -> ucx://127.0.0.1:39703
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f028d15a1c0, tag: 0xc63d2a66acde84b2, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-21 07:40:29,314 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39703
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #017] ep: 0x7f4a21576200, tag: 0xa25a2fdaada2b502, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #017] ep: 0x7f4a21576200, tag: 0xa25a2fdaada2b502, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-11-21 07:40:29,314 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39703
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f028d15a300, tag: 0x4a2928a497bf8fb4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f028d15a300, tag: 0x4a2928a497bf8fb4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
Task exception was never retrieved
future: <Task finished name='Task-1796' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-11-21 07:40:29,316 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39703
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7ff2fd323240, tag: 0x41305f7082569649, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7ff2fd323240, tag: 0x41305f7082569649, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-21 07:40:31,418 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
Function:  shuffle_group
args:      (                 key   payload
0         1518863339  99051695
1         1552208751  78953234
2          689778035  81293845
3         1553618300  74974697
4         1502090160  62919800
...              ...       ...
99999995    87713713  72068972
99999996  1549206659  33160153
99999997  1559655348  40095643
99999998  1552540937  33956372
99999999  1512812172  90324512

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:32,065 - distributed.nanny - WARNING - Restarting worker
2023-11-21 07:40:33,374 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 4)
Function:  shuffle_group
args:      (               key   payload
shuffle                     
4            81507   1140505
4            11975  84513553
4            81516  30826533
4            11978  94210713
4            81519  64097613
...            ...       ...
4        799980140  42994732
4        799980141  75275386
4        799980151   1331310
4        799980162  79193233
4        799980178  16969716

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:33,391 - distributed.worker - ERROR - Exception during execution of task ('split-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 5, 5).
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2382, in _prepare_args_for_execution
    data[k] = self.data[k]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/device_host_file.py", line 273, in __getitem__
    raise KeyError(key)
KeyError: ('group-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2234, in execute
    args2, kwargs2 = self._prepare_args_for_execution(ts, args, kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2386, in _prepare_args_for_execution
    data[k] = Actor(type(self.state.actors[k]), self.address, k, self)
KeyError: ('group-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 5)
2023-11-21 07:40:37,970 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 0)
Function:  _concat
args:      ([                key   payload
11364     848733317  46815590
11365     803137889  90812872
11368     825410529  49683112
72804     863275291  91870079
11370     848191710  48989202
...             ...       ...
99971859  807991774  49239071
99971860  800862512  36183597
99971865  838089784  76414127
99971867  859837898   7599173
99971892  814273086  32744341

[12497168 rows x 2 columns],                 key   payload
31958      23184622   2385294
31962     421220972  87280185
31965     949805110  37734301
134373    904404109  72859226
31966      17552821  66035881
...             ...       ...
99985748  958722273  24806328
99985753  953403513  14761569
99985665  908490913  59948422
99985673  932290771   9590431
99985691  927848705  75535813

[12502889 rows x 2 columns],                  key   payload
18624     1009991261  35767682
18630     1040067945  92767883
18637     1068586646   9276079
18642     1023978941  82501448
18643     1014674628  39134595
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,255 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,256 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,278 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,278 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,303 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 1)
Function:  _concat
args:      ([                key   payload
11380     601055712  51972572
11381     857121761  96881703
11383      11520467  38455156
72802     829141029  54617172
11389     819411763  92925940
...             ...       ...
99971887  809989934  32140837
99971890  100516745  28737646
99971894  100010693  41511732
99971895  209224170   7899762
99971898  829644976   3755705

[12502120 rows x 2 columns],                 key   payload
31942     927379439  62224902
31943     961949071   3238074
31949     116370422  41987069
134368    939605561  83124806
31950     966448963   3076866
...             ...       ...
99985671  936670762  41401758
99985679  519977766  68281832
99985680  940766061  64235390
99985694  116780288  81893800
99985695  906263766  99959353

[12499414 rows x 2 columns],                  key   payload
18627     1058096360  76773061
18628     1057697959  85152572
18633      530123847  60495102
18634     1009207622  83157707
18635      234089049  59451852
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,383 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 2)
Function:  _concat
args:      ([                key   payload
93315     853363793  32105320
93322     825079527  39318298
93330     810223205  90243019
39584     825580225  37797774
93333     841316263  10031672
...             ...       ...
99971883  805568492  23231111
99971885  819503172  73314744
99971896  867651217  76656798
99971900  805196527  31011742
99971901  825365867  15915281

[12503879 rows x 2 columns],                 key   payload
31947     223018432  72848216
134390    956221331  64521690
134396    937543128  77556855
123561    964364227  77071248
123578    927526675  84896050
...             ...       ...
99985745  905543407  14076426
99985754  951039379  89408071
99985755  914963051    184956
99985684  934084444  31813280
99985687  422870416   4216429

[12499576 rows x 2 columns],                  key   payload
18629     1040887053  76430794
39658     1024352186  14630313
39667     1036043108  90557819
59470      236293156  33698578
11904     1055350019  65860337
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,384 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-11-21 07:40:38,386 - distributed.worker - ERROR - tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: tuple indices must be integers or slices, not str
2023-11-21 07:40:38,391 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1069, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1784, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {('split-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 5, 4)}, 'who': 'ucx://127.0.0.1:50687', 'reply': True}
2023-11-21 07:40:38,405 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50303
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 364, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #212] ep: 0x7f028d15a280, tag: 0x65bb26917c7c3ece, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #212] ep: 0x7f028d15a280, tag: 0x65bb26917c7c3ece, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2023-11-21 07:40:38,425 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  _concat
args:      ([                key   payload
11362     806993478  62155918
11372     829765530  33707326
11385     835957983  21651213
72800     850264506  46400236
11386     809432709  21649426
...             ...       ...
99971844  824361361  94125055
99971849  824129509  22220703
99971853  860670618  19807334
99971868  857284641  41296657
99971889  858645839  48724493

[12498632 rows x 2 columns],                 key   payload
31936     920885180  16944189
31945     937461033  55265990
31951     939043728  27465554
134376    214384797  28428334
31960     903029669  53191733
...             ...       ...
99985669  915743526  21667542
99985677  906573103  79895900
99985682  926214503  26704119
99985683  960088906  56018810
99985690  900180471  74909743

[12502237 rows x 2 columns],                  key   payload
18632     1045318445  23951953
18646     1005437560  70733445
18652     1010944905  96376790
18654     1065331499  93536537
39648      537413695  45280144
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,507 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  _concat
args:      ([                key   payload
11369     847654026  53108740
11371     800360115  48247496
11378     701577190  62801247
72813     708022640  85904651
11379     858089873  69771605
...             ...       ...
99971848  856322492  38126769
99971863  830294513  12194333
99971872  111092928  22688726
99971880  860769765  80672350
99971888  812039450  40760043

[12500589 rows x 2 columns],                 key   payload
31939     905748287   3418246
31944     928659154   2773486
31946     621801766  90562334
134377    961219372  61310889
31948     941238054  33373967
...             ...       ...
99985752  317064053  70878606
99985759  214566431  99774480
99985674  314134465  46204712
99985675  959491642  32041672
99985681  216605847  80075098

[12501715 rows x 2 columns],                  key   payload
18651     1046801171  93776218
18655      230746689  67050702
39654      227141085  50665278
39659     1039776926  25628635
59456     1028366063  37255152
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,548 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 4)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           294670   3792327
0           570490  32784493
0            33061  76543459
0           642086  93945537
0             1341   3838739
...            ...       ...
0        799972802  96721123
0        799898259  60133858
0        799970786  44589902
0        799965570  29023902
0        799923764  64399944

[12503392 rows x 2 columns],                key   payload
shuffle                     
1           930279  28125044
1           908772  73006821
1          1061580  60821180
1           963862  27108487
1           896323  24148088
...            ...       ...
1        799999301  37397272
1        799860442  57129626
1        799984582  24764631
1        799999322  38033697
1        799887438  34225979

[12500389 rows x 2 columns],                key   payload
shuffle                     
2          1241163  40308854
2          1045313  75666262
2           953830  26050008
2           993691  23998337
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,551 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:53359
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #236] ep: 0x7f028d15a240, tag: 0x2613b9f6000f4fe1, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #236] ep: 0x7f028d15a240, tag: 0x2613b9f6000f4fe1, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2023-11-21 07:40:38,607 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,607 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,629 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,629 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-11-21 07:40:38,633 - distributed.worker - ERROR - ('Unexpected response', {'op': 'get_data', 'keys': {('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6, 3)}, 'who': 'ucx://127.0.0.1:53359', 'reply': True})
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
KeyError: 'status'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    raise ValueError("Unexpected response", response)
ValueError: ('Unexpected response', {'op': 'get_data', 'keys': {('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6, 3)}, 'who': 'ucx://127.0.0.1:53359', 'reply': True})
2023-11-21 07:40:38,870 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5)
Function:  _concat
args:      ([                key   payload
11363     202069130   1684915
11366     860162340  66599419
11367     864264311  53230441
72803     860866207  70200368
11374     864964009  40511370
...             ...       ...
99971855  508997821  50776130
99971862  842654209  74208295
99971876  845991715   7299844
99971879  860733417  36321448
99971902  845598690  20688460

[12498923 rows x 2 columns],                 key   payload
31938     122357104   9560516
31940     943294877  70317547
31941     914889211  19930599
134381    939576778   1378094
31952     117877046  16645531
...             ...       ...
99985736  910399339  14331580
99985737  944856385  73128805
99985740  124945142  77204351
99985756  942133231  65284923
99985672  934049393   8750189

[12501128 rows x 2 columns],                  key   payload
18625     1015914657  52804018
18631     1020410157  93494296
18647     1015881779    176359
39666     1053295591  77045354
39672     1005409064  72770611
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:38,874 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  _concat
args:      ([                key   payload
11360     407678083  26424789
11361     606092337   8659668
11375     208859481  89454859
72805     311669973  89618642
11382     800842710  84478955
...             ...       ...
99971877  822649556  90956345
99971884  819345069  84272156
99971891  102714803  63932281
99971897  306378459  35167001
99971899  609609203  51074704

[12497796 rows x 2 columns],                 key   payload
31937     944910701  55504790
31956     616968880  51963656
31963     954546604  51738101
134370    966890627  97610886
134371    906483104  95111846
...             ...       ...
99985751  954518975  58360610
99985664  935368402  87266133
99985668  121670733   2129511
99985670  413843336  14567050
99985693  968269913  40199586

[12497151 rows x 2 columns],                  key   payload
18626     1062416292  37267052
18638      334844816  70628347
18639     1041983500  90685184
18640      427096461  49767935
39663     1064726955  91454578
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-11-21 07:40:43,862 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
