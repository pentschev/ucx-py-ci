============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-20 05:29:47,953 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:29:47,957 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:29:47,960 - distributed.scheduler - INFO - State start
2023-10-20 05:29:47,983 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:29:47,984 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-20 05:29:47,984 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:29:47,984 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:29:48,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33871'
2023-10-20 05:29:48,101 - distributed.scheduler - INFO - Receive client connection: Client-ae33a8c4-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:29:48,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35122
2023-10-20 05:29:48,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43911'
2023-10-20 05:29:48,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44461'
2023-10-20 05:29:48,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46371'
2023-10-20 05:29:49,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:49,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:49,861 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-20 05:29:49,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45891
2023-10-20 05:29:49,872 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45891
2023-10-20 05:29:49,872 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32993
2023-10-20 05:29:49,872 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-20 05:29:49,872 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:49,873 - distributed.worker - INFO -               Threads:                          4
2023-10-20 05:29:49,873 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-20 05:29:49,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ql7ldo_7
2023-10-20 05:29:49,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e364770d-a2b5-4562-965f-ae6c5096774f
2023-10-20 05:29:49,873 - distributed.worker - INFO - Starting Worker plugin PreImport-d903ec6c-75aa-4d60-ac10-5f3a7ff01dca
2023-10-20 05:29:49,873 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87a4871e-4d6d-477d-8f51-d9825fe34de3
2023-10-20 05:29:49,873 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:49,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:49,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:49,885 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:49,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:49,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:49,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:49,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:49,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:49,910 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:49,932 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45891', status: init, memory: 0, processing: 0>
2023-10-20 05:29:49,933 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45891
2023-10-20 05:29:49,933 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35168
2023-10-20 05:29:49,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:49,934 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-20 05:29:49,934 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:49,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-20 05:29:51,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38505
2023-10-20 05:29:51,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38505
2023-10-20 05:29:51,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35921
2023-10-20 05:29:51,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-20 05:29:51,136 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,136 - distributed.worker - INFO -               Threads:                          4
2023-10-20 05:29:51,137 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-20 05:29:51,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-yskhb3ij
2023-10-20 05:29:51,137 - distributed.worker - INFO - Starting Worker plugin PreImport-28641b54-7ef0-45ca-be89-292154c52624
2023-10-20 05:29:51,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c134c5ca-b5ec-49a6-a28b-a293a114b396
2023-10-20 05:29:51,138 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ca9cb72-4eab-4b97-8c32-be5df5b29ffb
2023-10-20 05:29:51,138 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,163 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38505', status: init, memory: 0, processing: 0>
2023-10-20 05:29:51,164 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38505
2023-10-20 05:29:51,164 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34312
2023-10-20 05:29:51,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:51,166 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-20 05:29:51,166 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,168 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-20 05:29:51,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40093
2023-10-20 05:29:51,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40093
2023-10-20 05:29:51,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41571
2023-10-20 05:29:51,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33131
2023-10-20 05:29:51,190 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-20 05:29:51,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33131
2023-10-20 05:29:51,190 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41131
2023-10-20 05:29:51,190 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-20 05:29:51,191 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,191 - distributed.worker - INFO -               Threads:                          4
2023-10-20 05:29:51,191 - distributed.worker - INFO -               Threads:                          4
2023-10-20 05:29:51,191 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-20 05:29:51,191 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-20 05:29:51,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-flbx25vx
2023-10-20 05:29:51,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-5hiugu8b
2023-10-20 05:29:51,191 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c903d05-79d6-41ae-9c6e-ae120660873e
2023-10-20 05:29:51,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80deac81-bb84-4459-8bf3-f998a73f50ba
2023-10-20 05:29:51,192 - distributed.worker - INFO - Starting Worker plugin PreImport-a51be9ab-2768-4e37-b626-adb50d0d3cf3
2023-10-20 05:29:51,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26fc6b32-9e85-4b2b-a1c1-e76dcac9abca
2023-10-20 05:29:51,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a69a0c97-5b4d-4b04-b8c7-f31ee975a523
2023-10-20 05:29:51,192 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,192 - distributed.worker - INFO - Starting Worker plugin PreImport-23b2a03f-4c8a-466c-890c-f317399cabbd
2023-10-20 05:29:51,193 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40093', status: init, memory: 0, processing: 0>
2023-10-20 05:29:51,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40093
2023-10-20 05:29:51,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34320
2023-10-20 05:29:51,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:51,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-20 05:29:51,216 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33131', status: init, memory: 0, processing: 0>
2023-10-20 05:29:51,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33131
2023-10-20 05:29:51,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34318
2023-10-20 05:29:51,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-20 05:29:51,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:51,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-20 05:29:51,219 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:51,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-20 05:29:51,261 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-20 05:29:51,261 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-20 05:29:51,261 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-20 05:29:51,261 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-20 05:29:51,265 - distributed.scheduler - INFO - Remove client Client-ae33a8c4-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:29:51,266 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35122; closing.
2023-10-20 05:29:51,266 - distributed.scheduler - INFO - Remove client Client-ae33a8c4-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:29:51,266 - distributed.scheduler - INFO - Close client connection: Client-ae33a8c4-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:29:51,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33871'. Reason: nanny-close
2023-10-20 05:29:51,267 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:29:51,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43911'. Reason: nanny-close
2023-10-20 05:29:51,268 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:29:51,269 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33131. Reason: nanny-close
2023-10-20 05:29:51,269 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44461'. Reason: nanny-close
2023-10-20 05:29:51,269 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:29:51,269 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40093. Reason: nanny-close
2023-10-20 05:29:51,269 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46371'. Reason: nanny-close
2023-10-20 05:29:51,270 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:29:51,270 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38505. Reason: nanny-close
2023-10-20 05:29:51,270 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45891. Reason: nanny-close
2023-10-20 05:29:51,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34318; closing.
2023-10-20 05:29:51,271 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-20 05:29:51,271 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-20 05:29:51,271 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779791.2713954')
2023-10-20 05:29:51,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34320; closing.
2023-10-20 05:29:51,272 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-20 05:29:51,272 - distributed.nanny - INFO - Worker closed
2023-10-20 05:29:51,272 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-20 05:29:51,272 - distributed.nanny - INFO - Worker closed
2023-10-20 05:29:51,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40093', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779791.2729511')
2023-10-20 05:29:51,273 - distributed.nanny - INFO - Worker closed
2023-10-20 05:29:51,273 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34312; closing.
2023-10-20 05:29:51,274 - distributed.nanny - INFO - Worker closed
2023-10-20 05:29:51,274 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35168; closing.
2023-10-20 05:29:51,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38505', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779791.2743816')
2023-10-20 05:29:51,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779791.2748284')
2023-10-20 05:29:51,275 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:29:52,384 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:29:52,384 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:29:52,384 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:29:52,386 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-20 05:29:52,386 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-20 05:29:54,420 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:29:54,424 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:29:54,427 - distributed.scheduler - INFO - State start
2023-10-20 05:29:54,449 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:29:54,449 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:29:54,450 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:29:54,450 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:29:54,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43465'
2023-10-20 05:29:54,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44981'
2023-10-20 05:29:54,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38719'
2023-10-20 05:29:54,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35725'
2023-10-20 05:29:54,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35637'
2023-10-20 05:29:54,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37773'
2023-10-20 05:29:54,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40421'
2023-10-20 05:29:54,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33435'
2023-10-20 05:29:56,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,392 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,514 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:56,567 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:29:56,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:29:56,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:29:58,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37057
2023-10-20 05:29:58,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37057
2023-10-20 05:29:58,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39879
2023-10-20 05:29:58,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:58,765 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:58,765 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:58,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:58,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-15egp7id
2023-10-20 05:29:58,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d22ffec4-8738-46c5-b881-5c7cf5f4bd63
2023-10-20 05:29:58,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16c5e8ea-96b1-4606-a9df-c171918bd49f
2023-10-20 05:29:58,873 - distributed.worker - INFO - Starting Worker plugin PreImport-e06965a7-71cd-4c72-b47a-457b3a216fa4
2023-10-20 05:29:58,873 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:58,928 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37057', status: init, memory: 0, processing: 0>
2023-10-20 05:29:58,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37057
2023-10-20 05:29:58,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55118
2023-10-20 05:29:58,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:58,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:58,960 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:58,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:58,986 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40107
2023-10-20 05:29:58,987 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40107
2023-10-20 05:29:58,987 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33451
2023-10-20 05:29:58,987 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:58,987 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:58,987 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:58,987 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:58,987 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bz_ucz4b
2023-10-20 05:29:58,988 - distributed.worker - INFO - Starting Worker plugin PreImport-d42532e7-0144-4b62-9608-41b32eaf15fb
2023-10-20 05:29:58,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0b39ea0-c326-4187-a3d2-6e071c2c5d3a
2023-10-20 05:29:58,988 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d69b767-adda-4019-8f28-64af0cf4762c
2023-10-20 05:29:58,991 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45077
2023-10-20 05:29:58,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45077
2023-10-20 05:29:58,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46155
2023-10-20 05:29:58,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:58,992 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:58,992 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:58,992 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:58,992 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dm4k3hfu
2023-10-20 05:29:58,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f878d8b7-26ff-43b4-b5b4-d465bc50342c
2023-10-20 05:29:58,997 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44737
2023-10-20 05:29:58,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44737
2023-10-20 05:29:58,998 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46405
2023-10-20 05:29:58,998 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:58,998 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:58,998 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:58,998 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:58,998 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ck7yv9p1
2023-10-20 05:29:58,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9abfd14d-07ab-4ccb-951c-e0c48e048084
2023-10-20 05:29:59,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34705
2023-10-20 05:29:59,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34705
2023-10-20 05:29:59,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42849
2023-10-20 05:29:59,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,008 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,008 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:59,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:59,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pbfob8pj
2023-10-20 05:29:59,009 - distributed.worker - INFO - Starting Worker plugin PreImport-c27f7292-f525-4d3d-9ed4-4cb10c715fb1
2023-10-20 05:29:59,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1c90a07-6523-4211-9e05-4d1f17a201bc
2023-10-20 05:29:59,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41285
2023-10-20 05:29:59,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41285
2023-10-20 05:29:59,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42627
2023-10-20 05:29:59,107 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,107 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,107 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:59,108 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:59,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ethv0fef
2023-10-20 05:29:59,107 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36061
2023-10-20 05:29:59,108 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36061
2023-10-20 05:29:59,108 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41789
2023-10-20 05:29:59,108 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b225f3d4-0bf3-4234-af0c-50e9a5a84ee2
2023-10-20 05:29:59,108 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,108 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,108 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:59,108 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:59,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-orkhh46t
2023-10-20 05:29:59,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1abb528b-0d43-4d6b-823b-adc67825e038
2023-10-20 05:29:59,109 - distributed.worker - INFO - Starting Worker plugin RMMSetup-adf90e4a-aa3b-4598-9c8d-85cb2232b9af
2023-10-20 05:29:59,108 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43043
2023-10-20 05:29:59,109 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43043
2023-10-20 05:29:59,109 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34357
2023-10-20 05:29:59,109 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,109 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,109 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:29:59,109 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:29:59,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xe_6r0xn
2023-10-20 05:29:59,110 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-588e24c6-a9d2-4848-a353-59bb4f5578af
2023-10-20 05:29:59,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f68b9078-83bb-494a-ba08-db9eefa8e97e
2023-10-20 05:29:59,120 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,133 - distributed.worker - INFO - Starting Worker plugin PreImport-df24e1ad-105a-4130-9fcb-47d5077daeed
2023-10-20 05:29:59,133 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dda5cccd-fd30-4300-a4e9-d1c477c86766
2023-10-20 05:29:59,133 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d64609b-c83c-43b9-891b-f90a2deda0f6
2023-10-20 05:29:59,133 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9f1e323-8684-4c8d-80e2-db110ded4c52
2023-10-20 05:29:59,133 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,133 - distributed.worker - INFO - Starting Worker plugin PreImport-d5817ecf-4ba2-4a7e-93cd-53ec56bd4186
2023-10-20 05:29:59,133 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,133 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,148 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40107', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,149 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40107
2023-10-20 05:29:59,149 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55130
2023-10-20 05:29:59,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,151 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,151 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:59,156 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45077', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45077
2023-10-20 05:29:59,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55144
2023-10-20 05:29:59,157 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34705', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,158 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34705
2023-10-20 05:29:59,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55156
2023-10-20 05:29:59,158 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,158 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,159 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44737', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,160 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44737
2023-10-20 05:29:59,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55164
2023-10-20 05:29:59,160 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,160 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:59,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:59,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,161 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:59,227 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4aa4846-e983-436a-9925-b5d587a4dd2d
2023-10-20 05:29:59,227 - distributed.worker - INFO - Starting Worker plugin PreImport-7ff86550-ca6e-4111-b832-24dab00fc54c
2023-10-20 05:29:59,227 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,234 - distributed.worker - INFO - Starting Worker plugin PreImport-56aaa1ec-b5d4-409a-8cc2-c0fe9353ef2f
2023-10-20 05:29:59,234 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,234 - distributed.worker - INFO - Starting Worker plugin PreImport-a33a227a-55e9-4b3e-b231-7400da84bc8d
2023-10-20 05:29:59,235 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36061', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36061
2023-10-20 05:29:59,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55166
2023-10-20 05:29:59,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,251 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,251 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:59,256 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43043', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,257 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43043
2023-10-20 05:29:59,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55168
2023-10-20 05:29:59,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,258 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,258 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:29:59,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41285', status: init, memory: 0, processing: 0>
2023-10-20 05:29:59,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41285
2023-10-20 05:29:59,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55170
2023-10-20 05:29:59,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:29:59,266 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:29:59,266 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:29:59,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:02,392 - distributed.scheduler - INFO - Receive client connection: Client-b2113f69-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:02,393 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54176
2023-10-20 05:30:02,405 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,405 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,406 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:02,410 - distributed.scheduler - INFO - Remove client Client-b2113f69-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:02,411 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54176; closing.
2023-10-20 05:30:02,411 - distributed.scheduler - INFO - Remove client Client-b2113f69-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:02,411 - distributed.scheduler - INFO - Close client connection: Client-b2113f69-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:02,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43465'. Reason: nanny-close
2023-10-20 05:30:02,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44981'. Reason: nanny-close
2023-10-20 05:30:02,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45077. Reason: nanny-close
2023-10-20 05:30:02,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38719'. Reason: nanny-close
2023-10-20 05:30:02,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34705. Reason: nanny-close
2023-10-20 05:30:02,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35725'. Reason: nanny-close
2023-10-20 05:30:02,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37057. Reason: nanny-close
2023-10-20 05:30:02,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35637'. Reason: nanny-close
2023-10-20 05:30:02,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40107. Reason: nanny-close
2023-10-20 05:30:02,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37773'. Reason: nanny-close
2023-10-20 05:30:02,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55144; closing.
2023-10-20 05:30:02,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44737. Reason: nanny-close
2023-10-20 05:30:02,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4170535')
2023-10-20 05:30:02,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40421'. Reason: nanny-close
2023-10-20 05:30:02,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41285. Reason: nanny-close
2023-10-20 05:30:02,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33435'. Reason: nanny-close
2023-10-20 05:30:02,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:02,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36061. Reason: nanny-close
2023-10-20 05:30:02,418 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43043. Reason: nanny-close
2023-10-20 05:30:02,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,418 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,419 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55118; closing.
2023-10-20 05:30:02,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55156; closing.
2023-10-20 05:30:02,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,420 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4205182')
2023-10-20 05:30:02,420 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34705', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4209163')
2023-10-20 05:30:02,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:02,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55130; closing.
2023-10-20 05:30:02,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55164; closing.
2023-10-20 05:30:02,421 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,421 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.422186')
2023-10-20 05:30:02,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44737', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4224758')
2023-10-20 05:30:02,422 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,422 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:02,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55170; closing.
2023-10-20 05:30:02,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55166; closing.
2023-10-20 05:30:02,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55168; closing.
2023-10-20 05:30:02,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4243157')
2023-10-20 05:30:02,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36061', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4247406')
2023-10-20 05:30:02,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779802.4252257')
2023-10-20 05:30:02,425 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:04,080 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:04,080 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:04,081 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:04,082 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:04,082 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-20 05:30:06,157 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:06,161 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:06,165 - distributed.scheduler - INFO - State start
2023-10-20 05:30:06,186 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:06,187 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:06,187 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:06,188 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:30:06,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43669'
2023-10-20 05:30:06,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39781'
2023-10-20 05:30:06,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44203'
2023-10-20 05:30:06,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45441'
2023-10-20 05:30:06,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46541'
2023-10-20 05:30:06,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39255'
2023-10-20 05:30:06,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38301'
2023-10-20 05:30:06,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34551'
2023-10-20 05:30:06,508 - distributed.scheduler - INFO - Receive client connection: Client-b908c5ac-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:06,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54360
2023-10-20 05:30:08,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,020 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,061 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,099 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:08,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:08,374 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,374 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,374 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:08,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:10,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45687
2023-10-20 05:30:10,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45687
2023-10-20 05:30:10,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33759
2023-10-20 05:30:10,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:10,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,028 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:10,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:10,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bi1mzxvj
2023-10-20 05:30:10,029 - distributed.worker - INFO - Starting Worker plugin PreImport-b16bcafe-1775-450e-b6a2-ae8adf9f7b1a
2023-10-20 05:30:10,029 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-281a1222-f748-45d6-aa62-b24f2c2b99c1
2023-10-20 05:30:10,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-739eec1a-f8e6-4f00-abba-910eae06275c
2023-10-20 05:30:10,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33913
2023-10-20 05:30:10,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33913
2023-10-20 05:30:10,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45715
2023-10-20 05:30:10,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:10,042 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,042 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:10,043 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:10,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-buczu0lb
2023-10-20 05:30:10,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62f1a701-0236-44b7-a34d-b1d34ba276b3
2023-10-20 05:30:10,116 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,122 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-700c953d-699b-411e-92e4-d7130c93b4e7
2023-10-20 05:30:10,122 - distributed.worker - INFO - Starting Worker plugin PreImport-f1f93e74-2710-46b1-b02c-8ba5a267b640
2023-10-20 05:30:10,122 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,146 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45687', status: init, memory: 0, processing: 0>
2023-10-20 05:30:10,147 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45687
2023-10-20 05:30:10,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38278
2023-10-20 05:30:10,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:10,149 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:10,149 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:10,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33913', status: init, memory: 0, processing: 0>
2023-10-20 05:30:10,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33913
2023-10-20 05:30:10,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38292
2023-10-20 05:30:10,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:10,158 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:10,158 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:10,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39099
2023-10-20 05:30:10,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39099
2023-10-20 05:30:10,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36473
2023-10-20 05:30:10,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:10,787 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,787 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:10,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:10,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_zk0vtz3
2023-10-20 05:30:10,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6a5c8a8-a045-431e-84ab-c40cfe23e3d8
2023-10-20 05:30:10,793 - distributed.worker - INFO - Starting Worker plugin PreImport-369dbc54-6584-4524-8a49-e34a03eabd11
2023-10-20 05:30:10,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-299e3b93-bdbe-4626-af8b-b5368d96c5fd
2023-10-20 05:30:10,794 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,821 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39099', status: init, memory: 0, processing: 0>
2023-10-20 05:30:10,822 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39099
2023-10-20 05:30:10,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38302
2023-10-20 05:30:10,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:10,824 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:10,824 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:10,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:11,039 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35327
2023-10-20 05:30:11,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35327
2023-10-20 05:30:11,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40411
2023-10-20 05:30:11,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,040 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,040 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:11,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:11,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7qdyq8nt
2023-10-20 05:30:11,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5eeda5c9-d070-40f6-b758-80854151d82b
2023-10-20 05:30:11,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46337
2023-10-20 05:30:11,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46337
2023-10-20 05:30:11,056 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44223
2023-10-20 05:30:11,056 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,056 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,056 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:11,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:11,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3xovwvex
2023-10-20 05:30:11,057 - distributed.worker - INFO - Starting Worker plugin PreImport-12ea8976-d2e3-4d40-912c-541689766fec
2023-10-20 05:30:11,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-829bae6c-6116-4a5a-aa23-f5399964ae02
2023-10-20 05:30:11,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35095
2023-10-20 05:30:11,059 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35095
2023-10-20 05:30:11,059 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45001
2023-10-20 05:30:11,059 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,059 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,059 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:11,059 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:11,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ftge3bx
2023-10-20 05:30:11,060 - distributed.worker - INFO - Starting Worker plugin PreImport-daed5c4d-a290-410c-88b9-54336c85ee70
2023-10-20 05:30:11,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96edc3af-c4c3-4b77-9038-52c8fb836105
2023-10-20 05:30:11,060 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e490c73-583b-4460-b94f-2c07483bfd83
2023-10-20 05:30:11,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42307
2023-10-20 05:30:11,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42307
2023-10-20 05:30:11,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33533
2023-10-20 05:30:11,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,063 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,063 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:11,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:11,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-17_ktavv
2023-10-20 05:30:11,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40449
2023-10-20 05:30:11,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40449
2023-10-20 05:30:11,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39847
2023-10-20 05:30:11,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,063 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,063 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:11,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e612bb3c-ec6b-472a-aabd-e589a090d981
2023-10-20 05:30:11,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:11,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wsfq0vrw
2023-10-20 05:30:11,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-507667ef-e00d-4657-803a-6bd1a0051f32
2023-10-20 05:30:11,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37c3be1f-33d5-41a6-9f29-161d02fcff39
2023-10-20 05:30:11,077 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc996da9-43a3-4946-b15b-2190163b9bd7
2023-10-20 05:30:11,077 - distributed.worker - INFO - Starting Worker plugin PreImport-1a764923-3f8f-48b2-b8c2-f975fba61f80
2023-10-20 05:30:11,078 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,084 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,088 - distributed.worker - INFO - Starting Worker plugin PreImport-07ae3777-1661-41fb-a3e5-b59f8029e193
2023-10-20 05:30:11,088 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,088 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7cca736-869b-4eac-be14-d4c3a283f223
2023-10-20 05:30:11,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1298eb9-abac-4879-9c65-015380e19cb4
2023-10-20 05:30:11,089 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,091 - distributed.worker - INFO - Starting Worker plugin PreImport-a16a2122-aeaa-4f7f-a3e9-7f0ca62e42d0
2023-10-20 05:30:11,092 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35327', status: init, memory: 0, processing: 0>
2023-10-20 05:30:11,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35327
2023-10-20 05:30:11,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38308
2023-10-20 05:30:11,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:11,101 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,101 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:11,111 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46337', status: init, memory: 0, processing: 0>
2023-10-20 05:30:11,112 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46337
2023-10-20 05:30:11,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38334
2023-10-20 05:30:11,113 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35095', status: init, memory: 0, processing: 0>
2023-10-20 05:30:11,113 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:11,114 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35095
2023-10-20 05:30:11,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38314
2023-10-20 05:30:11,114 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,114 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:11,116 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42307', status: init, memory: 0, processing: 0>
2023-10-20 05:30:11,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:11,116 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42307
2023-10-20 05:30:11,116 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38318
2023-10-20 05:30:11,116 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:11,118 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40449', status: init, memory: 0, processing: 0>
2023-10-20 05:30:11,118 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40449
2023-10-20 05:30:11,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38338
2023-10-20 05:30:11,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:11,119 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,119 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:11,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:11,121 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:11,121 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:11,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:11,180 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,180 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,180 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,180 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:11,185 - distributed.scheduler - INFO - Remove client Client-b908c5ac-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:11,185 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54360; closing.
2023-10-20 05:30:11,186 - distributed.scheduler - INFO - Remove client Client-b908c5ac-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:11,186 - distributed.scheduler - INFO - Close client connection: Client-b908c5ac-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:11,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43669'. Reason: nanny-close
2023-10-20 05:30:11,187 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,188 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39781'. Reason: nanny-close
2023-10-20 05:30:11,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39099. Reason: nanny-close
2023-10-20 05:30:11,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44203'. Reason: nanny-close
2023-10-20 05:30:11,189 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,189 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45687. Reason: nanny-close
2023-10-20 05:30:11,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45441'. Reason: nanny-close
2023-10-20 05:30:11,189 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42307. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46541'. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33913. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39255'. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35327. Reason: nanny-close
2023-10-20 05:30:11,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38301'. Reason: nanny-close
2023-10-20 05:30:11,191 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38302; closing.
2023-10-20 05:30:11,191 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,191 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46337. Reason: nanny-close
2023-10-20 05:30:11,191 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38278; closing.
2023-10-20 05:30:11,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34551'. Reason: nanny-close
2023-10-20 05:30:11,192 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:11,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,192 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35095. Reason: nanny-close
2023-10-20 05:30:11,192 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.192359')
2023-10-20 05:30:11,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40449. Reason: nanny-close
2023-10-20 05:30:11,192 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.192818')
2023-10-20 05:30:11,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38318; closing.
2023-10-20 05:30:11,193 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,193 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,193 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,193 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.193932')
2023-10-20 05:30:11,194 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,194 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38292; closing.
2023-10-20 05:30:11,194 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,194 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,195 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:11,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33913', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.1952634')
2023-10-20 05:30:11,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38308; closing.
2023-10-20 05:30:11,196 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,197 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:11,196 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38318>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-20 05:30:11,198 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.19808')
2023-10-20 05:30:11,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38334; closing.
2023-10-20 05:30:11,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38314; closing.
2023-10-20 05:30:11,199 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.1992154')
2023-10-20 05:30:11,199 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.199616')
2023-10-20 05:30:11,199 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38338; closing.
2023-10-20 05:30:11,200 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779811.200473')
2023-10-20 05:30:11,200 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:12,905 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:12,906 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:12,906 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:12,907 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:12,908 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-20 05:30:15,007 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:15,012 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:15,015 - distributed.scheduler - INFO - State start
2023-10-20 05:30:15,036 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:15,037 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:15,037 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:15,037 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:30:15,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38943'
2023-10-20 05:30:15,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43995'
2023-10-20 05:30:15,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41523'
2023-10-20 05:30:15,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42713'
2023-10-20 05:30:15,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33889'
2023-10-20 05:30:15,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35149'
2023-10-20 05:30:15,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33499'
2023-10-20 05:30:15,341 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42949'
2023-10-20 05:30:16,848 - distributed.scheduler - INFO - Receive client connection: Client-be5a0d88-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:16,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38486
2023-10-20 05:30:17,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,129 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,135 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,178 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,179 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,181 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,185 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:17,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:17,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:17,232 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:19,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43435
2023-10-20 05:30:19,287 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43435
2023-10-20 05:30:19,287 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40925
2023-10-20 05:30:19,287 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,287 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,287 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,287 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wxcoj9_8
2023-10-20 05:30:19,288 - distributed.worker - INFO - Starting Worker plugin PreImport-468946b4-fca9-404b-9d73-92b1b1c239d8
2023-10-20 05:30:19,288 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef669152-b131-4962-8107-002ccdcb41f4
2023-10-20 05:30:19,412 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-352eac44-b231-4371-9d0a-f3456358b26d
2023-10-20 05:30:19,412 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,434 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43435', status: init, memory: 0, processing: 0>
2023-10-20 05:30:19,436 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43435
2023-10-20 05:30:19,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38508
2023-10-20 05:30:19,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:19,437 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,437 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:19,778 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43291
2023-10-20 05:30:19,779 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43291
2023-10-20 05:30:19,780 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38205
2023-10-20 05:30:19,780 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,780 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,780 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,780 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p2y40rui
2023-10-20 05:30:19,781 - distributed.worker - INFO - Starting Worker plugin PreImport-34ee464e-c6a2-4423-a381-8a7e23bccd86
2023-10-20 05:30:19,781 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a29abbe0-1fed-4beb-99fb-94309e5aa3fa
2023-10-20 05:30:19,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38695
2023-10-20 05:30:19,785 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38695
2023-10-20 05:30:19,785 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41219
2023-10-20 05:30:19,785 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,785 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,785 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,785 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,785 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ncmrp45
2023-10-20 05:30:19,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-716a1ffc-b05a-4275-9f5a-00c8cd84d7b6
2023-10-20 05:30:19,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7197eb34-8477-4ab1-8d2a-94e9fc588884
2023-10-20 05:30:19,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44773
2023-10-20 05:30:19,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44773
2023-10-20 05:30:19,790 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39605
2023-10-20 05:30:19,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,790 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,791 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,791 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lt1x8nrs
2023-10-20 05:30:19,791 - distributed.worker - INFO - Starting Worker plugin PreImport-a70968be-58a2-4699-bcd5-f8b860c2f8a6
2023-10-20 05:30:19,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59155fa3-25bc-41b6-b3ae-1c33443947e3
2023-10-20 05:30:19,795 - distributed.worker - INFO - Starting Worker plugin RMMSetup-282c8578-c127-4963-9d2b-6d69ecdc241f
2023-10-20 05:30:19,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36655
2023-10-20 05:30:19,797 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36655
2023-10-20 05:30:19,797 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43747
2023-10-20 05:30:19,797 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,798 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,798 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a9kl5j7r
2023-10-20 05:30:19,798 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c14c93d-bd13-49e5-9f1c-434172c00b33
2023-10-20 05:30:19,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43805
2023-10-20 05:30:19,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43805
2023-10-20 05:30:19,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38445
2023-10-20 05:30:19,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,811 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,811 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_mnhck7b
2023-10-20 05:30:19,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f179be83-c0d8-46c2-aafb-80c6ba1a3ac1
2023-10-20 05:30:19,814 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37041
2023-10-20 05:30:19,815 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37041
2023-10-20 05:30:19,815 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36747
2023-10-20 05:30:19,815 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,815 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,815 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,815 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,815 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-igm0t4gt
2023-10-20 05:30:19,816 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f9f262e-cddb-47ff-a21a-1e334a64743e
2023-10-20 05:30:19,816 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33705
2023-10-20 05:30:19,816 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33705
2023-10-20 05:30:19,817 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33617
2023-10-20 05:30:19,817 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:19,817 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:19,817 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:19,817 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:19,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6p_5k1z7
2023-10-20 05:30:19,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23de4f2b-608c-438d-bc6a-76db923deed8
2023-10-20 05:30:19,818 - distributed.worker - INFO - Starting Worker plugin RMMSetup-abe10e49-ffde-4d7e-b4ea-79aaa176d630
2023-10-20 05:30:19,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3541b9ca-0213-4536-ae9c-e8445e919c6a
2023-10-20 05:30:19,990 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,019 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43291', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,020 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43291
2023-10-20 05:30:20,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55824
2023-10-20 05:30:20,021 - distributed.worker - INFO - Starting Worker plugin PreImport-78b4530c-6834-4010-9bd5-cf32d6decb74
2023-10-20 05:30:20,021 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,022 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,027 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b766321-66fb-413e-bea1-3fe7826532a0
2023-10-20 05:30:20,028 - distributed.worker - INFO - Starting Worker plugin PreImport-4d586819-4d1f-4095-8542-3704b0cdd839
2023-10-20 05:30:20,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,028 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc18cf0a-d1be-4f67-a959-1df94ee9250e
2023-10-20 05:30:20,029 - distributed.worker - INFO - Starting Worker plugin PreImport-87ae2552-49e4-4a93-a319-e116384a3d22
2023-10-20 05:30:20,029 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,033 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a091fc44-905c-456d-9dee-5e954037db76
2023-10-20 05:30:20,034 - distributed.worker - INFO - Starting Worker plugin PreImport-f092f769-48ca-4687-9909-1513db54de46
2023-10-20 05:30:20,034 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,035 - distributed.worker - INFO - Starting Worker plugin PreImport-7ae7d219-f695-43d8-be10-c3cc30d98ebc
2023-10-20 05:30:20,036 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,053 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38695', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38695
2023-10-20 05:30:20,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55832
2023-10-20 05:30:20,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37041', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,055 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37041
2023-10-20 05:30:20,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55842
2023-10-20 05:30:20,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,056 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,056 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,056 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43805', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,057 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43805
2023-10-20 05:30:20,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55864
2023-10-20 05:30:20,057 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,057 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,058 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33705', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,058 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,059 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,059 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33705
2023-10-20 05:30:20,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55878
2023-10-20 05:30:20,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,060 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,061 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,061 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,062 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36655', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,062 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36655
2023-10-20 05:30:20,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55854
2023-10-20 05:30:20,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,065 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,065 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,068 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44773', status: init, memory: 0, processing: 0>
2023-10-20 05:30:20,068 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44773
2023-10-20 05:30:20,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55868
2023-10-20 05:30:20,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:20,070 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:20,070 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:20,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:20,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:20,168 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,168 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,169 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,169 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,169 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,169 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,169 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,170 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:20,176 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:20,177 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:20,180 - distributed.scheduler - INFO - Remove client Client-be5a0d88-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:20,180 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38486; closing.
2023-10-20 05:30:20,181 - distributed.scheduler - INFO - Remove client Client-be5a0d88-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:20,181 - distributed.scheduler - INFO - Close client connection: Client-be5a0d88-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:20,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38943'. Reason: nanny-close
2023-10-20 05:30:20,183 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43995'. Reason: nanny-close
2023-10-20 05:30:20,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,184 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43435. Reason: nanny-close
2023-10-20 05:30:20,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41523'. Reason: nanny-close
2023-10-20 05:30:20,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37041. Reason: nanny-close
2023-10-20 05:30:20,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42713'. Reason: nanny-close
2023-10-20 05:30:20,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36655. Reason: nanny-close
2023-10-20 05:30:20,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33889'. Reason: nanny-close
2023-10-20 05:30:20,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,186 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,186 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44773. Reason: nanny-close
2023-10-20 05:30:20,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38508; closing.
2023-10-20 05:30:20,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35149'. Reason: nanny-close
2023-10-20 05:30:20,187 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,187 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.187157')
2023-10-20 05:30:20,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43805. Reason: nanny-close
2023-10-20 05:30:20,187 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33499'. Reason: nanny-close
2023-10-20 05:30:20,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,188 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33705. Reason: nanny-close
2023-10-20 05:30:20,188 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42949'. Reason: nanny-close
2023-10-20 05:30:20,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:20,189 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43291. Reason: nanny-close
2023-10-20 05:30:20,189 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,189 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55842; closing.
2023-10-20 05:30:20,189 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38695. Reason: nanny-close
2023-10-20 05:30:20,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55854; closing.
2023-10-20 05:30:20,190 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,191 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37041', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.1910386')
2023-10-20 05:30:20,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,191 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,192 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.1921716')
2023-10-20 05:30:20,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:20,192 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,192 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55868; closing.
2023-10-20 05:30:20,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55864; closing.
2023-10-20 05:30:20,193 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,193 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44773', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.1936648')
2023-10-20 05:30:20,193 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,194 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.1942143')
2023-10-20 05:30:20,194 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:20,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55878; closing.
2023-10-20 05:30:20,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33705', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.195525')
2023-10-20 05:30:20,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55824; closing.
2023-10-20 05:30:20,196 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55832; closing.
2023-10-20 05:30:20,196 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.1967564')
2023-10-20 05:30:20,197 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779820.197216')
2023-10-20 05:30:20,197 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:20,197 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55832>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-20 05:30:20,199 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55824>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-20 05:30:21,850 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:21,850 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:21,851 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:21,852 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:21,852 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-20 05:30:23,879 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:23,884 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:23,888 - distributed.scheduler - INFO - State start
2023-10-20 05:30:23,908 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:23,909 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:23,910 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:23,910 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:30:24,161 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37421'
2023-10-20 05:30:24,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35977'
2023-10-20 05:30:24,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44421'
2023-10-20 05:30:24,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44747'
2023-10-20 05:30:24,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34379'
2023-10-20 05:30:24,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36121'
2023-10-20 05:30:24,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46577'
2023-10-20 05:30:24,227 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36583'
2023-10-20 05:30:25,016 - distributed.scheduler - INFO - Receive client connection: Client-c39a2ba6-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:25,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56028
2023-10-20 05:30:26,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,179 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,179 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:26,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:26,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:26,198 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:28,640 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44739
2023-10-20 05:30:28,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44739
2023-10-20 05:30:28,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46351
2023-10-20 05:30:28,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,641 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,641 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,641 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wawmqa7w
2023-10-20 05:30:28,641 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf40c9d8-9f1f-4f95-8384-d6dff7ff3199
2023-10-20 05:30:28,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37301
2023-10-20 05:30:28,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37301
2023-10-20 05:30:28,751 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41387
2023-10-20 05:30:28,751 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,751 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,751 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,751 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k22nudh8
2023-10-20 05:30:28,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9550b10-c48d-4014-aa64-f0c58f79b153
2023-10-20 05:30:28,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e5f5047-0c73-4e4a-b00d-800ee94e611f
2023-10-20 05:30:28,754 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44503
2023-10-20 05:30:28,755 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44503
2023-10-20 05:30:28,755 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36467
2023-10-20 05:30:28,755 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,755 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,755 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,755 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,755 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-paqgtknc
2023-10-20 05:30:28,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95e9fa27-c4da-47fc-a4e0-e82caef92339
2023-10-20 05:30:28,756 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41695
2023-10-20 05:30:28,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41695
2023-10-20 05:30:28,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39699
2023-10-20 05:30:28,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,757 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,757 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,757 - distributed.worker - INFO - Starting Worker plugin PreImport-a77b212c-ab27-4711-8bb6-1e6dcf542322
2023-10-20 05:30:28,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9cp4jvu1
2023-10-20 05:30:28,757 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13c4967f-433a-4063-ba9c-067983ac644c
2023-10-20 05:30:28,757 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92e47972-79ff-4375-877e-cfde34cedd5b
2023-10-20 05:30:28,759 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39001
2023-10-20 05:30:28,760 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39001
2023-10-20 05:30:28,760 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39397
2023-10-20 05:30:28,760 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,761 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,761 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,761 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v7ir_wu5
2023-10-20 05:30:28,761 - distributed.worker - INFO - Starting Worker plugin PreImport-26bd7f3a-b75f-47e3-a146-3b9e3fc97f61
2023-10-20 05:30:28,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4535374e-cefd-4178-b8e4-3465376107c0
2023-10-20 05:30:28,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-937e67d8-5e2c-4384-abf4-f7446c620407
2023-10-20 05:30:28,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-baec7bff-31b9-4c51-86b5-910b06fc48aa
2023-10-20 05:30:28,786 - distributed.worker - INFO - Starting Worker plugin PreImport-1ff3518e-8156-4335-823c-81e8d7d26a3d
2023-10-20 05:30:28,786 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,817 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44739', status: init, memory: 0, processing: 0>
2023-10-20 05:30:28,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44739
2023-10-20 05:30:28,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56044
2023-10-20 05:30:28,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:28,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,821 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:28,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41463
2023-10-20 05:30:28,920 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41463
2023-10-20 05:30:28,920 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35279
2023-10-20 05:30:28,920 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,920 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,921 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,921 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bnubqkck
2023-10-20 05:30:28,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cac194af-a0a1-4c2c-95b7-3430edf645e4
2023-10-20 05:30:28,921 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45217
2023-10-20 05:30:28,922 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45217
2023-10-20 05:30:28,922 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39439
2023-10-20 05:30:28,922 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,922 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,922 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,922 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,922 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kethsiu2
2023-10-20 05:30:28,923 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9280aa1c-ccff-4f42-9230-36b3939c7af9
2023-10-20 05:30:28,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45801
2023-10-20 05:30:28,930 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45801
2023-10-20 05:30:28,930 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34537
2023-10-20 05:30:28,930 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:28,930 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,930 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:28,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:28,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qj80e9h_
2023-10-20 05:30:28,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca533e19-cb50-41ce-8d81-fe8279841e1e
2023-10-20 05:30:28,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0f5df9e-646c-481f-9fe5-d7cca7e662af
2023-10-20 05:30:28,973 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,974 - distributed.worker - INFO - Starting Worker plugin PreImport-a783b92a-9e15-409a-ac0b-167c262a4b0e
2023-10-20 05:30:28,974 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf8f5bc0-aa88-4287-9ace-70bc29d41d03
2023-10-20 05:30:28,974 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:28,975 - distributed.worker - INFO - Starting Worker plugin PreImport-08b17929-81f2-483e-bd5c-c54d4ba15b14
2023-10-20 05:30:28,975 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,001 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44503', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,002 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44503
2023-10-20 05:30:29,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56060
2023-10-20 05:30:29,003 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39001', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39001
2023-10-20 05:30:29,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56068
2023-10-20 05:30:29,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,005 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41695', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,005 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,005 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,005 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41695
2023-10-20 05:30:29,005 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56084
2023-10-20 05:30:29,006 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,006 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,007 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,007 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,010 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37301', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,011 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37301
2023-10-20 05:30:29,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56094
2023-10-20 05:30:29,012 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,013 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,013 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21fca4c6-067e-47c8-98b8-906f74138aca
2023-10-20 05:30:29,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc5c5a91-8109-4b99-ba6c-096eeda0e13e
2023-10-20 05:30:29,081 - distributed.worker - INFO - Starting Worker plugin PreImport-b393b278-9b45-448f-b0c8-70e137886f1d
2023-10-20 05:30:29,082 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,083 - distributed.worker - INFO - Starting Worker plugin PreImport-5609a5d5-d3e0-44c4-af08-d12906200396
2023-10-20 05:30:29,083 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,095 - distributed.worker - INFO - Starting Worker plugin PreImport-a0b64b1e-dd15-4292-ae20-25a61ee8bdae
2023-10-20 05:30:29,096 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45801', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,105 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45801
2023-10-20 05:30:29,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56108
2023-10-20 05:30:29,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,108 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,108 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,147 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45217', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45217
2023-10-20 05:30:29,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56112
2023-10-20 05:30:29,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,150 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,150 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,157 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41463', status: init, memory: 0, processing: 0>
2023-10-20 05:30:29,159 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41463
2023-10-20 05:30:29,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56116
2023-10-20 05:30:29,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:29,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:29,161 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:29,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:29,269 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,281 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,281 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,281 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,282 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,282 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,282 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,282 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,282 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:30:29,288 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,290 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:29,292 - distributed.scheduler - INFO - Remove client Client-c39a2ba6-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:29,293 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56028; closing.
2023-10-20 05:30:29,293 - distributed.scheduler - INFO - Remove client Client-c39a2ba6-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:29,293 - distributed.scheduler - INFO - Close client connection: Client-c39a2ba6-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:29,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37421'. Reason: nanny-close
2023-10-20 05:30:29,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35977'. Reason: nanny-close
2023-10-20 05:30:29,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,296 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44503. Reason: nanny-close
2023-10-20 05:30:29,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44421'. Reason: nanny-close
2023-10-20 05:30:29,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,297 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41463. Reason: nanny-close
2023-10-20 05:30:29,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44747'. Reason: nanny-close
2023-10-20 05:30:29,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44739. Reason: nanny-close
2023-10-20 05:30:29,298 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34379'. Reason: nanny-close
2023-10-20 05:30:29,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56060; closing.
2023-10-20 05:30:29,298 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39001. Reason: nanny-close
2023-10-20 05:30:29,299 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,299 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36121'. Reason: nanny-close
2023-10-20 05:30:29,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,299 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.2992854')
2023-10-20 05:30:29,299 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,299 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45217. Reason: nanny-close
2023-10-20 05:30:29,299 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46577'. Reason: nanny-close
2023-10-20 05:30:29,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,300 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37301. Reason: nanny-close
2023-10-20 05:30:29,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36583'. Reason: nanny-close
2023-10-20 05:30:29,300 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:29,300 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41695. Reason: nanny-close
2023-10-20 05:30:29,300 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,301 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,301 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56044; closing.
2023-10-20 05:30:29,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45801. Reason: nanny-close
2023-10-20 05:30:29,301 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,301 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56116; closing.
2023-10-20 05:30:29,301 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,302 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,302 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,302 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44739', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3023949')
2023-10-20 05:30:29,302 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,302 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3028514')
2023-10-20 05:30:29,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56068; closing.
2023-10-20 05:30:29,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:29,303 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39001', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3037672')
2023-10-20 05:30:29,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56112; closing.
2023-10-20 05:30:29,304 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,304 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,304 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45217', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3048618')
2023-10-20 05:30:29,305 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:29,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56094; closing.
2023-10-20 05:30:29,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56084; closing.
2023-10-20 05:30:29,306 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37301', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3059874')
2023-10-20 05:30:29,306 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3064277')
2023-10-20 05:30:29,306 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56108; closing.
2023-10-20 05:30:29,307 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779829.3072398')
2023-10-20 05:30:29,307 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:30,862 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:30,862 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:30,863 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:30,864 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:30,864 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-20 05:30:33,017 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:33,021 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:33,025 - distributed.scheduler - INFO - State start
2023-10-20 05:30:33,048 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:33,049 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:33,050 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:33,050 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:30:33,207 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34955'
2023-10-20 05:30:33,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46387'
2023-10-20 05:30:33,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42715'
2023-10-20 05:30:33,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43127'
2023-10-20 05:30:33,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34123'
2023-10-20 05:30:33,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35149'
2023-10-20 05:30:33,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42589'
2023-10-20 05:30:33,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41067'
2023-10-20 05:30:34,631 - distributed.scheduler - INFO - Receive client connection: Client-c8ffa667-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:34,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38452
2023-10-20 05:30:35,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,140 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,163 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:35,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:35,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:35,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:37,831 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44031
2023-10-20 05:30:37,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44031
2023-10-20 05:30:37,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38813
2023-10-20 05:30:37,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,832 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,832 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,832 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wgzo5gsa
2023-10-20 05:30:37,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cc80603-6735-44fb-b11f-0d7f9d91a6e4
2023-10-20 05:30:37,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34025
2023-10-20 05:30:37,851 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34025
2023-10-20 05:30:37,851 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32949
2023-10-20 05:30:37,851 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,851 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,851 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,852 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,852 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h6dqcxh_
2023-10-20 05:30:37,852 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bf40d30-e7a1-4853-af19-4117b68d55f3
2023-10-20 05:30:37,852 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29378c8f-94e7-4185-af8b-89d133476bea
2023-10-20 05:30:37,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41529
2023-10-20 05:30:37,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41529
2023-10-20 05:30:37,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37917
2023-10-20 05:30:37,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,855 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,855 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,855 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,855 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8vwrees
2023-10-20 05:30:37,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91c51a9d-1301-4945-b6d6-8663da61703c
2023-10-20 05:30:37,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80d34814-ee8e-460a-9c7d-b8b6170e5943
2023-10-20 05:30:37,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40061
2023-10-20 05:30:37,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40061
2023-10-20 05:30:37,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37013
2023-10-20 05:30:37,861 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,861 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,861 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,861 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,861 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a2zx28nh
2023-10-20 05:30:37,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a83ce196-301b-423e-be1f-8f120c126bd5
2023-10-20 05:30:37,881 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36811
2023-10-20 05:30:37,881 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36811
2023-10-20 05:30:37,881 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44629
2023-10-20 05:30:37,881 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,882 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,882 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,882 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,882 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k6wxoihs
2023-10-20 05:30:37,882 - distributed.worker - INFO - Starting Worker plugin PreImport-ad7c8ed8-c975-4418-b926-eec15e976e0f
2023-10-20 05:30:37,882 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5380c93-82c8-4ae5-a882-2bc1e73b63d0
2023-10-20 05:30:37,889 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41971
2023-10-20 05:30:37,889 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41971
2023-10-20 05:30:37,889 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42185
2023-10-20 05:30:37,889 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,890 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,890 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,890 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,890 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m7y77eja
2023-10-20 05:30:37,890 - distributed.worker - INFO - Starting Worker plugin PreImport-db886ac6-c6cf-4d67-95c7-42c69b2e3a1c
2023-10-20 05:30:37,890 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b36c8c0-7078-466f-9d5a-70439ecac65f
2023-10-20 05:30:37,891 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eeac4189-51ca-4b58-9a98-259c0bc88381
2023-10-20 05:30:37,890 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35313
2023-10-20 05:30:37,891 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35313
2023-10-20 05:30:37,891 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41929
2023-10-20 05:30:37,891 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,891 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,891 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,891 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,891 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h2lyhd03
2023-10-20 05:30:37,892 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ca44d03-d252-4b9e-8f90-df73a936988a
2023-10-20 05:30:37,892 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44387
2023-10-20 05:30:37,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44387
2023-10-20 05:30:37,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46525
2023-10-20 05:30:37,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:37,893 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:37,893 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:37,893 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:30:37,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_tltgf7l
2023-10-20 05:30:37,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c604ff0-f275-4f6a-9c89-44b9aaf7cc09
2023-10-20 05:30:38,032 - distributed.worker - INFO - Starting Worker plugin PreImport-aced1a40-7b1e-42ed-a995-76bdcea44b27
2023-10-20 05:30:38,033 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,041 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63f5fa66-2ac2-4f98-8487-7947d2d8c230
2023-10-20 05:30:38,042 - distributed.worker - INFO - Starting Worker plugin PreImport-c03ba1c0-1ac9-4095-a218-741562b5182e
2023-10-20 05:30:38,042 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,056 - distributed.worker - INFO - Starting Worker plugin PreImport-2f302368-37b8-4549-8142-f8ac76fb597e
2023-10-20 05:30:38,056 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fb27136-dcba-4f1c-8633-4a3a53527281
2023-10-20 05:30:38,057 - distributed.worker - INFO - Starting Worker plugin PreImport-183bcd54-fb24-4301-b1a2-4b03b81df2c1
2023-10-20 05:30:38,057 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41529', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,064 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41529
2023-10-20 05:30:38,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38472
2023-10-20 05:30:38,065 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,066 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,070 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41971', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,071 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41971
2023-10-20 05:30:38,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38474
2023-10-20 05:30:38,072 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,073 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,073 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,074 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,079 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbcba89d-3a47-462a-9389-4db2df74b29f
2023-10-20 05:30:38,080 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44031', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,083 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44031
2023-10-20 05:30:38,083 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38486
2023-10-20 05:30:38,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06b4686e-ee99-4488-af97-17d8c41e8016
2023-10-20 05:30:38,085 - distributed.worker - INFO - Starting Worker plugin PreImport-a42fd9bd-5f25-4c8e-b555-5db69857c922
2023-10-20 05:30:38,085 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,085 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,085 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79dfcc14-483b-4aa6-b4d0-677e60e9654f
2023-10-20 05:30:38,090 - distributed.worker - INFO - Starting Worker plugin PreImport-b01b8b2b-2be5-452b-b0fc-8498f28a077f
2023-10-20 05:30:38,090 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34025', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34025
2023-10-20 05:30:38,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38494
2023-10-20 05:30:38,092 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40061', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,092 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40061
2023-10-20 05:30:38,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38500
2023-10-20 05:30:38,093 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,093 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,095 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,101 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36811', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,102 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36811
2023-10-20 05:30:38,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38512
2023-10-20 05:30:38,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,104 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,104 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35313', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,109 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35313
2023-10-20 05:30:38,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38520
2023-10-20 05:30:38,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,110 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,110 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,119 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44387', status: init, memory: 0, processing: 0>
2023-10-20 05:30:38,120 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44387
2023-10-20 05:30:38,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38522
2023-10-20 05:30:38,121 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:38,122 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:38,122 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:38,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:38,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,177 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,178 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:30:38,182 - distributed.scheduler - INFO - Remove client Client-c8ffa667-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:38,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38452; closing.
2023-10-20 05:30:38,183 - distributed.scheduler - INFO - Remove client Client-c8ffa667-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:38,183 - distributed.scheduler - INFO - Close client connection: Client-c8ffa667-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:38,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34955'. Reason: nanny-close
2023-10-20 05:30:38,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46387'. Reason: nanny-close
2023-10-20 05:30:38,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,186 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36811. Reason: nanny-close
2023-10-20 05:30:38,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42715'. Reason: nanny-close
2023-10-20 05:30:38,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34025. Reason: nanny-close
2023-10-20 05:30:38,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43127'. Reason: nanny-close
2023-10-20 05:30:38,187 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34123'. Reason: nanny-close
2023-10-20 05:30:38,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41529. Reason: nanny-close
2023-10-20 05:30:38,187 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40061. Reason: nanny-close
2023-10-20 05:30:38,188 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35149'. Reason: nanny-close
2023-10-20 05:30:38,188 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38512; closing.
2023-10-20 05:30:38,188 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,188 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44031. Reason: nanny-close
2023-10-20 05:30:38,188 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.1889021')
2023-10-20 05:30:38,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42589'. Reason: nanny-close
2023-10-20 05:30:38,189 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35313. Reason: nanny-close
2023-10-20 05:30:38,189 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41067'. Reason: nanny-close
2023-10-20 05:30:38,189 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:38,190 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41971. Reason: nanny-close
2023-10-20 05:30:38,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38494; closing.
2023-10-20 05:30:38,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,190 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44387. Reason: nanny-close
2023-10-20 05:30:38,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,191 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.191703')
2023-10-20 05:30:38,191 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,192 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,192 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,193 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:38,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38500; closing.
2023-10-20 05:30:38,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38472; closing.
2023-10-20 05:30:38,193 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38486; closing.
2023-10-20 05:30:38,194 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,194 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40061', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.1946328')
2023-10-20 05:30:38,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.1949947')
2023-10-20 05:30:38,195 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:38,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44031', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.1954265')
2023-10-20 05:30:38,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38520; closing.
2023-10-20 05:30:38,196 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.1967144')
2023-10-20 05:30:38,197 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38474; closing.
2023-10-20 05:30:38,197 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38522; closing.
2023-10-20 05:30:38,198 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41971', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.197948')
2023-10-20 05:30:38,198 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779838.198472')
2023-10-20 05:30:38,198 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:39,803 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:39,803 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:39,804 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:39,806 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:39,806 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-20 05:30:42,150 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:42,155 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:42,159 - distributed.scheduler - INFO - State start
2023-10-20 05:30:42,182 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:42,183 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:42,184 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:42,184 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:30:42,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44317'
2023-10-20 05:30:43,999 - distributed.scheduler - INFO - Receive client connection: Client-ce673ef9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:44,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43950
2023-10-20 05:30:44,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:44,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:44,748 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:45,575 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43193
2023-10-20 05:30:45,575 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43193
2023-10-20 05:30:45,575 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-20 05:30:45,575 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:45,575 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:45,576 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:45,576 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-20 05:30:45,576 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hbmozmr6
2023-10-20 05:30:45,576 - distributed.worker - INFO - Starting Worker plugin PreImport-5004a967-7919-448d-af14-94bd3cd5c932
2023-10-20 05:30:45,576 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f99ccb6-9f3d-4fbe-8c75-e3affe57997d
2023-10-20 05:30:45,576 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-488806b0-066a-4c09-a530-8e4a9e9fe7ac
2023-10-20 05:30:45,577 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:45,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43193', status: init, memory: 0, processing: 0>
2023-10-20 05:30:45,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43193
2023-10-20 05:30:45,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43970
2023-10-20 05:30:45,600 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:45,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:45,601 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:45,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:45,649 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:45,651 - distributed.scheduler - INFO - Remove client Client-ce673ef9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:45,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43950; closing.
2023-10-20 05:30:45,652 - distributed.scheduler - INFO - Remove client Client-ce673ef9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:45,652 - distributed.scheduler - INFO - Close client connection: Client-ce673ef9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:45,653 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44317'. Reason: nanny-close
2023-10-20 05:30:45,653 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:45,654 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43193. Reason: nanny-close
2023-10-20 05:30:45,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43970; closing.
2023-10-20 05:30:45,656 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:45,657 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779845.6569362')
2023-10-20 05:30:45,657 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:45,658 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:46,619 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:46,619 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:46,620 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:46,621 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:46,621 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-20 05:30:50,751 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:50,755 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:50,758 - distributed.scheduler - INFO - State start
2023-10-20 05:30:50,778 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:50,779 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:50,780 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:50,780 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:30:50,786 - distributed.scheduler - INFO - Receive client connection: Client-d3a478d9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:50,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54044
2023-10-20 05:30:50,866 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34709'
2023-10-20 05:30:52,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:30:52,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:30:53,214 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:30:54,123 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37063
2023-10-20 05:30:54,124 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37063
2023-10-20 05:30:54,124 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34371
2023-10-20 05:30:54,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:30:54,124 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:54,124 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:30:54,124 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-20 05:30:54,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fmcz90a_
2023-10-20 05:30:54,124 - distributed.worker - INFO - Starting Worker plugin PreImport-28b214dd-664f-4181-bd65-f081ac9fa842
2023-10-20 05:30:54,126 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5e606ee-7a72-4788-894e-ace236c6db43
2023-10-20 05:30:54,126 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89514e76-e491-4fe5-add2-db400979da84
2023-10-20 05:30:54,126 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:54,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37063', status: init, memory: 0, processing: 0>
2023-10-20 05:30:54,295 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37063
2023-10-20 05:30:54,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54072
2023-10-20 05:30:54,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:30:54,299 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:30:54,299 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:30:54,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:30:54,336 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:30:54,339 - distributed.scheduler - INFO - Remove client Client-d3a478d9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:54,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54044; closing.
2023-10-20 05:30:54,340 - distributed.scheduler - INFO - Remove client Client-d3a478d9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:54,340 - distributed.scheduler - INFO - Close client connection: Client-d3a478d9-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:30:54,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34709'. Reason: nanny-close
2023-10-20 05:30:54,352 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:30:54,360 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37063. Reason: nanny-close
2023-10-20 05:30:54,369 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54072; closing.
2023-10-20 05:30:54,369 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:30:54,369 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37063', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779854.369904')
2023-10-20 05:30:54,370 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:30:54,372 - distributed.nanny - INFO - Worker closed
2023-10-20 05:30:55,607 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:30:55,607 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:30:55,608 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:30:55,609 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:30:55,609 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-20 05:30:57,629 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:57,633 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:30:57,636 - distributed.scheduler - INFO - State start
2023-10-20 05:30:57,657 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:30:57,658 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:30:57,659 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:30:57,659 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:31:01,631 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:54080'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54080>: Stream is closed
2023-10-20 05:31:01,944 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:31:01,944 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:31:01,945 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:31:01,945 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:31:01,946 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-20 05:31:03,974 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:03,978 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:31:03,981 - distributed.scheduler - INFO - State start
2023-10-20 05:31:04,002 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:04,003 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-20 05:31:04,003 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:31:04,003 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:31:04,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35661'
2023-10-20 05:31:04,448 - distributed.scheduler - INFO - Receive client connection: Client-db7efb08-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:04,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56746
2023-10-20 05:31:05,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:05,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:05,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:06,446 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41077
2023-10-20 05:31:06,447 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41077
2023-10-20 05:31:06,447 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46503
2023-10-20 05:31:06,447 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-20 05:31:06,447 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:06,447 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:06,447 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-20 05:31:06,447 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mpp93hx1
2023-10-20 05:31:06,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e6f6116-3f4e-4f2c-a46b-c5bb94287c43
2023-10-20 05:31:06,448 - distributed.worker - INFO - Starting Worker plugin PreImport-5296dd3f-81f6-46c5-b53f-fb676b83a5c1
2023-10-20 05:31:06,448 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77a440a5-0342-4b81-bb54-a38d1754b7f0
2023-10-20 05:31:06,448 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:06,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41077', status: init, memory: 0, processing: 0>
2023-10-20 05:31:06,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41077
2023-10-20 05:31:06,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56774
2023-10-20 05:31:06,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:06,467 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-20 05:31:06,467 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:06,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-20 05:31:06,497 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:06,499 - distributed.scheduler - INFO - Remove client Client-db7efb08-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:06,499 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56746; closing.
2023-10-20 05:31:06,499 - distributed.scheduler - INFO - Remove client Client-db7efb08-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:06,500 - distributed.scheduler - INFO - Close client connection: Client-db7efb08-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:06,500 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35661'. Reason: nanny-close
2023-10-20 05:31:06,501 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:06,502 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41077. Reason: nanny-close
2023-10-20 05:31:06,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56774; closing.
2023-10-20 05:31:06,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-20 05:31:06,504 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779866.5042005')
2023-10-20 05:31:06,504 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:31:06,505 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:07,265 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:31:07,266 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:31:07,266 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:31:07,267 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-20 05:31:07,267 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-20 05:31:09,283 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:09,287 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:31:09,290 - distributed.scheduler - INFO - State start
2023-10-20 05:31:09,310 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:09,311 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:31:09,312 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:31:09,312 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:31:09,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38119'
2023-10-20 05:31:09,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43183'
2023-10-20 05:31:09,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35375'
2023-10-20 05:31:09,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42073'
2023-10-20 05:31:09,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42243'
2023-10-20 05:31:09,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43863'
2023-10-20 05:31:09,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37251'
2023-10-20 05:31:09,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42401'
2023-10-20 05:31:10,310 - distributed.scheduler - INFO - Receive client connection: Client-dea511eb-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:10,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37268
2023-10-20 05:31:11,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,281 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,319 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,319 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:11,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:11,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:11,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:13,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35539
2023-10-20 05:31:13,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34951
2023-10-20 05:31:13,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34951
2023-10-20 05:31:13,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35539
2023-10-20 05:31:13,619 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38199
2023-10-20 05:31:13,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46209
2023-10-20 05:31:13,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,620 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,620 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,620 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,620 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v5gzkfa8
2023-10-20 05:31:13,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mv5eao48
2023-10-20 05:31:13,620 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f90c2b2-2a99-416b-b0a8-0b1810d471a5
2023-10-20 05:31:13,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05446eef-6714-4df8-ac37-bd02af23a78b
2023-10-20 05:31:13,695 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46627
2023-10-20 05:31:13,696 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46627
2023-10-20 05:31:13,696 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34703
2023-10-20 05:31:13,696 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,696 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,696 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,696 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,696 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m08q0ax6
2023-10-20 05:31:13,696 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d32cfd2-f305-4a6a-a884-1e3eb6bd8270
2023-10-20 05:31:13,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33281
2023-10-20 05:31:13,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33281
2023-10-20 05:31:13,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41585
2023-10-20 05:31:13,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,708 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,708 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m_71sfcn
2023-10-20 05:31:13,709 - distributed.worker - INFO - Starting Worker plugin PreImport-bb35b7f0-7e80-4155-99d4-9a1a959a0dea
2023-10-20 05:31:13,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c98a38e-a3c5-4b15-9ea3-0654f72d84d7
2023-10-20 05:31:13,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2e041e9-27ff-46ab-ac58-ad2a23f8e9df
2023-10-20 05:31:13,710 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39737
2023-10-20 05:31:13,712 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39737
2023-10-20 05:31:13,712 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45701
2023-10-20 05:31:13,712 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,712 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,712 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,712 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,712 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mvdb3oz5
2023-10-20 05:31:13,713 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c3be20e-8250-4d73-93c3-ba5e0bb0b972
2023-10-20 05:31:13,713 - distributed.worker - INFO - Starting Worker plugin PreImport-c72958d0-8caf-49b5-94f6-96c4b439efe1
2023-10-20 05:31:13,713 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92ae3d2c-8ec6-4067-b304-62a78a4f72c9
2023-10-20 05:31:13,712 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33335
2023-10-20 05:31:13,713 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33335
2023-10-20 05:31:13,713 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43043
2023-10-20 05:31:13,714 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,714 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,714 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,714 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,714 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5b0bois2
2023-10-20 05:31:13,714 - distributed.worker - INFO - Starting Worker plugin PreImport-17292b49-0b21-430e-bd6e-8819f6e73a5f
2023-10-20 05:31:13,715 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65acda9e-9345-42b8-b18b-8b5cc79ddbd5
2023-10-20 05:31:13,715 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a678e6e-401e-4d56-bef1-8fb2d11f46b9
2023-10-20 05:31:13,725 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39139
2023-10-20 05:31:13,726 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39139
2023-10-20 05:31:13,726 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41149
2023-10-20 05:31:13,726 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,726 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,726 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,726 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pw4pi5yz
2023-10-20 05:31:13,725 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41483
2023-10-20 05:31:13,726 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41483
2023-10-20 05:31:13,726 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38231
2023-10-20 05:31:13,726 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,726 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7390e86-59ac-4672-a7af-d3c5e8d30723
2023-10-20 05:31:13,727 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:13,727 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-20 05:31:13,727 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rwf35a_0
2023-10-20 05:31:13,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92a3d580-e6eb-41f8-a05d-658a3e59c76d
2023-10-20 05:31:13,727 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d8f50f3-b8b3-4ec3-9856-7cad3959e101
2023-10-20 05:31:13,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e41814e-16b8-414e-a745-5d17f7db92d5
2023-10-20 05:31:13,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a229a336-bfea-471a-bcea-5d50f2659bae
2023-10-20 05:31:13,756 - distributed.worker - INFO - Starting Worker plugin PreImport-82df08cf-d489-4852-8540-b723f06f1f8f
2023-10-20 05:31:13,756 - distributed.worker - INFO - Starting Worker plugin PreImport-5cc4538b-3b03-4123-812d-2dfb62595f1c
2023-10-20 05:31:13,756 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,756 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,778 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34951', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,779 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34951
2023-10-20 05:31:13,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37278
2023-10-20 05:31:13,780 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35539', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,780 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35539
2023-10-20 05:31:13,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37290
2023-10-20 05:31:13,781 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,781 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,783 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,783 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,844 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,844 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07b3315b-8d46-4b3e-b692-f17472bf3d5d
2023-10-20 05:31:13,851 - distributed.worker - INFO - Starting Worker plugin PreImport-7223ceff-10da-4b81-a4c2-997e22734983
2023-10-20 05:31:13,852 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,856 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d115c5aa-c9e0-45e1-96ff-50caae895f17
2023-10-20 05:31:13,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,856 - distributed.worker - INFO - Starting Worker plugin PreImport-c4d4c969-aea7-4f78-b719-19b14c1a4d0a
2023-10-20 05:31:13,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,858 - distributed.worker - INFO - Starting Worker plugin PreImport-410abd0c-e1ef-4a3f-9343-d4ca03e49953
2023-10-20 05:31:13,859 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,872 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33281', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33281
2023-10-20 05:31:13,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37304
2023-10-20 05:31:13,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,875 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33335', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,875 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33335
2023-10-20 05:31:13,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37310
2023-10-20 05:31:13,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,876 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,878 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39737', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,878 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,878 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,878 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39737
2023-10-20 05:31:13,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37334
2023-10-20 05:31:13,879 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41483', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,880 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41483
2023-10-20 05:31:13,880 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,880 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37350
2023-10-20 05:31:13,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,881 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46627', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,881 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,882 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46627
2023-10-20 05:31:13,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37326
2023-10-20 05:31:13,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,883 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,884 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,884 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39139', status: init, memory: 0, processing: 0>
2023-10-20 05:31:13,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39139
2023-10-20 05:31:13,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37358
2023-10-20 05:31:13,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:13,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:13,889 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:13,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,903 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-20 05:31:13,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:13,920 - distributed.scheduler - INFO - Remove client Client-dea511eb-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:13,920 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37268; closing.
2023-10-20 05:31:13,920 - distributed.scheduler - INFO - Remove client Client-dea511eb-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:13,921 - distributed.scheduler - INFO - Close client connection: Client-dea511eb-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:13,921 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38119'. Reason: nanny-close
2023-10-20 05:31:13,922 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43183'. Reason: nanny-close
2023-10-20 05:31:13,922 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35375'. Reason: nanny-close
2023-10-20 05:31:13,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42073'. Reason: nanny-close
2023-10-20 05:31:13,923 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34951. Reason: nanny-close
2023-10-20 05:31:13,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42243'. Reason: nanny-close
2023-10-20 05:31:13,923 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43863'. Reason: nanny-close
2023-10-20 05:31:13,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37251'. Reason: nanny-close
2023-10-20 05:31:13,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35539. Reason: nanny-close
2023-10-20 05:31:13,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42401'. Reason: nanny-close
2023-10-20 05:31:13,924 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,925 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,925 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37278; closing.
2023-10-20 05:31:13,925 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.925641')
2023-10-20 05:31:13,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33281. Reason: nanny-close
2023-10-20 05:31:13,926 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,926 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,927 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37290; closing.
2023-10-20 05:31:13,927 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,928 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,928 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,928 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35539', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.928309')
2023-10-20 05:31:13,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39139. Reason: nanny-close
2023-10-20 05:31:13,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33335. Reason: nanny-close
2023-10-20 05:31:13,929 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,929 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37304; closing.
2023-10-20 05:31:13,930 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41483. Reason: nanny-close
2023-10-20 05:31:13,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,930 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33281', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.9303522')
2023-10-20 05:31:13,930 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,930 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,931 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46627. Reason: nanny-close
2023-10-20 05:31:13,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37358; closing.
2023-10-20 05:31:13,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,932 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:13,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.9322379')
2023-10-20 05:31:13,932 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37310; closing.
2023-10-20 05:31:13,933 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39737. Reason: nanny-close
2023-10-20 05:31:13,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,933 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,933 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.9332566')
2023-10-20 05:31:13,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37350; closing.
2023-10-20 05:31:13,934 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:13,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41483', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.9346159')
2023-10-20 05:31:13,934 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,935 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37326; closing.
2023-10-20 05:31:13,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46627', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.935642')
2023-10-20 05:31:13,935 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:13,936 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37350>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-20 05:31:13,937 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37334; closing.
2023-10-20 05:31:13,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39737', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779873.9379385')
2023-10-20 05:31:13,938 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:31:15,439 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:31:15,440 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:31:15,440 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:31:15,442 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:31:15,442 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-20 05:31:17,556 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:17,561 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:31:17,564 - distributed.scheduler - INFO - State start
2023-10-20 05:31:17,586 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:17,586 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:31:17,587 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:31:17,587 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:31:17,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41811'
2023-10-20 05:31:19,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:19,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:19,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:19,404 - distributed.scheduler - INFO - Receive client connection: Client-e38f9619-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:19,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37484
2023-10-20 05:31:20,275 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43039
2023-10-20 05:31:20,276 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43039
2023-10-20 05:31:20,276 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44327
2023-10-20 05:31:20,276 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:20,276 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:20,276 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:20,276 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-20 05:31:20,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bjkcjniq
2023-10-20 05:31:20,277 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2601cb2-1515-4ca3-93ac-bb3c8f1df16c
2023-10-20 05:31:20,277 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aec869da-1450-49d0-a478-31afe5c9d729
2023-10-20 05:31:20,370 - distributed.worker - INFO - Starting Worker plugin PreImport-e9e5e5ed-fab4-404d-9e8b-7f32c50dc78c
2023-10-20 05:31:20,370 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:20,390 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43039', status: init, memory: 0, processing: 0>
2023-10-20 05:31:20,391 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43039
2023-10-20 05:31:20,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41178
2023-10-20 05:31:20,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:20,393 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:20,393 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:20,394 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:20,449 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:31:20,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:20,454 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:20,457 - distributed.scheduler - INFO - Remove client Client-e38f9619-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:20,457 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37484; closing.
2023-10-20 05:31:20,457 - distributed.scheduler - INFO - Remove client Client-e38f9619-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:20,457 - distributed.scheduler - INFO - Close client connection: Client-e38f9619-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:20,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41811'. Reason: nanny-close
2023-10-20 05:31:20,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:20,460 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43039. Reason: nanny-close
2023-10-20 05:31:20,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41178; closing.
2023-10-20 05:31:20,462 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:20,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779880.4624987')
2023-10-20 05:31:20,462 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:31:20,463 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:21,475 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:31:21,475 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:31:21,476 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:31:21,477 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:31:21,478 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-20 05:31:23,654 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:23,660 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-20 05:31:23,664 - distributed.scheduler - INFO - State start
2023-10-20 05:31:23,688 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-20 05:31:23,689 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-20 05:31:23,690 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-20 05:31:23,690 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-20 05:31:24,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34873'
2023-10-20 05:31:25,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-20 05:31:25,992 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-20 05:31:25,996 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-20 05:31:26,828 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36931
2023-10-20 05:31:26,828 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36931
2023-10-20 05:31:26,828 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32935
2023-10-20 05:31:26,828 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-20 05:31:26,829 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:26,829 - distributed.worker - INFO -               Threads:                          1
2023-10-20 05:31:26,829 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-20 05:31:26,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8j57svd9
2023-10-20 05:31:26,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20256343-0f1e-4fda-b4da-909ff629f657
2023-10-20 05:31:26,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a720da0-0be1-4f06-8312-5e628acbb8f7
2023-10-20 05:31:26,927 - distributed.worker - INFO - Starting Worker plugin PreImport-8850b001-9211-4df5-92ab-2d8f79fc204e
2023-10-20 05:31:26,928 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:26,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36931', status: init, memory: 0, processing: 0>
2023-10-20 05:31:26,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36931
2023-10-20 05:31:26,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41272
2023-10-20 05:31:26,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-20 05:31:26,969 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-20 05:31:26,969 - distributed.worker - INFO - -------------------------------------------------
2023-10-20 05:31:26,971 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-20 05:31:27,957 - distributed.scheduler - INFO - Receive client connection: Client-e7314284-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:27,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41278
2023-10-20 05:31:27,965 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-20 05:31:27,969 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-20 05:31:27,972 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:27,974 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-20 05:31:27,976 - distributed.scheduler - INFO - Remove client Client-e7314284-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:27,977 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41278; closing.
2023-10-20 05:31:27,977 - distributed.scheduler - INFO - Remove client Client-e7314284-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:27,977 - distributed.scheduler - INFO - Close client connection: Client-e7314284-6f09-11ee-b8d6-d8c49764f6bb
2023-10-20 05:31:27,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34873'. Reason: nanny-close
2023-10-20 05:31:27,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-20 05:31:27,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36931. Reason: nanny-close
2023-10-20 05:31:27,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-20 05:31:27,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41272; closing.
2023-10-20 05:31:27,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36931', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697779887.9822383')
2023-10-20 05:31:27,982 - distributed.scheduler - INFO - Lost all workers
2023-10-20 05:31:27,983 - distributed.nanny - INFO - Worker closed
2023-10-20 05:31:29,045 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-20 05:31:29,045 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-20 05:31:29,046 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-20 05:31:29,047 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-20 05:31:29,047 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] 2023-10-20 05:32:45,220 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-20 05:32:45,226 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1281' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] [1697779986.176866] [dgx13:69177:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:47123) failed: Address already in use
[1697779986.177350] [dgx13:69177:0]            sock.c:470  UCX  ERROR bind(fd=138 addr=0.0.0.0:44677) failed: Address already in use
[1697779987.702634] [dgx13:69266:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:59050) failed: Address already in use
[1697779987.702692] [dgx13:69266:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:57930) failed: Address already in use
[1697779987.704676] [dgx13:69270:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:49540) failed: Address already in use
[1697779991.424478] [dgx13:69263:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:43688) failed: Address already in use
[1697779991.424587] [dgx13:69263:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:60188) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] [1697780105.855251] [dgx13:71892:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:43616) failed: Address already in use
[1697780107.235068] [dgx13:71892:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:49437) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] [1697780122.755091] [dgx13:72100:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:54662) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41219 instead
  warnings.warn(
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5130, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42595 instead
  warnings.warn(
2023-10-20 05:38:09,937 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-20 05:38:09,944 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:35817'.
2023-10-20 05:38:09,945 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:35817'. Shutting down.
2023-10-20 05:38:09,947 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f140f9ee040>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-20 05:38:10,813 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-20 05:38:10,823 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:40163'.
2023-10-20 05:38:10,823 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:40163'. Shutting down.
2023-10-20 05:38:10,826 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f82cf100040>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-20 05:38:11,949 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-20 05:38:12,831 - distributed.nanny - ERROR - Worker process died unexpectedly
