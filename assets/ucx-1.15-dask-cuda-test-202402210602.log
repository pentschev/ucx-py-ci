============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-21 07:04:08,740 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:08,744 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40043 instead
  warnings.warn(
2024-02-21 07:04:08,749 - distributed.scheduler - INFO - State start
2024-02-21 07:04:09,278 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:09,279 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-21 07:04:09,280 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40043/status
2024-02-21 07:04:09,281 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:04:09,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46173'
2024-02-21 07:04:09,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39109'
2024-02-21 07:04:09,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35117'
2024-02-21 07:04:09,757 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42767'
2024-02-21 07:04:11,287 - distributed.scheduler - INFO - Receive client connection: Client-675df6a4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:11,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45532
2024-02-21 07:04:11,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:11,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:11,658 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:11,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43783
2024-02-21 07:04:11,659 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43783
2024-02-21 07:04:11,659 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42489
2024-02-21 07:04:11,659 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,659 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,659 - distributed.worker - INFO -               Threads:                          4
2024-02-21 07:04:11,659 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-21 07:04:11,659 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zye0k2xq
2024-02-21 07:04:11,660 - distributed.worker - INFO - Starting Worker plugin PreImport-2e9e35d6-e8bf-48a3-a686-baac1d945303
2024-02-21 07:04:11,660 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2411de59-bd5b-44fe-aae3-185c669df0a1
2024-02-21 07:04:11,660 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c704e05f-f400-40ec-9503-e588e9b79452
2024-02-21 07:04:11,660 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:11,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:11,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:11,677 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:11,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:11,680 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34673
2024-02-21 07:04:11,680 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34673
2024-02-21 07:04:11,681 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36719
2024-02-21 07:04:11,681 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,681 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,681 - distributed.worker - INFO -               Threads:                          4
2024-02-21 07:04:11,681 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-21 07:04:11,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-334wu0yc
2024-02-21 07:04:11,681 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b986ea8d-bdbb-4f63-bf8e-c84bd91153a7
2024-02-21 07:04:11,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df103653-ca31-41f8-b517-f68b46827271
2024-02-21 07:04:11,681 - distributed.worker - INFO - Starting Worker plugin PreImport-63bad14d-60a0-4b54-bc12-f3c3d2e253c8
2024-02-21 07:04:11,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:11,682 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,682 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34417
2024-02-21 07:04:11,682 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34417
2024-02-21 07:04:11,682 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41531
2024-02-21 07:04:11,683 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,683 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,683 - distributed.worker - INFO -               Threads:                          4
2024-02-21 07:04:11,683 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-21 07:04:11,683 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zaoxgnxc
2024-02-21 07:04:11,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffa92d5f-9c37-483d-a0a1-167ecd00fd61
2024-02-21 07:04:11,683 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-019dd7fb-ecb7-40e9-b82a-cc730ddcc4c2
2024-02-21 07:04:11,683 - distributed.worker - INFO - Starting Worker plugin PreImport-7f5c3a40-d1a3-4767-afc5-c709df958d33
2024-02-21 07:04:11,683 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,722 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43783', status: init, memory: 0, processing: 0>
2024-02-21 07:04:11,723 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43783
2024-02-21 07:04:11,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45546
2024-02-21 07:04:11,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:11,725 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,725 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-21 07:04:11,728 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:11,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:11,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:11,733 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40423
2024-02-21 07:04:11,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40423
2024-02-21 07:04:11,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33365
2024-02-21 07:04:11,733 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,733 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,733 - distributed.worker - INFO -               Threads:                          4
2024-02-21 07:04:11,733 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-21 07:04:11,733 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8y97637i
2024-02-21 07:04:11,733 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b168ea61-e30b-4026-a66d-c378cb52bb70
2024-02-21 07:04:11,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0515df7-f0f5-4812-9df0-c679d09a4874
2024-02-21 07:04:11,734 - distributed.worker - INFO - Starting Worker plugin PreImport-714129c7-9041-4fe1-b625-08195df52e32
2024-02-21 07:04:11,734 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,781 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34673', status: init, memory: 0, processing: 0>
2024-02-21 07:04:11,782 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34673
2024-02-21 07:04:11,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45554
2024-02-21 07:04:11,783 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34417', status: init, memory: 0, processing: 0>
2024-02-21 07:04:11,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:11,783 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34417
2024-02-21 07:04:11,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45558
2024-02-21 07:04:11,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,784 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,784 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:11,785 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,785 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-21 07:04:11,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-21 07:04:11,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40423', status: init, memory: 0, processing: 0>
2024-02-21 07:04:11,805 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40423
2024-02-21 07:04:11,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45572
2024-02-21 07:04:11,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:11,807 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-21 07:04:11,807 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:11,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-21 07:04:11,818 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-21 07:04:11,819 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-21 07:04:11,819 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-21 07:04:11,819 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-21 07:04:11,824 - distributed.scheduler - INFO - Remove client Client-675df6a4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:11,824 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45532; closing.
2024-02-21 07:04:11,824 - distributed.scheduler - INFO - Remove client Client-675df6a4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:11,824 - distributed.scheduler - INFO - Close client connection: Client-675df6a4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:11,826 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46173'. Reason: nanny-close
2024-02-21 07:04:11,826 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:11,827 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39109'. Reason: nanny-close
2024-02-21 07:04:11,827 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:11,827 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35117'. Reason: nanny-close
2024-02-21 07:04:11,827 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34673. Reason: nanny-close
2024-02-21 07:04:11,828 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:11,828 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42767'. Reason: nanny-close
2024-02-21 07:04:11,828 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34417. Reason: nanny-close
2024-02-21 07:04:11,829 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43783. Reason: nanny-close
2024-02-21 07:04:11,829 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45554; closing.
2024-02-21 07:04:11,829 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-21 07:04:11,830 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499051.8299851')
2024-02-21 07:04:11,830 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-21 07:04:11,830 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-21 07:04:11,831 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:11,831 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45546; closing.
2024-02-21 07:04:11,831 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45558; closing.
2024-02-21 07:04:11,831 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:11,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499051.831965')
2024-02-21 07:04:11,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34417', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499051.8323467')
2024-02-21 07:04:11,832 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:11,850 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:11,851 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40423. Reason: nanny-close
2024-02-21 07:04:11,853 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45572; closing.
2024-02-21 07:04:11,853 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-21 07:04:11,853 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499051.853514')
2024-02-21 07:04:11,853 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:04:11,854 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:12,591 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:04:12,592 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:04:12,592 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:04:12,594 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-21 07:04:12,595 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-21 07:04:15,073 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:15,079 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43253 instead
  warnings.warn(
2024-02-21 07:04:15,084 - distributed.scheduler - INFO - State start
2024-02-21 07:04:15,108 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:15,109 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-21 07:04:15,110 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:04:15,111 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-21 07:04:15,265 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36869'
2024-02-21 07:04:15,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34029'
2024-02-21 07:04:15,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38261'
2024-02-21 07:04:15,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37573'
2024-02-21 07:04:15,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37961'
2024-02-21 07:04:15,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40279'
2024-02-21 07:04:15,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35749'
2024-02-21 07:04:15,337 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33881'
2024-02-21 07:04:17,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,260 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,261 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34669
2024-02-21 07:04:17,261 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34669
2024-02-21 07:04:17,261 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39959
2024-02-21 07:04:17,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,261 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,261 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,261 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,262 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n11_otuu
2024-02-21 07:04:17,262 - distributed.worker - INFO - Starting Worker plugin PreImport-4d4dc592-6908-4825-a7f4-ca4af00136ff
2024-02-21 07:04:17,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8811d90-3ddf-4c89-9efa-7c9fc160a7aa
2024-02-21 07:04:17,291 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,303 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34671
2024-02-21 07:04:17,303 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34671
2024-02-21 07:04:17,303 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42047
2024-02-21 07:04:17,303 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,303 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,303 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,303 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,303 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6e2pyxph
2024-02-21 07:04:17,304 - distributed.worker - INFO - Starting Worker plugin PreImport-a6db31fe-afcd-4ced-af13-851e21e5b6a4
2024-02-21 07:04:17,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06eb6290-1469-401b-a8f2-5f11f57d7d26
2024-02-21 07:04:17,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed3bbcf1-f06c-461d-82a8-6c1fa72c7442
2024-02-21 07:04:17,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,307 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41765
2024-02-21 07:04:17,307 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41765
2024-02-21 07:04:17,307 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40527
2024-02-21 07:04:17,307 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,307 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,307 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,307 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uehnb5ph
2024-02-21 07:04:17,308 - distributed.worker - INFO - Starting Worker plugin PreImport-f5c0abf1-66c5-4f77-a523-d63bdd9953ab
2024-02-21 07:04:17,308 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ef1553f-a57a-40cd-ba0f-a925f8e0d3f9
2024-02-21 07:04:17,308 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a338a28d-1ee8-478d-94d9-2b774c3bed99
2024-02-21 07:04:17,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42729
2024-02-21 07:04:17,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42729
2024-02-21 07:04:17,323 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36459
2024-02-21 07:04:17,323 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,323 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,323 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,323 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,323 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ke5wz4ez
2024-02-21 07:04:17,323 - distributed.worker - INFO - Starting Worker plugin PreImport-90833a61-57b8-447c-8037-ca8d9c652221
2024-02-21 07:04:17,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0b8ea95-fdd0-49c2-99e4-6820e4e2d531
2024-02-21 07:04:17,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,518 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,531 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34871
2024-02-21 07:04:17,533 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34871
2024-02-21 07:04:17,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36781
2024-02-21 07:04:17,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,534 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,534 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x1lc4f2s
2024-02-21 07:04:17,534 - distributed.worker - INFO - Starting Worker plugin PreImport-f0127c55-ff62-445a-8c01-3b90279d7106
2024-02-21 07:04:17,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51175775-1439-4fcd-ab7c-7849aa92f20f
2024-02-21 07:04:17,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:17,548 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:17,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41723
2024-02-21 07:04:17,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41723
2024-02-21 07:04:17,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41939
2024-02-21 07:04:17,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,549 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,549 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-je9grgl0
2024-02-21 07:04:17,549 - distributed.worker - INFO - Starting Worker plugin PreImport-8de9412e-f9f1-4c08-b7ea-4ca4a9c5ba2c
2024-02-21 07:04:17,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c011376e-3ba7-4aa7-a6a1-ecec49fe88e5
2024-02-21 07:04:17,549 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42999
2024-02-21 07:04:17,550 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42999
2024-02-21 07:04:17,550 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35431
2024-02-21 07:04:17,550 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,550 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,550 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,550 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,550 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jcvgt711
2024-02-21 07:04:17,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e5f59fe-059e-4d80-b8f8-04a479976539
2024-02-21 07:04:17,551 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ef85561-23b5-4823-9086-b63dd9ac87c9
2024-02-21 07:04:17,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:17,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37611
2024-02-21 07:04:17,565 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37611
2024-02-21 07:04:17,565 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38745
2024-02-21 07:04:17,565 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:17,565 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:17,565 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:17,565 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:17,565 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-inggurj6
2024-02-21 07:04:17,566 - distributed.worker - INFO - Starting Worker plugin PreImport-4aac23ae-7c53-4a3a-9f36-ec1821b0c3bf
2024-02-21 07:04:17,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91d4fb50-5a32-42d0-8b33-f558e86b5709
2024-02-21 07:04:18,353 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88e7e4f7-5b00-4e21-861d-8996e69f3d1a
2024-02-21 07:04:18,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,459 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f8c275f-64a6-442b-876b-00d42df5c589
2024-02-21 07:04:19,460 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,472 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,504 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c4b604e-771b-4e22-841f-7cfd70feb64d
2024-02-21 07:04:19,541 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,564 - distributed.worker - INFO - Starting Worker plugin PreImport-8536cb74-7a33-4501-83d3-4aa4fe0fed08
2024-02-21 07:04:19,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-add56405-eec0-4f97-8ba0-8c080be4374f
2024-02-21 07:04:19,580 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:19,585 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3518c1f-638d-4666-a270-1b772ddc0fdd
2024-02-21 07:04:19,586 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:22,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:22,844 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:22,844 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:22,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:22,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33881'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-21 07:04:22,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-21 07:04:22,872 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37611. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-21 07:04:22,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:22,876 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:22,897 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:22,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:22,898 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:22,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:22,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35749'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-21 07:04:22,914 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-21 07:04:22,914 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42999. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-21 07:04:22,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:22,919 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:42786 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-21 07:04:23,278 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52685 parent=52492 started daemon>
2024-02-21 07:04:23,278 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52682 parent=52492 started daemon>
2024-02-21 07:04:23,278 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52678 parent=52492 started daemon>
2024-02-21 07:04:23,278 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52672 parent=52492 started daemon>
2024-02-21 07:04:23,278 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52667 parent=52492 started daemon>
2024-02-21 07:04:23,279 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52662 parent=52492 started daemon>
2024-02-21 07:04:23,279 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52658 parent=52492 started daemon>
2024-02-21 07:04:23,585 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52662 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-21 07:04:32,882 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:32,887 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34137 instead
  warnings.warn(
2024-02-21 07:04:32,891 - distributed.scheduler - INFO - State start
2024-02-21 07:04:32,892 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ke5wz4ez', purging
2024-02-21 07:04:32,893 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-je9grgl0', purging
2024-02-21 07:04:32,893 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x1lc4f2s', purging
2024-02-21 07:04:32,894 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uehnb5ph', purging
2024-02-21 07:04:32,894 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6e2pyxph', purging
2024-02-21 07:04:32,894 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-n11_otuu', purging
2024-02-21 07:04:32,923 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:32,925 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:04:32,925 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34137/status
2024-02-21 07:04:32,926 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:04:32,949 - distributed.scheduler - INFO - Receive client connection: Client-75b8b7f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:32,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53078
2024-02-21 07:04:33,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43831'
2024-02-21 07:04:33,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32913'
2024-02-21 07:04:33,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39325'
2024-02-21 07:04:33,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36287'
2024-02-21 07:04:33,204 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44617'
2024-02-21 07:04:33,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36051'
2024-02-21 07:04:33,220 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41353'
2024-02-21 07:04:33,229 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39093'
2024-02-21 07:04:35,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,122 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,123 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,123 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40801
2024-02-21 07:04:35,123 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40801
2024-02-21 07:04:35,123 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35399
2024-02-21 07:04:35,123 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32943
2024-02-21 07:04:35,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,124 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32943
2024-02-21 07:04:35,124 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,124 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35695
2024-02-21 07:04:35,124 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,124 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,124 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vxtaxrsb
2024-02-21 07:04:35,124 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,124 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p5l3d7r9
2024-02-21 07:04:35,124 - distributed.worker - INFO - Starting Worker plugin PreImport-aca1cec6-8b59-435f-a988-415053bfb766
2024-02-21 07:04:35,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85dbc984-e4ed-4092-ba5a-be8ac3a39f82
2024-02-21 07:04:35,124 - distributed.worker - INFO - Starting Worker plugin PreImport-5f22adfc-25fa-408e-a763-040c330e293a
2024-02-21 07:04:35,124 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9441f208-aa85-4acf-b57c-322737f0a511
2024-02-21 07:04:35,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30304a52-aaf6-42f7-b85a-fe5a93ce0239
2024-02-21 07:04:35,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,133 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,134 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45881
2024-02-21 07:04:35,134 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45881
2024-02-21 07:04:35,134 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36735
2024-02-21 07:04:35,134 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,134 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,134 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,134 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,134 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4agi8ggz
2024-02-21 07:04:35,134 - distributed.worker - INFO - Starting Worker plugin PreImport-e106ed1c-aac6-49c4-876f-88359ee206bb
2024-02-21 07:04:35,134 - distributed.worker - INFO - Starting Worker plugin RMMSetup-612a1c60-3da9-41f1-b4af-56c18bc60591
2024-02-21 07:04:35,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,156 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41815
2024-02-21 07:04:35,156 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41815
2024-02-21 07:04:35,157 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41861
2024-02-21 07:04:35,157 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,157 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,157 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,157 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,157 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-onhba90c
2024-02-21 07:04:35,157 - distributed.worker - INFO - Starting Worker plugin PreImport-bc381e84-b2de-4f1d-91f1-dcf6e93c8c76
2024-02-21 07:04:35,157 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56d9bb16-dc55-4ab0-b8bf-8233bd9c10f0
2024-02-21 07:04:35,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90bfd7e2-9e9c-432c-8822-b0a022ab52fb
2024-02-21 07:04:35,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,159 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33073
2024-02-21 07:04:35,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33073
2024-02-21 07:04:35,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35357
2024-02-21 07:04:35,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,159 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,159 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,159 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rfe_fl_y
2024-02-21 07:04:35,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe7208fc-bc55-43dc-8fcc-828e28f5db80
2024-02-21 07:04:35,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea3e9182-86ea-4478-825d-1a53d336975f
2024-02-21 07:04:35,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44609
2024-02-21 07:04:35,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37023
2024-02-21 07:04:35,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44609
2024-02-21 07:04:35,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37023
2024-02-21 07:04:35,218 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35169
2024-02-21 07:04:35,218 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41899
2024-02-21 07:04:35,218 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,218 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,218 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,218 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,218 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,218 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,218 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,218 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,218 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rtg0n5mb
2024-02-21 07:04:35,218 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0uwwvnxn
2024-02-21 07:04:35,218 - distributed.worker - INFO - Starting Worker plugin PreImport-4a9fd2bc-e898-439a-bb31-0f07c51e545e
2024-02-21 07:04:35,218 - distributed.worker - INFO - Starting Worker plugin PreImport-1841c1f2-52a1-468c-b18f-d901484c9a0d
2024-02-21 07:04:35,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14e0cfdc-857e-460d-b8a0-4721aacd26c4
2024-02-21 07:04:35,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-025ffd72-90af-4ede-963c-06773e9b6c50
2024-02-21 07:04:35,228 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:35,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:35,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:35,236 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44099
2024-02-21 07:04:35,236 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44099
2024-02-21 07:04:35,236 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36009
2024-02-21 07:04:35,236 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:35,237 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:35,237 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:35,237 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:35,237 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-naez8tq3
2024-02-21 07:04:35,237 - distributed.worker - INFO - Starting Worker plugin PreImport-eea01c46-443e-456f-b693-ff84bba90618
2024-02-21 07:04:35,237 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a38f9b1-ceb5-41e4-9d4b-292ccf099ad6
2024-02-21 07:04:37,206 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,208 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51200ca7-f5a6-4ff9-9c09-4189fd9add8d
2024-02-21 07:04:37,208 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,212 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b8778b7-972d-421d-b9a2-fb62b92aeb38
2024-02-21 07:04:37,213 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,231 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45881', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,232 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45881
2024-02-21 07:04:37,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53178
2024-02-21 07:04:37,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,234 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,234 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,239 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32943', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,240 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32943
2024-02-21 07:04:37,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53190
2024-02-21 07:04:37,242 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,243 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,243 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,251 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40801', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,252 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40801
2024-02-21 07:04:37,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53196
2024-02-21 07:04:37,254 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,255 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,255 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,259 - distributed.worker - INFO - Starting Worker plugin PreImport-ebbd5a0c-1794-4d64-9898-9274612e184c
2024-02-21 07:04:37,259 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,260 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,285 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33073', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,286 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33073
2024-02-21 07:04:37,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53202
2024-02-21 07:04:37,287 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,288 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,288 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41815', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,291 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41815
2024-02-21 07:04:37,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53214
2024-02-21 07:04:37,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,294 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,294 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,301 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e0d17f2-3587-41b8-892e-0927f309bfa9
2024-02-21 07:04:37,301 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,316 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0a2aebe-f3c7-48bc-af3c-f58e6d9b1d79
2024-02-21 07:04:37,317 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3202bf7e-2570-4eb2-b27e-b549e8358918
2024-02-21 07:04:37,322 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44609', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44609
2024-02-21 07:04:37,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53226
2024-02-21 07:04:37,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,326 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,326 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44099', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44099
2024-02-21 07:04:37,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53240
2024-02-21 07:04:37,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,340 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,340 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37023', status: init, memory: 0, processing: 0>
2024-02-21 07:04:37,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37023
2024-02-21 07:04:37,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53254
2024-02-21 07:04:37,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:37,356 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:37,357 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:37,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:04:37,457 - distributed.scheduler - INFO - Remove client Client-75b8b7f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:37,457 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53078; closing.
2024-02-21 07:04:37,457 - distributed.scheduler - INFO - Remove client Client-75b8b7f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:37,458 - distributed.scheduler - INFO - Close client connection: Client-75b8b7f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:04:37,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43831'. Reason: nanny-close
2024-02-21 07:04:37,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,460 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32913'. Reason: nanny-close
2024-02-21 07:04:37,460 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39325'. Reason: nanny-close
2024-02-21 07:04:37,461 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41815. Reason: nanny-close
2024-02-21 07:04:37,461 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36287'. Reason: nanny-close
2024-02-21 07:04:37,461 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40801. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44617'. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45881. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36051'. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44099. Reason: nanny-close
2024-02-21 07:04:37,462 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41353'. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32943. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39093'. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37023. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:37,463 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,464 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44609. Reason: nanny-close
2024-02-21 07:04:37,464 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53214; closing.
2024-02-21 07:04:37,464 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53178; closing.
2024-02-21 07:04:37,464 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.464746')
2024-02-21 07:04:37,464 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33073. Reason: nanny-close
2024-02-21 07:04:37,465 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.465237')
2024-02-21 07:04:37,465 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,465 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,465 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,465 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,465 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,466 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53196; closing.
2024-02-21 07:04:37,466 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,466 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53240; closing.
2024-02-21 07:04:37,466 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,466 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:37,467 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,467 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.4674144')
2024-02-21 07:04:37,467 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,467 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.4677832')
2024-02-21 07:04:37,468 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53190; closing.
2024-02-21 07:04:37,468 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,468 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:37,469 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.4692183')
2024-02-21 07:04:37,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53226; closing.
2024-02-21 07:04:37,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53254; closing.
2024-02-21 07:04:37,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53202; closing.
2024-02-21 07:04:37,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.4702318')
2024-02-21 07:04:37,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.470575')
2024-02-21 07:04:37,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499077.4709072')
2024-02-21 07:04:37,471 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:04:38,375 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:04:38,375 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:04:38,376 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:04:38,377 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:04:38,377 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-21 07:04:40,853 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:40,859 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39971 instead
  warnings.warn(
2024-02-21 07:04:40,864 - distributed.scheduler - INFO - State start
2024-02-21 07:04:40,890 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:04:40,890 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-21 07:04:40,892 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:04:40,892 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-21 07:04:41,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39787'
2024-02-21 07:04:41,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41949'
2024-02-21 07:04:41,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37205'
2024-02-21 07:04:41,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37519'
2024-02-21 07:04:41,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37561'
2024-02-21 07:04:41,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39081'
2024-02-21 07:04:41,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36079'
2024-02-21 07:04:41,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45753'
2024-02-21 07:04:43,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44403
2024-02-21 07:04:43,074 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44403
2024-02-21 07:04:43,074 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34869
2024-02-21 07:04:43,074 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,074 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,074 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,074 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ilpweqjn
2024-02-21 07:04:43,075 - distributed.worker - INFO - Starting Worker plugin PreImport-f4867b6d-95ae-4d13-a355-fbd87de53713
2024-02-21 07:04:43,075 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb4e52c7-c514-48f2-9100-3e468b5ae55c
2024-02-21 07:04:43,075 - distributed.worker - INFO - Starting Worker plugin RMMSetup-385ff10b-fce1-429d-9d0d-fbcc90091726
2024-02-21 07:04:43,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41159
2024-02-21 07:04:43,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41159
2024-02-21 07:04:43,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44533
2024-02-21 07:04:43,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,094 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,094 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q00jzvi2
2024-02-21 07:04:43,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbd86060-bb47-462f-acf8-83a6fc996505
2024-02-21 07:04:43,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,104 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42071
2024-02-21 07:04:43,104 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42071
2024-02-21 07:04:43,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36169
2024-02-21 07:04:43,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,105 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,105 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,105 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i7ib69gf
2024-02-21 07:04:43,105 - distributed.worker - INFO - Starting Worker plugin PreImport-8e7ae92d-8a53-4926-8366-3c51fea3af96
2024-02-21 07:04:43,105 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfcd20a3-1332-4f6a-b0bb-6fb9ddb20b7f
2024-02-21 07:04:43,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,109 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39503
2024-02-21 07:04:43,109 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39503
2024-02-21 07:04:43,109 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39135
2024-02-21 07:04:43,109 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,109 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,110 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,110 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,110 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-24wzaqz7
2024-02-21 07:04:43,110 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9732389-f9c6-4337-9829-69dffa14149b
2024-02-21 07:04:43,111 - distributed.worker - INFO - Starting Worker plugin PreImport-ba358d95-4ac6-469c-9e93-9c1ac1b848a3
2024-02-21 07:04:43,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1abdaac8-763d-4588-91a9-57f71db3675e
2024-02-21 07:04:43,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,118 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45293
2024-02-21 07:04:43,118 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45293
2024-02-21 07:04:43,118 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41245
2024-02-21 07:04:43,118 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,118 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,118 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,118 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c4u8289p
2024-02-21 07:04:43,119 - distributed.worker - INFO - Starting Worker plugin PreImport-be7bda1b-4851-41fe-90dd-f3f51c589f8c
2024-02-21 07:04:43,119 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfe494fc-c004-4ffb-9b76-8c3d3a0dba29
2024-02-21 07:04:43,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38151
2024-02-21 07:04:43,157 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38151
2024-02-21 07:04:43,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:04:43,157 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38333
2024-02-21 07:04:43,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:04:43,157 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,157 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,157 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,157 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,157 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_hmhkmte
2024-02-21 07:04:43,158 - distributed.worker - INFO - Starting Worker plugin PreImport-1d562a4b-7020-4bcf-be8c-1d914783e177
2024-02-21 07:04:43,158 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d52d680-f77f-43c9-8d12-0378aa94fe0d
2024-02-21 07:04:43,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:04:43,162 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42911
2024-02-21 07:04:43,162 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40629
2024-02-21 07:04:43,162 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42911
2024-02-21 07:04:43,162 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40629
2024-02-21 07:04:43,162 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35355
2024-02-21 07:04:43,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,162 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40511
2024-02-21 07:04:43,162 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:04:43,162 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:43,162 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,162 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:04:43,162 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:04:43,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gaiwubk8
2024-02-21 07:04:43,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1c72zz9a
2024-02-21 07:04:43,163 - distributed.worker - INFO - Starting Worker plugin PreImport-153f7db9-1d91-4b0c-b7bc-5ffa2304db7f
2024-02-21 07:04:43,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85e43f31-7599-4520-9366-630eb6d79997
2024-02-21 07:04:43,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d96ad25-6bd3-4c6c-bf75-0f50b93fa65d
2024-02-21 07:04:43,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e568cbb-52d7-4388-bba6-c12b04dc3b48
2024-02-21 07:04:47,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c09101a-5df1-41d7-9068-c5fc70a974c9
2024-02-21 07:04:47,255 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,280 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,280 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,296 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c73e0a83-4b2e-41b1-9523-facaafbfa8db
2024-02-21 07:04:47,297 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,319 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,321 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,322 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,322 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e9f0a7f-b991-450b-8295-df077ca3a546
2024-02-21 07:04:47,330 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,343 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,344 - distributed.worker - INFO - Starting Worker plugin PreImport-1e7bc76d-9a2e-49cb-814c-53a47230a141
2024-02-21 07:04:47,344 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5b05bfd-d5e7-4836-96d0-4897d7b917b7
2024-02-21 07:04:47,344 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,344 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,346 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,347 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,350 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10b3a14d-4e31-4112-9607-fbc3ad98bdcf
2024-02-21 07:04:47,350 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,352 - distributed.worker - INFO - Starting Worker plugin PreImport-745638f6-bb1c-46be-b42d-27d9bd2dc499
2024-02-21 07:04:47,354 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,370 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,372 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,372 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,376 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,387 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,387 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,388 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,388 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:04:47,390 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:04:47,390 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:04:47,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:47,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:04:57,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39787'. Reason: nanny-close
2024-02-21 07:04:57,503 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41949'. Reason: nanny-close
2024-02-21 07:04:57,504 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37205'. Reason: nanny-close
2024-02-21 07:04:57,505 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,505 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44403. Reason: nanny-close
2024-02-21 07:04:57,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37519'. Reason: nanny-close
2024-02-21 07:04:57,505 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45293. Reason: nanny-close
2024-02-21 07:04:57,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37561'. Reason: nanny-close
2024-02-21 07:04:57,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39081'. Reason: nanny-close
2024-02-21 07:04:57,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36079'. Reason: nanny-close
2024-02-21 07:04:57,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42071. Reason: nanny-close
2024-02-21 07:04:57,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45753'. Reason: nanny-close
2024-02-21 07:04:57,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41159. Reason: nanny-close
2024-02-21 07:04:57,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38151. Reason: nanny-close
2024-02-21 07:04:57,508 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:04:57,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42911. Reason: nanny-close
2024-02-21 07:04:57,508 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40629. Reason: nanny-close
2024-02-21 07:04:57,509 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,509 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39503. Reason: nanny-close
2024-02-21 07:04:57,509 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,510 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,511 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,511 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,511 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:04:57,512 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,513 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,514 - distributed.nanny - INFO - Worker closed
2024-02-21 07:04:57,515 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-21 07:05:00,931 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:05:00,936 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37925 instead
  warnings.warn(
2024-02-21 07:05:00,941 - distributed.scheduler - INFO - State start
2024-02-21 07:05:00,964 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:05:00,965 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:05:00,966 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37925/status
2024-02-21 07:05:00,967 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:05:01,091 - distributed.scheduler - INFO - Receive client connection: Client-8669d10f-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:05:01,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37630
2024-02-21 07:05:01,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33547'
2024-02-21 07:05:01,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34235'
2024-02-21 07:05:01,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38135'
2024-02-21 07:05:01,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46605'
2024-02-21 07:05:01,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38665'
2024-02-21 07:05:01,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40377'
2024-02-21 07:05:01,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37471'
2024-02-21 07:05:01,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44903'
2024-02-21 07:05:02,531 - distributed.scheduler - INFO - Receive client connection: Client-86c8d6b9-d087-11ee-8a8d-d8c49764f6bb
2024-02-21 07:05:02,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37744
2024-02-21 07:05:03,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,130 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,131 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35731
2024-02-21 07:05:03,131 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35731
2024-02-21 07:05:03,131 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39911
2024-02-21 07:05:03,131 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,131 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,131 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,131 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,131 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ws3fawwu
2024-02-21 07:05:03,132 - distributed.worker - INFO - Starting Worker plugin PreImport-071aa8ff-1291-4e45-a34e-b8508eed6b71
2024-02-21 07:05:03,132 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f805ea3b-c0d2-479d-b87c-8781455894db
2024-02-21 07:05:03,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,148 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44967
2024-02-21 07:05:03,148 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44967
2024-02-21 07:05:03,148 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44515
2024-02-21 07:05:03,148 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,148 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,148 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,148 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,148 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-snutekcp
2024-02-21 07:05:03,149 - distributed.worker - INFO - Starting Worker plugin PreImport-e4836c55-4e58-4b47-8344-2cd29a1da43c
2024-02-21 07:05:03,149 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0d9ba7f-ace4-4b92-9ecc-957ff58e82d2
2024-02-21 07:05:03,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,159 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42313
2024-02-21 07:05:03,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42313
2024-02-21 07:05:03,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45963
2024-02-21 07:05:03,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,159 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,159 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,159 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c2ch9nw7
2024-02-21 07:05:03,160 - distributed.worker - INFO - Starting Worker plugin PreImport-76332188-7d12-4a26-a621-97ab8f3737b2
2024-02-21 07:05:03,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9645e09d-121f-4692-a561-db5fd7a5dc45
2024-02-21 07:05:03,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d13409ee-66da-4f36-bda1-db7e00817441
2024-02-21 07:05:03,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,178 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42649
2024-02-21 07:05:03,178 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42649
2024-02-21 07:05:03,178 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44255
2024-02-21 07:05:03,178 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,178 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,178 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,179 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,179 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ebm8n9bt
2024-02-21 07:05:03,179 - distributed.worker - INFO - Starting Worker plugin PreImport-479559c3-c73c-4b5e-b10d-014bdddb0301
2024-02-21 07:05:03,179 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42f6655f-6b2a-42b7-a408-dc0aa37a6e77
2024-02-21 07:05:03,218 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,218 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,222 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,223 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44233
2024-02-21 07:05:03,224 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44233
2024-02-21 07:05:03,224 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43203
2024-02-21 07:05:03,224 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,224 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,224 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,224 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,224 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n47lfzfp
2024-02-21 07:05:03,224 - distributed.worker - INFO - Starting Worker plugin PreImport-e3712ebe-0949-406c-8500-521709b6c3fa
2024-02-21 07:05:03,224 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79f5f4fa-f748-488d-a2b1-c433a8775a98
2024-02-21 07:05:03,234 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,243 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40147
2024-02-21 07:05:03,243 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40147
2024-02-21 07:05:03,243 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40477
2024-02-21 07:05:03,243 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,243 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,243 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,243 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z7f655bq
2024-02-21 07:05:03,243 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f065c47-7fd4-46f6-86a7-ae045bf48303
2024-02-21 07:05:03,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fb6dcd3-3a8d-4d06-9105-ccd1bc4f941c
2024-02-21 07:05:03,432 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,432 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,438 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39811
2024-02-21 07:05:03,438 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39811
2024-02-21 07:05:03,438 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43953
2024-02-21 07:05:03,438 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,438 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,438 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,438 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,438 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ui7gq3ya
2024-02-21 07:05:03,439 - distributed.worker - INFO - Starting Worker plugin PreImport-0b574ac2-2398-40a6-8a89-36588a6cc6c7
2024-02-21 07:05:03,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7dac89a-b632-4cea-b34a-1b2b4c4c19a0
2024-02-21 07:05:03,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:03,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:03,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:03,458 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41993
2024-02-21 07:05:03,458 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41993
2024-02-21 07:05:03,458 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36519
2024-02-21 07:05:03,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:03,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:03,458 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:03,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:03,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wiw41wnr
2024-02-21 07:05:03,458 - distributed.worker - INFO - Starting Worker plugin PreImport-6ec05032-11c3-444d-953e-bda3784caf98
2024-02-21 07:05:03,459 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-baab692f-ed65-4ef8-a7b2-98b929710a49
2024-02-21 07:05:03,459 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ccc84b7-61d0-4eb3-a206-a2eada0c27d4
2024-02-21 07:05:04,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6dd551b4-cece-438f-8296-286ec3e47fd3
2024-02-21 07:05:04,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:04,650 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35731', status: init, memory: 0, processing: 0>
2024-02-21 07:05:04,651 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35731
2024-02-21 07:05:04,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37754
2024-02-21 07:05:04,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:04,653 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:04,653 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:04,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:04,675 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:04,679 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:04,681 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:04,684 - distributed.scheduler - INFO - Remove client Client-86c8d6b9-d087-11ee-8a8d-d8c49764f6bb
2024-02-21 07:05:04,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37744; closing.
2024-02-21 07:05:04,685 - distributed.scheduler - INFO - Remove client Client-86c8d6b9-d087-11ee-8a8d-d8c49764f6bb
2024-02-21 07:05:04,685 - distributed.scheduler - INFO - Close client connection: Client-86c8d6b9-d087-11ee-8a8d-d8c49764f6bb
2024-02-21 07:05:05,457 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54f96f9b-ddfc-4ad9-9c55-28598f7bc44e
2024-02-21 07:05:05,459 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,491 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44967', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44967
2024-02-21 07:05:05,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37770
2024-02-21 07:05:05,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,494 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,494 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,515 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,538 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42313', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42313
2024-02-21 07:05:05,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37786
2024-02-21 07:05:05,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,540 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,540 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,542 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6b3a255-ca46-4c5d-aa2c-903eac44e211
2024-02-21 07:05:05,615 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a64fb066-cc72-4b1b-a7b7-2de8c4d97cfb
2024-02-21 07:05:05,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42649', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,644 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42649
2024-02-21 07:05:05,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37794
2024-02-21 07:05:05,645 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,647 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,647 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,656 - distributed.worker - INFO - Starting Worker plugin PreImport-136d9cac-f726-4fb1-9a7a-7b981db874ca
2024-02-21 07:05:05,657 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,672 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-006eb010-d73f-4a40-bf10-6d493741d691
2024-02-21 07:05:05,673 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,676 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44233', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,677 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44233
2024-02-21 07:05:05,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37798
2024-02-21 07:05:05,678 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40147', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,678 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,679 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40147
2024-02-21 07:05:05,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37808
2024-02-21 07:05:05,679 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,679 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,680 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,680 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39811', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39811
2024-02-21 07:05:05,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37820
2024-02-21 07:05:05,707 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,708 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,708 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,716 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42677', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,738 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42677
2024-02-21 07:05:05,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37828
2024-02-21 07:05:05,749 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41993', status: init, memory: 0, processing: 0>
2024-02-21 07:05:05,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41993
2024-02-21 07:05:05,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37832
2024-02-21 07:05:05,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:05:05,753 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:05:05,753 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:05,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:05:05,761 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37828; closing.
2024-02-21 07:05:05,761 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42677', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.7615104')
2024-02-21 07:05:05,781 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,782 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,782 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,782 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,783 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,783 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,783 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,783 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,795 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,795 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:05:05,805 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,806 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:05:05,809 - distributed.scheduler - INFO - Remove client Client-8669d10f-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:05:05,809 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37630; closing.
2024-02-21 07:05:05,809 - distributed.scheduler - INFO - Remove client Client-8669d10f-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:05:05,809 - distributed.scheduler - INFO - Close client connection: Client-8669d10f-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:05:05,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33547'. Reason: nanny-close
2024-02-21 07:05:05,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34235'. Reason: nanny-close
2024-02-21 07:05:05,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38135'. Reason: nanny-close
2024-02-21 07:05:05,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41993. Reason: nanny-close
2024-02-21 07:05:05,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46605'. Reason: nanny-close
2024-02-21 07:05:05,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44967. Reason: nanny-close
2024-02-21 07:05:05,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38665'. Reason: nanny-close
2024-02-21 07:05:05,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42313. Reason: nanny-close
2024-02-21 07:05:05,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40377'. Reason: nanny-close
2024-02-21 07:05:05,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35731. Reason: nanny-close
2024-02-21 07:05:05,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37471'. Reason: nanny-close
2024-02-21 07:05:05,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39811. Reason: nanny-close
2024-02-21 07:05:05,815 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37832; closing.
2024-02-21 07:05:05,815 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44903'. Reason: nanny-close
2024-02-21 07:05:05,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,815 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.8152444')
2024-02-21 07:05:05,815 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:05:05,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42649. Reason: nanny-close
2024-02-21 07:05:05,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40147. Reason: nanny-close
2024-02-21 07:05:05,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,816 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44233. Reason: nanny-close
2024-02-21 07:05:05,816 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37754; closing.
2024-02-21 07:05:05,817 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37786; closing.
2024-02-21 07:05:05,817 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37770; closing.
2024-02-21 07:05:05,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,817 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,818 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.8182833')
2024-02-21 07:05:05,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.8187351')
2024-02-21 07:05:05,819 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.8192043')
2024-02-21 07:05:05,819 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,819 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,819 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:05:05,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37820; closing.
2024-02-21 07:05:05,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37808; closing.
2024-02-21 07:05:05,820 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,820 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.8208473')
2024-02-21 07:05:05,821 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.821235')
2024-02-21 07:05:05,821 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37794; closing.
2024-02-21 07:05:05,821 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37798; closing.
2024-02-21 07:05:05,822 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42649', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.822163')
2024-02-21 07:05:05,822 - distributed.nanny - INFO - Worker closed
2024-02-21 07:05:05,822 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499105.822597')
2024-02-21 07:05:05,822 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:05:06,771 - distributed.scheduler - INFO - Receive client connection: Client-8b152ed5-d087-11ee-8a8d-d8c49764f6bb
2024-02-21 07:05:06,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37846
2024-02-21 07:05:06,877 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:05:06,878 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:05:06,878 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:05:06,880 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:05:06,881 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-21 07:05:09,344 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:05:09,348 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41411 instead
  warnings.warn(
2024-02-21 07:05:09,352 - distributed.scheduler - INFO - State start
2024-02-21 07:05:09,378 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:05:09,379 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-21 07:05:09,380 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:05:09,381 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-21 07:05:09,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35611'
2024-02-21 07:05:09,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41343'
2024-02-21 07:05:09,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39441'
2024-02-21 07:05:09,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34529'
2024-02-21 07:05:09,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42219'
2024-02-21 07:05:09,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34511'
2024-02-21 07:05:09,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44591'
2024-02-21 07:05:09,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41987'
2024-02-21 07:05:11,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,534 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40511
2024-02-21 07:05:11,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40511
2024-02-21 07:05:11,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46513
2024-02-21 07:05:11,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,539 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,539 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-itamg_6i
2024-02-21 07:05:11,539 - distributed.worker - INFO - Starting Worker plugin PreImport-582f1fed-4f20-4635-a12e-efada2c7453d
2024-02-21 07:05:11,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8c2bd76-06ca-4874-8275-13ae93b411e1
2024-02-21 07:05:11,540 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44187
2024-02-21 07:05:11,540 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44187
2024-02-21 07:05:11,540 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45829
2024-02-21 07:05:11,540 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,540 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,540 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,540 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,540 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gnediknt
2024-02-21 07:05:11,540 - distributed.worker - INFO - Starting Worker plugin PreImport-fc55b162-fb8d-4047-a19c-23e0b5e34c4d
2024-02-21 07:05:11,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59a99e6d-8e8c-4168-b45d-8fb127c457fe
2024-02-21 07:05:11,541 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cdfe8b9-c4c9-4638-9d5a-6a64353c1e48
2024-02-21 07:05:11,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36295
2024-02-21 07:05:11,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36295
2024-02-21 07:05:11,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45717
2024-02-21 07:05:11,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,597 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,597 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,598 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mchrc2lm
2024-02-21 07:05:11,598 - distributed.worker - INFO - Starting Worker plugin PreImport-86484b74-a5ff-4a0d-94a7-e7b8c457636b
2024-02-21 07:05:11,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4051ac26-6e6e-4bc8-b294-9e35cf94de64
2024-02-21 07:05:11,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f08122a6-cd67-4411-ac04-743445f111aa
2024-02-21 07:05:11,599 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,604 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,605 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38047
2024-02-21 07:05:11,605 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38047
2024-02-21 07:05:11,605 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35443
2024-02-21 07:05:11,605 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,605 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,605 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,605 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,605 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_h8kk60u
2024-02-21 07:05:11,605 - distributed.worker - INFO - Starting Worker plugin PreImport-662c7eb5-f6ca-441e-9653-f23142b0d128
2024-02-21 07:05:11,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5ad4e9b-2d64-4291-9658-54bd6032f87d
2024-02-21 07:05:11,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,617 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,620 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,621 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37631
2024-02-21 07:05:11,621 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37631
2024-02-21 07:05:11,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36555
2024-02-21 07:05:11,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,621 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,621 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,622 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-13axrizl
2024-02-21 07:05:11,622 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38497
2024-02-21 07:05:11,622 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38497
2024-02-21 07:05:11,622 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39989
2024-02-21 07:05:11,622 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,622 - distributed.worker - INFO - Starting Worker plugin PreImport-ca7410cd-e36c-4bd4-9d90-adca1c788a60
2024-02-21 07:05:11,622 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,622 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d684ab2-54a0-4163-9dec-1463e623fe71
2024-02-21 07:05:11,622 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x9bisj9g
2024-02-21 07:05:11,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5ba94e9-05a2-475f-b845-16f917f75737
2024-02-21 07:05:11,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f18de76-4471-4d27-871b-3082bf111796
2024-02-21 07:05:11,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38153
2024-02-21 07:05:11,637 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38153
2024-02-21 07:05:11,637 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35203
2024-02-21 07:05:11,637 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,637 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,637 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,638 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,638 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w3gbmjlo
2024-02-21 07:05:11,638 - distributed.worker - INFO - Starting Worker plugin PreImport-415678fc-6ef3-4a28-8e4a-9d6c4ac1bdae
2024-02-21 07:05:11,638 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4ab6e3d-681d-416b-95c7-40c5c1ae52a8
2024-02-21 07:05:11,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:05:11,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:05:11,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:05:11,722 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44293
2024-02-21 07:05:11,722 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44293
2024-02-21 07:05:11,722 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45707
2024-02-21 07:05:11,722 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:11,722 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:11,722 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:05:11,722 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:05:11,722 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-74n881m1
2024-02-21 07:05:11,723 - distributed.worker - INFO - Starting Worker plugin PreImport-ca308fdb-7dd7-4467-a7c2-8db4b92a96b5
2024-02-21 07:05:11,723 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d85f05fa-ef7a-453c-a328-950d12c83207
2024-02-21 07:05:14,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c6dacb0-2a11-4086-a057-c9ec7d4171c7
2024-02-21 07:05:14,043 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,143 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,152 - distributed.worker - INFO - Starting Worker plugin PreImport-34331c6a-bea4-4f1d-8eee-fc3f9dcbf891
2024-02-21 07:05:14,153 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15baa37e-9bd2-46ee-996d-17e9c2af3f84
2024-02-21 07:05:14,160 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,164 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31f206f0-2402-4d4d-8c74-fbd4f656b682
2024-02-21 07:05:14,181 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-058a8715-cba2-4c1f-904a-11277ffa4e5c
2024-02-21 07:05:14,184 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:14,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5da5ee56-e88e-47f8-bf4b-f000bf445b4a
2024-02-21 07:05:14,191 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:05:43,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35611'. Reason: nanny-close
2024-02-21 07:05:43,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41343'. Reason: nanny-close
2024-02-21 07:05:43,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39441'. Reason: nanny-close
2024-02-21 07:05:43,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34529'. Reason: nanny-close
2024-02-21 07:05:43,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42219'. Reason: nanny-close
2024-02-21 07:05:43,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34511'. Reason: nanny-close
2024-02-21 07:05:43,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44591'. Reason: nanny-close
2024-02-21 07:05:43,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41987'. Reason: nanny-close
2024-02-21 07:05:44,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,144 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,154 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,161 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,186 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:05:44,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-21 07:06:15,910 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:15,919 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41103 instead
  warnings.warn(
2024-02-21 07:06:15,926 - distributed.scheduler - INFO - State start
2024-02-21 07:06:15,928 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_h8kk60u', purging
2024-02-21 07:06:15,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x9bisj9g', purging
2024-02-21 07:06:15,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-itamg_6i', purging
2024-02-21 07:06:15,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-mchrc2lm', purging
2024-02-21 07:06:15,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gnediknt', purging
2024-02-21 07:06:15,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-13axrizl', purging
2024-02-21 07:06:15,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-74n881m1', purging
2024-02-21 07:06:15,933 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-w3gbmjlo', purging
2024-02-21 07:06:15,987 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:15,988 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:06:15,989 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41103/status
2024-02-21 07:06:15,989 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:15,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36331'
2024-02-21 07:06:16,077 - distributed.scheduler - INFO - Receive client connection: Client-b3178b27-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:16,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43724
2024-02-21 07:06:17,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:17,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:18,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:18,577 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33241
2024-02-21 07:06:18,577 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33241
2024-02-21 07:06:18,577 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-21 07:06:18,577 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:18,577 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:18,577 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:18,577 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-21 07:06:18,577 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gs3ax_2l
2024-02-21 07:06:18,577 - distributed.worker - INFO - Starting Worker plugin PreImport-6fe13b00-617c-4598-af6f-19b7f925169a
2024-02-21 07:06:18,577 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74dc9b18-5f17-4571-8cc6-36e3c3417224
2024-02-21 07:06:18,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48864bba-aee5-4ac5-a8ab-3f7844cf3926
2024-02-21 07:06:18,578 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:18,634 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33241', status: init, memory: 0, processing: 0>
2024-02-21 07:06:18,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33241
2024-02-21 07:06:18,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43738
2024-02-21 07:06:18,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:18,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:18,637 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:18,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:18,642 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:18,645 - distributed.scheduler - INFO - Remove client Client-b3178b27-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:18,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43724; closing.
2024-02-21 07:06:18,645 - distributed.scheduler - INFO - Remove client Client-b3178b27-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:18,646 - distributed.scheduler - INFO - Close client connection: Client-b3178b27-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:18,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36331'. Reason: nanny-close
2024-02-21 07:06:18,676 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:18,677 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33241. Reason: nanny-close
2024-02-21 07:06:18,679 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43738; closing.
2024-02-21 07:06:18,679 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:18,679 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499178.67953')
2024-02-21 07:06:18,679 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:06:18,680 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:19,262 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:06:19,263 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:06:19,263 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:06:19,265 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:06:19,265 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-21 07:06:24,179 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:24,185 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34567 instead
  warnings.warn(
2024-02-21 07:06:24,189 - distributed.scheduler - INFO - State start
2024-02-21 07:06:24,213 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:24,214 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:06:24,215 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34567/status
2024-02-21 07:06:24,215 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:24,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34597'
2024-02-21 07:06:25,097 - distributed.scheduler - INFO - Receive client connection: Client-b80186f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:25,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52504
2024-02-21 07:06:26,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:26,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:27,075 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:27,076 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40841
2024-02-21 07:06:27,076 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40841
2024-02-21 07:06:27,076 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35719
2024-02-21 07:06:27,077 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:27,077 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:27,077 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:27,077 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-21 07:06:27,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vexyxpqw
2024-02-21 07:06:27,077 - distributed.worker - INFO - Starting Worker plugin PreImport-3abd3055-f82f-4a4a-95bd-643b4a3edcf4
2024-02-21 07:06:27,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80ad009d-726d-4832-9d48-884877a7d579
2024-02-21 07:06:27,078 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-718ce50f-50fd-432c-8b9b-d46461e9942f
2024-02-21 07:06:27,078 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:27,334 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40841', status: init, memory: 0, processing: 0>
2024-02-21 07:06:27,335 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40841
2024-02-21 07:06:27,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52528
2024-02-21 07:06:27,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:27,337 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:27,337 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:27,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:27,396 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:27,399 - distributed.scheduler - INFO - Remove client Client-b80186f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:27,399 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52504; closing.
2024-02-21 07:06:27,400 - distributed.scheduler - INFO - Remove client Client-b80186f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:27,400 - distributed.scheduler - INFO - Close client connection: Client-b80186f6-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:27,401 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34597'. Reason: nanny-close
2024-02-21 07:06:27,401 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:27,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40841. Reason: nanny-close
2024-02-21 07:06:27,404 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:27,404 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52528; closing.
2024-02-21 07:06:27,405 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40841', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499187.405174')
2024-02-21 07:06:27,405 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:06:27,406 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:28,216 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:06:28,219 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:06:28,219 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:06:28,220 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:06:28,220 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-21 07:06:30,685 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:30,691 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-21 07:06:30,695 - distributed.scheduler - INFO - State start
2024-02-21 07:06:30,719 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:30,720 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:06:30,721 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-21 07:06:30,721 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:33,493 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:49326'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49326>: Stream is closed
2024-02-21 07:06:33,846 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:06:33,846 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:06:33,846 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:06:33,847 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:06:33,847 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-21 07:06:36,279 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:36,284 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36079 instead
  warnings.warn(
2024-02-21 07:06:36,288 - distributed.scheduler - INFO - State start
2024-02-21 07:06:36,313 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:36,314 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-21 07:06:36,315 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36079/status
2024-02-21 07:06:36,315 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:36,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37189'
2024-02-21 07:06:37,250 - distributed.scheduler - INFO - Receive client connection: Client-bf430ad9-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:37,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47486
2024-02-21 07:06:38,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:38,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:38,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:38,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40287
2024-02-21 07:06:38,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40287
2024-02-21 07:06:38,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40119
2024-02-21 07:06:38,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-21 07:06:38,418 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:38,418 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:38,418 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-21 07:06:38,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zadt7rat
2024-02-21 07:06:38,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99326cb8-6356-420e-8e3f-4166cccf4753
2024-02-21 07:06:38,419 - distributed.worker - INFO - Starting Worker plugin PreImport-b8022efb-a9f0-4a09-ac3a-ba4c1e73b46d
2024-02-21 07:06:38,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed1e45e1-4c9a-48ea-8e3f-fd72c12eb2f3
2024-02-21 07:06:38,419 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:39,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40287', status: init, memory: 0, processing: 0>
2024-02-21 07:06:39,596 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40287
2024-02-21 07:06:39,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47518
2024-02-21 07:06:39,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:39,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-21 07:06:39,598 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:39,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-21 07:06:39,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:39,691 - distributed.scheduler - INFO - Remove client Client-bf430ad9-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:39,691 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47486; closing.
2024-02-21 07:06:39,691 - distributed.scheduler - INFO - Remove client Client-bf430ad9-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:39,692 - distributed.scheduler - INFO - Close client connection: Client-bf430ad9-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:39,692 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37189'. Reason: nanny-close
2024-02-21 07:06:39,693 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:39,694 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40287. Reason: nanny-close
2024-02-21 07:06:39,695 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47518; closing.
2024-02-21 07:06:39,696 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-21 07:06:39,696 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40287', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499199.6962214')
2024-02-21 07:06:39,696 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:06:39,697 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:40,458 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:06:40,458 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:06:40,459 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:06:40,460 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-21 07:06:40,460 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-21 07:06:42,896 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:42,904 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36111 instead
  warnings.warn(
2024-02-21 07:06:42,909 - distributed.scheduler - INFO - State start
2024-02-21 07:06:43,124 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:43,125 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:06:43,127 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36111/status
2024-02-21 07:06:43,128 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:43,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34139'
2024-02-21 07:06:43,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35213'
2024-02-21 07:06:43,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40721'
2024-02-21 07:06:43,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45993'
2024-02-21 07:06:43,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35237'
2024-02-21 07:06:43,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38173'
2024-02-21 07:06:43,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45863'
2024-02-21 07:06:43,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37629'
2024-02-21 07:06:44,995 - distributed.scheduler - INFO - Receive client connection: Client-c3314d38-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:45,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60042
2024-02-21 07:06:45,235 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,239 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,240 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33273
2024-02-21 07:06:45,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33273
2024-02-21 07:06:45,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33975
2024-02-21 07:06:45,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,240 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,240 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qs7lkquj
2024-02-21 07:06:45,241 - distributed.worker - INFO - Starting Worker plugin PreImport-9ed01eca-b423-44ae-af95-ddd631e7ff8f
2024-02-21 07:06:45,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7eab005e-ff76-44a7-96e3-df72d7920a63
2024-02-21 07:06:45,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92acba5a-80c8-466a-92c7-7375c9198528
2024-02-21 07:06:45,267 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,267 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,272 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37337
2024-02-21 07:06:45,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37337
2024-02-21 07:06:45,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43571
2024-02-21 07:06:45,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,273 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,273 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xa829o6a
2024-02-21 07:06:45,273 - distributed.worker - INFO - Starting Worker plugin PreImport-fdf0a545-7c93-44a2-94a8-800c98199045
2024-02-21 07:06:45,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c7e4b80-daaf-463d-9d7c-1b3ebc08d825
2024-02-21 07:06:45,281 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38551
2024-02-21 07:06:45,286 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38551
2024-02-21 07:06:45,287 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44267
2024-02-21 07:06:45,287 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,287 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,287 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,287 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jzr1yhdj
2024-02-21 07:06:45,287 - distributed.worker - INFO - Starting Worker plugin PreImport-f0209da6-b43c-45e3-bfe6-3950f181e9d5
2024-02-21 07:06:45,287 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11acf457-f7ae-43d5-86da-f22bc61a4233
2024-02-21 07:06:45,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45513
2024-02-21 07:06:45,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45513
2024-02-21 07:06:45,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45927
2024-02-21 07:06:45,499 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,499 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,500 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tqc8xiyo
2024-02-21 07:06:45,500 - distributed.worker - INFO - Starting Worker plugin PreImport-d5e0c7cb-6e09-49f7-9169-16ac5b37a141
2024-02-21 07:06:45,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d54e4327-c2d4-4e64-8b4f-0b9d04114dbc
2024-02-21 07:06:45,500 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39727
2024-02-21 07:06:45,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39727
2024-02-21 07:06:45,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42703
2024-02-21 07:06:45,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,500 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,500 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,501 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-02znhyzq
2024-02-21 07:06:45,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,501 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bd579c6-3d63-42f2-b9c2-d048dc24ca57
2024-02-21 07:06:45,502 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97a06711-694e-495a-904b-7b56b28c3d57
2024-02-21 07:06:45,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44913
2024-02-21 07:06:45,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44913
2024-02-21 07:06:45,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40223
2024-02-21 07:06:45,513 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,513 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,513 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0d3dd4an
2024-02-21 07:06:45,514 - distributed.worker - INFO - Starting Worker plugin PreImport-266fce8d-5f46-4aac-a3e3-a043aa69a1ab
2024-02-21 07:06:45,514 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33512817-9a00-4c9b-b80c-6c6bb3f18d43
2024-02-21 07:06:45,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-252bceb9-ba86-4716-8349-53702af6d3c2
2024-02-21 07:06:45,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,530 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,532 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43115
2024-02-21 07:06:45,532 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43115
2024-02-21 07:06:45,532 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43015
2024-02-21 07:06:45,532 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,532 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,532 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,532 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,532 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0sctd6qr
2024-02-21 07:06:45,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57158d45-604a-4702-8a12-a28e1bad9e16
2024-02-21 07:06:45,533 - distributed.worker - INFO - Starting Worker plugin PreImport-d8402fe6-887d-492a-be75-61fcd087b363
2024-02-21 07:06:45,534 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b4df946-a0d4-4ee2-9528-f597b3c8acf4
2024-02-21 07:06:45,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:45,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:45,550 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:45,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39685
2024-02-21 07:06:45,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39685
2024-02-21 07:06:45,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42685
2024-02-21 07:06:45,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:45,552 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:45,552 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:45,553 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-21 07:06:45,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xlxza3ov
2024-02-21 07:06:45,553 - distributed.worker - INFO - Starting Worker plugin PreImport-44732a91-7978-4361-89e3-17c91340b326
2024-02-21 07:06:45,554 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d3be2b9-73a4-443a-a878-21cfad55a8f1
2024-02-21 07:06:47,205 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,226 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33273', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,227 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33273
2024-02-21 07:06:47,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60048
2024-02-21 07:06:47,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,229 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,229 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,430 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59e3effa-1c96-49bf-97b3-783dbfaab39c
2024-02-21 07:06:47,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,471 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37337', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37337
2024-02-21 07:06:47,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60050
2024-02-21 07:06:47,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,474 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,474 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,520 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-021f10ad-b5b4-4db1-bd55-c9ffd0e42e09
2024-02-21 07:06:47,521 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,552 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38551', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38551
2024-02-21 07:06:47,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60064
2024-02-21 07:06:47,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,554 - distributed.worker - INFO - Starting Worker plugin PreImport-58a75a70-fec5-4d9f-af08-4b41a918e6eb
2024-02-21 07:06:47,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,556 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,556 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17fce062-4cd9-40fc-9170-b25c0dd35657
2024-02-21 07:06:47,578 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39727', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39727
2024-02-21 07:06:47,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60070
2024-02-21 07:06:47,587 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,589 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,597 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,602 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45513', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,603 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45513
2024-02-21 07:06:47,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60076
2024-02-21 07:06:47,604 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,605 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,605 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73bc01df-b08d-4aa5-ae04-374c45fbe324
2024-02-21 07:06:47,611 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,618 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43115', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,619 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43115
2024-02-21 07:06:47,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60088
2024-02-21 07:06:47,620 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,621 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,621 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,627 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44913', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,628 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44913
2024-02-21 07:06:47,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60100
2024-02-21 07:06:47,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,630 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,630 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,642 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39685', status: init, memory: 0, processing: 0>
2024-02-21 07:06:47,642 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39685
2024-02-21 07:06:47,643 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60114
2024-02-21 07:06:47,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:47,645 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:47,645 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:47,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:47,743 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,744 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,744 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,744 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,744 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,744 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,745 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,745 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-21 07:06:47,761 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,761 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,761 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,761 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,761 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,762 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,762 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,762 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:47,767 - distributed.scheduler - INFO - Remove client Client-c3314d38-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:47,767 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60042; closing.
2024-02-21 07:06:47,767 - distributed.scheduler - INFO - Remove client Client-c3314d38-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:47,768 - distributed.scheduler - INFO - Close client connection: Client-c3314d38-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:47,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34139'. Reason: nanny-close
2024-02-21 07:06:47,769 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,769 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35213'. Reason: nanny-close
2024-02-21 07:06:47,770 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,770 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40721'. Reason: nanny-close
2024-02-21 07:06:47,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33273. Reason: nanny-close
2024-02-21 07:06:47,770 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45993'. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38551. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35237'. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44913. Reason: nanny-close
2024-02-21 07:06:47,771 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38173'. Reason: nanny-close
2024-02-21 07:06:47,772 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37337. Reason: nanny-close
2024-02-21 07:06:47,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45863'. Reason: nanny-close
2024-02-21 07:06:47,772 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,772 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60048; closing.
2024-02-21 07:06:47,772 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39685. Reason: nanny-close
2024-02-21 07:06:47,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37629'. Reason: nanny-close
2024-02-21 07:06:47,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45513. Reason: nanny-close
2024-02-21 07:06:47,773 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:47,773 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33273', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.7730134')
2024-02-21 07:06:47,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39727. Reason: nanny-close
2024-02-21 07:06:47,773 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60064; closing.
2024-02-21 07:06:47,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,773 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,774 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38551', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.774176')
2024-02-21 07:06:47,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43115. Reason: nanny-close
2024-02-21 07:06:47,774 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,774 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,774 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,774 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,775 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,776 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,776 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,776 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,776 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60064>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-21 07:06:47,777 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60100; closing.
2024-02-21 07:06:47,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60050; closing.
2024-02-21 07:06:47,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60114; closing.
2024-02-21 07:06:47,778 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:47,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60076; closing.
2024-02-21 07:06:47,778 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,778 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44913', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.7788892')
2024-02-21 07:06:47,779 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.779191')
2024-02-21 07:06:47,779 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.7794626')
2024-02-21 07:06:47,779 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.7798026')
2024-02-21 07:06:47,780 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:47,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60070; closing.
2024-02-21 07:06:47,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60088; closing.
2024-02-21 07:06:47,781 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.781185')
2024-02-21 07:06:47,781 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499207.7815561')
2024-02-21 07:06:47,781 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:06:48,785 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:06:48,785 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:06:48,786 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:06:48,787 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:06:48,787 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-21 07:06:51,313 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:51,319 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36469 instead
  warnings.warn(
2024-02-21 07:06:51,325 - distributed.scheduler - INFO - State start
2024-02-21 07:06:51,351 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:51,353 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:06:51,354 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36469/status
2024-02-21 07:06:51,355 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:51,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41595'
2024-02-21 07:06:52,684 - distributed.scheduler - INFO - Receive client connection: Client-c82d21e4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:52,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38940
2024-02-21 07:06:53,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:06:53,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:06:53,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:06:53,991 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40263
2024-02-21 07:06:53,991 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40263
2024-02-21 07:06:53,991 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37501
2024-02-21 07:06:53,991 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:06:53,991 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:53,991 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:06:53,991 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-21 07:06:53,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-050ag_bt
2024-02-21 07:06:53,992 - distributed.worker - INFO - Starting Worker plugin PreImport-0691048c-1fa3-4e5c-a84a-82a5b12bbba3
2024-02-21 07:06:53,992 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42782df7-cc2d-482e-bf49-f54c51d98b5e
2024-02-21 07:06:53,992 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e553e8bf-3832-4656-8fb3-2ac8e8bbfd9a
2024-02-21 07:06:55,417 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:55,468 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40263', status: init, memory: 0, processing: 0>
2024-02-21 07:06:55,469 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40263
2024-02-21 07:06:55,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38954
2024-02-21 07:06:55,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:06:55,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:06:55,471 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:06:55,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:06:55,537 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:06:55,541 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:55,543 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:06:55,545 - distributed.scheduler - INFO - Remove client Client-c82d21e4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:55,545 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38940; closing.
2024-02-21 07:06:55,546 - distributed.scheduler - INFO - Remove client Client-c82d21e4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:55,546 - distributed.scheduler - INFO - Close client connection: Client-c82d21e4-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:55,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41595'. Reason: nanny-close
2024-02-21 07:06:55,547 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:06:55,548 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40263. Reason: nanny-close
2024-02-21 07:06:55,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38954; closing.
2024-02-21 07:06:55,550 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:06:55,550 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40263', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499215.550647')
2024-02-21 07:06:55,550 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:06:55,551 - distributed.nanny - INFO - Worker closed
2024-02-21 07:06:56,162 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:06:56,162 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:06:56,163 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:06:56,164 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:06:56,164 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-21 07:06:58,555 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:58,563 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-21 07:06:58,570 - distributed.scheduler - INFO - State start
2024-02-21 07:06:58,607 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-21 07:06:58,608 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-21 07:06:58,609 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-21 07:06:58,610 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-21 07:06:58,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42029'
2024-02-21 07:06:58,794 - distributed.scheduler - INFO - Receive client connection: Client-cc93e5e2-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:06:58,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39062
2024-02-21 07:07:00,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-21 07:07:00,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-21 07:07:00,662 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-21 07:07:00,663 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34251
2024-02-21 07:07:00,663 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34251
2024-02-21 07:07:00,663 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42675
2024-02-21 07:07:00,663 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-21 07:07:00,663 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:07:00,663 - distributed.worker - INFO -               Threads:                          1
2024-02-21 07:07:00,663 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-21 07:07:00,663 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z61b5v_z
2024-02-21 07:07:00,663 - distributed.worker - INFO - Starting Worker plugin PreImport-34097d9e-b50e-4e44-b622-6f91078c40bd
2024-02-21 07:07:00,664 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-135fa746-569d-4d9e-b36b-a1e9b5886373
2024-02-21 07:07:00,664 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7132189c-c143-4f6a-8f30-3e96230cab8b
2024-02-21 07:07:01,212 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:07:01,264 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34251', status: init, memory: 0, processing: 0>
2024-02-21 07:07:01,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34251
2024-02-21 07:07:01,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54502
2024-02-21 07:07:01,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-21 07:07:01,266 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-21 07:07:01,266 - distributed.worker - INFO - -------------------------------------------------
2024-02-21 07:07:01,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-21 07:07:01,357 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-21 07:07:01,361 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-21 07:07:01,365 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:07:01,367 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-21 07:07:01,369 - distributed.scheduler - INFO - Remove client Client-cc93e5e2-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:07:01,370 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39062; closing.
2024-02-21 07:07:01,370 - distributed.scheduler - INFO - Remove client Client-cc93e5e2-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:07:01,370 - distributed.scheduler - INFO - Close client connection: Client-cc93e5e2-d087-11ee-8b82-d8c49764f6bb
2024-02-21 07:07:01,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42029'. Reason: nanny-close
2024-02-21 07:07:01,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-21 07:07:01,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34251. Reason: nanny-close
2024-02-21 07:07:01,374 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-21 07:07:01,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54502; closing.
2024-02-21 07:07:01,375 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34251', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708499221.3751276')
2024-02-21 07:07:01,375 - distributed.scheduler - INFO - Lost all workers
2024-02-21 07:07:01,376 - distributed.nanny - INFO - Worker closed
2024-02-21 07:07:01,936 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-21 07:07:01,936 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-21 07:07:01,937 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-21 07:07:01,938 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-21 07:07:01,938 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44347 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43133 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45103 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38301 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45147 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37963 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41733 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40129 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43103 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45125 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41257 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34811 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37861 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40403 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45931 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37845 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43307 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34999 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35275 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34479 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36737 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35649 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37849 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45599 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35601 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46621 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] [1708500087.367897] [dgx13:68357:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:38760) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38701 instead
  warnings.warn(
[1708500101.032050] [dgx13:68475:0]            sock.c:470  UCX  ERROR bind(fd=133 addr=0.0.0.0:40536) failed: Address already in use
[1708500104.644743] [dgx13:68562:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:58908) failed: Address already in use
[1708500104.644815] [dgx13:68562:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:46029) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39437 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35067 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38373 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32895 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39211 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44597 instead
  warnings.warn(
[1708500276.634120] [dgx13:71006:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:59148) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37907 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32913 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35067 instead
  warnings.warn(
[1708500370.658221] [dgx13:72160:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:57041) failed: Address already in use
[1708500372.657512] [dgx13:72335:0]            sock.c:470  UCX  ERROR bind(fd=121 addr=0.0.0.0:40518) failed: Address already in use
[1708500376.595108] [dgx13:72330:0]            sock.c:470  UCX  ERROR bind(fd=155 addr=0.0.0.0:35155) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46839 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35989 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41809 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40919 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37021 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36843 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45193 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34889 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42961 instead
  warnings.warn(
[1708500526.591507] [dgx13:74781:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:60335) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36611 instead
  warnings.warn(
[1708500544.387637] [dgx13:74988:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:35767) failed: Address already in use
2024-02-21 07:29:14,512 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,521 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,525 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,534 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,554 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:57501'.
2024-02-21 07:29:14,559 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:47657'.
2024-02-21 07:29:14,559 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:57501'. Shutting down.
2024-02-21 07:29:14,560 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:47657'. Shutting down.
2024-02-21 07:29:14,569 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,577 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,596 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,596 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:38716'.
2024-02-21 07:29:14,598 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:38716'. Shutting down.
2024-02-21 07:29:14,601 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,606 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc5baebc040>>, <Task finished name='Task-13' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-13' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,616 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe78d93c040>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,621 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:49085'.
2024-02-21 07:29:14,623 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:49085'. Shutting down.
2024-02-21 07:29:14,632 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f9b52b08040>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:14,649 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f78c6f5e040>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-21 07:29:16,609 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-02-21 07:29:16,620 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-02-21 07:29:16,636 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-02-21 07:29:16,653 - distributed.nanny - ERROR - Worker process died unexpectedly
