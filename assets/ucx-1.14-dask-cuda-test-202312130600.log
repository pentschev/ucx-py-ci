============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-13 06:40:32,152 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:32,157 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37753 instead
  warnings.warn(
2023-12-13 06:40:32,161 - distributed.scheduler - INFO - State start
2023-12-13 06:40:32,226 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:32,227 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-13 06:40:32,228 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37753/status
2023-12-13 06:40:32,228 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:33,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39951'
2023-12-13 06:40:33,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37831'
2023-12-13 06:40:33,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42645'
2023-12-13 06:40:33,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41029'
2023-12-13 06:40:33,680 - distributed.scheduler - INFO - Receive client connection: Client-821f2d65-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:33,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49126
2023-12-13 06:40:35,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:35,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:35,240 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:35,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:35,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:35,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:35,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:35,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:35,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-13 06:40:35,292 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37989
2023-12-13 06:40:35,292 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37989
2023-12-13 06:40:35,292 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44269
2023-12-13 06:40:35,292 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:35,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,292 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:35,292 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:35,292 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-jg3xrfio
2023-12-13 06:40:35,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a81a94ad-4654-4fca-967f-b262293840c1
2023-12-13 06:40:35,293 - distributed.worker - INFO - Starting Worker plugin PreImport-0411773e-867e-4ebc-9678-81c9598e3239
2023-12-13 06:40:35,293 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43c0d626-07d3-4cc6-b1a1-e1940772555b
2023-12-13 06:40:35,293 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,333 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:35,333 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:35,337 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:35,811 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37989', status: init, memory: 0, processing: 0>
2023-12-13 06:40:35,813 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37989
2023-12-13 06:40:35,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49150
2023-12-13 06:40:35,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:35,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:35,815 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:36,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34437
2023-12-13 06:40:36,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34437
2023-12-13 06:40:36,642 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35189
2023-12-13 06:40:36,642 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:36,642 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:36,642 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:36,642 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:36,642 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-3dwpnpjy
2023-12-13 06:40:36,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68e69018-1b33-4d04-83e8-9065a3387747
2023-12-13 06:40:36,643 - distributed.worker - INFO - Starting Worker plugin PreImport-92b08141-67c9-4d29-9df4-610b7308e6ce
2023-12-13 06:40:36,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf7b0dcc-5ed3-48b0-a7df-877651320226
2023-12-13 06:40:36,648 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:36,687 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34437', status: init, memory: 0, processing: 0>
2023-12-13 06:40:36,688 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34437
2023-12-13 06:40:36,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49156
2023-12-13 06:40:36,690 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:36,691 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:36,691 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:36,694 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:36,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45449
2023-12-13 06:40:36,920 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45449
2023-12-13 06:40:36,920 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39885
2023-12-13 06:40:36,920 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:36,920 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:36,920 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:36,920 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:36,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zww0gs9n
2023-12-13 06:40:36,921 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56630ed3-b03a-4b8f-830b-cd5c58e53af8
2023-12-13 06:40:36,921 - distributed.worker - INFO - Starting Worker plugin PreImport-0ac84b8e-3ce1-46f7-9d42-01d67353b8f3
2023-12-13 06:40:36,922 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31a44e88-3675-4cb7-9d2f-be775be65192
2023-12-13 06:40:36,922 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:36,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45449', status: init, memory: 0, processing: 0>
2023-12-13 06:40:36,958 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45449
2023-12-13 06:40:36,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49186
2023-12-13 06:40:36,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:36,961 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:36,961 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:36,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:37,275 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43783
2023-12-13 06:40:37,275 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43783
2023-12-13 06:40:37,275 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45507
2023-12-13 06:40:37,275 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:37,276 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:37,276 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:37,276 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:37,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-sbfz873f
2023-12-13 06:40:37,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0072c85-7258-417b-899f-4495654db537
2023-12-13 06:40:37,276 - distributed.worker - INFO - Starting Worker plugin PreImport-2d51ffd2-455a-4ab7-9586-8f2aabf9fbf5
2023-12-13 06:40:37,276 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54d4081f-9bdd-49d6-9ca8-41c47d78e9ca
2023-12-13 06:40:37,277 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:37,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43783', status: init, memory: 0, processing: 0>
2023-12-13 06:40:37,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43783
2023-12-13 06:40:37,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49194
2023-12-13 06:40:37,313 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:37,315 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:37,315 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:37,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:37,398 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:37,398 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:37,398 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:37,399 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:37,403 - distributed.scheduler - INFO - Remove client Client-821f2d65-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:37,404 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49126; closing.
2023-12-13 06:40:37,404 - distributed.scheduler - INFO - Remove client Client-821f2d65-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:37,404 - distributed.scheduler - INFO - Close client connection: Client-821f2d65-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:37,405 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39951'. Reason: nanny-close
2023-12-13 06:40:37,406 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:37,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37831'. Reason: nanny-close
2023-12-13 06:40:37,407 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:37,407 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43783. Reason: nanny-close
2023-12-13 06:40:37,408 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45449. Reason: nanny-close
2023-12-13 06:40:37,409 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:37,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42645'. Reason: nanny-close
2023-12-13 06:40:37,410 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49194; closing.
2023-12-13 06:40:37,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:37,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:37,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449637.410395')
2023-12-13 06:40:37,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41029'. Reason: nanny-close
2023-12-13 06:40:37,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:37,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34437. Reason: nanny-close
2023-12-13 06:40:37,411 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49186; closing.
2023-12-13 06:40:37,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37989. Reason: nanny-close
2023-12-13 06:40:37,411 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:37,412 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449637.4123209')
2023-12-13 06:40:37,412 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:37,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:37,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49150; closing.
2023-12-13 06:40:37,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:37,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37989', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449637.414568')
2023-12-13 06:40:37,415 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:37,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49156; closing.
2023-12-13 06:40:37,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34437', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449637.415964')
2023-12-13 06:40:37,416 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:37,416 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:38,722 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:40:38,723 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:40:38,723 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:38,724 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-13 06:40:38,725 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-13 06:40:41,064 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:41,069 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:40:41,072 - distributed.scheduler - INFO - State start
2023-12-13 06:40:41,095 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:41,096 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-13 06:40:41,097 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:41,098 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-13 06:40:41,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38215'
2023-12-13 06:40:41,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39735'
2023-12-13 06:40:41,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37081'
2023-12-13 06:40:41,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46317'
2023-12-13 06:40:41,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34595'
2023-12-13 06:40:41,337 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36381'
2023-12-13 06:40:41,346 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39029'
2023-12-13 06:40:41,355 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45827'
2023-12-13 06:40:43,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:43,438 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:43,441 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,442 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,442 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:47,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38215'. Reason: nanny-close
2023-12-13 06:40:47,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37081'. Reason: nanny-close
2023-12-13 06:40:47,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46317'. Reason: nanny-close
2023-12-13 06:40:47,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34595'. Reason: nanny-close
2023-12-13 06:40:47,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36381'. Reason: nanny-close
2023-12-13 06:40:47,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39029'. Reason: nanny-close
2023-12-13 06:40:47,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45827'. Reason: nanny-close
2023-12-13 06:40:47,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39735'. Reason: nanny-close
2023-12-13 06:40:48,306 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40327
2023-12-13 06:40:48,307 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40327
2023-12-13 06:40:48,307 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41007
2023-12-13 06:40:48,307 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,307 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,307 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,307 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f5e_8aeu
2023-12-13 06:40:48,308 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81134f6b-db48-42cc-9a09-3f581681cefe
2023-12-13 06:40:48,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce5c2187-2c64-4524-8737-759123cb362f
2023-12-13 06:40:48,311 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42019
2023-12-13 06:40:48,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42019
2023-12-13 06:40:48,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39811
2023-12-13 06:40:48,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,312 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,313 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,313 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ima11izz
2023-12-13 06:40:48,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e17396dd-99d2-4df1-a7d2-b5afd0b98933
2023-12-13 06:40:48,313 - distributed.worker - INFO - Starting Worker plugin PreImport-f805cbe7-1345-4141-9398-8808ded44b52
2023-12-13 06:40:48,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-983b30ee-3be1-4d17-b84b-a723e140bb9b
2023-12-13 06:40:48,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33007
2023-12-13 06:40:48,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33007
2023-12-13 06:40:48,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38573
2023-12-13 06:40:48,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,350 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,351 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,351 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,351 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9o3z7ocr
2023-12-13 06:40:48,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39477
2023-12-13 06:40:48,351 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39477
2023-12-13 06:40:48,351 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41091
2023-12-13 06:40:48,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74503cc9-ffc8-4190-8043-afe3cf2f72f1
2023-12-13 06:40:48,351 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,352 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,352 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,352 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,352 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e_fym933
2023-12-13 06:40:48,352 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c4d5a93-e134-41ec-90a1-b3b0985c5775
2023-12-13 06:40:48,352 - distributed.worker - INFO - Starting Worker plugin PreImport-a9981e4c-d7d6-4785-ae74-c4147d9acf35
2023-12-13 06:40:48,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0e519a9-67ab-4a8e-a4da-4cf6a108f883
2023-12-13 06:40:48,353 - distributed.worker - INFO - Starting Worker plugin PreImport-88baedec-1625-415b-8829-2d6f2f84ae61
2023-12-13 06:40:48,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20a3a778-71af-41a7-9395-5dbcfa97bda6
2023-12-13 06:40:48,377 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37003
2023-12-13 06:40:48,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37003
2023-12-13 06:40:48,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45555
2023-12-13 06:40:48,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,378 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,378 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x_7e5khg
2023-12-13 06:40:48,379 - distributed.worker - INFO - Starting Worker plugin PreImport-01743ec5-dce2-4086-b3f3-48ae8cbe5e6c
2023-12-13 06:40:48,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3996f1db-0429-46a3-812c-c2280753a8b2
2023-12-13 06:40:48,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-783ffc91-f812-419d-b2c8-1c2219439831
2023-12-13 06:40:48,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37371
2023-12-13 06:40:48,382 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37371
2023-12-13 06:40:48,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34951
2023-12-13 06:40:48,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,382 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,383 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,383 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tv9gw5hj
2023-12-13 06:40:48,383 - distributed.worker - INFO - Starting Worker plugin PreImport-acd39f8a-2c2c-47a0-a502-cf2699a39ec9
2023-12-13 06:40:48,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c17fbdfc-b164-4853-97ab-c8fcc1cba9dc
2023-12-13 06:40:48,383 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33313
2023-12-13 06:40:48,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33313
2023-12-13 06:40:48,384 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38825
2023-12-13 06:40:48,384 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,384 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,384 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hmvei0ut
2023-12-13 06:40:48,385 - distributed.worker - INFO - Starting Worker plugin PreImport-16a2e028-e279-49a1-ad18-11983976bc34
2023-12-13 06:40:48,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82494ade-2b10-4259-aba0-66cd0c51ea2a
2023-12-13 06:40:48,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d11bb988-4b7b-4ce7-b34a-52a9838e5d03
2023-12-13 06:40:48,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd399af2-b87f-4ca1-9454-01cc138076c7
2023-12-13 06:40:48,411 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35449
2023-12-13 06:40:48,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35449
2023-12-13 06:40:48,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36255
2023-12-13 06:40:48,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,413 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,413 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:48,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:48,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p_pdnchq
2023-12-13 06:40:48,415 - distributed.worker - INFO - Starting Worker plugin PreImport-57d2d01a-ddd9-4102-93cb-a991ea2cf3ea
2023-12-13 06:40:48,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49802f13-4775-4e55-acfe-993ee6c7bc85
2023-12-13 06:40:48,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc99bb52-caf4-4c0f-9291-a07a64836774
2023-12-13 06:40:48,818 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,821 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,851 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,863 - distributed.worker - INFO - Starting Worker plugin PreImport-a02ad54e-5520-48b9-a154-e1f3e45d82e8
2023-12-13 06:40:48,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,869 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,869 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,882 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,883 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,883 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,883 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,886 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,887 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,888 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39477. Reason: nanny-close
2023-12-13 06:40:48,890 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,891 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,896 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,898 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,913 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,917 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,917 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:48,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:48,926 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:48,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:48,934 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,934 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,935 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35449. Reason: nanny-close
2023-12-13 06:40:48,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,935 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42019. Reason: nanny-close
2023-12-13 06:40:48,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40327. Reason: nanny-close
2023-12-13 06:40:48,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37003. Reason: nanny-close
2023-12-13 06:40:48,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33313. Reason: nanny-close
2023-12-13 06:40:48,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:48,937 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37371. Reason: nanny-close
2023-12-13 06:40:48,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,938 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33007. Reason: nanny-close
2023-12-13 06:40:48,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,939 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,940 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,940 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,940 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,941 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,941 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:48,942 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:48,944 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-13 06:40:52,874 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:52,879 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44545 instead
  warnings.warn(
2023-12-13 06:40:52,883 - distributed.scheduler - INFO - State start
2023-12-13 06:40:52,906 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:52,907 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-13 06:40:52,907 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:52,908 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-13 06:40:53,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39501'
2023-12-13 06:40:53,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39087'
2023-12-13 06:40:53,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43577'
2023-12-13 06:40:53,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35233'
2023-12-13 06:40:53,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36193'
2023-12-13 06:40:53,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36549'
2023-12-13 06:40:53,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37129'
2023-12-13 06:40:53,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37995'
2023-12-13 06:40:55,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,167 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,174 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:55,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:55,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:55,360 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:58,955 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39501'. Reason: nanny-close
2023-12-13 06:40:58,956 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39087'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43577'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35233'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36193'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36549'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37129'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37995'. Reason: nanny-close
2023-12-13 06:41:00,546 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46399
2023-12-13 06:41:00,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46399
2023-12-13 06:41:00,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40643
2023-12-13 06:41:00,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,547 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,547 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,548 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-najvghkp
2023-12-13 06:41:00,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d950047-77b5-400d-a442-78eda0363abb
2023-12-13 06:41:00,548 - distributed.worker - INFO - Starting Worker plugin PreImport-82bb1451-31a8-47db-be9b-799d9df97d96
2023-12-13 06:41:00,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8aff31a6-6df0-46ea-80a8-a212cae0c1e3
2023-12-13 06:41:00,562 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33751
2023-12-13 06:41:00,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33751
2023-12-13 06:41:00,563 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44625
2023-12-13 06:41:00,563 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,563 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,564 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,564 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-caps0tr4
2023-12-13 06:41:00,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6dbc2953-9204-4586-84c0-65089a1e7266
2023-12-13 06:41:00,565 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c75768e4-1e78-467c-b4bb-26937b8374d6
2023-12-13 06:41:00,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34143
2023-12-13 06:41:00,566 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34143
2023-12-13 06:41:00,566 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39251
2023-12-13 06:41:00,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,567 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,567 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,567 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2c3_783m
2023-12-13 06:41:00,567 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1991602d-a988-4a6e-a4ca-8113c43abc19
2023-12-13 06:41:00,568 - distributed.worker - INFO - Starting Worker plugin PreImport-1f1f40d2-9df2-4e62-99b9-ca706e6dbf52
2023-12-13 06:41:00,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28387ef8-9f26-4e95-bef2-a491e24b8703
2023-12-13 06:41:00,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34975
2023-12-13 06:41:00,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34975
2023-12-13 06:41:00,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43841
2023-12-13 06:41:00,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,602 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,603 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,603 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pz668me
2023-12-13 06:41:00,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21ee642b-66fb-4a61-b95e-f056dc2f0a56
2023-12-13 06:41:00,604 - distributed.worker - INFO - Starting Worker plugin PreImport-0c1e21e1-4e27-4414-b971-ab70f08fb7fb
2023-12-13 06:41:00,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8a8f149-b036-44a8-a511-9cebf0d0892e
2023-12-13 06:41:00,640 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40503
2023-12-13 06:41:00,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40503
2023-12-13 06:41:00,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33061
2023-12-13 06:41:00,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,641 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,641 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,641 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0wx6ec8g
2023-12-13 06:41:00,642 - distributed.worker - INFO - Starting Worker plugin PreImport-58a438d8-7f42-480f-8c53-0ffc406f9465
2023-12-13 06:41:00,642 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d90c3e7b-a6e8-4c5b-a41f-f65fb6585adf
2023-12-13 06:41:00,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-386af9e7-bde4-4e01-8473-ea2df4f54c06
2023-12-13 06:41:00,649 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,673 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,681 - distributed.worker - INFO - Starting Worker plugin PreImport-d7760c33-f583-4895-b336-f78b26d24ed9
2023-12-13 06:41:00,681 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,684 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,689 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,711 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,716 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,716 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,718 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46399. Reason: nanny-close
2023-12-13 06:41:00,720 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45919
2023-12-13 06:41:00,720 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45919
2023-12-13 06:41:00,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,721 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37101
2023-12-13 06:41:00,721 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,721 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,721 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,721 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oiv2jf_f
2023-12-13 06:41:00,721 - distributed.worker - INFO - Starting Worker plugin PreImport-5d28ccef-26b5-421c-a8ec-33184bec51c1
2023-12-13 06:41:00,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-76ff4ca1-23e5-4f47-810b-4b83d03be909
2023-12-13 06:41:00,722 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,722 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28b77871-2081-4089-8561-2ae1867f15e5
2023-12-13 06:41:00,724 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,725 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,725 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,727 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,727 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32833
2023-12-13 06:41:00,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32833
2023-12-13 06:41:00,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37449
2023-12-13 06:41:00,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,750 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,751 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,751 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z45jy9z0
2023-12-13 06:41:00,751 - distributed.worker - INFO - Starting Worker plugin PreImport-6f7109b6-4e13-4cb3-affa-56c9cc829884
2023-12-13 06:41:00,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ac78a28-ef6b-43db-a390-f0c9282e8599
2023-12-13 06:41:00,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f3c58a4-75a3-4534-89e7-8919369c27c3
2023-12-13 06:41:00,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,758 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,758 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,758 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43277
2023-12-13 06:41:00,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43277
2023-12-13 06:41:00,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45467
2023-12-13 06:41:00,759 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,759 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,759 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:00,759 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:00,759 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3kfitz8_
2023-12-13 06:41:00,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48f879fc-8d6e-43c4-9c2f-88e5a1fee246
2023-12-13 06:41:00,760 - distributed.worker - INFO - Starting Worker plugin PreImport-0a2ba3c1-95a4-4a7c-96b0-d2f541ad6dc5
2023-12-13 06:41:00,760 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ed0a981-957c-4c65-9748-79ceb1817a32
2023-12-13 06:41:00,762 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,765 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,768 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,769 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,770 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40503. Reason: nanny-close
2023-12-13 06:41:00,770 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34143. Reason: nanny-close
2023-12-13 06:41:00,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34975. Reason: nanny-close
2023-12-13 06:41:00,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,773 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33751. Reason: nanny-close
2023-12-13 06:41:00,775 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,775 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,775 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,777 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,779 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,794 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,795 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,795 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,797 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,797 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:00,799 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:00,799 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:00,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:00,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:00,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43277. Reason: nanny-close
2023-12-13 06:41:00,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45919. Reason: nanny-close
2023-12-13 06:41:00,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32833. Reason: nanny-close
2023-12-13 06:41:00,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:00,827 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,827 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:00,828 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-13 06:41:04,364 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:04,368 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40873 instead
  warnings.warn(
2023-12-13 06:41:04,372 - distributed.scheduler - INFO - State start
2023-12-13 06:41:04,394 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:04,395 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-13 06:41:04,395 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:04,396 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-13 06:41:04,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41429'
2023-12-13 06:41:04,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39859'
2023-12-13 06:41:04,644 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34065'
2023-12-13 06:41:04,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35025'
2023-12-13 06:41:04,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45243'
2023-12-13 06:41:04,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35057'
2023-12-13 06:41:04,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34587'
2023-12-13 06:41:04,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33795'
2023-12-13 06:41:06,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,650 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,662 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,662 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:06,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:06,665 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,665 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,667 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:09,427 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35811
2023-12-13 06:41:09,428 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35811
2023-12-13 06:41:09,428 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40063
2023-12-13 06:41:09,428 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,429 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,429 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,429 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kf6qd35k
2023-12-13 06:41:09,430 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2189d93-6476-4720-bc1f-ed48e4437254
2023-12-13 06:41:09,431 - distributed.worker - INFO - Starting Worker plugin PreImport-66caa730-bf0d-44c4-a11c-0058084d00ba
2023-12-13 06:41:09,432 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3710637e-d124-4176-a248-1f55e041646f
2023-12-13 06:41:09,441 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33889
2023-12-13 06:41:09,442 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33889
2023-12-13 06:41:09,442 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46857
2023-12-13 06:41:09,442 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,442 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,442 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,442 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,443 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cdon9mx9
2023-12-13 06:41:09,443 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62ddae6e-bde4-4cc0-b185-5e615d9e1370
2023-12-13 06:41:09,443 - distributed.worker - INFO - Starting Worker plugin PreImport-fb2a646f-82da-4cf0-99d7-8abcaec2c49d
2023-12-13 06:41:09,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-621ab39e-9b95-4f7b-9600-65093cf3afda
2023-12-13 06:41:09,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35153
2023-12-13 06:41:09,471 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35153
2023-12-13 06:41:09,471 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45917
2023-12-13 06:41:09,471 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,471 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,471 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,471 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,471 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nanrtdao
2023-12-13 06:41:09,472 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0716e2fe-f7ef-41c7-86de-8cbb6072fffe
2023-12-13 06:41:09,474 - distributed.worker - INFO - Starting Worker plugin PreImport-29ab265a-bf98-4aea-bc12-4983efa57f06
2023-12-13 06:41:09,474 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35279fd9-7614-4e2e-91d8-e568e2a1aea9
2023-12-13 06:41:09,475 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36037
2023-12-13 06:41:09,476 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36037
2023-12-13 06:41:09,476 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36853
2023-12-13 06:41:09,476 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,476 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,476 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,477 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,477 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ovc43xmd
2023-12-13 06:41:09,477 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-511b54a2-5e4a-4539-bbc1-b0bc554d6fcc
2023-12-13 06:41:09,478 - distributed.worker - INFO - Starting Worker plugin PreImport-dfb93446-b225-4211-ae2a-dc3ad0045e3a
2023-12-13 06:41:09,478 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4975f686-870f-4856-8828-9c671350cc1b
2023-12-13 06:41:09,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33415
2023-12-13 06:41:09,487 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33415
2023-12-13 06:41:09,487 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39913
2023-12-13 06:41:09,487 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,487 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,487 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,487 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l0ljl7xz
2023-12-13 06:41:09,488 - distributed.worker - INFO - Starting Worker plugin PreImport-bfa149dd-01ff-45af-8ae8-cd2353417e61
2023-12-13 06:41:09,488 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd8f4274-6fb7-446f-92f3-2efc3961a4b7
2023-12-13 06:41:09,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa889f5b-0d9e-4494-9b70-855bd64c964a
2023-12-13 06:41:09,491 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38173
2023-12-13 06:41:09,492 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38173
2023-12-13 06:41:09,492 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44207
2023-12-13 06:41:09,492 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,492 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,492 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,492 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,492 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pnqy2pnq
2023-12-13 06:41:09,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e8b406a-c457-424f-beb2-bcd69af1c56d
2023-12-13 06:41:09,493 - distributed.worker - INFO - Starting Worker plugin PreImport-41f1c0df-0af4-4055-a08d-5031b9043b01
2023-12-13 06:41:09,493 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0de84f44-ecbe-45ae-98b7-91835b155601
2023-12-13 06:41:09,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40701
2023-12-13 06:41:09,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40701
2023-12-13 06:41:09,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37421
2023-12-13 06:41:09,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,500 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,500 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c5l0qfzh
2023-12-13 06:41:09,501 - distributed.worker - INFO - Starting Worker plugin PreImport-64657224-034b-4d7d-96d1-db7f0489fde6
2023-12-13 06:41:09,501 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4294df27-bfc7-452a-831b-df715ad9eb09
2023-12-13 06:41:09,502 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80520358-5c67-4354-b319-5ab87292e732
2023-12-13 06:41:09,502 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34751
2023-12-13 06:41:09,503 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34751
2023-12-13 06:41:09,503 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44699
2023-12-13 06:41:09,503 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:09,503 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,503 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:09,504 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:09,504 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-usag_o9w
2023-12-13 06:41:09,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f88308a8-693b-4806-b8a5-b15c8e27271a
2023-12-13 06:41:09,505 - distributed.worker - INFO - Starting Worker plugin PreImport-d3877936-0746-42f7-ac2b-625d520a8ad8
2023-12-13 06:41:09,505 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69bfa3e0-9ed7-48ca-bdd6-2ac177f1fc2a
2023-12-13 06:41:09,631 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,662 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,704 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,711 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,716 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,717 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,718 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:09,719 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:13,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:13,076 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:13,076 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:13,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:13,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34587'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:13,126 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:13,127 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35811. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:13,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:13,131 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:13,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:13,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:13,386 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:13,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:13,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41429'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:13,428 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:13,429 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40701. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:13,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:13,434 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 376, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:59630 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-12-13 06:41:13,923 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63887 parent=63698 started daemon>
2023-12-13 06:41:13,924 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63881 parent=63698 started daemon>
2023-12-13 06:41:13,924 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63879 parent=63698 started daemon>
2023-12-13 06:41:13,924 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63874 parent=63698 started daemon>
2023-12-13 06:41:13,925 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63870 parent=63698 started daemon>
2023-12-13 06:41:13,925 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63866 parent=63698 started daemon>
2023-12-13 06:41:13,925 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63863 parent=63698 started daemon>
2023-12-13 06:41:14,033 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63863 exit status was already read will report exitcode 255
2023-12-13 06:41:14,595 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63870 exit status was already read will report exitcode 255
2023-12-13 06:41:14,690 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 63866 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-13 06:41:22,608 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:22,612 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45699 instead
  warnings.warn(
2023-12-13 06:41:22,616 - distributed.scheduler - INFO - State start
2023-12-13 06:41:22,617 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ovc43xmd', purging
2023-12-13 06:41:22,618 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pnqy2pnq', purging
2023-12-13 06:41:22,618 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-usag_o9w', purging
2023-12-13 06:41:22,619 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l0ljl7xz', purging
2023-12-13 06:41:22,619 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cdon9mx9', purging
2023-12-13 06:41:22,619 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nanrtdao', purging
2023-12-13 06:41:22,845 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:22,846 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-13 06:41:22,846 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:22,847 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-13 06:41:23,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45923'
2023-12-13 06:41:23,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44979'
2023-12-13 06:41:23,098 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42191'
2023-12-13 06:41:23,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34601'
2023-12-13 06:41:23,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45067'
2023-12-13 06:41:23,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41501'
2023-12-13 06:41:23,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43643'
2023-12-13 06:41:23,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37143'
2023-12-13 06:41:25,077 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,087 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,091 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,095 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,096 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:25,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:25,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:25,127 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:28,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45065
2023-12-13 06:41:28,013 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45065
2023-12-13 06:41:28,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45829
2023-12-13 06:41:28,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,014 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,014 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,014 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,014 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y9uhrlyx
2023-12-13 06:41:28,014 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25fcca51-6af8-4586-a2eb-d190d95d0234
2023-12-13 06:41:28,015 - distributed.worker - INFO - Starting Worker plugin PreImport-70c7ce74-a39a-4da3-9a63-b239c8d3238a
2023-12-13 06:41:28,015 - distributed.worker - INFO - Starting Worker plugin RMMSetup-566e97a4-d8ec-42d4-a675-c1fff05df98a
2023-12-13 06:41:28,158 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34741
2023-12-13 06:41:28,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34741
2023-12-13 06:41:28,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43285
2023-12-13 06:41:28,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,159 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,159 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,160 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-13s9jrzu
2023-12-13 06:41:28,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-247d3719-4bb5-4d2f-89aa-24ea2648f897
2023-12-13 06:41:28,160 - distributed.worker - INFO - Starting Worker plugin PreImport-705d1b9d-7d6e-4a88-9503-61a9ebec8922
2023-12-13 06:41:28,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cb8bc5d-d928-4570-968b-e506f68d137d
2023-12-13 06:41:28,172 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37517
2023-12-13 06:41:28,172 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43145
2023-12-13 06:41:28,173 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37517
2023-12-13 06:41:28,173 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43145
2023-12-13 06:41:28,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46647
2023-12-13 06:41:28,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34575
2023-12-13 06:41:28,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,174 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,174 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,174 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,174 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,174 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,174 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,174 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9dyj_woy
2023-12-13 06:41:28,174 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-98zdg62o
2023-12-13 06:41:28,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44fcbd1b-5909-488a-8f15-871cdba3301b
2023-12-13 06:41:28,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63170cb3-e455-4817-9100-1291585a12d0
2023-12-13 06:41:28,175 - distributed.worker - INFO - Starting Worker plugin PreImport-654992b0-ffc8-449a-8255-c06dc329cdfb
2023-12-13 06:41:28,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ed9a410-c76a-4897-aa67-afe08a8bb97d
2023-12-13 06:41:28,175 - distributed.worker - INFO - Starting Worker plugin PreImport-0035d01c-1398-4211-8182-ae032ad14026
2023-12-13 06:41:28,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-819cd008-99d1-430e-b228-6a05af7295ef
2023-12-13 06:41:28,184 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37257
2023-12-13 06:41:28,184 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37257
2023-12-13 06:41:28,185 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41895
2023-12-13 06:41:28,185 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,185 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,185 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,185 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r8vghfdr
2023-12-13 06:41:28,185 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40199
2023-12-13 06:41:28,185 - distributed.worker - INFO - Starting Worker plugin PreImport-0d0171d7-3694-4ca3-a2ee-d862b47d0078
2023-12-13 06:41:28,185 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40199
2023-12-13 06:41:28,186 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45229
2023-12-13 06:41:28,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e33e6dfe-6201-46aa-b014-e6b32e51d44b
2023-12-13 06:41:28,186 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,186 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,186 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26e4b006-cf98-4ca5-90d3-2601029e20e2
2023-12-13 06:41:28,186 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,186 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,186 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xwv1l9mj
2023-12-13 06:41:28,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-916d3497-94eb-44fa-b7b1-8ca2f4d9c888
2023-12-13 06:41:28,187 - distributed.worker - INFO - Starting Worker plugin PreImport-d34acce1-fce1-4a11-af5e-94a26614de8e
2023-12-13 06:41:28,188 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59fe58bf-c60d-407c-8e7d-0d25de89a661
2023-12-13 06:41:28,187 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33103
2023-12-13 06:41:28,188 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33103
2023-12-13 06:41:28,188 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39807
2023-12-13 06:41:28,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,189 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,189 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,189 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qtvg0v2z
2023-12-13 06:41:28,190 - distributed.worker - INFO - Starting Worker plugin PreImport-d8b3d3d7-c504-40db-83b8-7036041b7290
2023-12-13 06:41:28,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3462910-e115-4b4d-baef-75d456c1abe1
2023-12-13 06:41:28,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36063
2023-12-13 06:41:28,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36063
2023-12-13 06:41:28,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41141
2023-12-13 06:41:28,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:28,191 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,191 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:28,191 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8e24cbd-91ff-48f0-92b0-517e3f24a65f
2023-12-13 06:41:28,191 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:28,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mq1k9zh4
2023-12-13 06:41:28,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fbb9f22-d974-4c14-9966-31b50ba23213
2023-12-13 06:41:28,192 - distributed.worker - INFO - Starting Worker plugin PreImport-30f75870-09cb-42f0-991e-a13963695cc8
2023-12-13 06:41:28,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76d5500d-7d5e-4722-ad3d-d2ff19e8ca2f
2023-12-13 06:41:28,206 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,358 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,405 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,408 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,410 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,414 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,415 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:28,416 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:33,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:33,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:33,684 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:33,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:33,715 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41501'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:33,715 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:33,717 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40199. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:33,719 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:33,721 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:34,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:34,289 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:34,289 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:34,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:34,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45923'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:34,328 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:34,329 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33103. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:34,332 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:34,334 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:34,369 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:34,369 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:34,369 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:34,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:34,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43643'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:34,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:34,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34741. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2023-12-13 06:41:34,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:34,383 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 376, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:45920 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-12-13 06:41:34,524 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64148 parent=63959 started daemon>
2023-12-13 06:41:34,524 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64145 parent=63959 started daemon>
2023-12-13 06:41:34,524 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64139 parent=63959 started daemon>
2023-12-13 06:41:34,524 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64135 parent=63959 started daemon>
2023-12-13 06:41:34,525 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64131 parent=63959 started daemon>
2023-12-13 06:41:34,525 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64127 parent=63959 started daemon>
2023-12-13 06:41:34,525 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64124 parent=63959 started daemon>
2023-12-13 06:41:34,996 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64131 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-13 06:41:38,973 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:38,978 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44697 instead
  warnings.warn(
2023-12-13 06:41:38,983 - distributed.scheduler - INFO - State start
2023-12-13 06:41:38,984 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-r8vghfdr', purging
2023-12-13 06:41:38,985 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-9dyj_woy', purging
2023-12-13 06:41:38,986 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y9uhrlyx', purging
2023-12-13 06:41:38,986 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-98zdg62o', purging
2023-12-13 06:41:38,986 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-mq1k9zh4', purging
2023-12-13 06:41:39,012 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:39,013 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-13 06:41:39,014 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:39,015 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-13 06:41:39,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40443'
2023-12-13 06:41:39,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42537'
2023-12-13 06:41:39,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32951'
2023-12-13 06:41:39,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38855'
2023-12-13 06:41:39,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39831'
2023-12-13 06:41:39,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33577'
2023-12-13 06:41:39,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43293'
2023-12-13 06:41:39,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39467'
2023-12-13 06:41:39,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40443'. Reason: nanny-close
2023-12-13 06:41:39,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42537'. Reason: nanny-close
2023-12-13 06:41:39,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32951'. Reason: nanny-close
2023-12-13 06:41:39,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38855'. Reason: nanny-close
2023-12-13 06:41:39,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39831'. Reason: nanny-close
2023-12-13 06:41:39,237 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33577'. Reason: nanny-close
2023-12-13 06:41:39,238 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43293'. Reason: nanny-close
2023-12-13 06:41:39,238 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39467'. Reason: nanny-close
2023-12-13 06:41:41,023 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,023 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,027 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,112 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,124 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:41,363 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:41,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:41,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:42,682 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45839
2023-12-13 06:41:42,683 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45839
2023-12-13 06:41:42,683 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35551
2023-12-13 06:41:42,683 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:42,683 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:42,683 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:42,683 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:42,683 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5x354ref
2023-12-13 06:41:42,684 - distributed.worker - INFO - Starting Worker plugin PreImport-6bc2f7ac-8e6d-4807-908c-3332ce7a53bf
2023-12-13 06:41:42,684 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01bce9d7-787e-4ff8-bc4e-7cc95889f71c
2023-12-13 06:41:42,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4fbd332-251e-4d7d-a8cc-d9529ccdfe74
2023-12-13 06:41:43,245 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:43,571 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:43,572 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:43,572 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:43,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:43,589 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:43,591 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45839. Reason: nanny-close
2023-12-13 06:41:43,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:43,596 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:43,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36765
2023-12-13 06:41:43,965 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36765
2023-12-13 06:41:43,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45165
2023-12-13 06:41:43,966 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:43,966 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:43,966 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:43,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:43,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7rs7uzlp
2023-12-13 06:41:43,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43253
2023-12-13 06:41:43,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43253
2023-12-13 06:41:43,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46193
2023-12-13 06:41:43,966 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:43,966 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:43,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a566565-4a6e-4789-b0e5-3fde7900f125
2023-12-13 06:41:43,966 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:43,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:43,967 - distributed.worker - INFO - Starting Worker plugin PreImport-c7a605ec-c1d5-453c-b013-e3aa25f12286
2023-12-13 06:41:43,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-udzesix0
2023-12-13 06:41:43,967 - distributed.worker - INFO - Starting Worker plugin RMMSetup-831e4577-df78-4d4a-9f80-59f829a23f8a
2023-12-13 06:41:43,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9cd79a10-84be-4bf5-8623-eda6825ee246
2023-12-13 06:41:43,968 - distributed.worker - INFO - Starting Worker plugin PreImport-b102d10a-1692-4fe1-b5b0-74d27983cdb3
2023-12-13 06:41:43,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4875c8c2-f217-490e-a001-0d9ca0f116db
2023-12-13 06:41:43,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33095
2023-12-13 06:41:43,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33095
2023-12-13 06:41:43,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46695
2023-12-13 06:41:43,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:43,993 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:43,994 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:43,994 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:43,994 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7pkwrijd
2023-12-13 06:41:43,994 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90baa856-75f4-4541-881e-cffa2c3ce3a8
2023-12-13 06:41:43,994 - distributed.worker - INFO - Starting Worker plugin PreImport-46fc6318-e1a4-4f59-9a5c-93e6bdad5820
2023-12-13 06:41:43,995 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d5e6d0d-e27f-4f5f-8148-1808d6cae78d
2023-12-13 06:41:44,038 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39595
2023-12-13 06:41:44,039 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44219
2023-12-13 06:41:44,039 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39595
2023-12-13 06:41:44,039 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44219
2023-12-13 06:41:44,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45319
2023-12-13 06:41:44,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46395
2023-12-13 06:41:44,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,040 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,040 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,040 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:44,040 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:44,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:44,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:44,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6nqn3mhd
2023-12-13 06:41:44,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k5x5ik5w
2023-12-13 06:41:44,040 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4e7ba32-67d2-4c9f-b772-d2a343a1e495
2023-12-13 06:41:44,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7001886a-6778-4304-bde8-04ee569efd66
2023-12-13 06:41:44,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43953
2023-12-13 06:41:44,041 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43953
2023-12-13 06:41:44,041 - distributed.worker - INFO - Starting Worker plugin PreImport-fd522e0e-60c7-4db4-a457-eb1996fe7ac5
2023-12-13 06:41:44,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43837
2023-12-13 06:41:44,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f906e86-3aa7-45a9-a114-1c9fbaea1291
2023-12-13 06:41:44,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,042 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,042 - distributed.worker - INFO - Starting Worker plugin PreImport-5c4f0e79-57bc-446e-88e9-d9666a31c135
2023-12-13 06:41:44,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae416656-ecde-489f-9152-01b571637267
2023-12-13 06:41:44,042 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:44,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:44,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ct4ei5b
2023-12-13 06:41:44,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eaec4dd4-2470-405b-8db9-1d55b0b0a427
2023-12-13 06:41:44,043 - distributed.worker - INFO - Starting Worker plugin PreImport-1078a009-5007-486f-9777-44f85cc3b36c
2023-12-13 06:41:44,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45473
2023-12-13 06:41:44,043 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45473
2023-12-13 06:41:44,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44f44995-3a1a-4675-99a6-583af95ace84
2023-12-13 06:41:44,043 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36317
2023-12-13 06:41:44,043 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,043 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,043 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:44,043 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:44,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g7p6ydwb
2023-12-13 06:41:44,044 - distributed.worker - INFO - Starting Worker plugin PreImport-60803e81-43c2-400a-b0e0-9273e14f8819
2023-12-13 06:41:44,044 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70962ee0-ba86-47a9-8204-fb046d304bf9
2023-12-13 06:41:44,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63373191-a3fe-497c-8302-8577f4955564
2023-12-13 06:41:44,171 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,171 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,171 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,177 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,184 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,184 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,190 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,209 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,211 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,211 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,213 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,213 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,221 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,221 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,226 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:44,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,229 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,229 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,233 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:44,233 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:44,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39595. Reason: nanny-close
2023-12-13 06:41:44,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36765. Reason: nanny-close
2023-12-13 06:41:44,235 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,235 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33095. Reason: nanny-close
2023-12-13 06:41:44,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:44,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43253. Reason: nanny-close
2023-12-13 06:41:44,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43953. Reason: nanny-close
2023-12-13 06:41:44,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,238 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,238 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,238 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:44,239 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:44,239 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,239 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:44,240 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:44,241 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:44,283 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,284 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:44,284 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45473. Reason: nanny-close
2023-12-13 06:41:44,285 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44219. Reason: nanny-close
2023-12-13 06:41:44,286 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,287 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:44,288 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:44,289 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 376, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:41628 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-12-13 06:41:44,511 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64412 parent=64226 started daemon>
2023-12-13 06:41:44,511 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64409 parent=64226 started daemon>
2023-12-13 06:41:44,511 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64406 parent=64226 started daemon>
2023-12-13 06:41:44,511 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64402 parent=64226 started daemon>
2023-12-13 06:41:44,511 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64398 parent=64226 started daemon>
2023-12-13 06:41:44,511 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64394 parent=64226 started daemon>
2023-12-13 06:41:44,512 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64391 parent=64226 started daemon>
2023-12-13 06:41:45,172 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64406 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 183, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    import dask.dataframe._pyarrow_compat
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py", line 5, in <module>
    import pandas as pd
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/compat/__init__.py", line 18, in <module>
    from pandas.compat.numpy import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/util/__init__.py", line 8, in <module>
    from pandas.core.util.hashing import (  # noqa:F401
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/util/hashing.py", line 24, in <module>
    from pandas.core.dtypes.common import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/dtypes/common.py", line 28, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 629, in <module>
    class DatetimeTZDtype(PandasExtensionDtype):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 672, in DatetimeTZDtype
    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 183, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    import dask.dataframe._pyarrow_compat
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py", line 5, in <module>
    import pandas as pd
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/__init__.py", line 48, in <module>
    from pandas.core.api import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/api.py", line 47, in <module>
    from pandas.core.groupby import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/groupby/__init__.py", line 1, in <module>
    from pandas.core.groupby.generic import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/groupby/generic.py", line 76, in <module>
    from pandas.core.frame import DataFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/frame.py", line 208, in <module>
    from pandas.core.series import Series
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/series.py", line 128, in <module>
    from pandas.core.indexes.accessors import CombinedDatetimelikeProperties
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/indexes/accessors.py", line 418, in <module>
    class PeriodProperties(Properties):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/accessor.py", line 142, in add_delegate_accessors
    cls._add_delegate_accessors(delegate, accessors, typ, overwrite=overwrite)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/accessor.py", line 104, in _add_delegate_accessors
    f = _create_delegator_property(name)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/accessor.py", line 78, in _create_delegator_property
    def _create_delegator_property(name):
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-13 06:41:50,917 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:50,922 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36089 instead
  warnings.warn(
2023-12-13 06:41:50,926 - distributed.scheduler - INFO - State start
2023-12-13 06:41:50,949 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:50,950 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-12-13 06:41:50,950 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:50,951 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-12-13 06:41:51,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41933'
2023-12-13 06:41:52,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:52,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:53,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:54,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35275
2023-12-13 06:41:54,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35275
2023-12-13 06:41:54,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34899
2023-12-13 06:41:54,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:54,530 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:54,530 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:54,531 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:41:54,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-unxny42v
2023-12-13 06:41:54,531 - distributed.worker - INFO - Starting Worker plugin PreImport-2e6b97f8-cbb7-4445-af0f-9ad6f4123d27
2023-12-13 06:41:54,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-120412ba-26fc-4c23-b68a-b041bcfcb8b6
2023-12-13 06:41:54,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-00459b6a-74b3-4177-8cfe-70d35e5b87f5
2023-12-13 06:41:54,533 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:42:24,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41933'. Reason: nanny-close
2023-12-13 06:42:24,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-12-13 06:42:56,449 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:42:56,459 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34521 instead
  warnings.warn(
2023-12-13 06:42:56,466 - distributed.scheduler - INFO - State start
2023-12-13 06:42:56,468 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-unxny42v', purging
2023-12-13 06:42:56,501 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:42:56,502 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:42:56,503 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34521/status
2023-12-13 06:42:56,503 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:43:01,200 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:54324'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54324>: Stream is closed
2023-12-13 06:43:01,490 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:43:01,491 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:43:01,492 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:43:01,493 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:43:01,493 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-13 06:43:04,148 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:04,153 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45069 instead
  warnings.warn(
2023-12-13 06:43:04,158 - distributed.scheduler - INFO - State start
2023-12-13 06:43:04,183 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:04,184 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-13 06:43:04,185 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45069/status
2023-12-13 06:43:04,185 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:43:04,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39741'
2023-12-13 06:43:05,496 - distributed.scheduler - INFO - Receive client connection: Client-dca1c220-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:05,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54406
2023-12-13 06:43:05,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:05,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:06,000 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:07,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37573
2023-12-13 06:43:07,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37573
2023-12-13 06:43:07,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40245
2023-12-13 06:43:07,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:43:07,825 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:07,826 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:07,826 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:43:07,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-0lz8yo19
2023-12-13 06:43:07,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-700b1bcd-d313-4f06-9111-76df9181c8b3
2023-12-13 06:43:07,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dca2ce6b-bc4e-4cc1-9b5a-96d1e9a75acf
2023-12-13 06:43:07,827 - distributed.worker - INFO - Starting Worker plugin PreImport-932c51be-f3b3-4ac6-8623-7c62d56a737d
2023-12-13 06:43:07,827 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:07,847 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37573', status: init, memory: 0, processing: 0>
2023-12-13 06:43:07,848 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37573
2023-12-13 06:43:07,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54434
2023-12-13 06:43:07,849 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:07,850 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:43:07,850 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:07,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:43:07,875 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:07,878 - distributed.scheduler - INFO - Remove client Client-dca1c220-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:07,879 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54406; closing.
2023-12-13 06:43:07,879 - distributed.scheduler - INFO - Remove client Client-dca1c220-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:07,879 - distributed.scheduler - INFO - Close client connection: Client-dca1c220-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:07,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39741'. Reason: nanny-close
2023-12-13 06:43:07,882 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:07,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37573. Reason: nanny-close
2023-12-13 06:43:07,885 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:43:07,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54434; closing.
2023-12-13 06:43:07,885 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449787.8854003')
2023-12-13 06:43:07,885 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:43:07,886 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:08,846 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:43:08,846 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:43:08,847 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:43:08,848 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-13 06:43:08,849 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-13 06:43:11,200 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:11,205 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37403 instead
  warnings.warn(
2023-12-13 06:43:11,209 - distributed.scheduler - INFO - State start
2023-12-13 06:43:11,450 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:11,451 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:43:11,452 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37403/status
2023-12-13 06:43:11,452 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:43:12,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32859'
2023-12-13 06:43:12,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35129'
2023-12-13 06:43:12,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33953'
2023-12-13 06:43:12,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35195'
2023-12-13 06:43:12,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45597'
2023-12-13 06:43:12,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39077'
2023-12-13 06:43:12,093 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33203'
2023-12-13 06:43:12,103 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44621'
2023-12-13 06:43:12,171 - distributed.scheduler - INFO - Receive client connection: Client-e0ea1ca1-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:12,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45352
2023-12-13 06:43:14,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,127 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:14,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:14,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,208 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:14,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:21,880 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34235
2023-12-13 06:43:21,881 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34235
2023-12-13 06:43:21,881 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44547
2023-12-13 06:43:21,881 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:21,881 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:21,881 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:21,881 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:21,881 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oumy2p0j
2023-12-13 06:43:21,882 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bdd7042-b00a-4f03-95bc-5d2ee866c11c
2023-12-13 06:43:21,882 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2ad5961-111c-4409-8921-7a513b674137
2023-12-13 06:43:21,884 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37329
2023-12-13 06:43:21,885 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37329
2023-12-13 06:43:21,885 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33697
2023-12-13 06:43:21,885 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:21,885 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:21,885 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:21,886 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:21,886 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r3r6_6z6
2023-12-13 06:43:21,886 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18d8c5df-8a08-4d23-ab12-20aa293e304a
2023-12-13 06:43:21,887 - distributed.worker - INFO - Starting Worker plugin PreImport-638c0e3a-faae-4dc7-b317-df7c930ae49f
2023-12-13 06:43:21,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a5df3f9-6ec8-43fe-b09f-d4a6ac75ec48
2023-12-13 06:43:22,070 - distributed.worker - INFO - Starting Worker plugin PreImport-29135aac-79ec-4d6a-a359-0fb100473036
2023-12-13 06:43:22,070 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,071 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37329', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,110 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37329
2023-12-13 06:43:22,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55358
2023-12-13 06:43:22,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34235', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34235
2023-12-13 06:43:22,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55368
2023-12-13 06:43:22,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,112 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,112 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,113 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,114 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,114 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41325
2023-12-13 06:43:22,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41325
2023-12-13 06:43:22,689 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44497
2023-12-13 06:43:22,689 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,689 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,689 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:22,689 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:22,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m2qt2phr
2023-12-13 06:43:22,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-808c7947-8cb4-41aa-bb46-0625f0616614
2023-12-13 06:43:22,690 - distributed.worker - INFO - Starting Worker plugin PreImport-df2ef29a-1752-487d-8c23-263cb5376275
2023-12-13 06:43:22,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-95680716-c42c-44db-b380-af97dd4de0ad
2023-12-13 06:43:22,694 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44259
2023-12-13 06:43:22,695 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34451
2023-12-13 06:43:22,696 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44259
2023-12-13 06:43:22,696 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34451
2023-12-13 06:43:22,696 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38571
2023-12-13 06:43:22,696 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33771
2023-12-13 06:43:22,696 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,696 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,696 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,696 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,696 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:22,696 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:22,697 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:22,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gv00oy9x
2023-12-13 06:43:22,697 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:22,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2pg4hp6v
2023-12-13 06:43:22,697 - distributed.worker - INFO - Starting Worker plugin PreImport-ccd6ecd5-2a4a-48e5-803a-e468d077b45a
2023-12-13 06:43:22,697 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-993dbce4-e710-47dd-970d-7f83d7e91120
2023-12-13 06:43:22,697 - distributed.worker - INFO - Starting Worker plugin RMMSetup-077c980b-9655-46e2-844e-3dcc546d8c7f
2023-12-13 06:43:22,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63d38838-ccc2-4403-af68-6e3247a419f8
2023-12-13 06:43:22,698 - distributed.worker - INFO - Starting Worker plugin PreImport-67f81a01-137c-4369-abe9-d88fa298b296
2023-12-13 06:43:22,698 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eaa20c54-1251-40e1-83c7-64cc4b357622
2023-12-13 06:43:22,702 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34725
2023-12-13 06:43:22,703 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34725
2023-12-13 06:43:22,703 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40603
2023-12-13 06:43:22,703 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,703 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,703 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:22,704 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:22,704 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oj_014y0
2023-12-13 06:43:22,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4dc22d9b-7936-423f-a3e1-8ca16c928c54
2023-12-13 06:43:22,704 - distributed.worker - INFO - Starting Worker plugin PreImport-bb620551-77db-482b-946e-a7e8ca2366c4
2023-12-13 06:43:22,704 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f604dcc-5693-41b7-9181-f3e48e20eefa
2023-12-13 06:43:22,717 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46479
2023-12-13 06:43:22,718 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46479
2023-12-13 06:43:22,718 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34699
2023-12-13 06:43:22,718 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,718 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,718 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:22,718 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:22,718 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w1wlo2kd
2023-12-13 06:43:22,719 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-116bb457-3bf7-4941-948f-d63fa62768b3
2023-12-13 06:43:22,719 - distributed.worker - INFO - Starting Worker plugin PreImport-fb015261-d208-4c70-8bc0-d7f6bc0bfd6e
2023-12-13 06:43:22,720 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81b01d46-9578-4398-8762-5df1e550018d
2023-12-13 06:43:22,723 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40897
2023-12-13 06:43:22,724 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40897
2023-12-13 06:43:22,724 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33685
2023-12-13 06:43:22,724 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,724 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,724 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:22,724 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:43:22,724 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-10pyusqc
2023-12-13 06:43:22,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17212c1a-6c91-4a52-a71b-dad5e757a530
2023-12-13 06:43:22,725 - distributed.worker - INFO - Starting Worker plugin PreImport-30039769-9e83-4da4-81b2-27b1436fd87d
2023-12-13 06:43:22,725 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5ee8d9d-5a0e-4cfe-ae96-0e615a6440af
2023-12-13 06:43:22,877 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,879 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,885 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40897', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40897
2023-12-13 06:43:22,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55404
2023-12-13 06:43:22,908 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34725', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,908 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34725
2023-12-13 06:43:22,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55384
2023-12-13 06:43:22,909 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,909 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41325', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,910 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41325
2023-12-13 06:43:22,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55420
2023-12-13 06:43:22,911 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,911 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,911 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34451', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,912 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34451
2023-12-13 06:43:22,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55394
2023-12-13 06:43:22,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,912 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,914 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,917 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44259', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,918 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44259
2023-12-13 06:43:22,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55430
2023-12-13 06:43:22,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,921 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,921 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46479', status: init, memory: 0, processing: 0>
2023-12-13 06:43:22,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46479
2023-12-13 06:43:22,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55436
2023-12-13 06:43:22,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:22,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:22,927 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:22,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:22,963 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,964 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,964 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,964 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,964 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,964 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,965 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,965 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:43:22,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,980 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,980 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:22,984 - distributed.scheduler - INFO - Remove client Client-e0ea1ca1-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:22,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45352; closing.
2023-12-13 06:43:22,985 - distributed.scheduler - INFO - Remove client Client-e0ea1ca1-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:22,985 - distributed.scheduler - INFO - Close client connection: Client-e0ea1ca1-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:22,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32859'. Reason: nanny-close
2023-12-13 06:43:22,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35129'. Reason: nanny-close
2023-12-13 06:43:22,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34451. Reason: nanny-close
2023-12-13 06:43:22,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33953'. Reason: nanny-close
2023-12-13 06:43:22,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41325. Reason: nanny-close
2023-12-13 06:43:22,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35195'. Reason: nanny-close
2023-12-13 06:43:22,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37329. Reason: nanny-close
2023-12-13 06:43:22,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45597'. Reason: nanny-close
2023-12-13 06:43:22,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,989 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55394; closing.
2023-12-13 06:43:22,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44259. Reason: nanny-close
2023-12-13 06:43:22,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39077'. Reason: nanny-close
2023-12-13 06:43:22,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9903424')
2023-12-13 06:43:22,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34725. Reason: nanny-close
2023-12-13 06:43:22,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33203'. Reason: nanny-close
2023-12-13 06:43:22,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40897. Reason: nanny-close
2023-12-13 06:43:22,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44621'. Reason: nanny-close
2023-12-13 06:43:22,991 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:22,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46479. Reason: nanny-close
2023-12-13 06:43:22,992 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55420; closing.
2023-12-13 06:43:22,992 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,992 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55358; closing.
2023-12-13 06:43:22,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34235. Reason: nanny-close
2023-12-13 06:43:22,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,992 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41325', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.992855')
2023-12-13 06:43:22,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,993 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,993 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37329', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9935286')
2023-12-13 06:43:22,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55430; closing.
2023-12-13 06:43:22,994 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,994 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,994 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,994 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9947062')
2023-12-13 06:43:22,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:22,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55384; closing.
2023-12-13 06:43:22,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55404; closing.
2023-12-13 06:43:22,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34725', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9959075')
2023-12-13 06:43:22,996 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40897', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9962773')
2023-12-13 06:43:22,996 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55436; closing.
2023-12-13 06:43:22,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55368; closing.
2023-12-13 06:43:22,996 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:22,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9972825')
2023-12-13 06:43:22,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34235', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449802.9976027')
2023-12-13 06:43:22,997 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:43:25,105 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:43:25,105 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:43:25,105 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:43:25,106 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:43:25,107 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-13 06:43:27,381 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:27,386 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41889 instead
  warnings.warn(
2023-12-13 06:43:27,390 - distributed.scheduler - INFO - State start
2023-12-13 06:43:27,500 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:27,501 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:43:27,502 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41889/status
2023-12-13 06:43:27,502 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:43:28,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36817'
2023-12-13 06:43:30,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:30,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:30,024 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:30,603 - distributed.scheduler - INFO - Receive client connection: Client-ea91c8b6-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:30,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57564
2023-12-13 06:43:31,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45933
2023-12-13 06:43:31,364 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45933
2023-12-13 06:43:31,364 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34201
2023-12-13 06:43:31,364 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:31,364 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:31,364 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:31,364 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:43:31,364 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9hars46r
2023-12-13 06:43:31,366 - distributed.worker - INFO - Starting Worker plugin PreImport-ccf2a737-9026-416f-bb84-5f2e882cf92e
2023-12-13 06:43:31,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8e1a7c6-2c93-4c7d-b473-0af263d43e75
2023-12-13 06:43:31,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ca18ff4-fcdb-424c-a591-588710d6aa0a
2023-12-13 06:43:31,555 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:31,585 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45933', status: init, memory: 0, processing: 0>
2023-12-13 06:43:31,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45933
2023-12-13 06:43:31,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57576
2023-12-13 06:43:31,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:31,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:31,589 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:31,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:31,627 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:43:31,632 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:31,634 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:31,636 - distributed.scheduler - INFO - Remove client Client-ea91c8b6-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:31,637 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57564; closing.
2023-12-13 06:43:31,637 - distributed.scheduler - INFO - Remove client Client-ea91c8b6-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:31,637 - distributed.scheduler - INFO - Close client connection: Client-ea91c8b6-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:31,638 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36817'. Reason: nanny-close
2023-12-13 06:43:31,639 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:31,640 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45933. Reason: nanny-close
2023-12-13 06:43:31,642 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57576; closing.
2023-12-13 06:43:31,642 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:31,642 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45933', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449811.6426463')
2023-12-13 06:43:31,643 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:43:31,644 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:33,256 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:43:33,257 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:43:33,257 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:43:33,259 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:43:33,259 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-13 06:43:35,590 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:35,594 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39193 instead
  warnings.warn(
2023-12-13 06:43:35,599 - distributed.scheduler - INFO - State start
2023-12-13 06:43:35,621 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:43:35,622 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:43:35,623 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39193/status
2023-12-13 06:43:35,623 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:43:35,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34439'
2023-12-13 06:43:36,749 - distributed.scheduler - INFO - Receive client connection: Client-ef7e3ea7-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:36,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57644
2023-12-13 06:43:37,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:43:37,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:43:37,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:43:38,799 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42009
2023-12-13 06:43:38,799 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42009
2023-12-13 06:43:38,799 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36561
2023-12-13 06:43:38,799 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:43:38,799 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:38,799 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:43:38,799 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:43:38,800 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vphxtj2f
2023-12-13 06:43:38,800 - distributed.worker - INFO - Starting Worker plugin PreImport-9d97331b-72af-4200-a784-dd841c70e839
2023-12-13 06:43:38,800 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30b2f5fe-1291-4f57-9443-eee671c7c739
2023-12-13 06:43:38,800 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d44dc70-b4e2-401d-827c-5c9f6369213d
2023-12-13 06:43:38,928 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:38,966 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42009', status: init, memory: 0, processing: 0>
2023-12-13 06:43:38,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42009
2023-12-13 06:43:38,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57668
2023-12-13 06:43:38,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:43:38,970 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:43:38,970 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:43:38,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:43:39,006 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-13 06:43:39,011 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:43:39,014 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:39,016 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:43:39,018 - distributed.scheduler - INFO - Remove client Client-ef7e3ea7-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:39,019 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57644; closing.
2023-12-13 06:43:39,019 - distributed.scheduler - INFO - Remove client Client-ef7e3ea7-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:39,019 - distributed.scheduler - INFO - Close client connection: Client-ef7e3ea7-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:43:39,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34439'. Reason: nanny-close
2023-12-13 06:43:39,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:43:39,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42009. Reason: nanny-close
2023-12-13 06:43:39,024 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:43:39,024 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57668; closing.
2023-12-13 06:43:39,024 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42009', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449819.024614')
2023-12-13 06:43:39,025 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:43:39,026 - distributed.nanny - INFO - Worker closed
2023-12-13 06:43:40,187 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:43:40,187 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:43:40,188 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:43:40,189 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:43:40,189 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44929 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43197 instead
  warnings.warn(
2023-12-13 06:44:07,605 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-13 06:44:07,608 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:59051', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44279 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35173 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46045 instead
  warnings.warn(
2023-12-13 06:45:07,110 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-13 06:45:07,122 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:40231', name: 3, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39319 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38917 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36279 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36355 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35239 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40205 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38581 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36221 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37155 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45685 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38529 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43645 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44701 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43569 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44277 instead
  warnings.warn(
2023-12-13 06:51:59,922 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:51:59,931 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f317c312fa0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:51:59,932 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:51:59,941 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe74c3defa0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:52:01,934 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-12-13 06:52:01,945 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
