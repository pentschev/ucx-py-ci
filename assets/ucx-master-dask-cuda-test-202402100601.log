============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.0, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-10 06:52:35,989 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:35,993 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38707 instead
  warnings.warn(
2024-02-10 06:52:35,996 - distributed.scheduler - INFO - State start
2024-02-10 06:52:36,018 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:36,018 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-10 06:52:36,019 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38707/status
2024-02-10 06:52:36,019 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:52:36,264 - distributed.scheduler - INFO - Receive client connection: Client-f81711e1-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:36,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56628
2024-02-10 06:52:36,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33447'
2024-02-10 06:52:36,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46049'
2024-02-10 06:52:36,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33975'
2024-02-10 06:52:36,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37761'
2024-02-10 06:52:38,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:38,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:38,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:38,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:38,160 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:38,160 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:38,161 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46877
2024-02-10 06:52:38,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46877
2024-02-10 06:52:38,161 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35769
2024-02-10 06:52:38,161 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33767
2024-02-10 06:52:38,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35769
2024-02-10 06:52:38,161 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,161 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41355
2024-02-10 06:52:38,161 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,162 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:52:38,162 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,162 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:52:38,162 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:52:38,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-1yrl6bbo
2024-02-10 06:52:38,162 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:52:38,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-p1lbbqkf
2024-02-10 06:52:38,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-785b9357-6830-4fb5-981e-6b7462cd80cc
2024-02-10 06:52:38,162 - distributed.worker - INFO - Starting Worker plugin PreImport-1d96b0da-6924-4595-954d-cc46b4f5c018
2024-02-10 06:52:38,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e56066c7-2c28-417b-b8c4-0394e986f524
2024-02-10 06:52:38,162 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1573ccc-aaa1-4b01-bbf7-eb4c23fdf6f0
2024-02-10 06:52:38,162 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,163 - distributed.worker - INFO - Starting Worker plugin PreImport-fc811818-c421-4397-96f6-92eb6338672a
2024-02-10 06:52:38,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a35f405-562d-48e1-b2eb-8b8a5f45f086
2024-02-10 06:52:38,164 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:38,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:38,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:38,178 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33971
2024-02-10 06:52:38,178 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33971
2024-02-10 06:52:38,178 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33413
2024-02-10 06:52:38,178 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,178 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,179 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:52:38,179 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:52:38,179 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-0c24ye55
2024-02-10 06:52:38,179 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e49f0f65-30ac-415f-9b14-e5e58ec835b9
2024-02-10 06:52:38,179 - distributed.worker - INFO - Starting Worker plugin PreImport-8a8d71c9-f554-4b46-9909-338074c16d35
2024-02-10 06:52:38,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6db7f634-ad1c-454d-b4ba-2b4f2dc00a53
2024-02-10 06:52:38,180 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:38,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:38,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:38,229 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37653
2024-02-10 06:52:38,229 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37653
2024-02-10 06:52:38,229 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33487
2024-02-10 06:52:38,229 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,229 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,229 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:52:38,229 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:52:38,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-euhj48dv
2024-02-10 06:52:38,230 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b462ea0-7fac-4e3c-91ef-8cd137e03d7b
2024-02-10 06:52:38,230 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa8aead3-b05c-4659-aa4f-b95868cd5bf0
2024-02-10 06:52:38,231 - distributed.worker - INFO - Starting Worker plugin PreImport-0bd27494-bde0-41d9-b9ec-7b53a36b271d
2024-02-10 06:52:38,232 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,255 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35769', status: init, memory: 0, processing: 0>
2024-02-10 06:52:38,256 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35769
2024-02-10 06:52:38,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56652
2024-02-10 06:52:38,257 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:38,257 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,258 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:52:38,275 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46877', status: init, memory: 0, processing: 0>
2024-02-10 06:52:38,276 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46877
2024-02-10 06:52:38,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56660
2024-02-10 06:52:38,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:38,277 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,278 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,279 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:52:38,302 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33971', status: init, memory: 0, processing: 0>
2024-02-10 06:52:38,303 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33971
2024-02-10 06:52:38,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56670
2024-02-10 06:52:38,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:38,305 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,305 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:52:38,322 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37653', status: init, memory: 0, processing: 0>
2024-02-10 06:52:38,323 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37653
2024-02-10 06:52:38,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56686
2024-02-10 06:52:38,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:38,325 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:52:38,325 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:38,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:52:38,421 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:52:38,421 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:52:38,421 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:52:38,421 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:52:38,426 - distributed.scheduler - INFO - Remove client Client-f81711e1-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:38,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56628; closing.
2024-02-10 06:52:38,427 - distributed.scheduler - INFO - Remove client Client-f81711e1-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:38,427 - distributed.scheduler - INFO - Close client connection: Client-f81711e1-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:38,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33447'. Reason: nanny-close
2024-02-10 06:52:38,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:38,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46049'. Reason: nanny-close
2024-02-10 06:52:38,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:38,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33975'. Reason: nanny-close
2024-02-10 06:52:38,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35769. Reason: nanny-close
2024-02-10 06:52:38,430 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:38,430 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37761'. Reason: nanny-close
2024-02-10 06:52:38,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46877. Reason: nanny-close
2024-02-10 06:52:38,430 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:38,431 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37653. Reason: nanny-close
2024-02-10 06:52:38,431 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33971. Reason: nanny-close
2024-02-10 06:52:38,432 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:52:38,432 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:52:38,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56652; closing.
2024-02-10 06:52:38,433 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56660; closing.
2024-02-10 06:52:38,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:52:38,433 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547958.4334073')
2024-02-10 06:52:38,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:52:38,433 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:38,434 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547958.4341476')
2024-02-10 06:52:38,434 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:38,434 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:38,435 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:38,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56670; closing.
2024-02-10 06:52:38,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56686; closing.
2024-02-10 06:52:38,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33971', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547958.4363642')
2024-02-10 06:52:38,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547958.4369144')
2024-02-10 06:52:38,437 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:52:38,437 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:56670>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:56670>: Stream is closed
2024-02-10 06:52:39,144 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:52:39,144 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:52:39,145 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:52:39,147 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-10 06:52:39,148 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-10 06:52:41,358 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:41,362 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:52:41,366 - distributed.scheduler - INFO - State start
2024-02-10 06:52:41,386 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:41,386 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:52:41,387 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:52:41,388 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:52:41,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46495'
2024-02-10 06:52:41,396 - distributed.scheduler - INFO - Receive client connection: Client-fc3e1c97-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:52:41,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49912
2024-02-10 06:52:41,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33965'
2024-02-10 06:52:41,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41463'
2024-02-10 06:52:41,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44629'
2024-02-10 06:52:41,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44701'
2024-02-10 06:52:41,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45411'
2024-02-10 06:52:41,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44099'
2024-02-10 06:52:41,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33355'
2024-02-10 06:52:41,482 - distributed.scheduler - INFO - Receive client connection: Client-fb245a03-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:41,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49988
2024-02-10 06:52:41,806 - distributed.scheduler - INFO - Receive client connection: Client-fbeb65d3-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:52:41,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50014
2024-02-10 06:52:43,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,266 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,266 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45077
2024-02-10 06:52:43,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45203
2024-02-10 06:52:43,266 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45077
2024-02-10 06:52:43,266 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45203
2024-02-10 06:52:43,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46789
2024-02-10 06:52:43,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35881
2024-02-10 06:52:43,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,267 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,267 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,267 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,267 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vc8x967e
2024-02-10 06:52:43,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xjvj7ocg
2024-02-10 06:52:43,267 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9b5e21d-0ba8-4dc2-9ad6-03728b01fbff
2024-02-10 06:52:43,267 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba24b920-e521-44e6-b03f-81786cf1de86
2024-02-10 06:52:43,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38339
2024-02-10 06:52:43,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39167
2024-02-10 06:52:43,336 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38339
2024-02-10 06:52:43,336 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39167
2024-02-10 06:52:43,336 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43233
2024-02-10 06:52:43,336 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33997
2024-02-10 06:52:43,336 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,336 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,336 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,336 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,336 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,336 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,336 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,336 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_4zuq9cp
2024-02-10 06:52:43,336 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,336 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uhpj6cul
2024-02-10 06:52:43,336 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a324635-95cf-41fe-8a5b-f886f35e0c7f
2024-02-10 06:52:43,337 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01d99dfd-86de-40dc-b507-033fe132d850
2024-02-10 06:52:43,337 - distributed.worker - INFO - Starting Worker plugin PreImport-2f56fd4a-8e76-47e1-8aa7-a2d0fb026679
2024-02-10 06:52:43,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddf9024d-8092-49df-85ca-b69f83483756
2024-02-10 06:52:43,337 - distributed.worker - INFO - Starting Worker plugin PreImport-879f5891-6e49-41f7-b480-b59934f8dbf0
2024-02-10 06:52:43,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2193abf6-ff7d-4eab-8c52-75ba8ece655e
2024-02-10 06:52:43,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,342 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,343 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37129
2024-02-10 06:52:43,343 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37129
2024-02-10 06:52:43,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41461
2024-02-10 06:52:43,344 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,344 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,344 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,344 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bbq5xpgq
2024-02-10 06:52:43,344 - distributed.worker - INFO - Starting Worker plugin PreImport-ae58e2ec-0447-4a9d-94bf-35438a44282f
2024-02-10 06:52:43,344 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9d0091f-314d-45f1-8ab2-3f0bf3b072f4
2024-02-10 06:52:43,344 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ca7d9b6-d48a-4205-9c24-10f2efe71e0d
2024-02-10 06:52:43,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,346 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44781
2024-02-10 06:52:43,346 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44781
2024-02-10 06:52:43,346 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46349
2024-02-10 06:52:43,346 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,346 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,346 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,346 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,346 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ncrqlqas
2024-02-10 06:52:43,346 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7fcb99d-9460-4e85-bf97-e42e6a33008c
2024-02-10 06:52:43,346 - distributed.worker - INFO - Starting Worker plugin PreImport-af992457-6459-4d41-84b5-acc4b46bd056
2024-02-10 06:52:43,346 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6198522d-2e85-4a7b-a99c-6e1bcf1add57
2024-02-10 06:52:43,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,359 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,360 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45677
2024-02-10 06:52:43,360 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45677
2024-02-10 06:52:43,360 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34119
2024-02-10 06:52:43,360 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,360 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,360 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,360 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,360 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jyqfluix
2024-02-10 06:52:43,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12bb4964-3639-4fa3-b87a-1748f6e15d94
2024-02-10 06:52:43,749 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:43,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:43,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:43,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37967
2024-02-10 06:52:43,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37967
2024-02-10 06:52:43,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46795
2024-02-10 06:52:43,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:43,766 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:43,766 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:43,766 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:43,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f220nind
2024-02-10 06:52:43,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a98fb120-1361-44b6-ab1d-adea381ed19b
2024-02-10 06:52:45,332 - distributed.worker - INFO - Starting Worker plugin PreImport-ac38dda6-921f-4d27-b13f-4d493773f8b8
2024-02-10 06:52:45,333 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5e61053-2460-4e8e-8ee7-91c7d0f02df5
2024-02-10 06:52:45,333 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45203', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45203
2024-02-10 06:52:45,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50050
2024-02-10 06:52:45,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,359 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,360 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,383 - distributed.worker - INFO - Starting Worker plugin PreImport-3d0aa108-fedd-4996-8e07-ba77bf334921
2024-02-10 06:52:45,384 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77f512a1-0252-4612-8e68-9e1360f63410
2024-02-10 06:52:45,385 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,401 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:52:45,402 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:52:45,405 - distributed.scheduler - INFO - Remove client Client-fbeb65d3-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:52:45,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50014; closing.
2024-02-10 06:52:45,405 - distributed.scheduler - INFO - Remove client Client-fbeb65d3-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:52:45,406 - distributed.scheduler - INFO - Close client connection: Client-fbeb65d3-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:52:45,406 - distributed.scheduler - INFO - Remove client Client-fc3e1c97-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:52:45,406 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49912; closing.
2024-02-10 06:52:45,406 - distributed.scheduler - INFO - Remove client Client-fc3e1c97-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:52:45,407 - distributed.scheduler - INFO - Close client connection: Client-fc3e1c97-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:52:45,422 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45077', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,422 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45077
2024-02-10 06:52:45,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50064
2024-02-10 06:52:45,424 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,425 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,425 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,609 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,622 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,634 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44781', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44781
2024-02-10 06:52:45,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50066
2024-02-10 06:52:45,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,636 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,637 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38339', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,653 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38339
2024-02-10 06:52:45,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50082
2024-02-10 06:52:45,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,656 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,656 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39167', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39167
2024-02-10 06:52:45,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50096
2024-02-10 06:52:45,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,659 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,664 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,673 - distributed.worker - INFO - Starting Worker plugin PreImport-b6f2a9b4-c68c-4c5f-bfd2-b5170031f2fc
2024-02-10 06:52:45,673 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4facb43f-a62a-4b9f-b5db-20aa7af6dd4d
2024-02-10 06:52:45,674 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,683 - distributed.worker - INFO - Starting Worker plugin PreImport-8925b65a-31a5-435a-8afb-ace6ea966dd3
2024-02-10 06:52:45,683 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-addb3680-07e8-43a1-962d-120d7eec3da4
2024-02-10 06:52:45,683 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,694 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45677', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45677
2024-02-10 06:52:45,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50118
2024-02-10 06:52:45,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37129', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37129
2024-02-10 06:52:45,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50114
2024-02-10 06:52:45,697 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,697 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,698 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,699 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,699 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,706 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37967', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,707 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37967
2024-02-10 06:52:45,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50134
2024-02-10 06:52:45,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:45,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:45,709 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:45,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,814 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:45,817 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36599', status: init, memory: 0, processing: 0>
2024-02-10 06:52:45,817 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36599
2024-02-10 06:52:45,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50100
2024-02-10 06:52:45,819 - distributed.scheduler - INFO - Remove client Client-fb245a03-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:45,819 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49988; closing.
2024-02-10 06:52:45,819 - distributed.scheduler - INFO - Remove client Client-fb245a03-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:45,820 - distributed.scheduler - INFO - Close client connection: Client-fb245a03-c7e0-11ee-b928-d8c49764f6bb
2024-02-10 06:52:45,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45411'. Reason: nanny-close
2024-02-10 06:52:45,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,821 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41463'. Reason: nanny-close
2024-02-10 06:52:45,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44629'. Reason: nanny-close
2024-02-10 06:52:45,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37129. Reason: nanny-close
2024-02-10 06:52:45,823 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33965'. Reason: nanny-close
2024-02-10 06:52:45,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38339. Reason: nanny-close
2024-02-10 06:52:45,823 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44701'. Reason: nanny-close
2024-02-10 06:52:45,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39167. Reason: nanny-close
2024-02-10 06:52:45,824 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,824 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45203. Reason: nanny-close
2024-02-10 06:52:45,824 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46495'. Reason: nanny-close
2024-02-10 06:52:45,824 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,825 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44099'. Reason: nanny-close
2024-02-10 06:52:45,825 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37967. Reason: nanny-close
2024-02-10 06:52:45,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,825 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,825 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50114; closing.
2024-02-10 06:52:45,825 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33355'. Reason: nanny-close
2024-02-10 06:52:45,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,825 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45077. Reason: nanny-close
2024-02-10 06:52:45,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37129', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8258765')
2024-02-10 06:52:45,826 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:45,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,826 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45677. Reason: nanny-close
2024-02-10 06:52:45,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,826 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44781. Reason: nanny-close
2024-02-10 06:52:45,826 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50050; closing.
2024-02-10 06:52:45,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50082; closing.
2024-02-10 06:52:45,827 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,827 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,827 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,827 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,828 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45203', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.828304')
2024-02-10 06:52:45,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8286724')
2024-02-10 06:52:45,829 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50096; closing.
2024-02-10 06:52:45,829 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,829 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:45,829 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,829 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,830 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,831 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:45,829 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50050>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50050>: Stream is closed
2024-02-10 06:52:45,831 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39167', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8318799')
2024-02-10 06:52:45,832 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50134; closing.
2024-02-10 06:52:45,833 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8330736')
2024-02-10 06:52:45,833 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50100; closing.
2024-02-10 06:52:45,833 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50118; closing.
2024-02-10 06:52:45,833 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50064; closing.
2024-02-10 06:52:45,834 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36599', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.834377')
2024-02-10 06:52:45,834 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45677', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8347685')
2024-02-10 06:52:45,835 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8352613')
2024-02-10 06:52:45,835 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50066; closing.
2024-02-10 06:52:45,836 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547965.8361602')
2024-02-10 06:52:45,836 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:52:45,836 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50066>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-10 06:52:47,579 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35867', status: init, memory: 0, processing: 0>
2024-02-10 06:52:47,580 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35867
2024-02-10 06:52:47,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50146
2024-02-10 06:52:47,594 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50146; closing.
2024-02-10 06:52:47,594 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547967.5944207')
2024-02-10 06:52:47,594 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:52:47,740 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:52:47,740 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:52:47,741 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:52:47,742 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:52:47,743 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-10 06:52:49,965 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:49,969 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:52:49,972 - distributed.scheduler - INFO - State start
2024-02-10 06:52:49,993 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:49,994 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:52:49,995 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:52:49,995 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:52:50,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40757'
2024-02-10 06:52:50,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45117'
2024-02-10 06:52:50,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34897'
2024-02-10 06:52:50,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43393'
2024-02-10 06:52:50,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46145'
2024-02-10 06:52:50,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46883'
2024-02-10 06:52:50,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34335'
2024-02-10 06:52:50,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44203'
2024-02-10 06:52:50,350 - distributed.scheduler - INFO - Receive client connection: Client-00b64962-c7e1-11ee-833f-d8c49764f6bb
2024-02-10 06:52:50,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56336
2024-02-10 06:52:50,688 - distributed.scheduler - INFO - Receive client connection: Client-01cc9510-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:52:50,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56358
2024-02-10 06:52:52,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,132 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33871
2024-02-10 06:52:52,132 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40969
2024-02-10 06:52:52,132 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33871
2024-02-10 06:52:52,132 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40969
2024-02-10 06:52:52,132 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40117
2024-02-10 06:52:52,132 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43813
2024-02-10 06:52:52,132 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,132 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,132 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,132 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,132 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,132 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,132 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,132 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ue4ci87j
2024-02-10 06:52:52,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-we49l4o9
2024-02-10 06:52:52,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6853ad8-7585-47c5-98e8-11d182d8f09b
2024-02-10 06:52:52,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35e6898b-6e1c-44d8-8f0e-48ef7083ce07
2024-02-10 06:52:52,132 - distributed.worker - INFO - Starting Worker plugin PreImport-b9a3f54e-5c1f-4124-a57a-4221c6a37d94
2024-02-10 06:52:52,133 - distributed.worker - INFO - Starting Worker plugin RMMSetup-647f50cc-9cbd-4e53-ba52-0d0d4932e7a8
2024-02-10 06:52:52,135 - distributed.worker - INFO - Starting Worker plugin PreImport-7393cf45-5323-41a4-bd6c-38d99803466a
2024-02-10 06:52:52,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc8c09f1-734b-4846-9bda-997b2f5719e6
2024-02-10 06:52:52,161 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,161 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,166 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,167 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34951
2024-02-10 06:52:52,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34951
2024-02-10 06:52:52,167 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46693
2024-02-10 06:52:52,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,167 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,167 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-shz30gif
2024-02-10 06:52:52,167 - distributed.worker - INFO - Starting Worker plugin PreImport-a4d9e493-b3e8-4cd8-8ba0-2eb7b49d978d
2024-02-10 06:52:52,167 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8fe9a1a-0c3a-4d95-9bd0-1cd8151b3476
2024-02-10 06:52:52,168 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,168 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36059
2024-02-10 06:52:52,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36059
2024-02-10 06:52:52,169 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37579
2024-02-10 06:52:52,169 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,169 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,169 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,169 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tpyxef0y
2024-02-10 06:52:52,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2f90cd1-5951-4a8e-8d6f-3a13085130e5
2024-02-10 06:52:52,171 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ef5d2b8-e8a1-43d2-875b-b0a587be44f9
2024-02-10 06:52:52,171 - distributed.worker - INFO - Starting Worker plugin PreImport-0a506e11-d844-4c34-8399-a20ef33ea438
2024-02-10 06:52:52,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af3282d8-6c73-478d-9369-9376064bfec1
2024-02-10 06:52:52,177 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,177 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,181 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,182 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35823
2024-02-10 06:52:52,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35823
2024-02-10 06:52:52,183 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33017
2024-02-10 06:52:52,183 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,183 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,183 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,183 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ytmdedw
2024-02-10 06:52:52,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42cad085-b03b-4f02-bbf0-f8f91a1a54b7
2024-02-10 06:52:52,191 - distributed.worker - INFO - Starting Worker plugin PreImport-f0e3054b-723f-4a92-b48a-44a674c48093
2024-02-10 06:52:52,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d9c7ef8-c7aa-4d88-a931-5383c1db836b
2024-02-10 06:52:52,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,221 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,227 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,228 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36671
2024-02-10 06:52:52,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36671
2024-02-10 06:52:52,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46599
2024-02-10 06:52:52,228 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,228 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,228 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,228 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,228 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e7fqn4sv
2024-02-10 06:52:52,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,229 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb8f1339-a52a-4d96-a68f-9340dc860edc
2024-02-10 06:52:52,229 - distributed.worker - INFO - Starting Worker plugin PreImport-e100324a-4458-40c0-9720-9055aa3a1556
2024-02-10 06:52:52,229 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfb3e379-383c-487b-b0c1-3a48e25edd59
2024-02-10 06:52:52,230 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43365
2024-02-10 06:52:52,230 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43365
2024-02-10 06:52:52,230 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36319
2024-02-10 06:52:52,230 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,230 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,230 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,230 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,230 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a0s1h2hj
2024-02-10 06:52:52,230 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbed6733-1f35-4a18-a457-2c80100f9a1f
2024-02-10 06:52:52,234 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:52:52,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:52:52,239 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:52:52,240 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39957
2024-02-10 06:52:52,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39957
2024-02-10 06:52:52,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43485
2024-02-10 06:52:52,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:52:52,240 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:52,240 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:52:52,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:52:52,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7qtwthot
2024-02-10 06:52:52,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8969a3e8-f74d-44c2-92c2-ae242a694542
2024-02-10 06:52:52,240 - distributed.worker - INFO - Starting Worker plugin PreImport-2ba04a72-0979-412e-bc39-dff684468a32
2024-02-10 06:52:52,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-939ab08d-33f2-4fb9-b2e7-bd3e68110b14
2024-02-10 06:52:52,653 - distributed.scheduler - INFO - Receive client connection: Client-004374d5-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:52:52,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56384
2024-02-10 06:52:53,939 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:53,962 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40969', status: init, memory: 0, processing: 0>
2024-02-10 06:52:53,964 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40969
2024-02-10 06:52:53,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56428
2024-02-10 06:52:53,965 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:53,966 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:53,966 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:53,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,048 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,064 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:52:54,065 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:52:54,067 - distributed.scheduler - INFO - Remove client Client-00b64962-c7e1-11ee-833f-d8c49764f6bb
2024-02-10 06:52:54,067 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56336; closing.
2024-02-10 06:52:54,068 - distributed.scheduler - INFO - Remove client Client-00b64962-c7e1-11ee-833f-d8c49764f6bb
2024-02-10 06:52:54,068 - distributed.scheduler - INFO - Remove client Client-01cc9510-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:52:54,068 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56358; closing.
2024-02-10 06:52:54,069 - distributed.scheduler - INFO - Close client connection: Client-00b64962-c7e1-11ee-833f-d8c49764f6bb
2024-02-10 06:52:54,069 - distributed.scheduler - INFO - Remove client Client-01cc9510-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:52:54,069 - distributed.scheduler - INFO - Close client connection: Client-01cc9510-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:52:54,080 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33871', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,081 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33871
2024-02-10 06:52:54,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56442
2024-02-10 06:52:54,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,083 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,161 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,169 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,188 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36059', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36059
2024-02-10 06:52:54,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56456
2024-02-10 06:52:54,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,196 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,196 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,199 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35823', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35823
2024-02-10 06:52:54,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56468
2024-02-10 06:52:54,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,202 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,202 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34951', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34951
2024-02-10 06:52:54,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56474
2024-02-10 06:52:54,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,220 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,263 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,268 - distributed.worker - INFO - Starting Worker plugin PreImport-966df8b9-791c-4b71-a175-8ac3a149b7b6
2024-02-10 06:52:54,269 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7789ff00-1690-4588-bc38-b7b094c9508b
2024-02-10 06:52:54,269 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,269 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44849', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,270 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44849
2024-02-10 06:52:54,270 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56418
2024-02-10 06:52:54,279 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36671', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36671
2024-02-10 06:52:54,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56476
2024-02-10 06:52:54,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,289 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,289 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43365', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,292 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43365
2024-02-10 06:52:54,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56478
2024-02-10 06:52:54,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,293 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,294 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,295 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56418; closing.
2024-02-10 06:52:54,296 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44849', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.2960534')
2024-02-10 06:52:54,300 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39957', status: init, memory: 0, processing: 0>
2024-02-10 06:52:54,301 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39957
2024-02-10 06:52:54,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56482
2024-02-10 06:52:54,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:52:54,302 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:52:54,302 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:52:54,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:52:54,381 - distributed.scheduler - INFO - Remove client Client-004374d5-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:52:54,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56384; closing.
2024-02-10 06:52:54,381 - distributed.scheduler - INFO - Remove client Client-004374d5-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:52:54,381 - distributed.scheduler - INFO - Close client connection: Client-004374d5-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:52:54,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40757'. Reason: nanny-close
2024-02-10 06:52:54,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45117'. Reason: nanny-close
2024-02-10 06:52:54,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,384 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34897'. Reason: nanny-close
2024-02-10 06:52:54,384 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34951. Reason: nanny-close
2024-02-10 06:52:54,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43393'. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33871. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46145'. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36671. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46883'. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40969. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34335'. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35823. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44203'. Reason: nanny-close
2024-02-10 06:52:54,386 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56474; closing.
2024-02-10 06:52:54,387 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:52:54,387 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36059. Reason: nanny-close
2024-02-10 06:52:54,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.3873348')
2024-02-10 06:52:54,387 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39957. Reason: nanny-close
2024-02-10 06:52:54,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,387 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43365. Reason: nanny-close
2024-02-10 06:52:54,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56476; closing.
2024-02-10 06:52:54,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56442; closing.
2024-02-10 06:52:54,388 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,388 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,389 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,389 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36671', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.3892443')
2024-02-10 06:52:54,389 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.389586')
2024-02-10 06:52:54,389 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,389 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56428; closing.
2024-02-10 06:52:54,390 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,390 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:52:54,390 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.390708')
2024-02-10 06:52:54,391 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,391 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56468; closing.
2024-02-10 06:52:54,391 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,391 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56482; closing.
2024-02-10 06:52:54,392 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56456; closing.
2024-02-10 06:52:54,392 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56478; closing.
2024-02-10 06:52:54,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35823', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.392432')
2024-02-10 06:52:54,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39957', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.3929')
2024-02-10 06:52:54,393 - distributed.nanny - INFO - Worker closed
2024-02-10 06:52:54,393 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36059', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.3932729')
2024-02-10 06:52:54,393 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547974.3936262')
2024-02-10 06:52:54,393 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:52:55,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33279', status: init, memory: 0, processing: 0>
2024-02-10 06:52:55,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33279
2024-02-10 06:52:55,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56484
2024-02-10 06:52:55,203 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56484; closing.
2024-02-10 06:52:55,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33279', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547975.2038164')
2024-02-10 06:52:55,204 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:52:55,249 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:52:55,249 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:52:55,249 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:52:55,251 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:52:55,251 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-10 06:52:57,397 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:57,402 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41587 instead
  warnings.warn(
2024-02-10 06:52:57,406 - distributed.scheduler - INFO - State start
2024-02-10 06:52:57,427 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:52:57,428 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-10 06:52:57,428 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:52:57,429 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-10 06:52:57,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40053'
2024-02-10 06:52:57,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43077'
2024-02-10 06:52:57,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42781'
2024-02-10 06:52:57,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34457'
2024-02-10 06:52:57,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42303'
2024-02-10 06:52:57,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36765'
2024-02-10 06:52:57,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39531'
2024-02-10 06:52:57,550 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33753'
2024-02-10 06:53:00,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,011 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,017 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,019 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35609
2024-02-10 06:53:00,019 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35609
2024-02-10 06:53:00,019 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46157
2024-02-10 06:53:00,019 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,019 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,019 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,019 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pmbmema
2024-02-10 06:53:00,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ab142bd-2bf9-440e-9957-4d45d3ddfa39
2024-02-10 06:53:00,026 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45575
2024-02-10 06:53:00,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,027 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45575
2024-02-10 06:53:00,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,027 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43683
2024-02-10 06:53:00,027 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,027 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,027 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,027 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xwf6p834
2024-02-10 06:53:00,028 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ed02294-5209-4275-9cd3-4d05314fa3ec
2024-02-10 06:53:00,028 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55592560-6664-4d2b-9249-dea00cbe0cde
2024-02-10 06:53:00,034 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37199
2024-02-10 06:53:00,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37199
2024-02-10 06:53:00,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36065
2024-02-10 06:53:00,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,035 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,035 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z65i9rg5
2024-02-10 06:53:00,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-846c4528-495f-4c23-b23b-a5b97e642136
2024-02-10 06:53:00,036 - distributed.worker - INFO - Starting Worker plugin PreImport-63145d55-bcd2-457a-b0f5-0e06f20f9dd2
2024-02-10 06:53:00,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-767ace58-a6be-4507-911d-71ac0e02cff4
2024-02-10 06:53:00,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,042 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,044 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33337
2024-02-10 06:53:00,044 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33337
2024-02-10 06:53:00,044 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41615
2024-02-10 06:53:00,044 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,044 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,044 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,044 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,044 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z2_u9t3e
2024-02-10 06:53:00,044 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9e56c93-2e5a-4f31-8dea-c5daf3aaa9eb
2024-02-10 06:53:00,045 - distributed.worker - INFO - Starting Worker plugin PreImport-c2904209-fd88-47ca-bca7-dcfef344353c
2024-02-10 06:53:00,045 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0df23509-0722-4491-8c2a-22b5a3cf20e9
2024-02-10 06:53:00,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41235
2024-02-10 06:53:00,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41235
2024-02-10 06:53:00,049 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42679
2024-02-10 06:53:00,049 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,049 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,049 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,050 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8wp95xtx
2024-02-10 06:53:00,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa2484a3-e099-4722-b4af-70c18fdc1b77
2024-02-10 06:53:00,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:00,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:00,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43233
2024-02-10 06:53:00,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43233
2024-02-10 06:53:00,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36389
2024-02-10 06:53:00,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,055 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,055 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lqa2jon1
2024-02-10 06:53:00,055 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-409e024e-763d-4dc3-bf59-851019569225
2024-02-10 06:53:00,055 - distributed.worker - INFO - Starting Worker plugin PreImport-cd5290a9-ee04-44c4-ae44-1d958c569e1e
2024-02-10 06:53:00,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1116346c-3622-4665-89ca-9a1ab818265d
2024-02-10 06:53:00,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40019
2024-02-10 06:53:00,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40019
2024-02-10 06:53:00,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43681
2024-02-10 06:53:00,058 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,058 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,058 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,058 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-52ldgu1d
2024-02-10 06:53:00,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1e1b078-d2b0-43ac-a3a1-cab1e12e8757
2024-02-10 06:53:00,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:00,065 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45143
2024-02-10 06:53:00,065 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45143
2024-02-10 06:53:00,065 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45545
2024-02-10 06:53:00,065 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:00,065 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:00,065 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:00,065 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:00,065 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-95cm4mjg
2024-02-10 06:53:00,066 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57e96164-effd-4e27-bfce-a5b7b416b4ab
2024-02-10 06:53:03,020 - distributed.worker - INFO - Starting Worker plugin PreImport-b3179f9d-105f-471d-894b-a3b13338442d
2024-02-10 06:53:03,020 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93ea57fc-15c5-4df5-bdab-bf131f3099fe
2024-02-10 06:53:03,021 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,047 - distributed.worker - INFO - Starting Worker plugin PreImport-1d936740-62c4-4ebb-9841-433804dc64dd
2024-02-10 06:53:03,048 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5751d7c-5722-4a60-8e21-d707407962bb
2024-02-10 06:53:03,049 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,058 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,069 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,085 - distributed.worker - INFO - Starting Worker plugin PreImport-56d65a30-6b55-47f6-8542-607a9fb324f6
2024-02-10 06:53:03,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcdcdb7e-e6a5-4a4b-9069-ec96625bf7f4
2024-02-10 06:53:03,086 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,094 - distributed.worker - INFO - Starting Worker plugin PreImport-51b9cd8e-5c95-48e8-ab36-5d7f8d593363
2024-02-10 06:53:03,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-580962fb-582b-4b96-b2cd-77be2b57ccc6
2024-02-10 06:53:03,097 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,097 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:03,101 - distributed.worker - INFO - Starting Worker plugin PreImport-4215a6e8-f842-4803-857e-3ca05410931f
2024-02-10 06:53:03,103 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:08,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:08,566 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:08,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:08,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:08,607 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34457'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:08,607 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:08,609 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41235. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:08,611 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:08,612 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:08,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:08,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:08,797 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:08,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:08,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39531'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:08,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:08,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45143. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:08,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:08,817 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:56654 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-10 06:53:08,966 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48571 parent=48374 started daemon>
2024-02-10 06:53:08,966 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48567 parent=48374 started daemon>
2024-02-10 06:53:08,967 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48563 parent=48374 started daemon>
2024-02-10 06:53:08,967 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48559 parent=48374 started daemon>
2024-02-10 06:53:08,967 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48549 parent=48374 started daemon>
2024-02-10 06:53:08,967 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48544 parent=48374 started daemon>
2024-02-10 06:53:08,967 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48540 parent=48374 started daemon>
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-10 06:53:15,160 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:15,165 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36261 instead
  warnings.warn(
2024-02-10 06:53:15,169 - distributed.scheduler - INFO - State start
2024-02-10 06:53:15,171 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-lqa2jon1', purging
2024-02-10 06:53:15,171 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-52ldgu1d', purging
2024-02-10 06:53:15,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4pmbmema', purging
2024-02-10 06:53:15,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-z2_u9t3e', purging
2024-02-10 06:53:15,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xwf6p834', purging
2024-02-10 06:53:15,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-z65i9rg5', purging
2024-02-10 06:53:15,202 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:15,203 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:53:15,204 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36261/status
2024-02-10 06:53:15,204 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:53:15,242 - distributed.scheduler - INFO - Receive client connection: Client-0fc3c4c8-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:53:15,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37348
2024-02-10 06:53:15,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35727'
2024-02-10 06:53:15,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38801'
2024-02-10 06:53:15,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35331'
2024-02-10 06:53:15,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44843'
2024-02-10 06:53:15,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45587'
2024-02-10 06:53:15,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35349'
2024-02-10 06:53:15,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36339'
2024-02-10 06:53:15,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41447'
2024-02-10 06:53:16,649 - distributed.scheduler - INFO - Receive client connection: Client-0f375bff-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:53:16,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37444
2024-02-10 06:53:17,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39025
2024-02-10 06:53:17,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39025
2024-02-10 06:53:17,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35809
2024-02-10 06:53:17,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,369 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,369 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vbeeslsy
2024-02-10 06:53:17,370 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f14105d-ee7e-4e9b-8531-aedc67719f48
2024-02-10 06:53:17,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-675ec970-d8bc-42e7-958b-d21dad10c08d
2024-02-10 06:53:17,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43075
2024-02-10 06:53:17,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43075
2024-02-10 06:53:17,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33949
2024-02-10 06:53:17,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45707
2024-02-10 06:53:17,413 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45707
2024-02-10 06:53:17,413 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42771
2024-02-10 06:53:17,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ghqhvsup
2024-02-10 06:53:17,413 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,413 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lrc7fsgz
2024-02-10 06:53:17,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b350c5d5-e388-4831-8111-9d1f5f22c80c
2024-02-10 06:53:17,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e4c27c3-5a2e-4643-b075-97f7fea0c0af
2024-02-10 06:53:17,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,609 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,609 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,610 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37269
2024-02-10 06:53:17,610 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34467
2024-02-10 06:53:17,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37269
2024-02-10 06:53:17,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34467
2024-02-10 06:53:17,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37617
2024-02-10 06:53:17,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43357
2024-02-10 06:53:17,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,610 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,610 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,610 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,610 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,611 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,611 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,611 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1t2hl9q7
2024-02-10 06:53:17,611 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qtf4jth3
2024-02-10 06:53:17,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd4fcfa3-c724-4b3c-bc75-80db612de76d
2024-02-10 06:53:17,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd5f2750-8da7-4d8a-804b-bd0fddb87b81
2024-02-10 06:53:17,611 - distributed.worker - INFO - Starting Worker plugin PreImport-703dacfd-70d7-4c7b-9cff-85bd44690d39
2024-02-10 06:53:17,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddd6c57d-11a5-4600-8867-9aece0c07b5b
2024-02-10 06:53:17,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,620 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44559
2024-02-10 06:53:17,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44559
2024-02-10 06:53:17,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33767
2024-02-10 06:53:17,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,620 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,620 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ww82f16
2024-02-10 06:53:17,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4877bd09-94ee-488c-8afc-4a8ae894aeea
2024-02-10 06:53:17,621 - distributed.worker - INFO - Starting Worker plugin PreImport-ae59b289-db26-42bf-b51e-4158d6c65a60
2024-02-10 06:53:17,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0248441-e32e-4b19-a06e-3b7b78af785a
2024-02-10 06:53:17,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:17,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:17,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:17,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41929
2024-02-10 06:53:17,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41929
2024-02-10 06:53:17,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41493
2024-02-10 06:53:17,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37117
2024-02-10 06:53:17,627 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37117
2024-02-10 06:53:17,627 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43437
2024-02-10 06:53:17,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l9q8sxur
2024-02-10 06:53:17,628 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:17,628 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:17,628 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:17,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:17,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92c62404-b928-411d-a1ba-2f493cc1456d
2024-02-10 06:53:17,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lkdpx9wm
2024-02-10 06:53:17,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ac9bf48-452b-465a-a0dc-1fe16259a39e
2024-02-10 06:53:17,631 - distributed.worker - INFO - Starting Worker plugin PreImport-b88af32e-3c78-45e3-8bce-b40e2a479fa9
2024-02-10 06:53:17,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52e35237-53e7-4177-9e52-995348d80ca0
2024-02-10 06:53:19,372 - distributed.worker - INFO - Starting Worker plugin PreImport-a9efe973-8926-416c-bce6-5f5c6b0559c0
2024-02-10 06:53:19,373 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,394 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39025', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,396 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39025
2024-02-10 06:53:19,396 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37450
2024-02-10 06:53:19,397 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,398 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,398 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:19,405 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:19,407 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:19,411 - distributed.scheduler - INFO - Remove client Client-0fc3c4c8-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:53:19,411 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37348; closing.
2024-02-10 06:53:19,411 - distributed.scheduler - INFO - Remove client Client-0fc3c4c8-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:53:19,412 - distributed.scheduler - INFO - Close client connection: Client-0fc3c4c8-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:53:19,661 - distributed.worker - INFO - Starting Worker plugin PreImport-acc7f515-e62c-4262-897c-1390d12bebc2
2024-02-10 06:53:19,661 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2ed0d29-9448-4175-a6f9-99868aa0bee4
2024-02-10 06:53:19,663 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,666 - distributed.worker - INFO - Starting Worker plugin PreImport-0b22ad43-4bf2-46fc-a86d-009573282213
2024-02-10 06:53:19,666 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-705afc8d-0aa6-4ada-bbf4-4392457d639c
2024-02-10 06:53:19,667 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,679 - distributed.worker - INFO - Starting Worker plugin PreImport-07469810-1f2e-4539-b17a-bf78bf0ff03e
2024-02-10 06:53:19,680 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05592c71-5e33-48bb-acd1-98ec63fdfb58
2024-02-10 06:53:19,680 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,688 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45707', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,688 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45707
2024-02-10 06:53:19,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37454
2024-02-10 06:53:19,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,690 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,690 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,691 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43075', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43075
2024-02-10 06:53:19,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37452
2024-02-10 06:53:19,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,698 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,700 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37269', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,700 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37269
2024-02-10 06:53:19,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37470
2024-02-10 06:53:19,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,701 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,702 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,702 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,777 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,789 - distributed.worker - INFO - Starting Worker plugin PreImport-6b7de949-f7cb-4774-aaf4-a726455cecec
2024-02-10 06:53:19,790 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3b255ce-8f6a-4043-bafd-3458dd6cd94f
2024-02-10 06:53:19,791 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,791 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,792 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,797 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34467', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,797 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34467
2024-02-10 06:53:19,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37476
2024-02-10 06:53:19,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,799 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,799 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37117', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,825 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37117
2024-02-10 06:53:19,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37488
2024-02-10 06:53:19,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41929', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,827 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41929
2024-02-10 06:53:19,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37498
2024-02-10 06:53:19,828 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,828 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,828 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44559', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,829 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44559
2024-02-10 06:53:19,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37484
2024-02-10 06:53:19,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,830 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,830 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,830 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:19,831 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:19,831 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:19,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:19,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33429', status: init, memory: 0, processing: 0>
2024-02-10 06:53:19,895 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33429
2024-02-10 06:53:19,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37504
2024-02-10 06:53:19,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37504; closing.
2024-02-10 06:53:19,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547999.9449925')
2024-02-10 06:53:20,019 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,020 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,020 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,020 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,020 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,020 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,021 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,021 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,031 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,031 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,032 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,032 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,032 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,032 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,032 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,032 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:53:20,041 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,042 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:53:20,044 - distributed.scheduler - INFO - Remove client Client-0f375bff-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:53:20,045 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37444; closing.
2024-02-10 06:53:20,045 - distributed.scheduler - INFO - Remove client Client-0f375bff-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:53:20,045 - distributed.scheduler - INFO - Close client connection: Client-0f375bff-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:53:20,046 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35727'. Reason: nanny-close
2024-02-10 06:53:20,046 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,047 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38801'. Reason: nanny-close
2024-02-10 06:53:20,047 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,047 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35331'. Reason: nanny-close
2024-02-10 06:53:20,047 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39025. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44843'. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34467. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45587'. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44559. Reason: nanny-close
2024-02-10 06:53:20,048 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35349'. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43075. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37450; closing.
2024-02-10 06:53:20,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36339'. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,049 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45707. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.0497072')
2024-02-10 06:53:20,049 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41447'. Reason: nanny-close
2024-02-10 06:53:20,050 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,050 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37269. Reason: nanny-close
2024-02-10 06:53:20,050 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:53:20,050 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41929. Reason: nanny-close
2024-02-10 06:53:20,050 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,050 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,051 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37117. Reason: nanny-close
2024-02-10 06:53:20,051 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,051 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,051 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37484; closing.
2024-02-10 06:53:20,051 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,051 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37476; closing.
2024-02-10 06:53:20,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,052 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.0522876')
2024-02-10 06:53:20,052 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,052 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,052 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34467', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.052665')
2024-02-10 06:53:20,052 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,053 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37452; closing.
2024-02-10 06:53:20,053 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:20,053 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43075', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.053505')
2024-02-10 06:53:20,053 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,053 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37454; closing.
2024-02-10 06:53:20,054 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,054 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.0544949')
2024-02-10 06:53:20,054 - distributed.nanny - INFO - Worker closed
2024-02-10 06:53:20,054 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37470; closing.
2024-02-10 06:53:20,055 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37498; closing.
2024-02-10 06:53:20,055 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.0555308')
2024-02-10 06:53:20,055 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.0559163')
2024-02-10 06:53:20,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37488; closing.
2024-02-10 06:53:20,056 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548000.0566244')
2024-02-10 06:53:20,056 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:53:20,853 - distributed.scheduler - INFO - Receive client connection: Client-13c79721-c7e1-11ee-8170-d8c49764f6bb
2024-02-10 06:53:20,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48762
2024-02-10 06:53:21,062 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:53:21,063 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:53:21,063 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:53:21,065 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:53:21,065 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-10 06:53:23,418 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:23,423 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43655 instead
  warnings.warn(
2024-02-10 06:53:23,427 - distributed.scheduler - INFO - State start
2024-02-10 06:53:23,449 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:23,450 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-10 06:53:23,451 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:53:23,451 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-10 06:53:23,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41801'
2024-02-10 06:53:23,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36627'
2024-02-10 06:53:23,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33011'
2024-02-10 06:53:23,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41881'
2024-02-10 06:53:23,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38733'
2024-02-10 06:53:23,636 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37425'
2024-02-10 06:53:23,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38609'
2024-02-10 06:53:23,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42293'
2024-02-10 06:53:25,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,487 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39477
2024-02-10 06:53:25,487 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39477
2024-02-10 06:53:25,487 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37879
2024-02-10 06:53:25,487 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,487 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,487 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,488 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,488 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c41srg7o
2024-02-10 06:53:25,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f38a5c8-2c72-4f2f-8e67-9d49ad72b33d
2024-02-10 06:53:25,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,505 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38049
2024-02-10 06:53:25,505 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38049
2024-02-10 06:53:25,505 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42621
2024-02-10 06:53:25,505 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,505 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,505 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,505 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,505 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ivzccxg4
2024-02-10 06:53:25,506 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1f15ce1-026f-4272-97a0-0656149ae44d
2024-02-10 06:53:25,506 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53a75454-99aa-42db-ad52-2720eafe7e68
2024-02-10 06:53:25,507 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43491
2024-02-10 06:53:25,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43491
2024-02-10 06:53:25,508 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41165
2024-02-10 06:53:25,508 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,508 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,508 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,508 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,508 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-farsily8
2024-02-10 06:53:25,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb07eacc-fb1b-4fe6-b89f-67753d6c8250
2024-02-10 06:53:25,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,525 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41213
2024-02-10 06:53:25,526 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41213
2024-02-10 06:53:25,526 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46119
2024-02-10 06:53:25,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,526 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,527 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,527 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n38hed7b
2024-02-10 06:53:25,527 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2935faa1-5551-4862-a06e-fd3863166fb8
2024-02-10 06:53:25,528 - distributed.worker - INFO - Starting Worker plugin PreImport-7a9f6333-9b29-432a-afda-fb08c8727c41
2024-02-10 06:53:25,528 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9772c3c-cd19-4ddd-8e5c-ad7f0db82c14
2024-02-10 06:53:25,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,545 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,545 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,546 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34899
2024-02-10 06:53:25,546 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34899
2024-02-10 06:53:25,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,546 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39077
2024-02-10 06:53:25,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,546 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,546 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44723
2024-02-10 06:53:25,546 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,546 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44723
2024-02-10 06:53:25,546 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33767
2024-02-10 06:53:25,546 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b6mn2npe
2024-02-10 06:53:25,547 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,547 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,547 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ofaxyp1n
2024-02-10 06:53:25,547 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5aff33bf-3f95-4503-8540-aece733dd930
2024-02-10 06:53:25,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e26fc1d0-8b56-4f0f-bedb-193faffbe8ec
2024-02-10 06:53:25,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43275
2024-02-10 06:53:25,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43275
2024-02-10 06:53:25,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46339
2024-02-10 06:53:25,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,547 - distributed.worker - INFO - Starting Worker plugin PreImport-e96f5339-b9ac-4627-a7c1-51537bb35424
2024-02-10 06:53:25,547 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,547 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,547 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6aaf84b9-6830-4aa2-9d2a-d1631e49810a
2024-02-10 06:53:25,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r4u29_47
2024-02-10 06:53:25,548 - distributed.worker - INFO - Starting Worker plugin PreImport-2452262d-9700-4e4e-a1b7-0ac9ea0d89f3
2024-02-10 06:53:25,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29f1e3ee-a425-470d-99bb-bbdef2757c8d
2024-02-10 06:53:25,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d82f79fb-bb8c-4698-8a17-cf84ce2086e1
2024-02-10 06:53:25,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:53:25,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:53:25,559 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:53:25,560 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39903
2024-02-10 06:53:25,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39903
2024-02-10 06:53:25,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44423
2024-02-10 06:53:25,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:53:25,560 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:25,560 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:53:25,560 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:53:25,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gkg38alp
2024-02-10 06:53:25,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e3871fa-a192-42f0-87bc-fdf89698358d
2024-02-10 06:53:25,560 - distributed.worker - INFO - Starting Worker plugin PreImport-6fd81a29-2aaf-46e4-b9e6-0e8b2e0d4127
2024-02-10 06:53:25,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c04f644f-10ba-4c46-aac5-c7d8a6c5b1e8
2024-02-10 06:53:27,449 - distributed.worker - INFO - Starting Worker plugin PreImport-8537bd33-217f-4017-b711-3e23daf73384
2024-02-10 06:53:27,449 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a2d48bd-7b4f-485e-b208-ff5ac28397e9
2024-02-10 06:53:27,450 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,563 - distributed.worker - INFO - Starting Worker plugin PreImport-488e077c-2f1f-45f4-a8d7-dcc5f0dbd42b
2024-02-10 06:53:27,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-353e391f-819c-413c-b543-071804551eca
2024-02-10 06:53:27,565 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,582 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,625 - distributed.worker - INFO - Starting Worker plugin PreImport-b731ae09-d433-469a-8436-da196ffe587c
2024-02-10 06:53:27,625 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,653 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,661 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,672 - distributed.worker - INFO - Starting Worker plugin PreImport-6dbd4f56-2954-418b-8540-2e5e093a64f5
2024-02-10 06:53:27,673 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2871d560-fd14-4ad7-a18e-83131cf59088
2024-02-10 06:53:27,674 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:27,678 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:38,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:53:38,402 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:53:38,402 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:53:38,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:53:38,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33011'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:38,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:38,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41213. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-10 06:53:38,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:53:38,423 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:49088 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-10 06:53:38,855 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49120 parent=48923 started daemon>
2024-02-10 06:53:38,855 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49117 parent=48923 started daemon>
2024-02-10 06:53:38,856 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49112 parent=48923 started daemon>
2024-02-10 06:53:38,856 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49109 parent=48923 started daemon>
2024-02-10 06:53:38,856 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49103 parent=48923 started daemon>
2024-02-10 06:53:38,857 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49093 parent=48923 started daemon>
2024-02-10 06:53:38,857 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49089 parent=48923 started daemon>
2024-02-10 06:53:39,172 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 49120 exit status was already read will report exitcode 255
2024-02-10 06:53:39,202 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 49103 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-10 06:53:53,849 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:53,854 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36179 instead
  warnings.warn(
2024-02-10 06:53:53,858 - distributed.scheduler - INFO - State start
2024-02-10 06:53:53,860 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-r4u29_47', purging
2024-02-10 06:53:53,860 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-b6mn2npe', purging
2024-02-10 06:53:53,861 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-c41srg7o', purging
2024-02-10 06:53:53,861 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-farsily8', purging
2024-02-10 06:53:53,861 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ivzccxg4', purging
2024-02-10 06:53:53,862 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ofaxyp1n', purging
2024-02-10 06:53:53,862 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gkg38alp', purging
2024-02-10 06:53:53,884 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:53,884 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-10 06:53:53,885 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:53:53,886 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

Aborted!
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-10 06:53:58,630 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:58,634 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38995 instead
  warnings.warn(
2024-02-10 06:53:58,638 - distributed.scheduler - INFO - State start
2024-02-10 06:53:58,660 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:53:58,661 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:53:58,661 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38995/status
2024-02-10 06:53:58,661 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:53:58,672 - distributed.scheduler - INFO - Receive client connection: Client-292898a4-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:53:58,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58062
2024-02-10 06:53:58,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32961'
2024-02-10 06:54:00,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:00,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:01,338 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:01,339 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32803
2024-02-10 06:54:01,339 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32803
2024-02-10 06:54:01,339 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33149
2024-02-10 06:54:01,339 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:01,339 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:01,339 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:01,339 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:54:01,339 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wtrryy20
2024-02-10 06:54:01,339 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ccb95065-2b52-4c7b-8063-ceed237f01fa
2024-02-10 06:54:01,339 - distributed.worker - INFO - Starting Worker plugin PreImport-ec2a22c7-3d48-46e9-ba99-dbf44cd79b1f
2024-02-10 06:54:01,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de904059-dd9c-43e7-ac1b-40aeab2e5aaf
2024-02-10 06:54:01,341 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:01,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32803', status: init, memory: 0, processing: 0>
2024-02-10 06:54:01,399 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32803
2024-02-10 06:54:01,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60722
2024-02-10 06:54:01,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:01,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:01,401 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:01,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:01,436 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:01,439 - distributed.scheduler - INFO - Remove client Client-292898a4-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:01,439 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58062; closing.
2024-02-10 06:54:01,440 - distributed.scheduler - INFO - Remove client Client-292898a4-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:01,440 - distributed.scheduler - INFO - Close client connection: Client-292898a4-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:01,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32961'. Reason: nanny-close
2024-02-10 06:54:01,447 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:01,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32803. Reason: nanny-close
2024-02-10 06:54:01,449 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:01,449 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60722; closing.
2024-02-10 06:54:01,450 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548041.4501243')
2024-02-10 06:54:01,450 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:54:01,451 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:02,256 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:54:02,256 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:54:02,257 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:54:02,258 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:54:02,259 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-10 06:54:04,670 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:04,674 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38877 instead
  warnings.warn(
2024-02-10 06:54:04,678 - distributed.scheduler - INFO - State start
2024-02-10 06:54:04,710 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:04,711 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:54:04,711 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38877/status
2024-02-10 06:54:04,711 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:54:07,562 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:60732'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60732>: Stream is closed
2024-02-10 06:54:07,882 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:54:07,883 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:54:07,885 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:54:07,886 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:54:07,887 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-10 06:54:10,242 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:10,247 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38223 instead
  warnings.warn(
2024-02-10 06:54:10,251 - distributed.scheduler - INFO - State start
2024-02-10 06:54:10,272 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:10,273 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-10 06:54:10,274 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38223/status
2024-02-10 06:54:10,274 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:54:10,390 - distributed.scheduler - INFO - Receive client connection: Client-3016f6bd-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:10,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38191'
2024-02-10 06:54:10,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50590
2024-02-10 06:54:12,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:12,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:12,384 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:12,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46213
2024-02-10 06:54:12,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46213
2024-02-10 06:54:12,384 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40531
2024-02-10 06:54:12,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:54:12,385 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:12,385 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:12,385 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:54:12,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-neddx3id
2024-02-10 06:54:12,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-244a3a59-efd4-4d5b-bd28-09919d782cd8
2024-02-10 06:54:12,385 - distributed.worker - INFO - Starting Worker plugin PreImport-8f4957d5-2228-4acb-b3c4-02390a90050c
2024-02-10 06:54:12,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55c80ae3-7da9-4cd6-b50e-9cd9e9c89f08
2024-02-10 06:54:12,385 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:14,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46213', status: init, memory: 0, processing: 0>
2024-02-10 06:54:14,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46213
2024-02-10 06:54:14,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50632
2024-02-10 06:54:14,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:14,109 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:54:14,109 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:14,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:54:14,166 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:14,169 - distributed.scheduler - INFO - Remove client Client-3016f6bd-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:14,169 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50590; closing.
2024-02-10 06:54:14,169 - distributed.scheduler - INFO - Remove client Client-3016f6bd-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:14,170 - distributed.scheduler - INFO - Close client connection: Client-3016f6bd-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:14,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38191'. Reason: nanny-close
2024-02-10 06:54:14,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:14,172 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46213. Reason: nanny-close
2024-02-10 06:54:14,174 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:54:14,174 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50632; closing.
2024-02-10 06:54:14,174 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46213', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548054.1748505')
2024-02-10 06:54:14,175 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:54:14,175 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:14,685 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:54:14,685 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:54:14,686 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:54:14,687 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-10 06:54:14,687 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-10 06:54:17,055 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:17,059 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36701 instead
  warnings.warn(
2024-02-10 06:54:17,063 - distributed.scheduler - INFO - State start
2024-02-10 06:54:17,233 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:17,234 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:54:17,235 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36701/status
2024-02-10 06:54:17,235 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:54:17,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36633'
2024-02-10 06:54:17,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44813'
2024-02-10 06:54:17,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43107'
2024-02-10 06:54:17,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32961'
2024-02-10 06:54:17,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45907'
2024-02-10 06:54:17,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38999'
2024-02-10 06:54:17,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34841'
2024-02-10 06:54:17,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33263'
2024-02-10 06:54:19,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36193
2024-02-10 06:54:19,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36193
2024-02-10 06:54:19,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43897
2024-02-10 06:54:19,468 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,469 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,469 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,469 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wrjx2ypp
2024-02-10 06:54:19,469 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8eaa3ee6-1061-446a-bac5-55cc3b5dc9d1
2024-02-10 06:54:19,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,532 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36477
2024-02-10 06:54:19,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36477
2024-02-10 06:54:19,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33939
2024-02-10 06:54:19,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,539 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,539 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3kia34kt
2024-02-10 06:54:19,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74a2ba61-2a70-4e9d-a67f-f31c5c8f8923
2024-02-10 06:54:19,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,724 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,725 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43977
2024-02-10 06:54:19,725 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43977
2024-02-10 06:54:19,725 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36421
2024-02-10 06:54:19,725 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,725 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,725 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,725 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kvrvcb76
2024-02-10 06:54:19,726 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75d5c4b2-a065-42ea-a007-298ca77d3f9a
2024-02-10 06:54:19,726 - distributed.worker - INFO - Starting Worker plugin PreImport-92c616b6-9981-4a75-88c4-b9cf1836359e
2024-02-10 06:54:19,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-758a0e95-9980-423c-afe8-0a0ca7abbe95
2024-02-10 06:54:19,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,760 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34585
2024-02-10 06:54:19,762 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34585
2024-02-10 06:54:19,762 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35005
2024-02-10 06:54:19,762 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,762 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,762 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,762 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,762 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztzszzuc
2024-02-10 06:54:19,763 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a293107-c04e-4ac3-9bb9-0b7b20b6070c
2024-02-10 06:54:19,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,766 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32795
2024-02-10 06:54:19,766 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32795
2024-02-10 06:54:19,766 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45077
2024-02-10 06:54:19,766 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,766 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,766 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,766 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uop8t7r5
2024-02-10 06:54:19,767 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96cc5486-7517-4e82-9b5f-4f4a140cdc60
2024-02-10 06:54:19,798 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,798 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,804 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:19,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:19,805 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44495
2024-02-10 06:54:19,805 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44495
2024-02-10 06:54:19,806 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37843
2024-02-10 06:54:19,806 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,806 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,806 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,806 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,806 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a5q5sxa0
2024-02-10 06:54:19,806 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9979548a-2046-447c-8f05-520fb1b66ff4
2024-02-10 06:54:19,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,807 - distributed.worker - INFO - Starting Worker plugin PreImport-51166d8a-103e-4b0a-b8b3-d8f4cdce095e
2024-02-10 06:54:19,807 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3bcb419-edee-496c-b73f-06306b76bc43
2024-02-10 06:54:19,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37199
2024-02-10 06:54:19,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37199
2024-02-10 06:54:19,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37607
2024-02-10 06:54:19,808 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,808 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,808 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,808 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1h56fijn
2024-02-10 06:54:19,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2935353-9958-476b-8854-172ea1bcbc45
2024-02-10 06:54:19,809 - distributed.worker - INFO - Starting Worker plugin PreImport-4d7fc626-754a-44a0-b240-a30bb6cdda4c
2024-02-10 06:54:19,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e36f8f64-cbe5-44eb-bbdd-899249db9b43
2024-02-10 06:54:19,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:19,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34935
2024-02-10 06:54:19,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34935
2024-02-10 06:54:19,812 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42533
2024-02-10 06:54:19,812 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:19,812 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:19,812 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:19,812 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:54:19,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8rjv8nwh
2024-02-10 06:54:19,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63fafea2-4e95-4ae9-b775-165aac888445
2024-02-10 06:54:19,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ebe1a14-5147-43fa-9996-7fe98111b445
2024-02-10 06:54:20,935 - distributed.worker - INFO - Starting Worker plugin PreImport-b7948fc8-40fe-47f7-899b-45bb193f6a01
2024-02-10 06:54:20,936 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f5126a8-7d92-4017-aa7d-a0dad5f8e5bd
2024-02-10 06:54:20,937 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:20,963 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36193', status: init, memory: 0, processing: 0>
2024-02-10 06:54:20,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36193
2024-02-10 06:54:20,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50346
2024-02-10 06:54:20,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:20,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:20,977 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:20,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,564 - distributed.worker - INFO - Starting Worker plugin PreImport-24ba5f23-5b6c-4e59-81e1-36a9145bba78
2024-02-10 06:54:21,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33e4dcec-325d-4aba-b2ba-5132103b1a62
2024-02-10 06:54:21,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36477', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36477
2024-02-10 06:54:21,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50356
2024-02-10 06:54:21,600 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,601 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,601 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,639 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43977', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,661 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43977
2024-02-10 06:54:21,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50366
2024-02-10 06:54:21,662 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,663 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,663 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,669 - distributed.worker - INFO - Starting Worker plugin PreImport-a034d20d-2ed5-4548-ad3f-80a4ae609566
2024-02-10 06:54:21,670 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01c75917-5bd2-44b4-9351-e90479b22cef
2024-02-10 06:54:21,670 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,686 - distributed.worker - INFO - Starting Worker plugin PreImport-b0e1e47a-b26e-463d-b69d-36dca1c90145
2024-02-10 06:54:21,687 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,689 - distributed.worker - INFO - Starting Worker plugin PreImport-3a05f40f-53f4-4e88-82a9-0e9f6b1559de
2024-02-10 06:54:21,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9438c01-01f2-405d-9c25-0757de406a67
2024-02-10 06:54:21,690 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,694 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,694 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34585', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34585
2024-02-10 06:54:21,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50380
2024-02-10 06:54:21,707 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34935', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,707 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34935
2024-02-10 06:54:21,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50394
2024-02-10 06:54:21,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,709 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,709 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32795', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32795
2024-02-10 06:54:21,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50406
2024-02-10 06:54:21,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,713 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,714 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,719 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37199', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37199
2024-02-10 06:54:21,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50420
2024-02-10 06:54:21,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44495', status: init, memory: 0, processing: 0>
2024-02-10 06:54:21,721 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44495
2024-02-10 06:54:21,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50430
2024-02-10 06:54:21,721 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,722 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,722 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:21,723 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:21,723 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:21,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:21,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:25,532 - distributed.scheduler - INFO - Receive client connection: Client-342aa8b3-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:25,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50492
2024-02-10 06:54:25,544 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,545 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,545 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,545 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,545 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,546 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,546 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,576 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:54:25,587 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,587 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,587 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,587 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,588 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,588 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,588 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,588 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:25,592 - distributed.scheduler - INFO - Remove client Client-342aa8b3-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:25,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50492; closing.
2024-02-10 06:54:25,592 - distributed.scheduler - INFO - Remove client Client-342aa8b3-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:25,593 - distributed.scheduler - INFO - Close client connection: Client-342aa8b3-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:25,593 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45907'. Reason: nanny-close
2024-02-10 06:54:25,594 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,594 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38999'. Reason: nanny-close
2024-02-10 06:54:25,594 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34841'. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34935. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33263'. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43977. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,595 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36633'. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37199. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44813'. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43107'. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34585. Reason: nanny-close
2024-02-10 06:54:25,596 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36193. Reason: nanny-close
2024-02-10 06:54:25,597 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50394; closing.
2024-02-10 06:54:25,597 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,597 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,597 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32961'. Reason: nanny-close
2024-02-10 06:54:25,597 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.5972536')
2024-02-10 06:54:25,597 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,597 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32795. Reason: nanny-close
2024-02-10 06:54:25,597 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:25,598 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36477. Reason: nanny-close
2024-02-10 06:54:25,598 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50366; closing.
2024-02-10 06:54:25,598 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,598 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44495. Reason: nanny-close
2024-02-10 06:54:25,598 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,598 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,599 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,599 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,599 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50420; closing.
2024-02-10 06:54:25,599 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,599 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43977', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.599495')
2024-02-10 06:54:25,600 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,600 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,600 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,600 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,600 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.6009116')
2024-02-10 06:54:25,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50346; closing.
2024-02-10 06:54:25,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50380; closing.
2024-02-10 06:54:25,602 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,602 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:25,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.6023')
2024-02-10 06:54:25,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34585', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.602637')
2024-02-10 06:54:25,603 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50406; closing.
2024-02-10 06:54:25,603 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.603692')
2024-02-10 06:54:25,604 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50356; closing.
2024-02-10 06:54:25,604 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50430; closing.
2024-02-10 06:54:25,604 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,604 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:25,604 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.60461')
2024-02-10 06:54:25,605 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548065.6049879')
2024-02-10 06:54:25,605 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:54:27,210 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:54:27,210 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:54:27,211 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:54:27,212 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:54:27,213 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-10 06:54:29,399 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:29,403 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41063 instead
  warnings.warn(
2024-02-10 06:54:29,407 - distributed.scheduler - INFO - State start
2024-02-10 06:54:29,902 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:29,904 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:54:29,905 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41063/status
2024-02-10 06:54:29,906 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:54:30,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39843'
2024-02-10 06:54:31,906 - distributed.scheduler - INFO - Receive client connection: Client-3b8a6cd8-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:31,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37778
2024-02-10 06:54:32,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:32,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:32,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:32,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33321
2024-02-10 06:54:32,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33321
2024-02-10 06:54:32,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39229
2024-02-10 06:54:32,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:32,040 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:32,040 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:32,040 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:54:32,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ma2wl4ar
2024-02-10 06:54:32,041 - distributed.worker - INFO - Starting Worker plugin PreImport-fc97bcca-c329-4831-9884-74a975d3a28e
2024-02-10 06:54:32,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-959d5e97-9ad1-48d7-833f-5d07e1ec5243
2024-02-10 06:54:32,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6da5f607-a3ac-413b-921d-78541a3e4973
2024-02-10 06:54:32,382 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:32,473 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33321', status: init, memory: 0, processing: 0>
2024-02-10 06:54:32,475 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33321
2024-02-10 06:54:32,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37804
2024-02-10 06:54:32,476 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:32,477 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:32,477 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:32,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:32,541 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:54:32,546 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:32,548 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:32,551 - distributed.scheduler - INFO - Remove client Client-3b8a6cd8-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:32,551 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37778; closing.
2024-02-10 06:54:32,551 - distributed.scheduler - INFO - Remove client Client-3b8a6cd8-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:32,552 - distributed.scheduler - INFO - Close client connection: Client-3b8a6cd8-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:32,552 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39843'. Reason: nanny-close
2024-02-10 06:54:32,553 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:32,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33321. Reason: nanny-close
2024-02-10 06:54:32,555 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:32,555 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37804; closing.
2024-02-10 06:54:32,556 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548072.556234')
2024-02-10 06:54:32,556 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:54:32,557 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:33,117 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:54:33,118 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:54:33,118 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:54:33,120 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:54:33,120 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-10 06:54:35,503 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:35,507 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45407 instead
  warnings.warn(
2024-02-10 06:54:35,511 - distributed.scheduler - INFO - State start
2024-02-10 06:54:35,534 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:54:35,535 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:54:35,535 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45407/status
2024-02-10 06:54:35,535 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:54:35,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41037'
2024-02-10 06:54:35,868 - distributed.scheduler - INFO - Receive client connection: Client-3f23b221-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:35,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37864
2024-02-10 06:54:37,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:54:37,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:54:37,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:54:37,578 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33597
2024-02-10 06:54:37,578 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33597
2024-02-10 06:54:37,578 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43321
2024-02-10 06:54:37,578 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:54:37,578 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:37,578 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:54:37,578 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:54:37,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yfxcqjz_
2024-02-10 06:54:37,578 - distributed.worker - INFO - Starting Worker plugin PreImport-9b457fc0-6040-445f-96af-711213bad160
2024-02-10 06:54:37,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42fa27fc-6acf-4e3c-a091-56fc6e69a71b
2024-02-10 06:54:37,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b93cde7a-f48d-4026-bbbc-c00088e33870
2024-02-10 06:54:38,166 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:38,223 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33597', status: init, memory: 0, processing: 0>
2024-02-10 06:54:38,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33597
2024-02-10 06:54:38,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37876
2024-02-10 06:54:38,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:54:38,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:54:38,227 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:54:38,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:54:38,233 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-10 06:54:38,238 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:54:38,241 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:38,243 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:54:38,245 - distributed.scheduler - INFO - Remove client Client-3f23b221-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:38,245 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37864; closing.
2024-02-10 06:54:38,245 - distributed.scheduler - INFO - Remove client Client-3f23b221-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:38,246 - distributed.scheduler - INFO - Close client connection: Client-3f23b221-c7e1-11ee-b928-d8c49764f6bb
2024-02-10 06:54:38,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41037'. Reason: nanny-close
2024-02-10 06:54:38,255 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:54:38,256 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33597. Reason: nanny-close
2024-02-10 06:54:38,257 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37876; closing.
2024-02-10 06:54:38,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:54:38,258 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33597', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707548078.2579813')
2024-02-10 06:54:38,258 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:54:38,259 - distributed.nanny - INFO - Worker closed
2024-02-10 06:54:38,912 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:54:38,912 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:54:38,913 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:54:38,914 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:54:38,915 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46101 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39761 instead
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
