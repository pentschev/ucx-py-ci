============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-17 05:36:43,494 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:36:43,498 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35353 instead
  warnings.warn(
2023-10-17 05:36:43,502 - distributed.scheduler - INFO - State start
2023-10-17 05:36:43,526 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:36:43,527 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-17 05:36:43,527 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35353/status
2023-10-17 05:36:43,528 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:36:43,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34063'
2023-10-17 05:36:43,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36003'
2023-10-17 05:36:43,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46409'
2023-10-17 05:36:43,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41673'
2023-10-17 05:36:45,522 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:45,522 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:45,527 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:45,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:45,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:45,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:45,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:45,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:45,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:45,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:45,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:45,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-17 05:36:45,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34935
2023-10-17 05:36:45,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34935
2023-10-17 05:36:45,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39009
2023-10-17 05:36:45,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-17 05:36:45,567 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:45,567 - distributed.worker - INFO -               Threads:                          4
2023-10-17 05:36:45,567 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-17 05:36:45,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nxw2gwan
2023-10-17 05:36:45,567 - distributed.worker - INFO - Starting Worker plugin PreImport-ee3e11d5-e8c1-4f21-86c1-6465014837e0
2023-10-17 05:36:45,567 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0af05c0-22b2-457d-b015-53778196a0cf
2023-10-17 05:36:45,568 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2feede36-f287-4559-ac6a-e6c96a44b76b
2023-10-17 05:36:45,568 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:46,264 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34935', status: init, memory: 0, processing: 0>
2023-10-17 05:36:46,277 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34935
2023-10-17 05:36:46,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37866
2023-10-17 05:36:46,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:46,279 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-17 05:36:46,279 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:46,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-17 05:36:46,653 - distributed.scheduler - INFO - Receive client connection: Client-268d4785-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:46,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37872
2023-10-17 05:36:47,300 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42453
2023-10-17 05:36:47,302 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42453
2023-10-17 05:36:47,302 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34559
2023-10-17 05:36:47,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-17 05:36:47,302 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,302 - distributed.worker - INFO -               Threads:                          4
2023-10-17 05:36:47,303 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-17 05:36:47,303 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-dc6f7qal
2023-10-17 05:36:47,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e6567da-86ee-40a8-ba5b-13764bbce96a
2023-10-17 05:36:47,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b07a28a3-1092-4539-a3f5-17f1e6e9a6e4
2023-10-17 05:36:47,304 - distributed.worker - INFO - Starting Worker plugin PreImport-bc620df8-cb09-4b9b-9778-e4f35092e53a
2023-10-17 05:36:47,305 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,339 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42453', status: init, memory: 0, processing: 0>
2023-10-17 05:36:47,339 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42453
2023-10-17 05:36:47,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37880
2023-10-17 05:36:47,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:47,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-17 05:36:47,342 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-17 05:36:47,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44067
2023-10-17 05:36:47,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39601
2023-10-17 05:36:47,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44067
2023-10-17 05:36:47,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39601
2023-10-17 05:36:47,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36155
2023-10-17 05:36:47,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40729
2023-10-17 05:36:47,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-17 05:36:47,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-17 05:36:47,407 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,407 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,407 - distributed.worker - INFO -               Threads:                          4
2023-10-17 05:36:47,407 - distributed.worker - INFO -               Threads:                          4
2023-10-17 05:36:47,407 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-17 05:36:47,407 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-17 05:36:47,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-emc5w2wm
2023-10-17 05:36:47,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8c6736he
2023-10-17 05:36:47,408 - distributed.worker - INFO - Starting Worker plugin PreImport-25144c5a-3fe4-46de-a8fd-1d252e43dc4d
2023-10-17 05:36:47,408 - distributed.worker - INFO - Starting Worker plugin PreImport-771faffb-5403-4d75-afd1-d74b235acfee
2023-10-17 05:36:47,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf778fe2-55e7-4660-bc0b-72c0a93fdb8a
2023-10-17 05:36:47,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e87e4cd0-5625-49f4-b014-d259ad954f6a
2023-10-17 05:36:47,408 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2518172-adf5-44c4-a46b-b5d982c37e03
2023-10-17 05:36:47,408 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ef9a1b1-4451-4199-8e7c-4a39bfbd5087
2023-10-17 05:36:47,408 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,408 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,437 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44067', status: init, memory: 0, processing: 0>
2023-10-17 05:36:47,438 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44067
2023-10-17 05:36:47,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37906
2023-10-17 05:36:47,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:47,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-17 05:36:47,440 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-17 05:36:47,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39601', status: init, memory: 0, processing: 0>
2023-10-17 05:36:47,444 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39601
2023-10-17 05:36:47,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37892
2023-10-17 05:36:47,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:47,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-17 05:36:47,446 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:47,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-17 05:36:47,462 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-17 05:36:47,463 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-17 05:36:47,463 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-17 05:36:47,909 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-17 05:36:47,914 - distributed.scheduler - INFO - Remove client Client-268d4785-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:47,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37872; closing.
2023-10-17 05:36:47,915 - distributed.scheduler - INFO - Remove client Client-268d4785-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:47,915 - distributed.scheduler - INFO - Close client connection: Client-268d4785-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:47,916 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34063'. Reason: nanny-close
2023-10-17 05:36:47,917 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:47,917 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36003'. Reason: nanny-close
2023-10-17 05:36:47,918 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:47,918 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46409'. Reason: nanny-close
2023-10-17 05:36:47,918 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42453. Reason: nanny-close
2023-10-17 05:36:47,918 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:47,918 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41673'. Reason: nanny-close
2023-10-17 05:36:47,918 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39601. Reason: nanny-close
2023-10-17 05:36:47,919 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:47,919 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44067. Reason: nanny-close
2023-10-17 05:36:47,919 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34935. Reason: nanny-close
2023-10-17 05:36:47,920 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37880; closing.
2023-10-17 05:36:47,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-17 05:36:47,920 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-17 05:36:47,920 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521007.9206932')
2023-10-17 05:36:47,921 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-17 05:36:47,921 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-17 05:36:47,921 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:47,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37892; closing.
2023-10-17 05:36:47,922 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:47,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37906; closing.
2023-10-17 05:36:47,922 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:47,923 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37866; closing.
2023-10-17 05:36:47,923 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:47,923 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521007.9232721')
2023-10-17 05:36:47,924 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521007.9240348')
2023-10-17 05:36:47,924 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521007.9243784')
2023-10-17 05:36:47,924 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:36:49,233 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:36:49,233 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:36:49,234 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:36:49,235 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-17 05:36:49,235 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-17 05:36:51,382 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:36:51,386 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44333 instead
  warnings.warn(
2023-10-17 05:36:51,390 - distributed.scheduler - INFO - State start
2023-10-17 05:36:51,446 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:36:51,447 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:36:51,448 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44333/status
2023-10-17 05:36:51,448 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:36:51,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35899'
2023-10-17 05:36:51,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39861'
2023-10-17 05:36:51,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44355'
2023-10-17 05:36:51,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36425'
2023-10-17 05:36:51,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37641'
2023-10-17 05:36:51,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46119'
2023-10-17 05:36:51,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35929'
2023-10-17 05:36:51,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44069'
2023-10-17 05:36:52,507 - distributed.scheduler - INFO - Receive client connection: Client-2b518f35-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:52,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52970
2023-10-17 05:36:53,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,653 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:36:53,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:36:53,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,660 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,661 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:53,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:36:56,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43547
2023-10-17 05:36:56,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43547
2023-10-17 05:36:56,735 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36855
2023-10-17 05:36:56,735 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,735 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,735 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,735 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zwv8fvfv
2023-10-17 05:36:56,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b81af6b0-1711-4277-bb21-8cc6b92d2532
2023-10-17 05:36:56,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39749
2023-10-17 05:36:56,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39749
2023-10-17 05:36:56,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33777
2023-10-17 05:36:56,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,745 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,745 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qm_ywax8
2023-10-17 05:36:56,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a9fd83b-7439-445e-a68a-c692b241c46e
2023-10-17 05:36:56,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40885
2023-10-17 05:36:56,749 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40885
2023-10-17 05:36:56,749 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42163
2023-10-17 05:36:56,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44881
2023-10-17 05:36:56,749 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,749 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44881
2023-10-17 05:36:56,749 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,749 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34615
2023-10-17 05:36:56,749 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,749 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,749 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,749 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,749 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-be6wid2y
2023-10-17 05:36:56,749 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fwrjpthl
2023-10-17 05:36:56,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea4738c7-99ec-48ce-b74d-6057123bd0c6
2023-10-17 05:36:56,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54aea0cc-d23e-4e61-a0f8-8593a8927706
2023-10-17 05:36:56,750 - distributed.worker - INFO - Starting Worker plugin PreImport-a3cebf2b-98a5-45b3-8c63-1501ea21d00b
2023-10-17 05:36:56,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09b82218-1afe-4fc4-a18b-79ce3d3b5e4b
2023-10-17 05:36:56,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38961
2023-10-17 05:36:56,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38961
2023-10-17 05:36:56,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40223
2023-10-17 05:36:56,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,752 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,752 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tci6yl3p
2023-10-17 05:36:56,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-75458b89-5ee8-42c9-b7f7-f16e3a93cb89
2023-10-17 05:36:56,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39043
2023-10-17 05:36:56,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39043
2023-10-17 05:36:56,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38327
2023-10-17 05:36:56,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,754 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,754 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-thp_wxps
2023-10-17 05:36:56,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6b0042f-1411-4978-8efc-33737e1d950b
2023-10-17 05:36:56,754 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38707
2023-10-17 05:36:56,755 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38707
2023-10-17 05:36:56,756 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34321
2023-10-17 05:36:56,756 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,756 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,756 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,756 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,756 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wo497vb3
2023-10-17 05:36:56,757 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f78147a1-7dfb-480e-ba8e-4239cdeb5a1e
2023-10-17 05:36:56,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39157
2023-10-17 05:36:56,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39157
2023-10-17 05:36:56,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40751
2023-10-17 05:36:56,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,758 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,758 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:36:56,758 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:36:56,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yws7p4_h
2023-10-17 05:36:56,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a408994c-56ef-41eb-bd34-c2aad2c36443
2023-10-17 05:36:56,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be258487-2059-4409-888c-4b545310e0bc
2023-10-17 05:36:56,877 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a89923f-a42f-4cdb-a7da-82d1bef3b7d1
2023-10-17 05:36:56,878 - distributed.worker - INFO - Starting Worker plugin PreImport-0575e87f-256d-49bc-9ef1-5a5eceaa9cea
2023-10-17 05:36:56,878 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-893ed250-6d3d-4364-81d5-eff489f7ed31
2023-10-17 05:36:56,887 - distributed.worker - INFO - Starting Worker plugin PreImport-bde7d712-be18-4762-a1b9-42b4dbd6d797
2023-10-17 05:36:56,887 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-baa5f14b-5409-440c-9e83-10ebce4f1f05
2023-10-17 05:36:56,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39d978f2-3a33-45e1-9e29-a465c2c64d6e
2023-10-17 05:36:56,903 - distributed.worker - INFO - Starting Worker plugin PreImport-1d44388f-ffd5-4121-badc-018d9d62fdd1
2023-10-17 05:36:56,904 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be60214a-6176-4c50-aa50-4b1aa3a52046
2023-10-17 05:36:56,904 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c215961-e9ea-415e-87bc-f2f58f43d450
2023-10-17 05:36:56,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,904 - distributed.worker - INFO - Starting Worker plugin PreImport-ceb229c3-3f64-4a91-8534-7b80ffa28b48
2023-10-17 05:36:56,904 - distributed.worker - INFO - Starting Worker plugin PreImport-bdc8b4d0-06f9-40cc-8023-ca9d3faa1c11
2023-10-17 05:36:56,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,906 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38707', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38707
2023-10-17 05:36:56,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52990
2023-10-17 05:36:56,907 - distributed.worker - INFO - Starting Worker plugin PreImport-99389700-bce3-414c-9e58-86529b14a128
2023-10-17 05:36:56,907 - distributed.worker - INFO - Starting Worker plugin PreImport-851bdbf9-04b6-404c-bfd0-94fdbc9ca559
2023-10-17 05:36:56,908 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,908 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,910 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,912 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43547', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,912 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43547
2023-10-17 05:36:56,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52998
2023-10-17 05:36:56,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,914 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,934 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38961', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,934 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38961
2023-10-17 05:36:56,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53018
2023-10-17 05:36:56,935 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44881', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,936 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44881
2023-10-17 05:36:56,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53008
2023-10-17 05:36:56,937 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,937 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,937 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39157', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,937 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39157
2023-10-17 05:36:56,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53014
2023-10-17 05:36:56,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,938 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40885', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,939 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40885
2023-10-17 05:36:56,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53028
2023-10-17 05:36:56,939 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,939 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39749', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,940 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39749
2023-10-17 05:36:56,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53040
2023-10-17 05:36:56,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,941 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,941 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,943 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,943 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,943 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39043', status: init, memory: 0, processing: 0>
2023-10-17 05:36:56,943 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39043
2023-10-17 05:36:56,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53034
2023-10-17 05:36:56,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:56,945 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:36:56,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:36:56,946 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:36:56,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,011 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:36:57,015 - distributed.scheduler - INFO - Remove client Client-2b518f35-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:57,015 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52970; closing.
2023-10-17 05:36:57,016 - distributed.scheduler - INFO - Remove client Client-2b518f35-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:57,016 - distributed.scheduler - INFO - Close client connection: Client-2b518f35-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:36:57,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35899'. Reason: nanny-close
2023-10-17 05:36:57,018 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,018 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39861'. Reason: nanny-close
2023-10-17 05:36:57,019 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,019 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44881. Reason: nanny-close
2023-10-17 05:36:57,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44355'. Reason: nanny-close
2023-10-17 05:36:57,019 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,019 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38961. Reason: nanny-close
2023-10-17 05:36:57,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36425'. Reason: nanny-close
2023-10-17 05:36:57,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,020 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39043. Reason: nanny-close
2023-10-17 05:36:57,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37641'. Reason: nanny-close
2023-10-17 05:36:57,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53008; closing.
2023-10-17 05:36:57,021 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40885. Reason: nanny-close
2023-10-17 05:36:57,021 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0213323')
2023-10-17 05:36:57,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46119'. Reason: nanny-close
2023-10-17 05:36:57,021 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39157. Reason: nanny-close
2023-10-17 05:36:57,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35929'. Reason: nanny-close
2023-10-17 05:36:57,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43547. Reason: nanny-close
2023-10-17 05:36:57,022 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44069'. Reason: nanny-close
2023-10-17 05:36:57,022 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53018; closing.
2023-10-17 05:36:57,023 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:36:57,023 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,023 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0237')
2023-10-17 05:36:57,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38707. Reason: nanny-close
2023-10-17 05:36:57,024 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39749. Reason: nanny-close
2023-10-17 05:36:57,024 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53034; closing.
2023-10-17 05:36:57,024 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0244355')
2023-10-17 05:36:57,024 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53028; closing.
2023-10-17 05:36:57,025 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,025 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53014; closing.
2023-10-17 05:36:57,025 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,025 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:36:57,026 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40885', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.026')
2023-10-17 05:36:57,026 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,026 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0263946')
2023-10-17 05:36:57,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52998; closing.
2023-10-17 05:36:57,027 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43547', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0275373')
2023-10-17 05:36:57,027 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,027 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52990; closing.
2023-10-17 05:36:57,028 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53040; closing.
2023-10-17 05:36:57,028 - distributed.nanny - INFO - Worker closed
2023-10-17 05:36:57,028 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0285618')
2023-10-17 05:36:57,028 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521017.0288794')
2023-10-17 05:36:57,029 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:36:58,535 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:36:58,535 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:36:58,535 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:36:58,536 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:36:58,537 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-17 05:37:00,600 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:00,604 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-17 05:37:00,607 - distributed.scheduler - INFO - State start
2023-10-17 05:37:00,627 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:00,628 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:00,628 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-17 05:37:00,628 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:37:00,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44813'
2023-10-17 05:37:00,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33209'
2023-10-17 05:37:00,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33327'
2023-10-17 05:37:00,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33031'
2023-10-17 05:37:00,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38681'
2023-10-17 05:37:00,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39695'
2023-10-17 05:37:00,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42237'
2023-10-17 05:37:00,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41285'
2023-10-17 05:37:02,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,595 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,663 - distributed.scheduler - INFO - Receive client connection: Client-30d405de-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:02,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36378
2023-10-17 05:37:02,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,718 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,718 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,736 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,736 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:02,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:02,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:02,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:05,052 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41195
2023-10-17 05:37:05,054 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41195
2023-10-17 05:37:05,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39631
2023-10-17 05:37:05,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,055 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,055 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i7037m5g
2023-10-17 05:37:05,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c770fc80-eb93-486b-a2f0-15396d25ca69
2023-10-17 05:37:05,057 - distributed.worker - INFO - Starting Worker plugin PreImport-cd3c5494-fb33-4ca4-8a87-7170893de202
2023-10-17 05:37:05,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20888705-5690-4e76-9f30-8d2d7befb500
2023-10-17 05:37:05,067 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,094 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41195', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,095 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41195
2023-10-17 05:37:05,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36408
2023-10-17 05:37:05,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,098 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,098 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,307 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46299
2023-10-17 05:37:05,308 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46299
2023-10-17 05:37:05,308 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38881
2023-10-17 05:37:05,308 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,308 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,308 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,308 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5s5uifpg
2023-10-17 05:37:05,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1000a776-723f-4097-9d3e-e2b5388dc91f
2023-10-17 05:37:05,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-044ac91c-ae0a-4c94-876f-98ef1b8c5614
2023-10-17 05:37:05,317 - distributed.worker - INFO - Starting Worker plugin PreImport-96b64e15-cc3b-470a-8e7b-bf31f4bb5d45
2023-10-17 05:37:05,318 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,350 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46299', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46299
2023-10-17 05:37:05,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36424
2023-10-17 05:37:05,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,354 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,355 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42575
2023-10-17 05:37:05,580 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42575
2023-10-17 05:37:05,580 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35665
2023-10-17 05:37:05,581 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,581 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,581 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,581 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,581 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f5m8g9y_
2023-10-17 05:37:05,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0d0e330-a8f9-4cb5-9ba2-402e570dc037
2023-10-17 05:37:05,588 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36791
2023-10-17 05:37:05,589 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36791
2023-10-17 05:37:05,589 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46855
2023-10-17 05:37:05,589 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,589 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,589 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,589 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,589 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ct9ut94e
2023-10-17 05:37:05,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bafa0c81-9b18-42be-a824-ab2a068a6ec1
2023-10-17 05:37:05,590 - distributed.worker - INFO - Starting Worker plugin RMMSetup-944ec8f9-9e03-4821-804b-bceaa2c77541
2023-10-17 05:37:05,596 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40609
2023-10-17 05:37:05,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40609
2023-10-17 05:37:05,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35247
2023-10-17 05:37:05,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,597 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,597 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,597 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dzem58ns
2023-10-17 05:37:05,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abd6b21b-dedd-4b5e-9554-cbaaa3e1f673
2023-10-17 05:37:05,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56d12b70-d249-4d59-b6cb-5a3d159176ef
2023-10-17 05:37:05,599 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46529
2023-10-17 05:37:05,600 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46529
2023-10-17 05:37:05,600 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41115
2023-10-17 05:37:05,600 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,600 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,600 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,600 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,600 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_slv9iyi
2023-10-17 05:37:05,601 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0eb2648f-156e-4250-895e-c90fd7d26e1a
2023-10-17 05:37:05,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35871
2023-10-17 05:37:05,603 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35871
2023-10-17 05:37:05,603 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33175
2023-10-17 05:37:05,603 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,603 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,603 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,603 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rmqiuysu
2023-10-17 05:37:05,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9b308e7-aa66-4db2-b68f-57bfafeb4b33
2023-10-17 05:37:05,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-766611f0-ec05-48a2-bb14-1b27f2eddf50
2023-10-17 05:37:05,609 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36691
2023-10-17 05:37:05,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36691
2023-10-17 05:37:05,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35579
2023-10-17 05:37:05,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,610 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,610 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:05,611 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:05,611 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tybr93_p
2023-10-17 05:37:05,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9edb1fe4-a425-4314-8006-f1d665b6bac6
2023-10-17 05:37:05,625 - distributed.worker - INFO - Starting Worker plugin PreImport-5e5681a0-eb33-4b38-9465-9fa76304eea9
2023-10-17 05:37:05,626 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,630 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02a9a018-716e-4433-9d92-22dca037fe0f
2023-10-17 05:37:05,630 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8578e4f9-49ef-4ecf-8812-f65cd12bec2f
2023-10-17 05:37:05,630 - distributed.worker - INFO - Starting Worker plugin PreImport-ae7a16c4-5f5a-4c3c-be28-bba35cdb00c1
2023-10-17 05:37:05,630 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,631 - distributed.worker - INFO - Starting Worker plugin PreImport-0853f67d-af58-4726-bfc1-a803afe75426
2023-10-17 05:37:05,632 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,632 - distributed.worker - INFO - Starting Worker plugin PreImport-2e9ef8f4-4e3e-4f5c-9f7d-9e79e4f05b6f
2023-10-17 05:37:05,633 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,633 - distributed.worker - INFO - Starting Worker plugin PreImport-24cd1d03-538b-45ce-9a70-b8fe9af34fe9
2023-10-17 05:37:05,633 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,633 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27af7fdc-fda2-47b0-b693-94afe62a451a
2023-10-17 05:37:05,633 - distributed.worker - INFO - Starting Worker plugin PreImport-f701dff5-b441-4603-9f5e-94f6c3f7e666
2023-10-17 05:37:05,634 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46529', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46529
2023-10-17 05:37:05,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36432
2023-10-17 05:37:05,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35871', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35871
2023-10-17 05:37:05,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36434
2023-10-17 05:37:05,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,657 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,659 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,659 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40609', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,662 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40609
2023-10-17 05:37:05,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36440
2023-10-17 05:37:05,663 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36691', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36691
2023-10-17 05:37:05,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36460
2023-10-17 05:37:05,663 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,663 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,664 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36791', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,664 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36791
2023-10-17 05:37:05,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36428
2023-10-17 05:37:05,665 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,665 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,666 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,668 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,668 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,669 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42575', status: init, memory: 0, processing: 0>
2023-10-17 05:37:05,670 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42575
2023-10-17 05:37:05,670 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36446
2023-10-17 05:37:05,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:05,672 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:05,672 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:05,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:05,695 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,695 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,696 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,697 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:05,701 - distributed.scheduler - INFO - Remove client Client-30d405de-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:05,701 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36378; closing.
2023-10-17 05:37:05,701 - distributed.scheduler - INFO - Remove client Client-30d405de-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:05,702 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44813'. Reason: nanny-close
2023-10-17 05:37:05,703 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,703 - distributed.scheduler - INFO - Close client connection: Client-30d405de-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:05,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33209'. Reason: nanny-close
2023-10-17 05:37:05,704 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,704 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41195. Reason: nanny-close
2023-10-17 05:37:05,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33327'. Reason: nanny-close
2023-10-17 05:37:05,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,705 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46299. Reason: nanny-close
2023-10-17 05:37:05,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33031'. Reason: nanny-close
2023-10-17 05:37:05,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,706 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36691. Reason: nanny-close
2023-10-17 05:37:05,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38681'. Reason: nanny-close
2023-10-17 05:37:05,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,706 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46529. Reason: nanny-close
2023-10-17 05:37:05,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39695'. Reason: nanny-close
2023-10-17 05:37:05,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42575. Reason: nanny-close
2023-10-17 05:37:05,707 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,707 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36408; closing.
2023-10-17 05:37:05,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42237'. Reason: nanny-close
2023-10-17 05:37:05,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.7078896')
2023-10-17 05:37:05,708 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36791. Reason: nanny-close
2023-10-17 05:37:05,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41285'. Reason: nanny-close
2023-10-17 05:37:05,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:05,708 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35871. Reason: nanny-close
2023-10-17 05:37:05,708 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36432; closing.
2023-10-17 05:37:05,709 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40609. Reason: nanny-close
2023-10-17 05:37:05,709 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,709 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,709 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.7099233')
2023-10-17 05:37:05,710 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,710 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,710 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36460; closing.
2023-10-17 05:37:05,710 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36424; closing.
2023-10-17 05:37:05,710 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,711 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,711 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,711 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:05,712 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,711 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36432>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36432>: Stream is closed
2023-10-17 05:37:05,712 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.7129803')
2023-10-17 05:37:05,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46299', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.71335')
2023-10-17 05:37:05,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36446; closing.
2023-10-17 05:37:05,714 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:05,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42575', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.714547')
2023-10-17 05:37:05,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36434; closing.
2023-10-17 05:37:05,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36428; closing.
2023-10-17 05:37:05,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.715834')
2023-10-17 05:37:05,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.7162385')
2023-10-17 05:37:05,716 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36440; closing.
2023-10-17 05:37:05,717 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521025.717132')
2023-10-17 05:37:05,717 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:37:05,717 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36440>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-17 05:37:07,220 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:37:07,221 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:37:07,221 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:37:07,222 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:37:07,223 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-17 05:37:09,154 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:09,159 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-17 05:37:09,162 - distributed.scheduler - INFO - State start
2023-10-17 05:37:09,182 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:09,183 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:09,184 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-17 05:37:09,184 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:37:09,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37223'
2023-10-17 05:37:09,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45155'
2023-10-17 05:37:09,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44665'
2023-10-17 05:37:09,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37917'
2023-10-17 05:37:09,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33441'
2023-10-17 05:37:09,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42093'
2023-10-17 05:37:09,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34939'
2023-10-17 05:37:09,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42233'
2023-10-17 05:37:11,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,358 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,363 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:11,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:11,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:11,575 - distributed.scheduler - INFO - Receive client connection: Client-35f892c4-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:11,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37584
2023-10-17 05:37:13,895 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36439
2023-10-17 05:37:13,895 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36439
2023-10-17 05:37:13,895 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38285
2023-10-17 05:37:13,895 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:13,895 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:13,896 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:13,896 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:13,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l6c_zpq6
2023-10-17 05:37:13,896 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31151b7a-917c-47d2-b398-fabd5ef8edca
2023-10-17 05:37:14,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fd59a0e-8595-4c44-8782-bd6cdfe9f162
2023-10-17 05:37:14,063 - distributed.worker - INFO - Starting Worker plugin PreImport-6b80be38-0ad1-436a-9a9c-8c8ee5cb8043
2023-10-17 05:37:14,063 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,090 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36439', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36439
2023-10-17 05:37:14,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37600
2023-10-17 05:37:14,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,093 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,093 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,268 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38581
2023-10-17 05:37:14,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38581
2023-10-17 05:37:14,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46735
2023-10-17 05:37:14,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,269 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,269 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,270 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aietlpyv
2023-10-17 05:37:14,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-253a028d-3dea-4ede-89a2-dbc25921fa9e
2023-10-17 05:37:14,295 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46785
2023-10-17 05:37:14,296 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46785
2023-10-17 05:37:14,296 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44235
2023-10-17 05:37:14,296 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,296 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,296 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,296 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,296 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sodk63kb
2023-10-17 05:37:14,297 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5180d0bc-5b47-4245-a350-53d13a49496e
2023-10-17 05:37:14,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37165
2023-10-17 05:37:14,302 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37165
2023-10-17 05:37:14,302 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42445
2023-10-17 05:37:14,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,302 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,302 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4b1x684r
2023-10-17 05:37:14,303 - distributed.worker - INFO - Starting Worker plugin PreImport-f99794b4-5319-44c1-acb9-88392022c957
2023-10-17 05:37:14,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfa8f6c3-fcb8-420f-9d5e-91bfd025f7c5
2023-10-17 05:37:14,305 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46075
2023-10-17 05:37:14,307 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46075
2023-10-17 05:37:14,307 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42607
2023-10-17 05:37:14,307 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,307 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,307 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,307 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-my_nar57
2023-10-17 05:37:14,308 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddf49807-8c86-42ac-ae87-624c800ca0be
2023-10-17 05:37:14,317 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43043
2023-10-17 05:37:14,318 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43043
2023-10-17 05:37:14,318 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46293
2023-10-17 05:37:14,318 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,318 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,318 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,318 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_47a9jmw
2023-10-17 05:37:14,318 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40459
2023-10-17 05:37:14,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73725ab3-7361-46ea-bc0e-a9fe0cea7507
2023-10-17 05:37:14,319 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40459
2023-10-17 05:37:14,319 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45399
2023-10-17 05:37:14,319 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,319 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,319 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,319 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a27739a2-40d6-42f9-aa1e-fabf86921095
2023-10-17 05:37:14,319 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,319 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cemv_p4s
2023-10-17 05:37:14,320 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31c2c26a-1791-4464-a15a-4b1647523090
2023-10-17 05:37:14,359 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33999
2023-10-17 05:37:14,359 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33999
2023-10-17 05:37:14,360 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38799
2023-10-17 05:37:14,360 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,360 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,360 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:14,360 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:14,360 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6pf7v2zn
2023-10-17 05:37:14,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea82ca2d-a7d3-4f6a-9b04-0dea535f66b0
2023-10-17 05:37:14,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26d68160-1cf7-443f-baae-1f788860f1fb
2023-10-17 05:37:14,494 - distributed.worker - INFO - Starting Worker plugin PreImport-778a295a-ab8a-411b-ac60-42dcda0fdb78
2023-10-17 05:37:14,495 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,528 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38581', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,529 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38581
2023-10-17 05:37:14,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37612
2023-10-17 05:37:14,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,532 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,532 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03673d68-da6d-4f3a-80fe-bd4db42ea253
2023-10-17 05:37:14,540 - distributed.worker - INFO - Starting Worker plugin PreImport-629a749b-4d0c-4221-8034-883bf12f189c
2023-10-17 05:37:14,540 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e33e01bb-b9a5-4be9-8d1a-d083a33fb0b6
2023-10-17 05:37:14,552 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,556 - distributed.worker - INFO - Starting Worker plugin PreImport-d0fe21d3-92d8-41c0-a486-371293e9265e
2023-10-17 05:37:14,557 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,557 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a199467-5c8a-46a2-b6e8-c86f50884947
2023-10-17 05:37:14,558 - distributed.worker - INFO - Starting Worker plugin PreImport-bac30675-9a2d-4ab7-b753-b9a9c6e91557
2023-10-17 05:37:14,558 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82c152ce-3a1d-426b-805e-edffecea99e0
2023-10-17 05:37:14,561 - distributed.worker - INFO - Starting Worker plugin PreImport-7fd34c26-feda-4165-b76c-fd24c45f85af
2023-10-17 05:37:14,561 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46785', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,565 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46785
2023-10-17 05:37:14,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37614
2023-10-17 05:37:14,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,567 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82a351ac-29a3-4075-babf-c85a00c56962
2023-10-17 05:37:14,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,567 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,568 - distributed.worker - INFO - Starting Worker plugin PreImport-bff4caa0-5585-4f63-a075-36be17d0866b
2023-10-17 05:37:14,569 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37165', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37165
2023-10-17 05:37:14,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37622
2023-10-17 05:37:14,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40459', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,585 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40459
2023-10-17 05:37:14,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37638
2023-10-17 05:37:14,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46075', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,586 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46075
2023-10-17 05:37:14,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37656
2023-10-17 05:37:14,587 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,587 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,589 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,594 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43043', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,595 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43043
2023-10-17 05:37:14,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37644
2023-10-17 05:37:14,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,598 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,601 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33999', status: init, memory: 0, processing: 0>
2023-10-17 05:37:14,601 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33999
2023-10-17 05:37:14,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37658
2023-10-17 05:37:14,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:14,604 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:14,604 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:14,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:14,679 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,679 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,679 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,680 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,680 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,680 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,680 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,680 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:14,691 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,691 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,691 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,691 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,691 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,692 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,692 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,692 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:14,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:14,699 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:14,701 - distributed.scheduler - INFO - Remove client Client-35f892c4-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:14,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37584; closing.
2023-10-17 05:37:14,702 - distributed.scheduler - INFO - Remove client Client-35f892c4-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:14,702 - distributed.scheduler - INFO - Close client connection: Client-35f892c4-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:14,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37223'. Reason: nanny-close
2023-10-17 05:37:14,704 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45155'. Reason: nanny-close
2023-10-17 05:37:14,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44665'. Reason: nanny-close
2023-10-17 05:37:14,705 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37165. Reason: nanny-close
2023-10-17 05:37:14,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,706 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38581. Reason: nanny-close
2023-10-17 05:37:14,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37917'. Reason: nanny-close
2023-10-17 05:37:14,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,706 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36439. Reason: nanny-close
2023-10-17 05:37:14,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33441'. Reason: nanny-close
2023-10-17 05:37:14,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46785. Reason: nanny-close
2023-10-17 05:37:14,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42093'. Reason: nanny-close
2023-10-17 05:37:14,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,707 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43043. Reason: nanny-close
2023-10-17 05:37:14,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34939'. Reason: nanny-close
2023-10-17 05:37:14,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,708 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37600; closing.
2023-10-17 05:37:14,708 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33999. Reason: nanny-close
2023-10-17 05:37:14,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,708 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36439', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.7088094')
2023-10-17 05:37:14,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42233'. Reason: nanny-close
2023-10-17 05:37:14,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:14,709 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40459. Reason: nanny-close
2023-10-17 05:37:14,709 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37612; closing.
2023-10-17 05:37:14,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46075. Reason: nanny-close
2023-10-17 05:37:14,709 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,709 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.7098613')
2023-10-17 05:37:14,710 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,710 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,710 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,711 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37622; closing.
2023-10-17 05:37:14,711 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,711 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,711 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37614; closing.
2023-10-17 05:37:14,711 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,712 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:14,712 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,712 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,711 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37612>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-17 05:37:14,713 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,713 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:14,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.713673')
2023-10-17 05:37:14,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.714057')
2023-10-17 05:37:14,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37644; closing.
2023-10-17 05:37:14,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.7151127')
2023-10-17 05:37:14,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37658; closing.
2023-10-17 05:37:14,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37638; closing.
2023-10-17 05:37:14,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.7162237')
2023-10-17 05:37:14,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40459', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.7165973')
2023-10-17 05:37:14,717 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37656; closing.
2023-10-17 05:37:14,717 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46075', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521034.717526')
2023-10-17 05:37:14,717 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:37:14,717 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37656>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-17 05:37:16,221 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:37:16,221 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:37:16,221 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:37:16,222 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:37:16,223 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-17 05:37:18,224 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:18,228 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39871 instead
  warnings.warn(
2023-10-17 05:37:18,232 - distributed.scheduler - INFO - State start
2023-10-17 05:37:18,253 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:18,254 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:18,255 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39871/status
2023-10-17 05:37:18,255 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:37:18,260 - distributed.scheduler - INFO - Receive client connection: Client-3b56abdc-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:18,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37746
2023-10-17 05:37:18,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41863'
2023-10-17 05:37:18,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43609'
2023-10-17 05:37:18,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42611'
2023-10-17 05:37:18,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34419'
2023-10-17 05:37:18,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46447'
2023-10-17 05:37:18,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43119'
2023-10-17 05:37:18,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39985'
2023-10-17 05:37:18,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36203'
2023-10-17 05:37:20,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,371 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,394 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,407 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:20,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:20,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:20,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:23,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43787
2023-10-17 05:37:23,659 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43787
2023-10-17 05:37:23,659 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37917
2023-10-17 05:37:23,659 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:23,660 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:23,660 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:23,660 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:23,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vdu14jbg
2023-10-17 05:37:23,660 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ddfaf7e-96bb-4a89-abf0-6c49d266c778
2023-10-17 05:37:23,835 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35337
2023-10-17 05:37:23,836 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35337
2023-10-17 05:37:23,836 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40719
2023-10-17 05:37:23,836 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:23,836 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:23,836 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:23,836 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:23,836 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sxqbw744
2023-10-17 05:37:23,837 - distributed.worker - INFO - Starting Worker plugin RMMSetup-593ee0a5-168c-470b-9457-a2ede6efbd01
2023-10-17 05:37:24,599 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45193
2023-10-17 05:37:24,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45193
2023-10-17 05:37:24,600 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45043
2023-10-17 05:37:24,600 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:24,600 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:24,600 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:24,600 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:24,600 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tknj7ssk
2023-10-17 05:37:24,601 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d3db0a3-3719-4c55-b54a-421e7548273e
2023-10-17 05:37:24,609 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46345
2023-10-17 05:37:24,609 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46345
2023-10-17 05:37:24,609 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34195
2023-10-17 05:37:24,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:24,610 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:24,610 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:24,610 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:24,610 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ef8izgvb
2023-10-17 05:37:24,610 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24e00ac7-e352-448c-839e-09f42f20343b
2023-10-17 05:37:24,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33291
2023-10-17 05:37:24,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33291
2023-10-17 05:37:24,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39103
2023-10-17 05:37:24,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:24,620 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:24,620 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:24,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:24,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ft8ovh65
2023-10-17 05:37:24,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9936e389-c0c3-453a-9054-4d3131cedfbf
2023-10-17 05:37:24,621 - distributed.worker - INFO - Starting Worker plugin PreImport-f5a7a919-ed51-4544-aece-ace309a59bd2
2023-10-17 05:37:24,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a8a1d03-ae42-4565-bea4-3e26080eafc0
2023-10-17 05:37:24,624 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40529
2023-10-17 05:37:24,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40529
2023-10-17 05:37:24,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34493
2023-10-17 05:37:24,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:24,625 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:24,625 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:24,625 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:24,625 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jvqir824
2023-10-17 05:37:24,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e32f07ad-42db-43bc-be0d-12b31eca3573
2023-10-17 05:37:24,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42165
2023-10-17 05:37:24,626 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42165
2023-10-17 05:37:24,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34125
2023-10-17 05:37:24,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:24,627 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:24,627 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:24,627 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:24,627 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tu4tbok8
2023-10-17 05:37:24,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd328e0a-a92f-4331-984a-2f03328cfec9
2023-10-17 05:37:24,628 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36081
2023-10-17 05:37:24,629 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36081
2023-10-17 05:37:24,629 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41067
2023-10-17 05:37:24,630 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:24,630 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:24,630 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:24,630 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:24,630 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iks05_0q
2023-10-17 05:37:24,631 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a95510d4-720b-4a11-a1a8-a2a4a31fe4ba
2023-10-17 05:37:24,631 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0e8a51d-2132-4238-b8e9-62dc947d1ba8
2023-10-17 05:37:25,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70472d27-a61c-460c-9363-c53ae318368f
2023-10-17 05:37:25,012 - distributed.worker - INFO - Starting Worker plugin PreImport-4c9f3348-e639-40ad-a67d-4fc99fe1cb74
2023-10-17 05:37:25,012 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8b963c5-7677-4e0a-9e5b-ab21ee0146bf
2023-10-17 05:37:25,041 - distributed.worker - INFO - Starting Worker plugin PreImport-b420cb32-5511-40ef-9e82-f312d4e12c2f
2023-10-17 05:37:25,042 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,048 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43787', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43787
2023-10-17 05:37:25,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48194
2023-10-17 05:37:25,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,053 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,083 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35337', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35337
2023-10-17 05:37:25,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48200
2023-10-17 05:37:25,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,087 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,087 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,177 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15a74b04-ed9d-4702-ac89-2a4049fc2f83
2023-10-17 05:37:25,178 - distributed.worker - INFO - Starting Worker plugin PreImport-3d43e770-b2de-467d-a639-1fbcd25a4e7d
2023-10-17 05:37:25,178 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45193', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45193
2023-10-17 05:37:25,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48216
2023-10-17 05:37:25,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,216 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,229 - distributed.worker - INFO - Starting Worker plugin PreImport-b9b6ea0b-8bae-4238-b2c9-cfed10274217
2023-10-17 05:37:25,230 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,244 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d70fe462-cbf1-40ba-85c9-cbe1a06bc218
2023-10-17 05:37:25,244 - distributed.worker - INFO - Starting Worker plugin PreImport-1c915274-eab2-4d9d-894b-c7d2ee1dd58a
2023-10-17 05:37:25,245 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36081', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,272 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36081
2023-10-17 05:37:25,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48218
2023-10-17 05:37:25,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40529', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40529
2023-10-17 05:37:25,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48226
2023-10-17 05:37:25,274 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,274 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,276 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,276 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,298 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4eb02f14-4288-4fab-adfa-dd78b76e17ee
2023-10-17 05:37:25,298 - distributed.worker - INFO - Starting Worker plugin PreImport-3601bb4d-d163-4d2b-94fd-6f650318b54d
2023-10-17 05:37:25,298 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,301 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1af78cab-5bb0-44e7-8e6e-2d59b59635f5
2023-10-17 05:37:25,301 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,303 - distributed.worker - INFO - Starting Worker plugin PreImport-8b65a8ca-c1e7-4868-a499-4bd6a9bd0454
2023-10-17 05:37:25,303 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33291', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33291
2023-10-17 05:37:25,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48250
2023-10-17 05:37:25,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,334 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,334 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42165', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,339 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42165
2023-10-17 05:37:25,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48238
2023-10-17 05:37:25,340 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46345', status: init, memory: 0, processing: 0>
2023-10-17 05:37:25,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,341 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46345
2023-10-17 05:37:25,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48264
2023-10-17 05:37:25,341 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,341 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:25,343 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:25,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,343 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:25,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:25,374 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,374 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,375 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,375 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,375 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,375 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,375 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,376 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,392 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,392 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,392 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,392 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,392 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,393 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,393 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,393 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:37:25,401 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,403 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:25,406 - distributed.scheduler - INFO - Remove client Client-3b56abdc-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:25,407 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37746; closing.
2023-10-17 05:37:25,407 - distributed.scheduler - INFO - Remove client Client-3b56abdc-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:25,408 - distributed.scheduler - INFO - Close client connection: Client-3b56abdc-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:25,408 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41863'. Reason: nanny-close
2023-10-17 05:37:25,408 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,409 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43609'. Reason: nanny-close
2023-10-17 05:37:25,409 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,410 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33291. Reason: nanny-close
2023-10-17 05:37:25,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42611'. Reason: nanny-close
2023-10-17 05:37:25,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,410 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40529. Reason: nanny-close
2023-10-17 05:37:25,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34419'. Reason: nanny-close
2023-10-17 05:37:25,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,411 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46447'. Reason: nanny-close
2023-10-17 05:37:25,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43787. Reason: nanny-close
2023-10-17 05:37:25,411 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35337. Reason: nanny-close
2023-10-17 05:37:25,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48250; closing.
2023-10-17 05:37:25,412 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4124265')
2023-10-17 05:37:25,412 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,413 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,414 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43119'. Reason: nanny-close
2023-10-17 05:37:25,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48226; closing.
2023-10-17 05:37:25,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36081. Reason: nanny-close
2023-10-17 05:37:25,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39985'. Reason: nanny-close
2023-10-17 05:37:25,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45193. Reason: nanny-close
2023-10-17 05:37:25,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4156766')
2023-10-17 05:37:25,415 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36203'. Reason: nanny-close
2023-10-17 05:37:25,416 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:25,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48194; closing.
2023-10-17 05:37:25,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48200; closing.
2023-10-17 05:37:25,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42165. Reason: nanny-close
2023-10-17 05:37:25,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4168174')
2023-10-17 05:37:25,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46345. Reason: nanny-close
2023-10-17 05:37:25,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.417146')
2023-10-17 05:37:25,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,418 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48218; closing.
2023-10-17 05:37:25,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48216; closing.
2023-10-17 05:37:25,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,418 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,419 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:25,419 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36081', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4190912')
2023-10-17 05:37:25,419 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4194865')
2023-10-17 05:37:25,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48238; closing.
2023-10-17 05:37:25,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4204266')
2023-10-17 05:37:25,420 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48264; closing.
2023-10-17 05:37:25,421 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:25,421 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521045.4213378')
2023-10-17 05:37:25,421 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:37:25,421 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48264>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-17 05:37:27,176 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:37:27,176 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:37:27,177 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:37:27,178 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:37:27,178 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-17 05:37:29,241 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:29,245 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-17 05:37:29,249 - distributed.scheduler - INFO - State start
2023-10-17 05:37:29,273 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:29,274 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:29,274 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-17 05:37:29,275 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:37:29,376 - distributed.scheduler - INFO - Receive client connection: Client-41de938d-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:29,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48388
2023-10-17 05:37:29,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33271'
2023-10-17 05:37:29,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43227'
2023-10-17 05:37:29,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39861'
2023-10-17 05:37:29,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38081'
2023-10-17 05:37:29,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38033'
2023-10-17 05:37:29,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33033'
2023-10-17 05:37:29,636 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35223'
2023-10-17 05:37:29,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35019'
2023-10-17 05:37:31,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,443 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,443 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,446 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,447 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:31,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:31,545 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:31,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:34,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35645
2023-10-17 05:37:34,112 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35645
2023-10-17 05:37:34,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40411
2023-10-17 05:37:34,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,113 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44851
2023-10-17 05:37:34,113 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44851
2023-10-17 05:37:34,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k9iqrv0v
2023-10-17 05:37:34,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40463
2023-10-17 05:37:34,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,113 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,113 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ilzubsj6
2023-10-17 05:37:34,113 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44126e50-e2f7-4cd2-9040-7228050da1a2
2023-10-17 05:37:34,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17b8a1be-8b44-47d9-bf95-32afd2160917
2023-10-17 05:37:34,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41031
2023-10-17 05:37:34,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41031
2023-10-17 05:37:34,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36449
2023-10-17 05:37:34,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33947
2023-10-17 05:37:34,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36449
2023-10-17 05:37:34,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36423
2023-10-17 05:37:34,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,220 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,220 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qs3frxr5
2023-10-17 05:37:34,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5g94b0m5
2023-10-17 05:37:34,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38751
2023-10-17 05:37:34,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38751
2023-10-17 05:37:34,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37551
2023-10-17 05:37:34,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f949f141-236c-43a9-8390-f9ebac1f92d6
2023-10-17 05:37:34,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,221 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e660a040-53e2-43eb-ad00-d307e48ac415
2023-10-17 05:37:34,221 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,221 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xeb1ulq0
2023-10-17 05:37:34,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8413d7e0-96cd-40f8-b7a1-82de931a1e36
2023-10-17 05:37:34,230 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44983
2023-10-17 05:37:34,232 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44983
2023-10-17 05:37:34,232 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43769
2023-10-17 05:37:34,232 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,232 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,232 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,232 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yvinq3k1
2023-10-17 05:37:34,233 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6029eb2e-b270-4524-a529-87da2324325f
2023-10-17 05:37:34,417 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33699
2023-10-17 05:37:34,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33699
2023-10-17 05:37:34,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37755
2023-10-17 05:37:34,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,418 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,418 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x2hb1s66
2023-10-17 05:37:34,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c969ee2-a144-428a-80ae-e67ee36b151e
2023-10-17 05:37:34,423 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5609b086-3b6e-4a40-919d-7267f221dda7
2023-10-17 05:37:34,428 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36259
2023-10-17 05:37:34,428 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36259
2023-10-17 05:37:34,429 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35893
2023-10-17 05:37:34,429 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,429 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,429 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:34,429 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:37:34,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-glx3zim3
2023-10-17 05:37:34,429 - distributed.worker - INFO - Starting Worker plugin PreImport-d6a63a01-b8ce-42c2-9bcf-fb97eae9624d
2023-10-17 05:37:34,430 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56b8dc25-b21b-402e-abd3-7b234e8f5a00
2023-10-17 05:37:34,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1c8566c-3604-4ac2-b77c-2a9408ea7115
2023-10-17 05:37:34,641 - distributed.worker - INFO - Starting Worker plugin PreImport-0c824b9f-ec0a-4a02-8f79-65acf5cfd2c5
2023-10-17 05:37:34,642 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,795 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d30ad3a-1c71-49ae-a58e-a4ea4f2910fb
2023-10-17 05:37:34,796 - distributed.worker - INFO - Starting Worker plugin PreImport-2a24d10c-0f75-4565-b281-6aa6a3ad5d06
2023-10-17 05:37:34,796 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,809 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35645', status: init, memory: 0, processing: 0>
2023-10-17 05:37:34,810 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35645
2023-10-17 05:37:34,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48996
2023-10-17 05:37:34,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:34,813 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,813 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b50c9452-98d7-4a44-895c-38a4f5e486fc
2023-10-17 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7172875a-b92d-4807-8df9-77edd9590850
2023-10-17 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin PreImport-db5e8ec7-773a-4fdb-9eaa-5b2b7c4cc46b
2023-10-17 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin PreImport-7fe75fff-c77e-408d-b9a9-2da30e30568e
2023-10-17 05:37:34,851 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,851 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,853 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c9f9db1-5616-4659-9d01-2e8a4567f9f5
2023-10-17 05:37:34,855 - distributed.worker - INFO - Starting Worker plugin PreImport-fd1de1fa-b864-4918-96ff-cc5f34d21280
2023-10-17 05:37:34,855 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,857 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0cbc4f9d-efeb-4f39-ab0f-57760e255158
2023-10-17 05:37:34,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44851', status: init, memory: 0, processing: 0>
2023-10-17 05:37:34,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44851
2023-10-17 05:37:34,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49010
2023-10-17 05:37:34,858 - distributed.worker - INFO - Starting Worker plugin PreImport-4a6a0796-5f1a-4968-aa78-a37bd72e4195
2023-10-17 05:37:34,859 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,860 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:34,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:34,862 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:34,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,013 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44983', status: init, memory: 0, processing: 0>
2023-10-17 05:37:35,014 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44983
2023-10-17 05:37:35,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49044
2023-10-17 05:37:35,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:35,016 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:35,016 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,022 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36449', status: init, memory: 0, processing: 0>
2023-10-17 05:37:35,022 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36449
2023-10-17 05:37:35,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49022
2023-10-17 05:37:35,023 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38751', status: init, memory: 0, processing: 0>
2023-10-17 05:37:35,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:35,023 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38751
2023-10-17 05:37:35,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49012
2023-10-17 05:37:35,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41031', status: init, memory: 0, processing: 0>
2023-10-17 05:37:35,025 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:35,025 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,025 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:35,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41031
2023-10-17 05:37:35,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49036
2023-10-17 05:37:35,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:35,026 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:35,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,028 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:35,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,038 - distributed.worker - INFO - Starting Worker plugin PreImport-d6392b37-9d7c-4a3e-bf8e-05b7d0d07897
2023-10-17 05:37:35,039 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b35e176e-12e3-4307-ba8c-b49fd030b18e
2023-10-17 05:37:35,043 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,199 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36259', status: init, memory: 0, processing: 0>
2023-10-17 05:37:35,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36259
2023-10-17 05:37:35,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49072
2023-10-17 05:37:35,201 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33699', status: init, memory: 0, processing: 0>
2023-10-17 05:37:35,201 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33699
2023-10-17 05:37:35,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:35,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49060
2023-10-17 05:37:35,202 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:35,203 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:35,204 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:35,204 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:35,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,216 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,217 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:37:35,221 - distributed.scheduler - INFO - Remove client Client-41de938d-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:35,221 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48388; closing.
2023-10-17 05:37:35,221 - distributed.scheduler - INFO - Remove client Client-41de938d-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:35,222 - distributed.scheduler - INFO - Close client connection: Client-41de938d-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:35,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33271'. Reason: nanny-close
2023-10-17 05:37:35,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43227'. Reason: nanny-close
2023-10-17 05:37:35,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,225 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36259. Reason: nanny-close
2023-10-17 05:37:35,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39861'. Reason: nanny-close
2023-10-17 05:37:35,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,226 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41031. Reason: nanny-close
2023-10-17 05:37:35,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38081'. Reason: nanny-close
2023-10-17 05:37:35,226 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,226 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38751. Reason: nanny-close
2023-10-17 05:37:35,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38033'. Reason: nanny-close
2023-10-17 05:37:35,227 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,227 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35645. Reason: nanny-close
2023-10-17 05:37:35,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33033'. Reason: nanny-close
2023-10-17 05:37:35,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49072; closing.
2023-10-17 05:37:35,227 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,227 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,228 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33699. Reason: nanny-close
2023-10-17 05:37:35,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.2280545')
2023-10-17 05:37:35,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35223'. Reason: nanny-close
2023-10-17 05:37:35,228 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,228 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,228 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44851. Reason: nanny-close
2023-10-17 05:37:35,228 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,229 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35019'. Reason: nanny-close
2023-10-17 05:37:35,229 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,229 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:35,229 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36449. Reason: nanny-close
2023-10-17 05:37:35,229 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49012; closing.
2023-10-17 05:37:35,229 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,230 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44983. Reason: nanny-close
2023-10-17 05:37:35,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49036; closing.
2023-10-17 05:37:35,230 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,230 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,230 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.2306864')
2023-10-17 05:37:35,230 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,231 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48996; closing.
2023-10-17 05:37:35,231 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41031', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.2312484')
2023-10-17 05:37:35,231 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,231 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,231 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.231829')
2023-10-17 05:37:35,232 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,232 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:35,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49060; closing.
2023-10-17 05:37:35,233 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49010; closing.
2023-10-17 05:37:35,233 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,233 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49022; closing.
2023-10-17 05:37:35,233 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33699', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.233707')
2023-10-17 05:37:35,233 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,234 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.2340012')
2023-10-17 05:37:35,234 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,234 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.234367')
2023-10-17 05:37:35,234 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49044; closing.
2023-10-17 05:37:35,234 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:35,235 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521055.2350767')
2023-10-17 05:37:35,235 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:37:37,040 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:37:37,041 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:37:37,041 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:37:37,042 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:37:37,043 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-17 05:37:39,283 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:39,288 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39535 instead
  warnings.warn(
2023-10-17 05:37:39,292 - distributed.scheduler - INFO - State start
2023-10-17 05:37:39,315 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:39,316 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:39,316 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39535/status
2023-10-17 05:37:39,316 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:37:39,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34049'
2023-10-17 05:37:41,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:41,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:41,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:42,027 - distributed.scheduler - INFO - Receive client connection: Client-47cde90e-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:42,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49018
2023-10-17 05:37:44,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35423
2023-10-17 05:37:44,469 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35423
2023-10-17 05:37:44,469 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-17 05:37:44,469 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:44,469 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:44,469 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:44,469 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-17 05:37:44,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hr6mi21x
2023-10-17 05:37:44,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a8a1858-2ee1-4e58-a320-a2c6b31e36fa
2023-10-17 05:37:44,470 - distributed.worker - INFO - Starting Worker plugin PreImport-73b73192-015e-4844-9a5a-98526727af9c
2023-10-17 05:37:44,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd181bda-15a4-4042-9766-c06bb50bd457
2023-10-17 05:37:44,470 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:44,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35423', status: init, memory: 0, processing: 0>
2023-10-17 05:37:44,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35423
2023-10-17 05:37:44,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49042
2023-10-17 05:37:44,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:44,500 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:44,500 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:44,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:44,552 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:44,555 - distributed.scheduler - INFO - Remove client Client-47cde90e-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:44,555 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49018; closing.
2023-10-17 05:37:44,555 - distributed.scheduler - INFO - Remove client Client-47cde90e-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:44,556 - distributed.scheduler - INFO - Close client connection: Client-47cde90e-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:44,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34049'. Reason: nanny-close
2023-10-17 05:37:44,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:44,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35423. Reason: nanny-close
2023-10-17 05:37:44,561 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49042; closing.
2023-10-17 05:37:44,561 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:44,561 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521064.5613446')
2023-10-17 05:37:44,561 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:37:44,562 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:45,523 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:37:45,523 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:37:45,524 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:37:45,525 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:37:45,525 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-17 05:37:49,455 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:49,460 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43957 instead
  warnings.warn(
2023-10-17 05:37:49,464 - distributed.scheduler - INFO - State start
2023-10-17 05:37:49,490 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:49,492 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:49,493 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43957/status
2023-10-17 05:37:49,494 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:37:49,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41721'
2023-10-17 05:37:49,619 - distributed.scheduler - INFO - Receive client connection: Client-4de2b8d3-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:49,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49128
2023-10-17 05:37:51,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:37:51,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:37:51,949 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:37:53,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33115
2023-10-17 05:37:53,824 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33115
2023-10-17 05:37:53,824 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35903
2023-10-17 05:37:53,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:37:53,825 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:53,825 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:37:53,825 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-17 05:37:53,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0t48e4__
2023-10-17 05:37:53,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c645ca6a-9c0b-4065-bfaf-6146b7ebccb5
2023-10-17 05:37:53,826 - distributed.worker - INFO - Starting Worker plugin PreImport-e2aaedce-a187-4d55-8c03-2cf51b3fb97c
2023-10-17 05:37:53,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91272c56-323d-4693-bd50-96884461d130
2023-10-17 05:37:53,828 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:53,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33115', status: init, memory: 0, processing: 0>
2023-10-17 05:37:53,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33115
2023-10-17 05:37:53,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58412
2023-10-17 05:37:53,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:37:53,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:37:53,858 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:37:53,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:37:53,899 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:37:53,902 - distributed.scheduler - INFO - Remove client Client-4de2b8d3-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:53,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49128; closing.
2023-10-17 05:37:53,903 - distributed.scheduler - INFO - Remove client Client-4de2b8d3-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:53,903 - distributed.scheduler - INFO - Close client connection: Client-4de2b8d3-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:37:53,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41721'. Reason: nanny-close
2023-10-17 05:37:53,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:37:53,906 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33115. Reason: nanny-close
2023-10-17 05:37:53,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58412; closing.
2023-10-17 05:37:53,908 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:37:53,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521073.908434')
2023-10-17 05:37:53,908 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:37:53,909 - distributed.nanny - INFO - Worker closed
2023-10-17 05:37:55,421 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:37:55,422 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:37:55,422 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:37:55,423 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:37:55,423 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-17 05:37:57,728 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:57,733 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46493 instead
  warnings.warn(
2023-10-17 05:37:57,737 - distributed.scheduler - INFO - State start
2023-10-17 05:37:58,005 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:37:58,006 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:37:58,007 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46493/status
2023-10-17 05:37:58,007 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:38:02,488 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:58418'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58418>: Stream is closed
2023-10-17 05:38:02,928 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:38:02,929 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:38:02,929 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:38:02,930 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:38:02,930 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-17 05:38:05,085 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:05,089 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35873 instead
  warnings.warn(
2023-10-17 05:38:05,093 - distributed.scheduler - INFO - State start
2023-10-17 05:38:05,112 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:05,113 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-17 05:38:05,114 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35873/status
2023-10-17 05:38:05,114 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:38:05,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41791'
2023-10-17 05:38:06,329 - distributed.scheduler - INFO - Receive client connection: Client-572e2c3f-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:06,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35886
2023-10-17 05:38:07,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:07,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:07,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:08,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35661
2023-10-17 05:38:08,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35661
2023-10-17 05:38:08,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34467
2023-10-17 05:38:08,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-17 05:38:08,205 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:08,205 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:08,205 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-17 05:38:08,205 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-11fiku3z
2023-10-17 05:38:08,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0433ca4-660a-4564-a6dc-670d2da4cb93
2023-10-17 05:38:08,206 - distributed.worker - INFO - Starting Worker plugin PreImport-3b0b8d31-3ada-45ef-aff2-158373ee39e5
2023-10-17 05:38:08,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c72e65b0-e524-4f6a-8499-74d18225c79b
2023-10-17 05:38:08,207 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:08,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35661', status: init, memory: 0, processing: 0>
2023-10-17 05:38:08,248 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35661
2023-10-17 05:38:08,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35906
2023-10-17 05:38:08,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:08,250 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-17 05:38:08,251 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:08,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-17 05:38:08,318 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:08,321 - distributed.scheduler - INFO - Remove client Client-572e2c3f-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:08,321 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35886; closing.
2023-10-17 05:38:08,321 - distributed.scheduler - INFO - Remove client Client-572e2c3f-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:08,321 - distributed.scheduler - INFO - Close client connection: Client-572e2c3f-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:08,322 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41791'. Reason: nanny-close
2023-10-17 05:38:08,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:08,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35661. Reason: nanny-close
2023-10-17 05:38:08,326 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35906; closing.
2023-10-17 05:38:08,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-17 05:38:08,326 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35661', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521088.3264706')
2023-10-17 05:38:08,326 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:38:08,327 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:09,539 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:38:09,539 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:38:09,540 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:38:09,541 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-17 05:38:09,541 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-17 05:38:11,656 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:11,660 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44003 instead
  warnings.warn(
2023-10-17 05:38:11,664 - distributed.scheduler - INFO - State start
2023-10-17 05:38:11,684 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:11,685 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:38:11,685 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44003/status
2023-10-17 05:38:11,686 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:38:11,711 - distributed.scheduler - INFO - Receive client connection: Client-5b228119-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:11,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44624
2023-10-17 05:38:11,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38451'
2023-10-17 05:38:11,892 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43913'
2023-10-17 05:38:11,901 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41783'
2023-10-17 05:38:11,914 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38345'
2023-10-17 05:38:11,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37853'
2023-10-17 05:38:11,924 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41303'
2023-10-17 05:38:11,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36223'
2023-10-17 05:38:11,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36811'
2023-10-17 05:38:13,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,767 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,771 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,789 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,789 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:13,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:13,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:13,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:16,876 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40673
2023-10-17 05:38:16,877 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40673
2023-10-17 05:38:16,877 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41715
2023-10-17 05:38:16,877 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:16,877 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:16,877 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:16,878 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:16,878 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-293zl0q9
2023-10-17 05:38:16,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66a7bceb-6868-4244-93b9-d1b6b13780bc
2023-10-17 05:38:16,878 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45225
2023-10-17 05:38:16,879 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45225
2023-10-17 05:38:16,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40891
2023-10-17 05:38:16,879 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:16,880 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:16,880 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:16,880 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:16,880 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hl_zq4pp
2023-10-17 05:38:16,880 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04ea1f13-6d88-4d88-a83c-04cf953c5b23
2023-10-17 05:38:16,885 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42743
2023-10-17 05:38:16,886 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42743
2023-10-17 05:38:16,886 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42747
2023-10-17 05:38:16,886 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:16,886 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:16,886 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:16,886 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:16,886 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3zxegte3
2023-10-17 05:38:16,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-714375e5-3c9a-444c-b8a8-4be14f358075
2023-10-17 05:38:17,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39993
2023-10-17 05:38:17,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39993
2023-10-17 05:38:17,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40893
2023-10-17 05:38:17,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,024 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43961
2023-10-17 05:38:17,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43961
2023-10-17 05:38:17,024 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:17,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43219
2023-10-17 05:38:17,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:17,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dq0yqmb1
2023-10-17 05:38:17,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,024 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,024 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:17,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:17,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5q883sgb
2023-10-17 05:38:17,025 - distributed.worker - INFO - Starting Worker plugin PreImport-a6d08cb0-da3a-4ad2-a8a1-583566e8987a
2023-10-17 05:38:17,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b1a2364-67b8-45f8-8238-575cc66de529
2023-10-17 05:38:17,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71fcefcf-4e75-4d94-951c-82c46da493f7
2023-10-17 05:38:17,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45925
2023-10-17 05:38:17,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45925
2023-10-17 05:38:17,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34617
2023-10-17 05:38:17,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35493
2023-10-17 05:38:17,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35493
2023-10-17 05:38:17,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37575
2023-10-17 05:38:17,028 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:17,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:17,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ldtct0m_
2023-10-17 05:38:17,028 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:17,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:17,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e0_gfh_4
2023-10-17 05:38:17,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2556a62b-1a10-4af4-a23e-85a258199187
2023-10-17 05:38:17,029 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb43259e-a7bf-431a-be66-66c243c0be4c
2023-10-17 05:38:17,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61c2965b-3643-4dbc-9f20-10cb57750ea1
2023-10-17 05:38:17,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b434e6f-ec3d-4cf4-876b-131a61ed09ac
2023-10-17 05:38:17,033 - distributed.worker - INFO - Starting Worker plugin PreImport-7966eefd-389c-410a-a3dd-5b43bb0d7120
2023-10-17 05:38:17,033 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,039 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9209026-d07d-47cb-9a98-06e05a4a2eaf
2023-10-17 05:38:17,039 - distributed.worker - INFO - Starting Worker plugin PreImport-d57341fa-e161-4f80-8a9e-deda72db2b78
2023-10-17 05:38:17,039 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,048 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41897
2023-10-17 05:38:17,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41897
2023-10-17 05:38:17,049 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39031
2023-10-17 05:38:17,049 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,049 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,049 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:17,050 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-17 05:38:17,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mofxp_41
2023-10-17 05:38:17,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77683bd6-2188-4c8b-b2da-6ffed1405961
2023-10-17 05:38:17,061 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c46914ae-6768-4dbf-b978-0d0518127eb8
2023-10-17 05:38:17,061 - distributed.worker - INFO - Starting Worker plugin PreImport-397f31bc-affe-4bdf-9460-7b1c43bdc738
2023-10-17 05:38:17,061 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,063 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40673', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,066 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40673
2023-10-17 05:38:17,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44724
2023-10-17 05:38:17,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,068 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45225', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,068 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,068 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,069 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45225
2023-10-17 05:38:17,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44730
2023-10-17 05:38:17,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,070 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,071 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,071 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42743', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42743
2023-10-17 05:38:17,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44740
2023-10-17 05:38:17,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,096 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,096 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,162 - distributed.worker - INFO - Starting Worker plugin PreImport-a4dd85d7-bb7a-4200-b6dc-c1acc416d58e
2023-10-17 05:38:17,163 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85d810cd-53fe-4df5-aa3c-39740ec8e82a
2023-10-17 05:38:17,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2da5b87e-775e-442e-abe3-c3dac820ba29
2023-10-17 05:38:17,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28193c4e-699a-4907-a078-77c04577d70c
2023-10-17 05:38:17,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7daa96ba-83c4-4d37-b667-89e964a595a3
2023-10-17 05:38:17,183 - distributed.worker - INFO - Starting Worker plugin PreImport-e3d0f9c5-e123-421b-98a4-fc1b55b24644
2023-10-17 05:38:17,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,183 - distributed.worker - INFO - Starting Worker plugin PreImport-21ba5014-daee-4046-a8d9-8272f880e74a
2023-10-17 05:38:17,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,183 - distributed.worker - INFO - Starting Worker plugin PreImport-2f563093-8169-492e-a4b4-8acce8cd5358
2023-10-17 05:38:17,184 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35493', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35493
2023-10-17 05:38:17,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44756
2023-10-17 05:38:17,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,195 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,208 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41897', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,208 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41897
2023-10-17 05:38:17,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44770
2023-10-17 05:38:17,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,210 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,210 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45925', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45925
2023-10-17 05:38:17,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44786
2023-10-17 05:38:17,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39993', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,219 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39993
2023-10-17 05:38:17,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44776
2023-10-17 05:38:17,221 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43961', status: init, memory: 0, processing: 0>
2023-10-17 05:38:17,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43961
2023-10-17 05:38:17,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44802
2023-10-17 05:38:17,222 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,222 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:17,224 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:17,224 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:17,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:17,251 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,253 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-17 05:38:17,265 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,265 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,265 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,265 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,265 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,266 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,266 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,266 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:17,270 - distributed.scheduler - INFO - Remove client Client-5b228119-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:17,270 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44624; closing.
2023-10-17 05:38:17,270 - distributed.scheduler - INFO - Remove client Client-5b228119-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:17,270 - distributed.scheduler - INFO - Close client connection: Client-5b228119-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:17,272 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38451'. Reason: nanny-close
2023-10-17 05:38:17,272 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,273 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43913'. Reason: nanny-close
2023-10-17 05:38:17,274 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,274 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39993. Reason: nanny-close
2023-10-17 05:38:17,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41783'. Reason: nanny-close
2023-10-17 05:38:17,274 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,275 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45925. Reason: nanny-close
2023-10-17 05:38:17,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38345'. Reason: nanny-close
2023-10-17 05:38:17,275 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,275 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45225. Reason: nanny-close
2023-10-17 05:38:17,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37853'. Reason: nanny-close
2023-10-17 05:38:17,276 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,276 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40673. Reason: nanny-close
2023-10-17 05:38:17,276 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41303'. Reason: nanny-close
2023-10-17 05:38:17,276 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,276 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,276 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44776; closing.
2023-10-17 05:38:17,277 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2771494')
2023-10-17 05:38:17,277 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35493. Reason: nanny-close
2023-10-17 05:38:17,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36223'. Reason: nanny-close
2023-10-17 05:38:17,277 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,277 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,277 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,277 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43961. Reason: nanny-close
2023-10-17 05:38:17,278 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36811'. Reason: nanny-close
2023-10-17 05:38:17,278 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,278 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:17,278 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42743. Reason: nanny-close
2023-10-17 05:38:17,278 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44786; closing.
2023-10-17 05:38:17,279 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44730; closing.
2023-10-17 05:38:17,279 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41897. Reason: nanny-close
2023-10-17 05:38:17,279 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,279 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,279 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44724; closing.
2023-10-17 05:38:17,279 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,279 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2799227')
2023-10-17 05:38:17,280 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,280 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45225', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2802866')
2023-10-17 05:38:17,280 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,280 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2807605')
2023-10-17 05:38:17,280 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,281 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44756; closing.
2023-10-17 05:38:17,281 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:17,281 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44740; closing.
2023-10-17 05:38:17,282 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,282 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,282 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,282 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35493', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2824538')
2023-10-17 05:38:17,282 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42743', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2828393')
2023-10-17 05:38:17,283 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44802; closing.
2023-10-17 05:38:17,283 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44770; closing.
2023-10-17 05:38:17,283 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.2837088')
2023-10-17 05:38:17,284 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41897', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521097.284064')
2023-10-17 05:38:17,284 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:17,284 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:38:18,789 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:38:18,789 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:38:18,790 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:38:18,791 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:38:18,791 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-17 05:38:20,795 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:20,799 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35625 instead
  warnings.warn(
2023-10-17 05:38:20,803 - distributed.scheduler - INFO - State start
2023-10-17 05:38:21,005 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:21,006 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:38:21,007 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35625/status
2023-10-17 05:38:21,007 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:38:21,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38933'
2023-10-17 05:38:21,369 - distributed.scheduler - INFO - Receive client connection: Client-60a3ae11-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:21,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50252
2023-10-17 05:38:22,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:22,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:22,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:23,715 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39927
2023-10-17 05:38:23,715 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39927
2023-10-17 05:38:23,715 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32979
2023-10-17 05:38:23,716 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:23,716 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:23,716 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:23,716 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-17 05:38:23,716 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u4hquwwy
2023-10-17 05:38:23,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9dae7826-b4ae-46a4-8fc0-90e602a60a5b
2023-10-17 05:38:23,717 - distributed.worker - INFO - Starting Worker plugin PreImport-039e9350-4329-47fe-87da-fef42d06f485
2023-10-17 05:38:23,717 - distributed.worker - INFO - Starting Worker plugin RMMSetup-711acdde-3815-47b9-9e68-44ceab8ec078
2023-10-17 05:38:23,836 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:23,873 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39927', status: init, memory: 0, processing: 0>
2023-10-17 05:38:23,874 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39927
2023-10-17 05:38:23,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50264
2023-10-17 05:38:23,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:23,877 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:23,877 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:23,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:23,951 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:38:23,956 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:23,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:23,960 - distributed.scheduler - INFO - Remove client Client-60a3ae11-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:23,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50252; closing.
2023-10-17 05:38:23,960 - distributed.scheduler - INFO - Remove client Client-60a3ae11-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:23,961 - distributed.scheduler - INFO - Close client connection: Client-60a3ae11-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:23,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38933'. Reason: nanny-close
2023-10-17 05:38:23,962 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:23,964 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39927. Reason: nanny-close
2023-10-17 05:38:23,966 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50264; closing.
2023-10-17 05:38:23,966 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:23,966 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521103.9665873')
2023-10-17 05:38:23,967 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:38:23,968 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:25,129 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:38:25,129 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:38:25,130 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:38:25,132 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:38:25,132 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-17 05:38:27,096 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:27,100 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41495 instead
  warnings.warn(
2023-10-17 05:38:27,104 - distributed.scheduler - INFO - State start
2023-10-17 05:38:27,212 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-17 05:38:27,213 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-17 05:38:27,214 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41495/status
2023-10-17 05:38:27,214 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-17 05:38:27,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38097'
2023-10-17 05:38:28,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-17 05:38:28,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-17 05:38:28,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-17 05:38:29,143 - distributed.scheduler - INFO - Receive client connection: Client-64692d7c-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:29,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50378
2023-10-17 05:38:29,853 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34269
2023-10-17 05:38:29,854 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34269
2023-10-17 05:38:29,854 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34729
2023-10-17 05:38:29,854 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-17 05:38:29,854 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:29,854 - distributed.worker - INFO -               Threads:                          1
2023-10-17 05:38:29,854 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-17 05:38:29,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8oakzt69
2023-10-17 05:38:29,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb748559-c833-4f01-9978-3453f6b91d2a
2023-10-17 05:38:29,855 - distributed.worker - INFO - Starting Worker plugin PreImport-0eb03a27-dd62-4dcb-bf07-406d3423aa83
2023-10-17 05:38:29,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be2d27e0-1d5c-4f0b-a42c-b2a136591bcd
2023-10-17 05:38:30,001 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:30,032 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34269', status: init, memory: 0, processing: 0>
2023-10-17 05:38:30,033 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34269
2023-10-17 05:38:30,033 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35916
2023-10-17 05:38:30,034 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-17 05:38:30,035 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-17 05:38:30,035 - distributed.worker - INFO - -------------------------------------------------
2023-10-17 05:38:30,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-17 05:38:30,077 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-17 05:38:30,091 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-17 05:38:30,095 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:30,096 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-17 05:38:30,099 - distributed.scheduler - INFO - Remove client Client-64692d7c-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:30,099 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50378; closing.
2023-10-17 05:38:30,099 - distributed.scheduler - INFO - Remove client Client-64692d7c-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:30,100 - distributed.scheduler - INFO - Close client connection: Client-64692d7c-6caf-11ee-b036-d8c49764f6bb
2023-10-17 05:38:30,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38097'. Reason: nanny-close
2023-10-17 05:38:30,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-17 05:38:30,102 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34269. Reason: nanny-close
2023-10-17 05:38:30,104 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35916; closing.
2023-10-17 05:38:30,105 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-17 05:38:30,105 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697521110.1052601')
2023-10-17 05:38:30,105 - distributed.scheduler - INFO - Lost all workers
2023-10-17 05:38:30,106 - distributed.nanny - INFO - Worker closed
2023-10-17 05:38:31,067 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-17 05:38:31,067 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-17 05:38:31,067 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-17 05:38:31,068 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-17 05:38:31,069 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44431 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34609 instead
  warnings.warn(
2023-10-17 05:38:57,681 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-17 05:38:57,699 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:35151', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-17 05:38:57,713 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-17 05:38:57,715 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:56217', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35537 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39173 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34875 instead
  warnings.warn(
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-17 05:39:47,841 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:47,949 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-55' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-10-17 05:39:49,234 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f221031cb80>>, <Task finished name='Task-54' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Worker failed to start.')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 387, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 605, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,582 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 1
2023-10-17 05:39:55,591 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 1
2023-10-17 05:39:55,595 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 1', 'time': 1697521195.5826604}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,629 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 1', 'time': 1697521195.5826604}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,643 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 3
2023-10-17 05:39:55,644 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 3
2023-10-17 05:39:55,646 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 2
2023-10-17 05:39:55,647 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 2
2023-10-17 05:39:55,647 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 3', 'time': 1697521195.643194}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,650 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 2', 'time': 1697521195.6468399}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,651 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 5
2023-10-17 05:39:55,653 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 5
2023-10-17 05:39:55,656 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 7
2023-10-17 05:39:55,656 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 4
2023-10-17 05:39:55,657 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 7
2023-10-17 05:39:55,657 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 5', 'time': 1697521195.6520255}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,658 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 4
2023-10-17 05:39:55,662 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 6
2023-10-17 05:39:55,661 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 7', 'time': 1697521195.6561065}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,663 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 4', 'time': 1697521195.6567445}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,663 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 6
2023-10-17 05:39:55,670 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 6', 'time': 1697521195.6624813}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,675 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 6', 'time': 1697521195.6624813}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,684 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 2', 'time': 1697521195.6468399}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,687 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 3', 'time': 1697521195.643194}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,688 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 5', 'time': 1697521195.6520255}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,690 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 7', 'time': 1697521195.6561065}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:55,691 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 4', 'time': 1697521195.6567445}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-843' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 1', 'time': 1697521195.5826604}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-848' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 6', 'time': 1697521195.6624813}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-846' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 4', 'time': 1697521195.6567445}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-849' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 7', 'time': 1697521195.6561065}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-845' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 3', 'time': 1697521195.643194}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-844' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 2', 'time': 1697521195.6468399}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-847' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 5', 'time': 1697521195.6520255}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-10-17 05:39:56,222 - distributed.nanny - WARNING - Restarting worker
2023-10-17 05:39:56,314 - distributed.nanny - WARNING - Restarting worker
2023-10-17 05:39:56,446 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 66233 exit status was already read will report exitcode 255
2023-10-17 05:39:56,447 - distributed.nanny - WARNING - Restarting worker
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 1', 'time': 1697521195.5826604}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 144, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/local_cuda_cluster.py", line 375, in __init__
    self.sync(self._correct_state)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 387, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 605, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-17 05:39:56,532 - distributed.nanny - WARNING - Restarting worker
2023-10-17 05:39:56,533 - distributed.nanny - WARNING - Restarting worker
2023-10-17 05:39:56,573 - distributed.nanny - WARNING - Restarting worker
2023-10-17 05:39:56,575 - distributed.nanny - WARNING - Restarting worker
2023-10-17 05:39:56,592 - distributed.nanny - WARNING - Restarting worker
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 692, in close_clusters
    cluster.close(timeout=10)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 293, in close
    aw = super().close(timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/cluster.py", line 226, in close
    return self.sync(self._close, callback_timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 460, in _close
    assert w.status in {
AssertionError: Status.running
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42729 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36299 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41119 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37005 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39005 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36191 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33003 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39813 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] Process SpawnProcess-16:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 98, in _test_dataframe_shuffle
    cudf = pytest.importorskip("cudf")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/outcomes.py", line 292, in importorskip
    __import__(modname)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42313 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42523 instead
  warnings.warn(
2023-10-17 05:42:42,260 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-17 05:42:42,262 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-17 05:42:42,270 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:38181', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-17 05:42:42,271 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:39671', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43547 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45921 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39335 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35269 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41001 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38059 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41833 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39105 instead
  warnings.warn(
[1697521684.374450] [dgx13:75605:0]            sock.c:470  UCX  ERROR bind(fd=156 addr=0.0.0.0:35990) failed: Address already in use
[1697521685.899415] [dgx13:75782:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:40947) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43983 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38811 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35335 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35607 instead
  warnings.warn(
2023-10-17 05:49:24,928 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,929 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,937 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,938 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,945 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:40633'.
2023-10-17 05:49:24,945 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60596 remote=tcp://127.0.0.1:45573>: Stream is closed
2023-10-17 05:49:24,946 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:45959'.
2023-10-17 05:49:24,945 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60588 remote=tcp://127.0.0.1:45573>: Stream is closed
2023-10-17 05:49:24,948 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46025'.
2023-10-17 05:49:24,948 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60578 remote=tcp://127.0.0.1:45573>: Stream is closed
2023-10-17 05:49:24,948 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:46025'. Shutting down.
2023-10-17 05:49:24,948 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:36389'.
2023-10-17 05:49:24,948 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fed43708400>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,948 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6522283400>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,951 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fbbd873c400>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:24,952 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f2a6742e430>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-17 05:49:26,951 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-17 05:49:26,951 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-17 05:49:26,955 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
