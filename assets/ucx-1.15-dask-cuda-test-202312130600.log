============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-13 06:40:03,211 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:03,216 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38421 instead
  warnings.warn(
2023-12-13 06:40:03,221 - distributed.scheduler - INFO - State start
2023-12-13 06:40:03,246 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:03,247 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-13 06:40:03,248 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38421/status
2023-12-13 06:40:03,248 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:03,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38581'
2023-12-13 06:40:03,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44817'
2023-12-13 06:40:03,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39879'
2023-12-13 06:40:03,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46479'
2023-12-13 06:40:04,065 - distributed.scheduler - INFO - Receive client connection: Client-70ea47e9-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:04,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43920
2023-12-13 06:40:05,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:05,405 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:05,409 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:05,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:05,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:05,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:05,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:05,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:05,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:05,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:05,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:05,484 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-13 06:40:05,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36383
2023-12-13 06:40:05,509 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36383
2023-12-13 06:40:05,509 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37113
2023-12-13 06:40:05,509 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:05,509 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:05,509 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:05,509 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:05,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-hz8fvqbo
2023-12-13 06:40:05,509 - distributed.worker - INFO - Starting Worker plugin PreImport-a319c4c1-2d98-4dfa-b0df-1edd37f250a6
2023-12-13 06:40:05,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c43be414-0cf7-45b9-abf3-07fafdf643fd
2023-12-13 06:40:05,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bd6399d-1f55-47c2-aca4-6b26782b6067
2023-12-13 06:40:05,510 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,348 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36383', status: init, memory: 0, processing: 0>
2023-12-13 06:40:06,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36383
2023-12-13 06:40:06,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43932
2023-12-13 06:40:06,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:06,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,353 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:06,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35619
2023-12-13 06:40:06,851 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35619
2023-12-13 06:40:06,851 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45651
2023-12-13 06:40:06,851 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,851 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,851 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:06,851 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:06,851 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qpkcqc4f
2023-12-13 06:40:06,852 - distributed.worker - INFO - Starting Worker plugin PreImport-2caecba3-3bda-4160-826b-fb7fd48a3d73
2023-12-13 06:40:06,852 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ced6d06-ef10-4b74-93ba-1f2c4909fad7
2023-12-13 06:40:06,852 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88aeee8b-e76e-4ac0-9b9a-1c73b55567af
2023-12-13 06:40:06,852 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,889 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35619', status: init, memory: 0, processing: 0>
2023-12-13 06:40:06,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35619
2023-12-13 06:40:06,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43940
2023-12-13 06:40:06,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:06,892 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,892 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:06,906 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37923
2023-12-13 06:40:06,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37923
2023-12-13 06:40:06,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37591
2023-12-13 06:40:06,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,908 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,908 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:06,908 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:06,909 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mw7jdksd
2023-12-13 06:40:06,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08184b39-b44d-49b8-89cc-b801cae3b7b6
2023-12-13 06:40:06,910 - distributed.worker - INFO - Starting Worker plugin PreImport-62685438-aba1-48d0-8f53-643a07cc2451
2023-12-13 06:40:06,910 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87e2d660-5c24-4293-9529-bec3e72c5ab2
2023-12-13 06:40:06,910 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,922 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39887
2023-12-13 06:40:06,923 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39887
2023-12-13 06:40:06,923 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44409
2023-12-13 06:40:06,923 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,923 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,924 - distributed.worker - INFO -               Threads:                          4
2023-12-13 06:40:06,924 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-13 06:40:06,924 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ejcbw5r2
2023-12-13 06:40:06,924 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-538c549b-e80f-42f0-99bd-a6492a0095c3
2023-12-13 06:40:06,924 - distributed.worker - INFO - Starting Worker plugin PreImport-11965d15-2470-4986-a5a2-8340cf3e8d9d
2023-12-13 06:40:06,924 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd4e4c1d-e3fe-4780-81f3-4dd02b076783
2023-12-13 06:40:06,925 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,941 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37923', status: init, memory: 0, processing: 0>
2023-12-13 06:40:06,942 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37923
2023-12-13 06:40:06,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43954
2023-12-13 06:40:06,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:06,944 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,944 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:06,955 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39887', status: init, memory: 0, processing: 0>
2023-12-13 06:40:06,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39887
2023-12-13 06:40:06,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43958
2023-12-13 06:40:06,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:06,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:40:06,958 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:06,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:40:07,008 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:07,008 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:07,008 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:07,008 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-13 06:40:07,013 - distributed.scheduler - INFO - Remove client Client-70ea47e9-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:07,014 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43920; closing.
2023-12-13 06:40:07,014 - distributed.scheduler - INFO - Remove client Client-70ea47e9-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:07,014 - distributed.scheduler - INFO - Close client connection: Client-70ea47e9-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:07,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38581'. Reason: nanny-close
2023-12-13 06:40:07,016 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:07,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44817'. Reason: nanny-close
2023-12-13 06:40:07,017 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:07,017 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39887. Reason: nanny-close
2023-12-13 06:40:07,018 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39879'. Reason: nanny-close
2023-12-13 06:40:07,018 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:07,018 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35619. Reason: nanny-close
2023-12-13 06:40:07,019 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:07,019 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43958; closing.
2023-12-13 06:40:07,019 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39887', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449607.019844')
2023-12-13 06:40:07,020 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:07,020 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43940; closing.
2023-12-13 06:40:07,020 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:07,021 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449607.021318')
2023-12-13 06:40:07,021 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:07,023 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46479'. Reason: nanny-close
2023-12-13 06:40:07,024 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:07,024 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37923. Reason: nanny-close
2023-12-13 06:40:07,024 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36383. Reason: nanny-close
2023-12-13 06:40:07,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:07,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43954; closing.
2023-12-13 06:40:07,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43932; closing.
2023-12-13 06:40:07,026 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449607.0269077')
2023-12-13 06:40:07,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:40:07,027 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36383', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449607.0272737')
2023-12-13 06:40:07,027 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:07,027 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:07,027 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:08,533 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:40:08,533 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:40:08,534 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:08,535 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-13 06:40:08,535 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-13 06:40:10,972 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:10,977 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:40:10,980 - distributed.scheduler - INFO - State start
2023-12-13 06:40:11,002 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:11,003 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:40:11,004 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:40:11,004 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:11,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35923'
2023-12-13 06:40:11,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32889'
2023-12-13 06:40:11,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42199'
2023-12-13 06:40:11,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41265'
2023-12-13 06:40:11,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37269'
2023-12-13 06:40:11,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44179'
2023-12-13 06:40:11,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43847'
2023-12-13 06:40:11,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45841'
2023-12-13 06:40:12,343 - distributed.scheduler - INFO - Receive client connection: Client-757daf40-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:12,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40942
2023-12-13 06:40:12,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:12,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:12,979 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,064 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,069 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,106 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,128 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:13,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:13,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:13,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:14,815 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35947
2023-12-13 06:40:14,816 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35947
2023-12-13 06:40:14,816 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34483
2023-12-13 06:40:14,816 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:14,816 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:14,816 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:14,817 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:14,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fa8vm9j0
2023-12-13 06:40:14,817 - distributed.worker - INFO - Starting Worker plugin PreImport-9f6fc774-a1d8-4b30-ae92-629cd1421a13
2023-12-13 06:40:14,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5e9f86f-1c64-4c8e-8221-a7de3f1e7989
2023-12-13 06:40:14,817 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fbe3a18-804a-411e-87cc-aaa70e5328e1
2023-12-13 06:40:15,215 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:15,257 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35947', status: init, memory: 0, processing: 0>
2023-12-13 06:40:15,259 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35947
2023-12-13 06:40:15,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40960
2023-12-13 06:40:15,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:15,266 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:15,266 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:15,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,156 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34973
2023-12-13 06:40:16,157 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34973
2023-12-13 06:40:16,157 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40589
2023-12-13 06:40:16,156 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42241
2023-12-13 06:40:16,157 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,157 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,157 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42241
2023-12-13 06:40:16,157 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33123
2023-12-13 06:40:16,157 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,157 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,157 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,157 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,157 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i8cobeau
2023-12-13 06:40:16,157 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,157 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,157 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gkgqya1x
2023-12-13 06:40:16,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e811651-d947-4686-ac8e-41dba0ef05d6
2023-12-13 06:40:16,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b65056b-15f4-45dc-ad23-5fb98e099290
2023-12-13 06:40:16,158 - distributed.worker - INFO - Starting Worker plugin PreImport-268e77f8-96af-4f5c-997e-c7737f6c5f29
2023-12-13 06:40:16,158 - distributed.worker - INFO - Starting Worker plugin RMMSetup-810b069c-0a65-4137-9bf9-41a864770060
2023-12-13 06:40:16,159 - distributed.worker - INFO - Starting Worker plugin PreImport-58e559d2-a39b-4466-b552-924f75c11965
2023-12-13 06:40:16,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c17ebe9-2eb6-45b2-af40-f6c0b0b61dae
2023-12-13 06:40:16,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35661
2023-12-13 06:40:16,170 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35661
2023-12-13 06:40:16,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40797
2023-12-13 06:40:16,170 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34763
2023-12-13 06:40:16,170 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40797
2023-12-13 06:40:16,170 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,170 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39939
2023-12-13 06:40:16,170 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,170 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,170 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,170 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,170 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,170 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x5dwc8eg
2023-12-13 06:40:16,170 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,170 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6yww6opq
2023-12-13 06:40:16,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-179116c0-e417-4194-a111-7dfeacb83c87
2023-12-13 06:40:16,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-912d3a51-71d4-4527-b4e4-7a48a9c03f5d
2023-12-13 06:40:16,171 - distributed.worker - INFO - Starting Worker plugin RMMSetup-808c2c19-7abb-4ebb-84fb-fa6fb41fd9f9
2023-12-13 06:40:16,173 - distributed.worker - INFO - Starting Worker plugin PreImport-808d4348-1b14-4907-b541-529a1c6b57b4
2023-12-13 06:40:16,173 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea0932de-a032-424b-9a0c-d22eb8d18528
2023-12-13 06:40:16,173 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37089
2023-12-13 06:40:16,173 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37089
2023-12-13 06:40:16,173 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35471
2023-12-13 06:40:16,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,174 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,174 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,174 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,174 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rfp_98n2
2023-12-13 06:40:16,174 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcefeec9-3a52-4ad4-b2b9-b4669e883bd2
2023-12-13 06:40:16,175 - distributed.worker - INFO - Starting Worker plugin PreImport-51728253-2dbc-4951-960b-e2defca813ec
2023-12-13 06:40:16,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5233e9d8-9711-48ce-a848-b01605b6c68f
2023-12-13 06:40:16,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44405
2023-12-13 06:40:16,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44405
2023-12-13 06:40:16,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37881
2023-12-13 06:40:16,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32771
2023-12-13 06:40:16,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37881
2023-12-13 06:40:16,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,182 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41275
2023-12-13 06:40:16,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,182 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,182 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,182 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-51jhtx1k
2023-12-13 06:40:16,182 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:16,182 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:16,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_f3uauwi
2023-12-13 06:40:16,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bbc7874b-7f19-42de-830a-0320ecf9d8c9
2023-12-13 06:40:16,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6e8c8ef-9893-41cd-89bc-f707a65b6f9d
2023-12-13 06:40:16,183 - distributed.worker - INFO - Starting Worker plugin PreImport-8e04a49c-ea09-4c48-a9d4-5c292a706b9d
2023-12-13 06:40:16,183 - distributed.worker - INFO - Starting Worker plugin PreImport-e4d0d3dd-7587-4e62-9181-abe3796974f1
2023-12-13 06:40:16,183 - distributed.worker - INFO - Starting Worker plugin RMMSetup-500e262c-b3fe-462d-b7c5-0d86a8038f9a
2023-12-13 06:40:16,183 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2ab9090-fa49-4680-9eda-87bbe2da41a9
2023-12-13 06:40:16,394 - distributed.worker - INFO - Starting Worker plugin PreImport-dc966247-39ef-434f-be1f-8a0bd70689e0
2023-12-13 06:40:16,394 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,394 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,394 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,394 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,394 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,394 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,395 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,424 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37089', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,424 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37089
2023-12-13 06:40:16,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40988
2023-12-13 06:40:16,425 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37881', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,425 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37881
2023-12-13 06:40:16,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40996
2023-12-13 06:40:16,426 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,426 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,426 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44405', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,426 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,427 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44405
2023-12-13 06:40:16,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41010
2023-12-13 06:40:16,427 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,427 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,429 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,429 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42241', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,433 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42241
2023-12-13 06:40:16,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41048
2023-12-13 06:40:16,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,434 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34973', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,434 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34973
2023-12-13 06:40:16,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41060
2023-12-13 06:40:16,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,436 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,436 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,436 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40797', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,437 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,437 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,437 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40797
2023-12-13 06:40:16,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41034
2023-12-13 06:40:16,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35661', status: init, memory: 0, processing: 0>
2023-12-13 06:40:16,438 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35661
2023-12-13 06:40:16,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41018
2023-12-13 06:40:16,438 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,440 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:16,441 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:16,441 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:16,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:16,450 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,450 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,450 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,450 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,451 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,452 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,452 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:16,455 - distributed.scheduler - INFO - Remove client Client-757daf40-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:16,456 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40942; closing.
2023-12-13 06:40:16,456 - distributed.scheduler - INFO - Remove client Client-757daf40-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:16,456 - distributed.scheduler - INFO - Close client connection: Client-757daf40-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:16,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35923'. Reason: nanny-close
2023-12-13 06:40:16,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32889'. Reason: nanny-close
2023-12-13 06:40:16,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42199'. Reason: nanny-close
2023-12-13 06:40:16,458 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41265'. Reason: nanny-close
2023-12-13 06:40:16,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,459 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37881. Reason: nanny-close
2023-12-13 06:40:16,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37269'. Reason: nanny-close
2023-12-13 06:40:16,460 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,460 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37089. Reason: nanny-close
2023-12-13 06:40:16,460 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44179'. Reason: nanny-close
2023-12-13 06:40:16,460 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43847'. Reason: nanny-close
2023-12-13 06:40:16,460 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,460 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42241. Reason: nanny-close
2023-12-13 06:40:16,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45841'. Reason: nanny-close
2023-12-13 06:40:16,461 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,461 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,461 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40996; closing.
2023-12-13 06:40:16,461 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44405. Reason: nanny-close
2023-12-13 06:40:16,461 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.4618418')
2023-12-13 06:40:16,462 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35947. Reason: nanny-close
2023-12-13 06:40:16,462 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,462 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,463 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,463 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,463 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40988; closing.
2023-12-13 06:40:16,463 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41048; closing.
2023-12-13 06:40:16,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37089', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.4644754')
2023-12-13 06:40:16,464 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,465 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,465 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.4651155')
2023-12-13 06:40:16,465 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,465 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41010; closing.
2023-12-13 06:40:16,466 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44405', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.4660313')
2023-12-13 06:40:16,466 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40960; closing.
2023-12-13 06:40:16,466 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,466 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35947', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.4668775')
2023-12-13 06:40:16,495 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,496 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,496 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34973. Reason: nanny-close
2023-12-13 06:40:16,497 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:16,497 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40797. Reason: nanny-close
2023-12-13 06:40:16,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35661. Reason: nanny-close
2023-12-13 06:40:16,498 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41060; closing.
2023-12-13 06:40:16,498 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,498 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.4988606')
2023-12-13 06:40:16,499 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41034; closing.
2023-12-13 06:40:16,499 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,500 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.500091')
2023-12-13 06:40:16,500 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,501 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41018; closing.
2023-12-13 06:40:16,501 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:16,501 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35661', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449616.5012329')
2023-12-13 06:40:16,501 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:16,501 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:16,503 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:18,075 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:40:18,075 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:40:18,076 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:18,077 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:40:18,077 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-13 06:40:20,357 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:20,362 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41615 instead
  warnings.warn(
2023-12-13 06:40:20,366 - distributed.scheduler - INFO - State start
2023-12-13 06:40:20,390 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:20,391 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:40:20,391 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41615/status
2023-12-13 06:40:20,392 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:20,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46819'
2023-12-13 06:40:20,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41145'
2023-12-13 06:40:20,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44727'
2023-12-13 06:40:20,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36529'
2023-12-13 06:40:20,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44951'
2023-12-13 06:40:20,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45271'
2023-12-13 06:40:20,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43831'
2023-12-13 06:40:20,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36703'
2023-12-13 06:40:21,688 - distributed.scheduler - INFO - Receive client connection: Client-7b1b8fdd-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:21,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53804
2023-12-13 06:40:22,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,614 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,637 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,637 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:22,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:22,724 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:22,728 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:25,006 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42715
2023-12-13 06:40:25,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42715
2023-12-13 06:40:25,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44585
2023-12-13 06:40:25,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,008 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,008 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,008 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k1csfd73
2023-12-13 06:40:25,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c66e61b-62e6-4a7e-a7d2-072184ba3c37
2023-12-13 06:40:25,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fe931fa-2422-4358-9585-bfd3a2e8b378
2023-12-13 06:40:25,015 - distributed.worker - INFO - Starting Worker plugin PreImport-ef880478-a749-4887-b63b-f84378501cd4
2023-12-13 06:40:25,015 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44697
2023-12-13 06:40:25,041 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44697
2023-12-13 06:40:25,041 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33231
2023-12-13 06:40:25,041 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,041 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,041 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,041 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,041 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mfgycg1v
2023-12-13 06:40:25,042 - distributed.worker - INFO - Starting Worker plugin PreImport-d05f01bb-9f1b-43de-a90b-456e91435426
2023-12-13 06:40:25,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8f567ac-cce8-40af-a38e-8a43443c45f1
2023-12-13 06:40:25,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-859d6e34-8c57-469e-b9e2-e1e8c47f9f54
2023-12-13 06:40:25,047 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,052 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42715', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42715
2023-12-13 06:40:25,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53822
2023-12-13 06:40:25,057 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,059 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,059 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,074 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44697', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,075 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44697
2023-12-13 06:40:25,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53826
2023-12-13 06:40:25,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,077 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,077 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,164 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36017
2023-12-13 06:40:25,166 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36017
2023-12-13 06:40:25,166 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41253
2023-12-13 06:40:25,166 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,167 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,167 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-act_hxbv
2023-12-13 06:40:25,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e68b402a-b151-404f-96e1-463248a564ef
2023-12-13 06:40:25,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f72d8653-986b-4951-af1a-8c1620b20060
2023-12-13 06:40:25,175 - distributed.worker - INFO - Starting Worker plugin PreImport-4dfad382-73d5-4d9c-87f8-97489679b324
2023-12-13 06:40:25,176 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,199 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43805
2023-12-13 06:40:25,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43805
2023-12-13 06:40:25,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38657
2023-12-13 06:40:25,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,200 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,200 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,200 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,200 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eakw9ooe
2023-12-13 06:40:25,201 - distributed.worker - INFO - Starting Worker plugin PreImport-ab9212f5-d285-4419-8995-16ad6b6bb80b
2023-12-13 06:40:25,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-211869b4-3010-47af-94f1-9fae114ba7f1
2023-12-13 06:40:25,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48a9e7fa-a633-4393-89a9-46c0f50cfa90
2023-12-13 06:40:25,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36017', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36017
2023-12-13 06:40:25,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53842
2023-12-13 06:40:25,206 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,209 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,235 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43805', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,236 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43805
2023-12-13 06:40:25,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53850
2023-12-13 06:40:25,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,238 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,238 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,270 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39303
2023-12-13 06:40:25,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39303
2023-12-13 06:40:25,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46177
2023-12-13 06:40:25,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,272 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,272 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-731hmln5
2023-12-13 06:40:25,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fda760a0-16f8-4d4c-84f0-3c351c8e60bd
2023-12-13 06:40:25,273 - distributed.worker - INFO - Starting Worker plugin PreImport-febac292-b85b-40a2-a71c-416bee9ffcd0
2023-12-13 06:40:25,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-757573ff-2294-407b-ac4e-01278714f89d
2023-12-13 06:40:25,273 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37157
2023-12-13 06:40:25,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37157
2023-12-13 06:40:25,274 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44457
2023-12-13 06:40:25,274 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,274 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,274 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,274 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h3koo606
2023-12-13 06:40:25,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3b0af1f-d4a0-408a-9113-f15f18482714
2023-12-13 06:40:25,275 - distributed.worker - INFO - Starting Worker plugin PreImport-bf402c52-d92b-472c-9306-0bab0750b131
2023-12-13 06:40:25,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4007523e-892c-42ba-b8b1-59d2e5acaa07
2023-12-13 06:40:25,295 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,301 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,322 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39303', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,323 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39303
2023-12-13 06:40:25,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53858
2023-12-13 06:40:25,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37157', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,325 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,325 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37157
2023-12-13 06:40:25,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53860
2023-12-13 06:40:25,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,327 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40157
2023-12-13 06:40:25,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40157
2023-12-13 06:40:25,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37419
2023-12-13 06:40:25,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,330 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,330 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-20xlrpk3
2023-12-13 06:40:25,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,331 - distributed.worker - INFO - Starting Worker plugin PreImport-7aa401e9-ef9e-407f-bf9b-03e98a7b0a93
2023-12-13 06:40:25,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,331 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a87b8b6-72a2-41f3-bff1-b295c064efc6
2023-12-13 06:40:25,331 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddea4c21-cb0f-45a8-8bda-db94376cb448
2023-12-13 06:40:25,332 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35155
2023-12-13 06:40:25,333 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35155
2023-12-13 06:40:25,333 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37121
2023-12-13 06:40:25,333 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,333 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,333 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:25,333 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:25,333 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3hewdk_4
2023-12-13 06:40:25,334 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc67f27f-7819-4ad2-bb0b-e5567ad03bd4
2023-12-13 06:40:25,335 - distributed.worker - INFO - Starting Worker plugin PreImport-5015e27b-46ae-45c1-917b-c5324c0c75a3
2023-12-13 06:40:25,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9074f5e7-f470-4fbc-b0b3-eb0145a0c45c
2023-12-13 06:40:25,344 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,351 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,373 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40157', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,374 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40157
2023-12-13 06:40:25,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53874
2023-12-13 06:40:25,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,376 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,381 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35155', status: init, memory: 0, processing: 0>
2023-12-13 06:40:25,381 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35155
2023-12-13 06:40:25,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53886
2023-12-13 06:40:25,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:25,384 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:25,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,384 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:25,393 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:25,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:25,476 - distributed.scheduler - INFO - Remove client Client-7b1b8fdd-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:25,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53804; closing.
2023-12-13 06:40:25,477 - distributed.scheduler - INFO - Remove client Client-7b1b8fdd-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:25,477 - distributed.scheduler - INFO - Close client connection: Client-7b1b8fdd-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:25,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46819'. Reason: nanny-close
2023-12-13 06:40:25,479 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41145'. Reason: nanny-close
2023-12-13 06:40:25,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,480 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36017. Reason: nanny-close
2023-12-13 06:40:25,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44727'. Reason: nanny-close
2023-12-13 06:40:25,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36529'. Reason: nanny-close
2023-12-13 06:40:25,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,481 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42715. Reason: nanny-close
2023-12-13 06:40:25,481 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44697. Reason: nanny-close
2023-12-13 06:40:25,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44951'. Reason: nanny-close
2023-12-13 06:40:25,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,482 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43805. Reason: nanny-close
2023-12-13 06:40:25,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45271'. Reason: nanny-close
2023-12-13 06:40:25,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,483 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,483 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53842; closing.
2023-12-13 06:40:25,483 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40157. Reason: nanny-close
2023-12-13 06:40:25,483 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36017', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.4834406')
2023-12-13 06:40:25,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43831'. Reason: nanny-close
2023-12-13 06:40:25,483 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,483 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,484 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35155. Reason: nanny-close
2023-12-13 06:40:25,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36703'. Reason: nanny-close
2023-12-13 06:40:25,484 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,484 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:25,484 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39303. Reason: nanny-close
2023-12-13 06:40:25,485 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37157. Reason: nanny-close
2023-12-13 06:40:25,485 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,485 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,485 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53850; closing.
2023-12-13 06:40:25,485 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53826; closing.
2023-12-13 06:40:25,485 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53822; closing.
2023-12-13 06:40:25,485 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,486 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.4861665')
2023-12-13 06:40:25,486 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.486568')
2023-12-13 06:40:25,486 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,486 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.487009')
2023-12-13 06:40:25,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:25,488 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53886; closing.
2023-12-13 06:40:25,488 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53874; closing.
2023-12-13 06:40:25,488 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,488 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53858; closing.
2023-12-13 06:40:25,488 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,488 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,489 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.48904')
2023-12-13 06:40:25,489 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.4894345')
2023-12-13 06:40:25,489 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39303', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.4898434')
2023-12-13 06:40:25,490 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53860; closing.
2023-12-13 06:40:25,490 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:25,490 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449625.4905615')
2023-12-13 06:40:25,490 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:27,197 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:40:27,197 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:40:27,198 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:27,199 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:40:27,199 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-13 06:40:29,451 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:29,456 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37341 instead
  warnings.warn(
2023-12-13 06:40:29,460 - distributed.scheduler - INFO - State start
2023-12-13 06:40:29,486 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:29,486 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:40:29,487 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37341/status
2023-12-13 06:40:29,487 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:29,577 - distributed.scheduler - INFO - Receive client connection: Client-808115a4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:29,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53974
2023-12-13 06:40:29,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44255'
2023-12-13 06:40:29,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39563'
2023-12-13 06:40:29,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42119'
2023-12-13 06:40:29,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36029'
2023-12-13 06:40:29,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43883'
2023-12-13 06:40:29,780 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36659'
2023-12-13 06:40:29,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38253'
2023-12-13 06:40:29,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40713'
2023-12-13 06:40:31,695 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,696 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,698 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,698 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,702 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,702 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,706 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:31,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:31,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:31,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:34,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37451
2023-12-13 06:40:34,382 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37451
2023-12-13 06:40:34,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46013
2023-12-13 06:40:34,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,382 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,382 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,383 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-chp2t77_
2023-12-13 06:40:34,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d2f649e-1e7d-4057-88d0-a3b9b1284778
2023-12-13 06:40:34,384 - distributed.worker - INFO - Starting Worker plugin PreImport-905797a7-54f3-49ba-ae58-9c7065a3057a
2023-12-13 06:40:34,384 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a088c8d-3608-4980-8d96-d9df4fd839d8
2023-12-13 06:40:34,390 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35989
2023-12-13 06:40:34,391 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35989
2023-12-13 06:40:34,391 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45591
2023-12-13 06:40:34,391 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,391 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,391 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b4huytd5
2023-12-13 06:40:34,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c240e69-bf36-447d-be8e-0760143b62f8
2023-12-13 06:40:34,393 - distributed.worker - INFO - Starting Worker plugin PreImport-24995b07-272b-4555-90ea-f33343ab6631
2023-12-13 06:40:34,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53e14506-78ed-4e3b-9c5f-702af9b6de1e
2023-12-13 06:40:34,408 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40939
2023-12-13 06:40:34,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40939
2023-12-13 06:40:34,409 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34771
2023-12-13 06:40:34,409 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,409 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,409 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,409 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t7hbspsm
2023-12-13 06:40:34,410 - distributed.worker - INFO - Starting Worker plugin PreImport-9c14938d-9b4d-4f03-8e80-a5927d96b9c1
2023-12-13 06:40:34,410 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-806e7d6d-c6e2-4fef-98fb-871110ae86da
2023-12-13 06:40:34,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46f3bb26-1137-4cdf-9aa4-17aad6356ca6
2023-12-13 06:40:34,578 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46651
2023-12-13 06:40:34,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46651
2023-12-13 06:40:34,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41735
2023-12-13 06:40:34,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,579 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,579 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ee9n5_r5
2023-12-13 06:40:34,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1260b41e-cb3c-4750-b7ff-033a6799f708
2023-12-13 06:40:34,580 - distributed.worker - INFO - Starting Worker plugin PreImport-d86401ad-3351-4412-a666-7f27d550cf44
2023-12-13 06:40:34,580 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb2a11b2-80b2-4ab4-a965-7f29f500e07b
2023-12-13 06:40:34,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38613
2023-12-13 06:40:34,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38613
2023-12-13 06:40:34,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38113
2023-12-13 06:40:34,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,590 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,590 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ja3v6iz
2023-12-13 06:40:34,591 - distributed.worker - INFO - Starting Worker plugin PreImport-806f7346-f68f-4f7c-ac79-2611dfac3c69
2023-12-13 06:40:34,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1da37ff-62ed-4d25-922c-3e8c946abad6
2023-12-13 06:40:34,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0d1332e-233c-4e0e-96cf-fe2a718470bc
2023-12-13 06:40:34,594 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46437
2023-12-13 06:40:34,595 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46437
2023-12-13 06:40:34,595 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34077
2023-12-13 06:40:34,595 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,595 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,595 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qlt3qleb
2023-12-13 06:40:34,596 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14ff4bfc-0ae2-4cda-8ec6-a7abcd6058ef
2023-12-13 06:40:34,596 - distributed.worker - INFO - Starting Worker plugin PreImport-7ed0d242-016e-4abc-ba64-ac4a128c2f36
2023-12-13 06:40:34,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2ea7b8c-7dbe-4949-9c84-705e986a2477
2023-12-13 06:40:34,596 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45409
2023-12-13 06:40:34,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45409
2023-12-13 06:40:34,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46547
2023-12-13 06:40:34,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,597 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,597 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,597 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dh0felst
2023-12-13 06:40:34,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe246394-fa9a-43f8-8685-d540d2928f13
2023-12-13 06:40:34,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37071
2023-12-13 06:40:34,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37071
2023-12-13 06:40:34,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34033
2023-12-13 06:40:34,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,769 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,769 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:34,769 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:34,769 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kn6yhwoa
2023-12-13 06:40:34,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03111205-d2a0-4332-8a7a-45b3ce93ef9c
2023-12-13 06:40:34,771 - distributed.worker - INFO - Starting Worker plugin PreImport-c96accf4-1c97-4e4f-8b50-c0f1465fb8b3
2023-12-13 06:40:34,771 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d512df35-a8e2-4db0-b343-e263e03c5d7e
2023-12-13 06:40:34,838 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,844 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,846 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,879 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40939', status: init, memory: 0, processing: 0>
2023-12-13 06:40:34,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40939
2023-12-13 06:40:34,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44142
2023-12-13 06:40:34,883 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35989', status: init, memory: 0, processing: 0>
2023-12-13 06:40:34,883 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:34,884 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35989
2023-12-13 06:40:34,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44134
2023-12-13 06:40:34,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:34,887 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,887 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:34,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37451', status: init, memory: 0, processing: 0>
2023-12-13 06:40:34,891 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37451
2023-12-13 06:40:34,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44152
2023-12-13 06:40:34,892 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,892 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:34,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:34,899 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:34,899 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:34,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d522bd6-92b0-4f09-88af-4dbd934f2b48
2023-12-13 06:40:34,968 - distributed.worker - INFO - Starting Worker plugin PreImport-5aad5560-8524-4057-bc68-93f69e7d2a85
2023-12-13 06:40:34,968 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,972 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,974 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,979 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,979 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:34,997 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45409', status: init, memory: 0, processing: 0>
2023-12-13 06:40:34,998 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45409
2023-12-13 06:40:34,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44162
2023-12-13 06:40:34,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:35,002 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46437', status: init, memory: 0, processing: 0>
2023-12-13 06:40:35,002 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:35,002 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,002 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46437
2023-12-13 06:40:35,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44170
2023-12-13 06:40:35,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:35,003 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:35,003 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46651', status: init, memory: 0, processing: 0>
2023-12-13 06:40:35,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46651
2023-12-13 06:40:35,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44172
2023-12-13 06:40:35,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:35,007 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:35,007 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:35,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:35,008 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:35,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37071', status: init, memory: 0, processing: 0>
2023-12-13 06:40:35,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37071
2023-12-13 06:40:35,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44186
2023-12-13 06:40:35,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38613', status: init, memory: 0, processing: 0>
2023-12-13 06:40:35,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38613
2023-12-13 06:40:35,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44198
2023-12-13 06:40:35,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:35,017 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:35,017 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:35,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:35,024 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:35,024 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:35,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:35,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,106 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:35,114 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:35,115 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:35,118 - distributed.scheduler - INFO - Remove client Client-808115a4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:35,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53974; closing.
2023-12-13 06:40:35,119 - distributed.scheduler - INFO - Remove client Client-808115a4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:35,119 - distributed.scheduler - INFO - Close client connection: Client-808115a4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:35,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44255'. Reason: nanny-close
2023-12-13 06:40:35,120 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,121 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39563'. Reason: nanny-close
2023-12-13 06:40:35,122 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,122 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40939. Reason: nanny-close
2023-12-13 06:40:35,122 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42119'. Reason: nanny-close
2023-12-13 06:40:35,122 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,122 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45409. Reason: nanny-close
2023-12-13 06:40:35,122 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36029'. Reason: nanny-close
2023-12-13 06:40:35,123 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43883'. Reason: nanny-close
2023-12-13 06:40:35,123 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37071. Reason: nanny-close
2023-12-13 06:40:35,123 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,123 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44142; closing.
2023-12-13 06:40:35,123 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36659'. Reason: nanny-close
2023-12-13 06:40:35,124 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,124 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40939', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.124161')
2023-12-13 06:40:35,124 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35989. Reason: nanny-close
2023-12-13 06:40:35,124 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46437. Reason: nanny-close
2023-12-13 06:40:35,124 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38253'. Reason: nanny-close
2023-12-13 06:40:35,125 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,125 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46651. Reason: nanny-close
2023-12-13 06:40:35,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40713'. Reason: nanny-close
2023-12-13 06:40:35,125 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,125 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:35,125 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44162; closing.
2023-12-13 06:40:35,126 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37451. Reason: nanny-close
2023-12-13 06:40:35,126 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,126 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,126 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,126 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45409', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1267443')
2023-12-13 06:40:35,127 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44186; closing.
2023-12-13 06:40:35,127 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,127 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,127 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38613. Reason: nanny-close
2023-12-13 06:40:35,127 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1278338')
2023-12-13 06:40:35,128 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,128 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44170; closing.
2023-12-13 06:40:35,128 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,128 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,128 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,128 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46437', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1288717')
2023-12-13 06:40:35,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44134; closing.
2023-12-13 06:40:35,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44172; closing.
2023-12-13 06:40:35,129 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:35,130 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35989', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1299944')
2023-12-13 06:40:35,130 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1303742')
2023-12-13 06:40:35,130 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44152; closing.
2023-12-13 06:40:35,131 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1313496')
2023-12-13 06:40:35,131 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:35,131 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44198; closing.
2023-12-13 06:40:35,132 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38613', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449635.1321144')
2023-12-13 06:40:35,132 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:36,889 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:40:36,889 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:40:36,890 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:36,891 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:40:36,892 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-13 06:40:39,265 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:39,269 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46319 instead
  warnings.warn(
2023-12-13 06:40:39,273 - distributed.scheduler - INFO - State start
2023-12-13 06:40:39,296 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:39,297 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:40:39,298 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46319/status
2023-12-13 06:40:39,299 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:39,338 - distributed.scheduler - INFO - Receive client connection: Client-876725c2-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:39,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44328
2023-12-13 06:40:39,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33093'
2023-12-13 06:40:39,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45333'
2023-12-13 06:40:39,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46047'
2023-12-13 06:40:39,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42703'
2023-12-13 06:40:39,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35615'
2023-12-13 06:40:39,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39411'
2023-12-13 06:40:39,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41437'
2023-12-13 06:40:39,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39429'
2023-12-13 06:40:40,178 - distributed.scheduler - INFO - Receive client connection: Client-8659c0b4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:40,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38230
2023-12-13 06:40:41,404 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,405 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,409 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,463 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,466 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,466 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,470 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,528 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:41,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:41,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:41,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:43,693 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46657
2023-12-13 06:40:43,693 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46657
2023-12-13 06:40:43,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45327
2023-12-13 06:40:43,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:43,693 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:43,694 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:43,694 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:43,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ut3p8u5
2023-12-13 06:40:43,694 - distributed.worker - INFO - Starting Worker plugin PreImport-6e56afd2-194b-4ef2-ae7b-5b485632fffd
2023-12-13 06:40:43,694 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-761aa2fb-b621-4c7f-b5bd-d90fc01b38c8
2023-12-13 06:40:43,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dce295f1-4b17-45a2-988c-e5819964acab
2023-12-13 06:40:44,105 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,134 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46657', status: init, memory: 0, processing: 0>
2023-12-13 06:40:44,135 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46657
2023-12-13 06:40:44,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38344
2023-12-13 06:40:44,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:44,137 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,137 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:44,661 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33929
2023-12-13 06:40:44,661 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33929
2023-12-13 06:40:44,662 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46237
2023-12-13 06:40:44,662 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,662 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,662 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,662 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,662 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hz1tinla
2023-12-13 06:40:44,662 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22c2538b-1311-40a0-9cf3-942a0f942001
2023-12-13 06:40:44,663 - distributed.worker - INFO - Starting Worker plugin PreImport-6fd937f1-3c50-4dc0-a95b-d26391347b67
2023-12-13 06:40:44,663 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db235e6d-19b7-4dd9-a255-02982e9397a6
2023-12-13 06:40:44,671 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45559
2023-12-13 06:40:44,672 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45559
2023-12-13 06:40:44,672 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45251
2023-12-13 06:40:44,672 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,672 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,672 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,672 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,672 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j8hk8sp2
2023-12-13 06:40:44,673 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b799822f-403c-4a84-b055-5b929852aa4c
2023-12-13 06:40:44,673 - distributed.worker - INFO - Starting Worker plugin PreImport-ab23bac4-a850-4762-a7c9-fc57716154d2
2023-12-13 06:40:44,673 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fea1c73-7143-4017-9324-67dd8571b88b
2023-12-13 06:40:44,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39519
2023-12-13 06:40:44,677 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39519
2023-12-13 06:40:44,678 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45639
2023-12-13 06:40:44,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,678 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,678 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,678 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,678 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-znyjtq51
2023-12-13 06:40:44,678 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd19ba45-dbe3-4feb-8ac0-cd54ff204ebb
2023-12-13 06:40:44,679 - distributed.worker - INFO - Starting Worker plugin PreImport-4f8065a5-786b-4a74-a00d-ccf3c7c81a62
2023-12-13 06:40:44,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d3663e0-1f6f-4be2-be20-00a1dbc7b5d8
2023-12-13 06:40:44,679 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35531
2023-12-13 06:40:44,680 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35531
2023-12-13 06:40:44,680 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46269
2023-12-13 06:40:44,680 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,680 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,681 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,681 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tmezeq4m
2023-12-13 06:40:44,682 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7eacc2f-763e-448b-96a8-6e92ce952d8a
2023-12-13 06:40:44,731 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39857
2023-12-13 06:40:44,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39857
2023-12-13 06:40:44,732 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46049
2023-12-13 06:40:44,732 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,732 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,732 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,733 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,733 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fmwz5rxu
2023-12-13 06:40:44,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a1a8592-3231-4d09-ab0c-1d3888f0bd70
2023-12-13 06:40:44,734 - distributed.worker - INFO - Starting Worker plugin PreImport-4eddd539-d0ec-4beb-9c42-e8bff64149a4
2023-12-13 06:40:44,734 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01d4bf24-52b8-45fc-bc36-e4f549d02ff0
2023-12-13 06:40:44,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36753
2023-12-13 06:40:44,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36753
2023-12-13 06:40:44,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42347
2023-12-13 06:40:44,741 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,741 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,741 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,742 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ktldn8p1
2023-12-13 06:40:44,742 - distributed.worker - INFO - Starting Worker plugin PreImport-c45f4adb-bd72-4f95-a414-4440e9b0e7aa
2023-12-13 06:40:44,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2c0cbda-2029-4212-8c4d-2ee68be35583
2023-12-13 06:40:44,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a1ce0a7-af26-4e33-8cfe-7ed94decfb24
2023-12-13 06:40:44,921 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39605
2023-12-13 06:40:44,922 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39605
2023-12-13 06:40:44,922 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34093
2023-12-13 06:40:44,922 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:44,922 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:44,922 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:44,923 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:44,923 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g5cg8md4
2023-12-13 06:40:44,923 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a8894da-0757-4c3b-9889-918437d0bae9
2023-12-13 06:40:44,923 - distributed.worker - INFO - Starting Worker plugin PreImport-714c3637-f8f3-4631-b1fd-133e215913b1
2023-12-13 06:40:44,924 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ba11df8-85a2-40da-8cac-5e47f19d4c24
2023-12-13 06:40:46,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,848 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,875 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45559', status: init, memory: 0, processing: 0>
2023-12-13 06:40:46,876 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45559
2023-12-13 06:40:46,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38364
2023-12-13 06:40:46,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:46,881 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:46,881 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:46,887 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33929', status: init, memory: 0, processing: 0>
2023-12-13 06:40:46,888 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33929
2023-12-13 06:40:46,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38372
2023-12-13 06:40:46,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:46,893 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:46,893 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:46,911 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,919 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd64f24f-5b18-48e9-a0ad-4beb1d322775
2023-12-13 06:40:46,928 - distributed.worker - INFO - Starting Worker plugin PreImport-e001c404-68bf-4098-850c-e4600d0b17c8
2023-12-13 06:40:46,928 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,939 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39519', status: init, memory: 0, processing: 0>
2023-12-13 06:40:46,939 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39519
2023-12-13 06:40:46,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38382
2023-12-13 06:40:46,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:46,944 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:46,944 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:46,965 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39857', status: init, memory: 0, processing: 0>
2023-12-13 06:40:46,966 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39857
2023-12-13 06:40:46,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38398
2023-12-13 06:40:46,966 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:46,969 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35531', status: init, memory: 0, processing: 0>
2023-12-13 06:40:46,969 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35531
2023-12-13 06:40:46,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38410
2023-12-13 06:40:46,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:46,970 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,975 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:46,975 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:46,977 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:46,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:46,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:46,997 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39605', status: init, memory: 0, processing: 0>
2023-12-13 06:40:46,997 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39605
2023-12-13 06:40:46,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38424
2023-12-13 06:40:46,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:47,003 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:47,003 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:47,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:47,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36753', status: init, memory: 0, processing: 0>
2023-12-13 06:40:47,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36753
2023-12-13 06:40:47,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38432
2023-12-13 06:40:47,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:47,018 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:47,019 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:47,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:47,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,043 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,046 - distributed.scheduler - INFO - Remove client Client-876725c2-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:47,046 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44328; closing.
2023-12-13 06:40:47,047 - distributed.scheduler - INFO - Remove client Client-876725c2-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:47,047 - distributed.scheduler - INFO - Close client connection: Client-876725c2-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,055 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,056 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:40:47,062 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,064 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:40:47,066 - distributed.scheduler - INFO - Remove client Client-8659c0b4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:47,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38230; closing.
2023-12-13 06:40:47,067 - distributed.scheduler - INFO - Remove client Client-8659c0b4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:47,067 - distributed.scheduler - INFO - Close client connection: Client-8659c0b4-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:47,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33093'. Reason: nanny-close
2023-12-13 06:40:47,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45333'. Reason: nanny-close
2023-12-13 06:40:47,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46657. Reason: nanny-close
2023-12-13 06:40:47,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46047'. Reason: nanny-close
2023-12-13 06:40:47,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45559. Reason: nanny-close
2023-12-13 06:40:47,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42703'. Reason: nanny-close
2023-12-13 06:40:47,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,071 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35531. Reason: nanny-close
2023-12-13 06:40:47,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35615'. Reason: nanny-close
2023-12-13 06:40:47,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,071 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38344; closing.
2023-12-13 06:40:47,071 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,072 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39519. Reason: nanny-close
2023-12-13 06:40:47,072 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46657', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0721374')
2023-12-13 06:40:47,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39411'. Reason: nanny-close
2023-12-13 06:40:47,072 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,072 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33929. Reason: nanny-close
2023-12-13 06:40:47,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41437'. Reason: nanny-close
2023-12-13 06:40:47,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,073 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39605. Reason: nanny-close
2023-12-13 06:40:47,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39429'. Reason: nanny-close
2023-12-13 06:40:47,073 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:47,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,073 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38364; closing.
2023-12-13 06:40:47,073 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39857. Reason: nanny-close
2023-12-13 06:40:47,073 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,074 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,074 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,074 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0747077')
2023-12-13 06:40:47,074 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36753. Reason: nanny-close
2023-12-13 06:40:47,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38410; closing.
2023-12-13 06:40:47,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38382; closing.
2023-12-13 06:40:47,075 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,075 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,075 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0756526')
2023-12-13 06:40:47,076 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0759506')
2023-12-13 06:40:47,076 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,076 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,076 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,076 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:47,076 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,076 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38372; closing.
2023-12-13 06:40:47,077 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38424; closing.
2023-12-13 06:40:47,077 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0775125')
2023-12-13 06:40:47,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39605', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0779479')
2023-12-13 06:40:47,078 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38398; closing.
2023-12-13 06:40:47,078 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,078 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38432; closing.
2023-12-13 06:40:47,078 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:47,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0788143')
2023-12-13 06:40:47,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449647.0791922')
2023-12-13 06:40:47,079 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:48,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39477', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,864 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39477
2023-12-13 06:40:48,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38446
2023-12-13 06:40:48,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33007', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33007
2023-12-13 06:40:48,869 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38454
2023-12-13 06:40:48,877 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33313', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,878 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33313
2023-12-13 06:40:48,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38468
2023-12-13 06:40:48,890 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38446; closing.
2023-12-13 06:40:48,890 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.8905156')
2023-12-13 06:40:48,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37003', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37003
2023-12-13 06:40:48,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38472
2023-12-13 06:40:48,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42019', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,893 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42019
2023-12-13 06:40:48,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38488
2023-12-13 06:40:48,901 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37371', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,902 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37371
2023-12-13 06:40:48,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38496
2023-12-13 06:40:48,905 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40327', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,905 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40327
2023-12-13 06:40:48,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38502
2023-12-13 06:40:48,919 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35449', status: init, memory: 0, processing: 0>
2023-12-13 06:40:48,920 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35449
2023-12-13 06:40:48,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38506
2023-12-13 06:40:48,937 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38488; closing.
2023-12-13 06:40:48,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.9385047')
2023-12-13 06:40:48,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38506; closing.
2023-12-13 06:40:48,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.9402344')
2023-12-13 06:40:48,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38468; closing.
2023-12-13 06:40:48,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38502; closing.
2023-12-13 06:40:48,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38472; closing.
2023-12-13 06:40:48,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38506>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-13 06:40:48,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.94774')
2023-12-13 06:40:48,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.9484682')
2023-12-13 06:40:48,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.9490957')
2023-12-13 06:40:48,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38496; closing.
2023-12-13 06:40:48,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37371', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.9504037')
2023-12-13 06:40:48,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38454; closing.
2023-12-13 06:40:48,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449648.95122')
2023-12-13 06:40:48,951 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:49,287 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:40:49,288 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:40:49,288 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:40:49,290 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:40:49,291 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-13 06:40:51,890 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:51,895 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37425 instead
  warnings.warn(
2023-12-13 06:40:51,899 - distributed.scheduler - INFO - State start
2023-12-13 06:40:51,921 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:40:51,922 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:40:51,923 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37425/status
2023-12-13 06:40:51,923 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:40:52,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37473'
2023-12-13 06:40:52,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42755'
2023-12-13 06:40:52,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39575'
2023-12-13 06:40:52,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43859'
2023-12-13 06:40:52,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44369'
2023-12-13 06:40:52,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43615'
2023-12-13 06:40:52,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37891'
2023-12-13 06:40:52,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39295'
2023-12-13 06:40:52,507 - distributed.scheduler - INFO - Receive client connection: Client-8dd686f3-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:52,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60700
2023-12-13 06:40:53,017 - distributed.scheduler - INFO - Receive client connection: Client-8e74ee21-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:53,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60722
2023-12-13 06:40:54,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,067 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,099 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,101 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,129 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,129 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,133 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:54,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:40:54,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:40:54,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:40:57,124 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42529
2023-12-13 06:40:57,125 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42529
2023-12-13 06:40:57,125 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40943
2023-12-13 06:40:57,125 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,125 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,126 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,126 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,126 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k4lzj_di
2023-12-13 06:40:57,126 - distributed.worker - INFO - Starting Worker plugin PreImport-958e9dfe-6fdb-43d9-b967-692c39fd2366
2023-12-13 06:40:57,126 - distributed.worker - INFO - Starting Worker plugin RMMSetup-072e6525-425a-4ea7-ad40-9983097bc8a8
2023-12-13 06:40:57,128 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34431
2023-12-13 06:40:57,129 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34431
2023-12-13 06:40:57,128 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39875
2023-12-13 06:40:57,129 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41573
2023-12-13 06:40:57,129 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39875
2023-12-13 06:40:57,129 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,129 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40017
2023-12-13 06:40:57,129 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,129 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,129 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,129 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,129 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,129 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ee2qr4hd
2023-12-13 06:40:57,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zbiev94m
2023-12-13 06:40:57,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40b282ad-4696-463f-b4dc-11d50760023b
2023-12-13 06:40:57,130 - distributed.worker - INFO - Starting Worker plugin PreImport-5e6abbb7-dce1-460c-9bab-4c42be4fe988
2023-12-13 06:40:57,130 - distributed.worker - INFO - Starting Worker plugin PreImport-5e23c40c-a707-4a37-b1bc-720c44070bb4
2023-12-13 06:40:57,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08404033-dad5-4814-9717-255f453a1d3b
2023-12-13 06:40:57,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-18ec2670-2304-43ec-a7b0-7b1ad69fb6e3
2023-12-13 06:40:57,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c211211f-655e-4b20-bac1-f3e780e06c0c
2023-12-13 06:40:57,131 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43481
2023-12-13 06:40:57,132 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43481
2023-12-13 06:40:57,133 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39973
2023-12-13 06:40:57,133 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,133 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,133 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,133 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7if_hy1p
2023-12-13 06:40:57,134 - distributed.worker - INFO - Starting Worker plugin PreImport-022fe519-cb5e-48e6-b4a4-d12724f30879
2023-12-13 06:40:57,134 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-841edd31-6ea8-4d95-9fed-e59ede1bee5b
2023-12-13 06:40:57,134 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9a38e18-33d7-42b5-9ea3-c5dc156614a0
2023-12-13 06:40:57,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45929
2023-12-13 06:40:57,172 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45929
2023-12-13 06:40:57,172 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39473
2023-12-13 06:40:57,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,172 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,172 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,173 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,173 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zzuavtdw
2023-12-13 06:40:57,173 - distributed.worker - INFO - Starting Worker plugin PreImport-c268f249-bfe1-46cf-b1e5-d76faeb345b8
2023-12-13 06:40:57,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0dc5e17-70d0-4ebf-820f-5af7f73b7444
2023-12-13 06:40:57,174 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e56339c-defe-4e4b-afe4-76fbcd28227c
2023-12-13 06:40:57,755 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45211
2023-12-13 06:40:57,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45211
2023-12-13 06:40:57,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44147
2023-12-13 06:40:57,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,769 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,769 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,769 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,769 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6lgfzzzz
2023-12-13 06:40:57,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa6c115e-6bb8-48c1-b3be-c9e62bf8e630
2023-12-13 06:40:57,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-388de01c-5cba-4a88-a0a8-884f34db3a1c
2023-12-13 06:40:57,778 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21543ed2-b738-4fd0-a056-281172c3cb65
2023-12-13 06:40:57,778 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,780 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38245
2023-12-13 06:40:57,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38245
2023-12-13 06:40:57,790 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46159
2023-12-13 06:40:57,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,790 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,790 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,790 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6x4hv8cr
2023-12-13 06:40:57,791 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0656a6d9-e800-4221-aee2-698629db1bb2
2023-12-13 06:40:57,790 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,791 - distributed.worker - INFO - Starting Worker plugin PreImport-22d36682-b2b6-4b72-995b-4f229410909c
2023-12-13 06:40:57,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39821a3f-8932-462a-8b20-cd5709d0abbf
2023-12-13 06:40:57,794 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42961
2023-12-13 06:40:57,795 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42961
2023-12-13 06:40:57,795 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35833
2023-12-13 06:40:57,795 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,795 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,796 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:40:57,796 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:40:57,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4shrur5h
2023-12-13 06:40:57,797 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14561276-7493-483f-9a89-275cacba276c
2023-12-13 06:40:57,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43481', status: init, memory: 0, processing: 0>
2023-12-13 06:40:57,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43481
2023-12-13 06:40:57,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60836
2023-12-13 06:40:57,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:57,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,801 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:57,978 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39875', status: init, memory: 0, processing: 0>
2023-12-13 06:40:57,979 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39875
2023-12-13 06:40:57,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60854
2023-12-13 06:40:57,980 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42529', status: init, memory: 0, processing: 0>
2023-12-13 06:40:57,981 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42529
2023-12-13 06:40:57,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60838
2023-12-13 06:40:57,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:57,982 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34431', status: init, memory: 0, processing: 0>
2023-12-13 06:40:57,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:57,982 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,982 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,982 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34431
2023-12-13 06:40:57,982 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60856
2023-12-13 06:40:57,983 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,983 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:57,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:57,987 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:57,987 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:57,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:57,990 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:58,161 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45929', status: init, memory: 0, processing: 0>
2023-12-13 06:40:58,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45929
2023-12-13 06:40:58,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60872
2023-12-13 06:40:58,363 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:58,374 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:58,374 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:58,807 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87496491-c0f3-4d94-9467-072e57cd7251
2023-12-13 06:40:58,808 - distributed.worker - INFO - Starting Worker plugin PreImport-9170fd7c-608e-4586-a2ce-21c503811be3
2023-12-13 06:40:58,809 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,809 - distributed.worker - INFO - Starting Worker plugin PreImport-39f63ebb-ffc3-435a-a24d-598abb228ce1
2023-12-13 06:40:58,809 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,810 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,844 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45211', status: init, memory: 0, processing: 0>
2023-12-13 06:40:58,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45211
2023-12-13 06:40:58,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60880
2023-12-13 06:40:58,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:58,849 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:58,849 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:58,852 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42961', status: init, memory: 0, processing: 0>
2023-12-13 06:40:58,853 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42961
2023-12-13 06:40:58,853 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60892
2023-12-13 06:40:58,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38245', status: init, memory: 0, processing: 0>
2023-12-13 06:40:58,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:58,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38245
2023-12-13 06:40:58,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60896
2023-12-13 06:40:58,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:40:58,861 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:58,861 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:58,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:40:58,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:40:58,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:40:58,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,945 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,945 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,945 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:40:58,953 - distributed.scheduler - INFO - Remove client Client-8e74ee21-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:58,953 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60722; closing.
2023-12-13 06:40:58,954 - distributed.scheduler - INFO - Remove client Client-8e74ee21-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:58,954 - distributed.scheduler - INFO - Close client connection: Client-8e74ee21-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:40:58,955 - distributed.scheduler - INFO - Remove client Client-8dd686f3-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:58,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60700; closing.
2023-12-13 06:40:58,955 - distributed.scheduler - INFO - Remove client Client-8dd686f3-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:58,956 - distributed.scheduler - INFO - Close client connection: Client-8dd686f3-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37473'. Reason: nanny-close
2023-12-13 06:40:58,957 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42755'. Reason: nanny-close
2023-12-13 06:40:58,958 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,958 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42529. Reason: nanny-close
2023-12-13 06:40:58,959 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39575'. Reason: nanny-close
2023-12-13 06:40:58,959 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45211. Reason: nanny-close
2023-12-13 06:40:58,959 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43859'. Reason: nanny-close
2023-12-13 06:40:58,960 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45929. Reason: nanny-close
2023-12-13 06:40:58,960 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44369'. Reason: nanny-close
2023-12-13 06:40:58,960 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60838; closing.
2023-12-13 06:40:58,960 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43615'. Reason: nanny-close
2023-12-13 06:40:58,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39875. Reason: nanny-close
2023-12-13 06:40:58,961 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,961 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,961 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9612854')
2023-12-13 06:40:58,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43481. Reason: nanny-close
2023-12-13 06:40:58,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37891'. Reason: nanny-close
2023-12-13 06:40:58,961 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,962 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34431. Reason: nanny-close
2023-12-13 06:40:58,962 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39295'. Reason: nanny-close
2023-12-13 06:40:58,962 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:40:58,962 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,962 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42961. Reason: nanny-close
2023-12-13 06:40:58,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,963 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60880; closing.
2023-12-13 06:40:58,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38245. Reason: nanny-close
2023-12-13 06:40:58,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,964 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,964 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45211', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9644794')
2023-12-13 06:40:58,965 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60872; closing.
2023-12-13 06:40:58,965 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,965 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,965 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:40:58,965 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,966 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9659789')
2023-12-13 06:40:58,966 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60836; closing.
2023-12-13 06:40:58,966 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60854; closing.
2023-12-13 06:40:58,967 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43481', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9672954')
2023-12-13 06:40:58,967 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,967 - distributed.nanny - INFO - Worker closed
2023-12-13 06:40:58,967 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9676816')
2023-12-13 06:40:58,968 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60856; closing.
2023-12-13 06:40:58,968 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34431', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.968777')
2023-12-13 06:40:58,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60892; closing.
2023-12-13 06:40:58,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60896; closing.
2023-12-13 06:40:58,969 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9698157')
2023-12-13 06:40:58,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449658.9702327')
2023-12-13 06:40:58,970 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:40:58,970 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60896>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-13 06:40:58,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60892>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-13 06:41:00,678 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46399', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,679 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46399
2023-12-13 06:41:00,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59328
2023-12-13 06:41:00,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34975', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,710 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34975
2023-12-13 06:41:00,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59340
2023-12-13 06:41:00,720 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59328; closing.
2023-12-13 06:41:00,721 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.7212734')
2023-12-13 06:41:00,722 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33751', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,723 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33751
2023-12-13 06:41:00,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59342
2023-12-13 06:41:00,724 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34143', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34143
2023-12-13 06:41:00,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59350
2023-12-13 06:41:00,755 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40503', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40503
2023-12-13 06:41:00,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59358
2023-12-13 06:41:00,773 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59358; closing.
2023-12-13 06:41:00,773 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.7734692')
2023-12-13 06:41:00,774 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59340; closing.
2023-12-13 06:41:00,774 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59350; closing.
2023-12-13 06:41:00,775 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.7756288')
2023-12-13 06:41:00,776 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.776001')
2023-12-13 06:41:00,777 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59342; closing.
2023-12-13 06:41:00,778 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.7780602')
2023-12-13 06:41:00,778 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:00,792 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32833', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,793 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32833
2023-12-13 06:41:00,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59366
2023-12-13 06:41:00,794 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45919', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,794 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45919
2023-12-13 06:41:00,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59360
2023-12-13 06:41:00,796 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43277', status: init, memory: 0, processing: 0>
2023-12-13 06:41:00,797 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43277
2023-12-13 06:41:00,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59380
2023-12-13 06:41:00,825 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:00,825 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59380; closing.
2023-12-13 06:41:00,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43277', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.8258936')
2023-12-13 06:41:00,826 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59360; closing.
2023-12-13 06:41:00,826 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45919', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.826806')
2023-12-13 06:41:00,827 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:00,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59366; closing.
2023-12-13 06:41:00,827 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59360>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-13 06:41:00,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449660.8283184')
2023-12-13 06:41:00,828 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:00,829 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:00,830 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:00,831 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-13 06:41:03,299 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:03,304 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33169 instead
  warnings.warn(
2023-12-13 06:41:03,309 - distributed.scheduler - INFO - State start
2023-12-13 06:41:03,332 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:03,333 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:41:03,334 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33169/status
2023-12-13 06:41:03,334 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:03,365 - distributed.scheduler - INFO - Receive client connection: Client-95577715-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:03,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59548
2023-12-13 06:41:03,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37237'
2023-12-13 06:41:05,104 - distributed.scheduler - INFO - Receive client connection: Client-94a9f388-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:05,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59648
2023-12-13 06:41:05,272 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:05,272 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:05,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:06,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43463
2023-12-13 06:41:06,840 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43463
2023-12-13 06:41:06,840 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-13 06:41:06,840 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:06,840 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:06,841 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:06,841 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:41:06,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_oqjz6bd
2023-12-13 06:41:06,841 - distributed.worker - INFO - Starting Worker plugin PreImport-3f32260c-588e-4056-acb9-eb552597dd27
2023-12-13 06:41:06,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37d0a45c-9c62-4761-81a2-dd5be72752d6
2023-12-13 06:41:06,841 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffab7ecc-a550-488d-8478-8a2a5e7c3aa4
2023-12-13 06:41:06,842 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:06,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43463', status: init, memory: 0, processing: 0>
2023-12-13 06:41:06,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43463
2023-12-13 06:41:06,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59668
2023-12-13 06:41:06,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:06,888 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:06,888 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:06,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:06,946 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:06,949 - distributed.scheduler - INFO - Remove client Client-94a9f388-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:06,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59648; closing.
2023-12-13 06:41:06,949 - distributed.scheduler - INFO - Remove client Client-94a9f388-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:06,950 - distributed.scheduler - INFO - Close client connection: Client-94a9f388-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:06,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37237'. Reason: nanny-close
2023-12-13 06:41:06,951 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:06,952 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43463. Reason: nanny-close
2023-12-13 06:41:06,955 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:06,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59668; closing.
2023-12-13 06:41:06,955 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449666.9554317')
2023-12-13 06:41:06,955 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:06,956 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:08,417 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:08,417 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:08,418 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:08,420 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:08,420 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-13 06:41:12,888 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:12,893 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:41:12,897 - distributed.scheduler - INFO - State start
2023-12-13 06:41:12,919 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:12,920 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:41:12,921 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:41:12,921 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:12,933 - distributed.scheduler - INFO - Receive client connection: Client-9a712550-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:12,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49702
2023-12-13 06:41:13,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42781'
2023-12-13 06:41:13,074 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35811', status: init, memory: 0, processing: 0>
2023-12-13 06:41:13,075 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35811
2023-12-13 06:41:13,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49732
2023-12-13 06:41:13,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49732; closing.
2023-12-13 06:41:13,130 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449673.1299586')
2023-12-13 06:41:13,130 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:13,383 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40701', status: init, memory: 0, processing: 0>
2023-12-13 06:41:13,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40701
2023-12-13 06:41:13,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49738
2023-12-13 06:41:13,431 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49738; closing.
2023-12-13 06:41:13,432 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449673.4323773')
2023-12-13 06:41:13,432 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:14,629 - distributed.scheduler - INFO - Receive client connection: Client-95577715-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:14,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49740
2023-12-13 06:41:14,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:14,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:15,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:16,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44467
2023-12-13 06:41:16,266 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44467
2023-12-13 06:41:16,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38437
2023-12-13 06:41:16,266 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:16,266 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:16,266 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:16,266 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:41:16,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-76gyimjm
2023-12-13 06:41:16,266 - distributed.worker - INFO - Starting Worker plugin PreImport-6631f194-9d17-434d-aa84-2530ac3b4025
2023-12-13 06:41:16,267 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4f6712b-29b3-4832-bd73-bce373231691
2023-12-13 06:41:16,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7cb6071-3ab0-4531-8ba3-5caacc81a73d
2023-12-13 06:41:16,268 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:16,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44467', status: init, memory: 0, processing: 0>
2023-12-13 06:41:16,299 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44467
2023-12-13 06:41:16,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49746
2023-12-13 06:41:16,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:16,300 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:16,301 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:16,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:16,314 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:16,317 - distributed.scheduler - INFO - Remove client Client-9a712550-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:16,317 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49702; closing.
2023-12-13 06:41:16,317 - distributed.scheduler - INFO - Remove client Client-9a712550-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:16,318 - distributed.scheduler - INFO - Close client connection: Client-9a712550-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:16,319 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42781'. Reason: nanny-close
2023-12-13 06:41:16,348 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:16,349 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44467. Reason: nanny-close
2023-12-13 06:41:16,352 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49746; closing.
2023-12-13 06:41:16,352 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:16,352 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44467', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449676.3523772')
2023-12-13 06:41:16,352 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:16,353 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:17,435 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:17,436 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:17,436 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:17,438 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:17,439 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-13 06:41:19,798 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:19,803 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:41:19,806 - distributed.scheduler - INFO - State start
2023-12-13 06:41:19,828 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:19,829 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:41:19,830 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:41:19,830 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:20,774 - distributed.scheduler - INFO - Receive client connection: Client-a034de15-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:20,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45854
2023-12-13 06:41:24,492 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45826'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45826>: Stream is closed
2023-12-13 06:41:24,755 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:24,755 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:24,756 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:24,758 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:24,758 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-13 06:41:27,152 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:27,157 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:41:27,161 - distributed.scheduler - INFO - State start
2023-12-13 06:41:27,366 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:27,367 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-13 06:41:27,369 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:41:27,369 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:27,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33925'
2023-12-13 06:41:28,357 - distributed.scheduler - INFO - Receive client connection: Client-a2e3794d-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:28,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34986
2023-12-13 06:41:29,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:29,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:29,204 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:29,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46773
2023-12-13 06:41:29,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46773
2023-12-13 06:41:29,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33933
2023-12-13 06:41:29,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-13 06:41:29,961 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:29,961 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:29,961 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:41:29,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-orua6icv
2023-12-13 06:41:29,962 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b121bb28-6dfd-47f3-a446-ad8288329b4d
2023-12-13 06:41:29,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d153dce-c025-43c2-95e9-2571de8805ec
2023-12-13 06:41:29,963 - distributed.worker - INFO - Starting Worker plugin PreImport-a54053ac-abea-4b61-a12e-7d8fdfcc6c6e
2023-12-13 06:41:29,963 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:29,987 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46773', status: init, memory: 0, processing: 0>
2023-12-13 06:41:29,988 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46773
2023-12-13 06:41:29,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40568
2023-12-13 06:41:29,989 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:29,990 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-13 06:41:29,990 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:29,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-13 06:41:30,030 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:30,033 - distributed.scheduler - INFO - Remove client Client-a2e3794d-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:30,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34986; closing.
2023-12-13 06:41:30,034 - distributed.scheduler - INFO - Remove client Client-a2e3794d-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:30,034 - distributed.scheduler - INFO - Close client connection: Client-a2e3794d-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:30,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33925'. Reason: nanny-close
2023-12-13 06:41:30,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:30,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46773. Reason: nanny-close
2023-12-13 06:41:30,038 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40568; closing.
2023-12-13 06:41:30,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-13 06:41:30,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46773', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449690.038868')
2023-12-13 06:41:30,039 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:30,040 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:31,151 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:31,152 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:31,153 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:31,154 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-13 06:41:31,154 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-13 06:41:33,436 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:33,440 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:41:33,444 - distributed.scheduler - INFO - State start
2023-12-13 06:41:33,466 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:33,468 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:41:33,468 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:41:33,469 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:33,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39301'
2023-12-13 06:41:33,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42159'
2023-12-13 06:41:33,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34735'
2023-12-13 06:41:33,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33941'
2023-12-13 06:41:33,664 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40199', status: init, memory: 0, processing: 0>
2023-12-13 06:41:33,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41037'
2023-12-13 06:41:33,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34277'
2023-12-13 06:41:33,681 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40199
2023-12-13 06:41:33,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41450
2023-12-13 06:41:33,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35929'
2023-12-13 06:41:33,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34643'
2023-12-13 06:41:33,719 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41450; closing.
2023-12-13 06:41:33,720 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449693.7203748')
2023-12-13 06:41:33,720 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:34,286 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33103', status: init, memory: 0, processing: 0>
2023-12-13 06:41:34,286 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33103
2023-12-13 06:41:34,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41456
2023-12-13 06:41:34,332 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41456; closing.
2023-12-13 06:41:34,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449694.3325522')
2023-12-13 06:41:34,332 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:34,367 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34741', status: init, memory: 0, processing: 0>
2023-12-13 06:41:34,368 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34741
2023-12-13 06:41:34,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41464
2023-12-13 06:41:34,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41464; closing.
2023-12-13 06:41:34,382 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449694.3819714')
2023-12-13 06:41:34,382 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:34,437 - distributed.scheduler - INFO - Receive client connection: Client-a6a9422b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:34,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41476
2023-12-13 06:41:35,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,729 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,729 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,735 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,736 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,736 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,741 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,744 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,744 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:35,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:35,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:35,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:37,134 - distributed.scheduler - INFO - Receive client connection: Client-a9eb5118-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:37,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41496
2023-12-13 06:41:38,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45645
2023-12-13 06:41:38,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45645
2023-12-13 06:41:38,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44617
2023-12-13 06:41:38,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,519 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,519 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,520 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-08cvy1kh
2023-12-13 06:41:38,520 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2ad64cf-c427-4198-8bfa-6371113168c1
2023-12-13 06:41:38,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40671
2023-12-13 06:41:38,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40671
2023-12-13 06:41:38,520 - distributed.worker - INFO - Starting Worker plugin PreImport-1ab44c73-afde-432a-81fd-6243dde10b52
2023-12-13 06:41:38,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36735
2023-12-13 06:41:38,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-623c20cd-fd4e-47de-a2d4-2dd191055683
2023-12-13 06:41:38,520 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,520 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,521 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,521 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-610gnogz
2023-12-13 06:41:38,521 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efc3bf7b-585d-4380-99fb-e5d04c2dd70f
2023-12-13 06:41:38,522 - distributed.worker - INFO - Starting Worker plugin PreImport-ce579f0d-5f61-4e17-9af3-401282488129
2023-12-13 06:41:38,522 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03447a31-4c4e-43d6-b101-676aeb04e7f2
2023-12-13 06:41:38,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37647
2023-12-13 06:41:38,632 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37647
2023-12-13 06:41:38,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35599
2023-12-13 06:41:38,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,633 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,633 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,633 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,633 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v2bjqgph
2023-12-13 06:41:38,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-972e26c7-3194-4952-af3b-0af45f41a01a
2023-12-13 06:41:38,634 - distributed.worker - INFO - Starting Worker plugin PreImport-560997e8-84bf-4de2-a2b2-893d76e2e071
2023-12-13 06:41:38,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c336e797-0c88-4be4-a389-7b6b8c683cdd
2023-12-13 06:41:38,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35859
2023-12-13 06:41:38,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35859
2023-12-13 06:41:38,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41341
2023-12-13 06:41:38,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,639 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,639 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,640 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,640 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yfu8q3vk
2023-12-13 06:41:38,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c6bbc70-ed2e-4dcf-b6f8-5cca459811be
2023-12-13 06:41:38,641 - distributed.worker - INFO - Starting Worker plugin PreImport-e9e5d425-ddf8-4dc2-843c-06ae38c14685
2023-12-13 06:41:38,641 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be593fce-f761-476d-bfd6-176152015418
2023-12-13 06:41:38,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41513
2023-12-13 06:41:38,642 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41513
2023-12-13 06:41:38,643 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38273
2023-12-13 06:41:38,643 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,643 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,643 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,643 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ql38x8bg
2023-12-13 06:41:38,644 - distributed.worker - INFO - Starting Worker plugin PreImport-57a9f2c8-9114-4070-a0e9-aabbe3f11cf9
2023-12-13 06:41:38,644 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d1293c5-8edc-4284-85ad-507f7b3a5376
2023-12-13 06:41:38,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daab465c-658e-4f3d-94fd-69abcbffbdb7
2023-12-13 06:41:38,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33523
2023-12-13 06:41:38,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33523
2023-12-13 06:41:38,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44021
2023-12-13 06:41:38,647 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,647 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,647 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,647 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9jn8dmhb
2023-12-13 06:41:38,648 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f307b25f-0ad4-4da8-a066-0ec88be71432
2023-12-13 06:41:38,648 - distributed.worker - INFO - Starting Worker plugin PreImport-144d8d95-b780-4fd7-8e06-781de90c38a8
2023-12-13 06:41:38,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-009063a1-892b-4c79-be0f-f6a2d4a6d081
2023-12-13 06:41:38,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43929
2023-12-13 06:41:38,657 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43929
2023-12-13 06:41:38,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46377
2023-12-13 06:41:38,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,657 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,657 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2uqt7vym
2023-12-13 06:41:38,658 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-544aaa84-f992-4dca-b795-028a874e8065
2023-12-13 06:41:38,659 - distributed.worker - INFO - Starting Worker plugin PreImport-ca794b9c-a0ef-40cc-bf7b-9ad8d16eb1d7
2023-12-13 06:41:38,659 - distributed.worker - INFO - Starting Worker plugin RMMSetup-540f4b82-4758-4372-9a47-0aca6f87577a
2023-12-13 06:41:38,834 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,843 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43461
2023-12-13 06:41:38,844 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43461
2023-12-13 06:41:38,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35609
2023-12-13 06:41:38,844 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,844 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,844 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:38,845 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-13 06:41:38,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uu5fz6f5
2023-12-13 06:41:38,845 - distributed.worker - INFO - Starting Worker plugin PreImport-886c545d-52fe-404f-8431-43ceedf8cd04
2023-12-13 06:41:38,845 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db462db8-9dd9-4c48-aba0-5543ff3a0e8e
2023-12-13 06:41:38,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e410d289-961a-4723-8206-05def08f188a
2023-12-13 06:41:38,866 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45645', status: init, memory: 0, processing: 0>
2023-12-13 06:41:38,867 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45645
2023-12-13 06:41:38,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41514
2023-12-13 06:41:38,867 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,868 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:38,872 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,872 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:38,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40671', status: init, memory: 0, processing: 0>
2023-12-13 06:41:38,914 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40671
2023-12-13 06:41:38,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41518
2023-12-13 06:41:38,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:38,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:38,924 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:38,981 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,981 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,981 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,984 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:38,992 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,010 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33523', status: init, memory: 0, processing: 0>
2023-12-13 06:41:39,012 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33523
2023-12-13 06:41:39,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41532
2023-12-13 06:41:39,013 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37647', status: init, memory: 0, processing: 0>
2023-12-13 06:41:39,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:39,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37647
2023-12-13 06:41:39,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41538
2023-12-13 06:41:39,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:39,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41513', status: init, memory: 0, processing: 0>
2023-12-13 06:41:39,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41513
2023-12-13 06:41:39,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41548
2023-12-13 06:41:39,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:39,016 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:39,017 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,018 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:39,018 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:39,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:39,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:39,021 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:39,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43929', status: init, memory: 0, processing: 0>
2023-12-13 06:41:39,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43929
2023-12-13 06:41:39,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41560
2023-12-13 06:41:39,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:39,029 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35859', status: init, memory: 0, processing: 0>
2023-12-13 06:41:39,030 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35859
2023-12-13 06:41:39,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41566
2023-12-13 06:41:39,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:39,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:39,033 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:39,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:39,036 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:39,047 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43461', status: init, memory: 0, processing: 0>
2023-12-13 06:41:39,048 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43461
2023-12-13 06:41:39,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41574
2023-12-13 06:41:39,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:39,050 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:39,050 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:39,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:39,155 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,156 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,163 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,163 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-13 06:41:39,174 - distributed.scheduler - INFO - Remove client Client-a9eb5118-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:39,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41496; closing.
2023-12-13 06:41:39,175 - distributed.scheduler - INFO - Remove client Client-a9eb5118-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:39,180 - distributed.scheduler - INFO - Close client connection: Client-a9eb5118-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:39,181 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,181 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,181 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,182 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,182 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,182 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,182 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,182 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:39,187 - distributed.scheduler - INFO - Remove client Client-a6a9422b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:39,187 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41476; closing.
2023-12-13 06:41:39,187 - distributed.scheduler - INFO - Remove client Client-a6a9422b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:39,188 - distributed.scheduler - INFO - Close client connection: Client-a6a9422b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:39,188 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39301'. Reason: nanny-close
2023-12-13 06:41:39,189 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42159'. Reason: nanny-close
2023-12-13 06:41:39,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,190 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43461. Reason: nanny-close
2023-12-13 06:41:39,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34735'. Reason: nanny-close
2023-12-13 06:41:39,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,191 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43929. Reason: nanny-close
2023-12-13 06:41:39,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33941'. Reason: nanny-close
2023-12-13 06:41:39,191 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,191 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45645. Reason: nanny-close
2023-12-13 06:41:39,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41037'. Reason: nanny-close
2023-12-13 06:41:39,192 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37647. Reason: nanny-close
2023-12-13 06:41:39,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34277'. Reason: nanny-close
2023-12-13 06:41:39,192 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40671. Reason: nanny-close
2023-12-13 06:41:39,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41574; closing.
2023-12-13 06:41:39,193 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35929'. Reason: nanny-close
2023-12-13 06:41:39,193 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,193 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.1933198')
2023-12-13 06:41:39,193 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,193 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,193 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35859. Reason: nanny-close
2023-12-13 06:41:39,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34643'. Reason: nanny-close
2023-12-13 06:41:39,193 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:39,194 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33523. Reason: nanny-close
2023-12-13 06:41:39,194 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,194 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41513. Reason: nanny-close
2023-12-13 06:41:39,194 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,195 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,195 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41538; closing.
2023-12-13 06:41:39,195 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41514; closing.
2023-12-13 06:41:39,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41560; closing.
2023-12-13 06:41:39,196 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,196 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,196 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,196 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37647', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.1966233')
2023-12-13 06:41:39,197 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,197 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.197155')
2023-12-13 06:41:39,197 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:39,197 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,197 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.1976998')
2023-12-13 06:41:39,198 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41518; closing.
2023-12-13 06:41:39,198 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:39,199 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40671', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.199202')
2023-12-13 06:41:39,199 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41566; closing.
2023-12-13 06:41:39,200 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.2008176')
2023-12-13 06:41:39,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41532; closing.
2023-12-13 06:41:39,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41548; closing.
2023-12-13 06:41:39,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.2022808')
2023-12-13 06:41:39,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449699.2028983')
2023-12-13 06:41:39,203 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:40,806 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:40,807 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:40,807 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:40,810 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:40,810 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-13 06:41:43,383 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:43,388 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:41:43,391 - distributed.scheduler - INFO - State start
2023-12-13 06:41:43,467 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:43,468 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:41:43,469 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:41:43,469 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:43,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42803'
2023-12-13 06:41:43,556 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45839', status: init, memory: 0, processing: 0>
2023-12-13 06:41:43,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45839
2023-12-13 06:41:43,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46474
2023-12-13 06:41:43,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46474; closing.
2023-12-13 06:41:43,594 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449703.5941348')
2023-12-13 06:41:43,594 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:44,011 - distributed.scheduler - INFO - Receive client connection: Client-ac8f94bc-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:44,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46484
2023-12-13 06:41:44,204 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33095', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,205 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33095
2023-12-13 06:41:44,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46498
2023-12-13 06:41:44,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36765', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36765
2023-12-13 06:41:44,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46492
2023-12-13 06:41:44,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43953', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43953
2023-12-13 06:41:44,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46514
2023-12-13 06:41:44,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43253', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,213 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43253
2023-12-13 06:41:44,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46508
2023-12-13 06:41:44,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39595', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,220 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39595
2023-12-13 06:41:44,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46528
2023-12-13 06:41:44,225 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45473', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45473
2023-12-13 06:41:44,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46536
2023-12-13 06:41:44,226 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44219', status: init, memory: 0, processing: 0>
2023-12-13 06:41:44,226 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44219
2023-12-13 06:41:44,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46550
2023-12-13 06:41:44,236 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46528; closing.
2023-12-13 06:41:44,237 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.2370064')
2023-12-13 06:41:44,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46498; closing.
2023-12-13 06:41:44,239 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46492; closing.
2023-12-13 06:41:44,239 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.2398572')
2023-12-13 06:41:44,240 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46508; closing.
2023-12-13 06:41:44,240 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.2404275')
2023-12-13 06:41:44,241 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43253', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.2409813')
2023-12-13 06:41:44,241 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46514; closing.
2023-12-13 06:41:44,241 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43953', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.2418962')
2023-12-13 06:41:44,286 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46536; closing.
2023-12-13 06:41:44,287 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45473', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.2870276')
2023-12-13 06:41:44,287 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46550; closing.
2023-12-13 06:41:44,288 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449704.288181')
2023-12-13 06:41:44,288 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:45,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:45,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:45,427 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:45,730 - distributed.scheduler - INFO - Receive client connection: Client-af14fc7d-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:45,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46556
2023-12-13 06:41:46,357 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42069
2023-12-13 06:41:46,358 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42069
2023-12-13 06:41:46,359 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44325
2023-12-13 06:41:46,359 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:46,359 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:46,359 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:46,359 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:41:46,359 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_nqpr6eo
2023-12-13 06:41:46,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e75aa1e-fd76-423a-9767-8d23d3d671bf
2023-12-13 06:41:46,361 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eca8e9ca-8e31-4cd8-9151-9c815042465c
2023-12-13 06:41:46,463 - distributed.worker - INFO - Starting Worker plugin PreImport-7edb40cf-4dfb-4316-abd4-a718fc772ebc
2023-12-13 06:41:46,463 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:46,492 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42069', status: init, memory: 0, processing: 0>
2023-12-13 06:41:46,493 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42069
2023-12-13 06:41:46,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46564
2023-12-13 06:41:46,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:46,495 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:46,495 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:46,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:46,552 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:46,556 - distributed.scheduler - INFO - Remove client Client-af14fc7d-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:46,556 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:41:46,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46556; closing.
2023-12-13 06:41:46,556 - distributed.scheduler - INFO - Remove client Client-af14fc7d-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:46,556 - distributed.scheduler - INFO - Close client connection: Client-af14fc7d-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:46,559 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:46,561 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:46,563 - distributed.scheduler - INFO - Remove client Client-ac8f94bc-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:46,563 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46484; closing.
2023-12-13 06:41:46,564 - distributed.scheduler - INFO - Remove client Client-ac8f94bc-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:46,564 - distributed.scheduler - INFO - Close client connection: Client-ac8f94bc-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:46,565 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42803'. Reason: nanny-close
2023-12-13 06:41:46,565 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:46,567 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42069. Reason: nanny-close
2023-12-13 06:41:46,568 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:46,568 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46564; closing.
2023-12-13 06:41:46,569 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42069', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449706.568963')
2023-12-13 06:41:46,569 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:46,570 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:47,631 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:47,631 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:47,632 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:47,633 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:47,633 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-13 06:41:49,817 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:49,821 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-13 06:41:49,825 - distributed.scheduler - INFO - State start
2023-12-13 06:41:49,848 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-13 06:41:49,849 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-13 06:41:49,850 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-13 06:41:49,850 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-13 06:41:49,939 - distributed.scheduler - INFO - Receive client connection: Client-b0704d1b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:49,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48504
2023-12-13 06:41:49,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35321'
2023-12-13 06:41:50,916 - distributed.scheduler - INFO - Receive client connection: Client-b1125882-9982-11ee-b4f0-d8c49764f6bb
2023-12-13 06:41:50,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48536
2023-12-13 06:41:51,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-13 06:41:51,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-13 06:41:51,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-13 06:41:52,633 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36889
2023-12-13 06:41:52,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36889
2023-12-13 06:41:52,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41053
2023-12-13 06:41:52,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-13 06:41:52,634 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:52,634 - distributed.worker - INFO -               Threads:                          1
2023-12-13 06:41:52,634 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-13 06:41:52,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ymu9fd0
2023-12-13 06:41:52,635 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d181026f-bf7c-4765-b4d8-3e38f4fd537c
2023-12-13 06:41:52,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5eafdef9-c8fc-465d-9092-6bc00d9afcf8
2023-12-13 06:41:52,740 - distributed.worker - INFO - Starting Worker plugin PreImport-eec2154f-ad96-45c8-a832-7adea1fefd27
2023-12-13 06:41:52,741 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:52,775 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36889', status: init, memory: 0, processing: 0>
2023-12-13 06:41:52,776 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36889
2023-12-13 06:41:52,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48560
2023-12-13 06:41:52,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-13 06:41:52,779 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-13 06:41:52,779 - distributed.worker - INFO - -------------------------------------------------
2023-12-13 06:41:52,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-13 06:41:52,805 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-13 06:41:52,810 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-13 06:41:52,814 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:52,816 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-13 06:41:52,818 - distributed.scheduler - INFO - Remove client Client-b0704d1b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:52,818 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48504; closing.
2023-12-13 06:41:52,818 - distributed.scheduler - INFO - Remove client Client-b0704d1b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:52,819 - distributed.scheduler - INFO - Close client connection: Client-b0704d1b-9982-11ee-b688-d8c49764f6bb
2023-12-13 06:41:52,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35321'. Reason: nanny-close
2023-12-13 06:41:52,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-13 06:41:52,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36889. Reason: nanny-close
2023-12-13 06:41:52,824 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-13 06:41:52,824 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48560; closing.
2023-12-13 06:41:52,824 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1702449712.824467')
2023-12-13 06:41:52,824 - distributed.scheduler - INFO - Lost all workers
2023-12-13 06:41:52,825 - distributed.nanny - INFO - Worker closed
2023-12-13 06:41:54,036 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-13 06:41:54,037 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-13 06:41:54,037 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-13 06:41:54,039 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-13 06:41:54,039 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41279 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44621 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39879 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] [1702449828.263031] [dgx13:68663:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:47688) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35445 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38469 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45515 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35631 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43887 instead
  warnings.warn(
2023-12-13 06:46:06,529 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-13 06:46:06,531 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:43273', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43773 instead
  warnings.warn(
2023-12-13 06:46:36,185 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-13 06:46:36,186 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:37039', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35997 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39945 instead
  warnings.warn(
[1702450033.199698] [dgx13:71788:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:48598) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41005 instead
  warnings.warn(
[1702450073.335046] [dgx13:72604:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:48746) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41359 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36373 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33755 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42257 instead
  warnings.warn(
2023-12-13 06:49:20,781 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-13 06:49:20,786 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:44557', name: 2, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44259 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40539 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37199 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42733 instead
  warnings.warn(
2023-12-13 06:51:59,796 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:52:00,047 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:52:00,073 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:54947'.
2023-12-13 06:52:00,074 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:54947'. Shutting down.
2023-12-13 06:52:00,104 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6b54005fa0>>, <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-13 06:52:02,108 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
