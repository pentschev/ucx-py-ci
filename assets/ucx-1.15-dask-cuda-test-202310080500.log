============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-08 05:37:03,294 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:03,298 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-08 05:37:03,301 - distributed.scheduler - INFO - State start
2023-10-08 05:37:03,338 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:03,339 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-08 05:37:03,340 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-08 05:37:03,340 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:03,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35005'
2023-10-08 05:37:03,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35915'
2023-10-08 05:37:03,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42131'
2023-10-08 05:37:03,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37681'
2023-10-08 05:37:03,473 - distributed.scheduler - INFO - Receive client connection: Client-b4c044d9-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:03,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46344
2023-10-08 05:37:05,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:05,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:05,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:05,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:05,177 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:05,177 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:05,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:05,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:05,182 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:05,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:05,216 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:05,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-08 05:37:05,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38701
2023-10-08 05:37:05,248 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38701
2023-10-08 05:37:05,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43791
2023-10-08 05:37:05,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-08 05:37:05,248 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:05,248 - distributed.worker - INFO -               Threads:                          4
2023-10-08 05:37:05,248 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-08 05:37:05,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-e8c2rlu3
2023-10-08 05:37:05,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb92a119-6856-4266-911c-fbda94c2f6be
2023-10-08 05:37:05,249 - distributed.worker - INFO - Starting Worker plugin PreImport-ceb506af-0822-4f27-b5d5-fe90c5f2c116
2023-10-08 05:37:05,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d32e278-a9da-45c6-bf28-ea3c0ddd9858
2023-10-08 05:37:05,249 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:05,955 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38701', status: init, memory: 0, processing: 0>
2023-10-08 05:37:05,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38701
2023-10-08 05:37:05,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46372
2023-10-08 05:37:05,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:05,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-08 05:37:05,958 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:05,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-08 05:37:06,831 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36811
2023-10-08 05:37:06,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36811
2023-10-08 05:37:06,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36685
2023-10-08 05:37:06,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-08 05:37:06,832 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,832 - distributed.worker - INFO -               Threads:                          4
2023-10-08 05:37:06,832 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-08 05:37:06,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-w2r6ewuh
2023-10-08 05:37:06,833 - distributed.worker - INFO - Starting Worker plugin PreImport-3c23f084-3cf7-4572-88ca-2652d1184a56
2023-10-08 05:37:06,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13238a1a-36b4-4ed5-8008-205d88a7319c
2023-10-08 05:37:06,833 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fa7cee7-57c3-4374-be89-d230130227f2
2023-10-08 05:37:06,833 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,834 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43621
2023-10-08 05:37:06,834 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43621
2023-10-08 05:37:06,834 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39557
2023-10-08 05:37:06,835 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-08 05:37:06,835 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,835 - distributed.worker - INFO -               Threads:                          4
2023-10-08 05:37:06,835 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-08 05:37:06,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-eb1642be
2023-10-08 05:37:06,835 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3825d586-f63f-4d33-9fd6-3e7552154416
2023-10-08 05:37:06,835 - distributed.worker - INFO - Starting Worker plugin PreImport-cdd039dd-f42b-4eb6-aba4-d38309da78b8
2023-10-08 05:37:06,835 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7de2b9aa-39c6-4a80-a739-a4c9a8022bf1
2023-10-08 05:37:06,836 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42513
2023-10-08 05:37:06,837 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42513
2023-10-08 05:37:06,837 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39193
2023-10-08 05:37:06,837 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-08 05:37:06,838 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,838 - distributed.worker - INFO -               Threads:                          4
2023-10-08 05:37:06,838 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-08 05:37:06,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-43bx7xr_
2023-10-08 05:37:06,838 - distributed.worker - INFO - Starting Worker plugin PreImport-7433aead-7a36-42e1-9538-8cf7c45e6351
2023-10-08 05:37:06,838 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a20e8f2-f50e-4e6f-9018-e6738a9a9d9f
2023-10-08 05:37:06,839 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d562f2a-97a8-4965-b84d-f339071d1ce0
2023-10-08 05:37:06,844 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43621', status: init, memory: 0, processing: 0>
2023-10-08 05:37:06,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43621
2023-10-08 05:37:06,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46398
2023-10-08 05:37:06,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:06,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-08 05:37:06,859 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-08 05:37:06,877 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36811', status: init, memory: 0, processing: 0>
2023-10-08 05:37:06,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36811
2023-10-08 05:37:06,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46386
2023-10-08 05:37:06,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:06,881 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-08 05:37:06,881 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-08 05:37:06,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42513', status: init, memory: 0, processing: 0>
2023-10-08 05:37:06,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42513
2023-10-08 05:37:06,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46400
2023-10-08 05:37:06,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:06,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-08 05:37:06,889 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:06,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-08 05:37:06,938 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-08 05:37:06,938 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-08 05:37:06,939 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-08 05:37:06,939 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-08 05:37:06,943 - distributed.scheduler - INFO - Remove client Client-b4c044d9-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:06,943 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46344; closing.
2023-10-08 05:37:06,944 - distributed.scheduler - INFO - Remove client Client-b4c044d9-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:06,944 - distributed.scheduler - INFO - Close client connection: Client-b4c044d9-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:06,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35005'. Reason: nanny-close
2023-10-08 05:37:06,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:06,946 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35915'. Reason: nanny-close
2023-10-08 05:37:06,946 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:06,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42131'. Reason: nanny-close
2023-10-08 05:37:06,946 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42513. Reason: nanny-close
2023-10-08 05:37:06,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:06,947 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43621. Reason: nanny-close
2023-10-08 05:37:06,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37681'. Reason: nanny-close
2023-10-08 05:37:06,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:06,948 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36811. Reason: nanny-close
2023-10-08 05:37:06,948 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38701. Reason: nanny-close
2023-10-08 05:37:06,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46398; closing.
2023-10-08 05:37:06,949 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-08 05:37:06,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743426.9494312')
2023-10-08 05:37:06,949 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-08 05:37:06,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-08 05:37:06,950 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:06,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-08 05:37:06,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46372; closing.
2023-10-08 05:37:06,951 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46386; closing.
2023-10-08 05:37:06,951 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46400; closing.
2023-10-08 05:37:06,951 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:06,951 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:06,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743426.951766')
2023-10-08 05:37:06,952 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743426.9520645')
2023-10-08 05:37:06,952 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:06,952 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743426.9524617')
2023-10-08 05:37:06,952 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:37:08,112 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:37:08,112 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:37:08,113 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:37:08,113 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-08 05:37:08,114 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-08 05:37:10,239 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:10,243 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-08 05:37:10,247 - distributed.scheduler - INFO - State start
2023-10-08 05:37:10,268 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:10,269 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:37:10,270 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-08 05:37:10,270 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:10,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36327'
2023-10-08 05:37:10,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36673'
2023-10-08 05:37:10,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46869'
2023-10-08 05:37:10,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37751'
2023-10-08 05:37:10,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35551'
2023-10-08 05:37:10,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46437'
2023-10-08 05:37:10,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42381'
2023-10-08 05:37:10,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39019'
2023-10-08 05:37:11,796 - distributed.scheduler - INFO - Receive client connection: Client-b8cc6529-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:11,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60684
2023-10-08 05:37:12,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,719 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,730 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,730 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,736 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,736 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,739 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:12,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:12,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:12,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:16,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46457
2023-10-08 05:37:16,354 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46457
2023-10-08 05:37:16,354 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33743
2023-10-08 05:37:16,354 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,354 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,354 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,354 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,354 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bvawkk5v
2023-10-08 05:37:16,355 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d70d5e63-be08-4db1-a071-067b4524d7c4
2023-10-08 05:37:16,355 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42139
2023-10-08 05:37:16,356 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42139
2023-10-08 05:37:16,356 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35553
2023-10-08 05:37:16,356 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,356 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,356 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,356 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,356 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ntyq93s
2023-10-08 05:37:16,356 - distributed.worker - INFO - Starting Worker plugin PreImport-45e0fd12-0f8b-4ab1-b402-f5641640ac08
2023-10-08 05:37:16,357 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a8e8c5f-f148-4b80-9c37-c86c2a7bfd5d
2023-10-08 05:37:16,357 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5de54042-37a5-4f93-9a62-9aebb50f8a97
2023-10-08 05:37:16,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36421
2023-10-08 05:37:16,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36421
2023-10-08 05:37:16,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34385
2023-10-08 05:37:16,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,366 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,366 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,366 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,366 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-arhsyhlk
2023-10-08 05:37:16,367 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55af3700-0df2-47e7-a9df-f16e39459cb6
2023-10-08 05:37:16,368 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37475
2023-10-08 05:37:16,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37475
2023-10-08 05:37:16,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36379
2023-10-08 05:37:16,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,369 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,369 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pm1x0ndb
2023-10-08 05:37:16,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60335544-939c-4be8-a233-821d019f4e50
2023-10-08 05:37:16,379 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40137
2023-10-08 05:37:16,379 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40137
2023-10-08 05:37:16,379 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39107
2023-10-08 05:37:16,380 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,380 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,380 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,380 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,380 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p5c50yk7
2023-10-08 05:37:16,380 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7cb772e7-c70d-4c65-8cf6-a30ab24406f8
2023-10-08 05:37:16,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34543
2023-10-08 05:37:16,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34543
2023-10-08 05:37:16,389 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45619
2023-10-08 05:37:16,389 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,389 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,389 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-67ilf7h9
2023-10-08 05:37:16,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5dc3af82-78c2-4f79-99e0-01a2e8c59aaa
2023-10-08 05:37:16,390 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41095
2023-10-08 05:37:16,391 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41095
2023-10-08 05:37:16,391 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43161
2023-10-08 05:37:16,391 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,391 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,391 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,391 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,391 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aieoqjbq
2023-10-08 05:37:16,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-924d7d90-4cda-4be6-8ad7-b1ed25805fb6
2023-10-08 05:37:16,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45713
2023-10-08 05:37:16,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45713
2023-10-08 05:37:16,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41635
2023-10-08 05:37:16,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,392 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,393 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:16,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:16,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hynifto6
2023-10-08 05:37:16,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0f58a3d7-cd65-4df8-994b-831c6e64c94e
2023-10-08 05:37:16,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a7872a9-4821-4f47-877a-050edabacb07
2023-10-08 05:37:16,539 - distributed.worker - INFO - Starting Worker plugin PreImport-b33c070e-d7f1-4170-85c5-01428ad3f0ba
2023-10-08 05:37:16,539 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,539 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-436220d9-efd9-48ee-b78c-30f19c71e76a
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin PreImport-a8faa26e-a160-49e6-bebe-a50fd8149b4a
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91f4e04d-6cb7-432e-b1be-e669a2c5670b
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8f7b53a-e2f9-489f-853e-9a20d2a4afd1
2023-10-08 05:37:16,540 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin PreImport-fc5e9bc9-1454-4504-bcd8-ce0d207a3e5c
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin PreImport-680b5961-caea-418e-9711-656c979c98c7
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin PreImport-aad28deb-9179-44b2-9642-5ca788d304f7
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-938b6f94-d5bd-46eb-ac9d-f6bab0c06848
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19586fc2-5cf3-47c4-90f1-e8e2a998bdde
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin PreImport-661dd0d9-d5a4-4e82-91d5-5847bf3b7851
2023-10-08 05:37:16,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5170d583-9cd9-42f2-9e96-d352f274859e
2023-10-08 05:37:16,540 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,540 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,541 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,541 - distributed.worker - INFO - Starting Worker plugin PreImport-c8be264d-ed52-4138-b7f0-bdfda9c502a4
2023-10-08 05:37:16,542 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,544 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46457', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46457
2023-10-08 05:37:16,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60710
2023-10-08 05:37:16,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,567 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36421', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,567 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,568 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36421
2023-10-08 05:37:16,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60712
2023-10-08 05:37:16,569 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34543', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34543
2023-10-08 05:37:16,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60720
2023-10-08 05:37:16,570 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,570 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,570 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42139', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,571 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42139
2023-10-08 05:37:16,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60708
2023-10-08 05:37:16,571 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,571 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,572 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,573 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,574 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45713', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,574 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45713
2023-10-08 05:37:16,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60734
2023-10-08 05:37:16,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,577 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,577 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40137', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,583 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40137
2023-10-08 05:37:16,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60724
2023-10-08 05:37:16,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41095', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,586 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41095
2023-10-08 05:37:16,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60764
2023-10-08 05:37:16,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,589 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,590 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37475', status: init, memory: 0, processing: 0>
2023-10-08 05:37:16,591 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37475
2023-10-08 05:37:16,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60750
2023-10-08 05:37:16,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,592 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:16,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:16,593 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:16,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:16,700 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,700 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,701 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,701 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,701 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,701 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,701 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,701 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:16,705 - distributed.scheduler - INFO - Remove client Client-b8cc6529-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:16,705 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60684; closing.
2023-10-08 05:37:16,705 - distributed.scheduler - INFO - Remove client Client-b8cc6529-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:16,706 - distributed.scheduler - INFO - Close client connection: Client-b8cc6529-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:16,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36327'. Reason: nanny-close
2023-10-08 05:37:16,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36673'. Reason: nanny-close
2023-10-08 05:37:16,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37475. Reason: nanny-close
2023-10-08 05:37:16,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46869'. Reason: nanny-close
2023-10-08 05:37:16,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,710 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45713. Reason: nanny-close
2023-10-08 05:37:16,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37751'. Reason: nanny-close
2023-10-08 05:37:16,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,710 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46457. Reason: nanny-close
2023-10-08 05:37:16,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35551'. Reason: nanny-close
2023-10-08 05:37:16,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,711 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42139. Reason: nanny-close
2023-10-08 05:37:16,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46437'. Reason: nanny-close
2023-10-08 05:37:16,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,712 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40137. Reason: nanny-close
2023-10-08 05:37:16,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42381'. Reason: nanny-close
2023-10-08 05:37:16,712 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60750; closing.
2023-10-08 05:37:16,712 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,712 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37475', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7128677')
2023-10-08 05:37:16,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39019'. Reason: nanny-close
2023-10-08 05:37:16,713 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,713 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41095. Reason: nanny-close
2023-10-08 05:37:16,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:16,713 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36421. Reason: nanny-close
2023-10-08 05:37:16,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60708; closing.
2023-10-08 05:37:16,713 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,714 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34543. Reason: nanny-close
2023-10-08 05:37:16,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7140255')
2023-10-08 05:37:16,714 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60710; closing.
2023-10-08 05:37:16,714 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,715 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,715 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,715 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46457', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7153873')
2023-10-08 05:37:16,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60734; closing.
2023-10-08 05:37:16,715 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,716 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,717 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,717 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,717 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:16,716 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60708>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-08 05:37:16,718 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,718 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45713', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7182643')
2023-10-08 05:37:16,718 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60724; closing.
2023-10-08 05:37:16,719 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7193527')
2023-10-08 05:37:16,719 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60712; closing.
2023-10-08 05:37:16,719 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60764; closing.
2023-10-08 05:37:16,719 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:16,720 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.720605')
2023-10-08 05:37:16,721 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7209582')
2023-10-08 05:37:16,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60720; closing.
2023-10-08 05:37:16,721 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34543', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743436.7218194')
2023-10-08 05:37:16,722 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:37:16,722 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60720>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-08 05:37:18,274 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:37:18,275 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:37:18,275 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:37:18,276 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:37:18,276 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-08 05:37:20,497 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:20,501 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34297 instead
  warnings.warn(
2023-10-08 05:37:20,505 - distributed.scheduler - INFO - State start
2023-10-08 05:37:20,527 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:20,528 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:37:20,528 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34297/status
2023-10-08 05:37:20,529 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:20,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39581'
2023-10-08 05:37:20,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39725'
2023-10-08 05:37:20,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39859'
2023-10-08 05:37:20,693 - distributed.scheduler - INFO - Receive client connection: Client-bedddbad-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:20,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36093'
2023-10-08 05:37:20,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41033'
2023-10-08 05:37:20,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42971'
2023-10-08 05:37:20,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36862
2023-10-08 05:37:20,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37999'
2023-10-08 05:37:20,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32955'
2023-10-08 05:37:22,566 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,566 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,570 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,574 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,574 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:22,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:22,708 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:22,712 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:26,021 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37983
2023-10-08 05:37:26,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37983
2023-10-08 05:37:26,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37877
2023-10-08 05:37:26,022 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36665
2023-10-08 05:37:26,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36665
2023-10-08 05:37:26,022 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41787
2023-10-08 05:37:26,023 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,022 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,023 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,023 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q9iup2jd
2023-10-08 05:37:26,023 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,023 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-27oumnno
2023-10-08 05:37:26,023 - distributed.worker - INFO - Starting Worker plugin PreImport-e9765244-f264-4a70-beac-55996cde4a53
2023-10-08 05:37:26,023 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bc5d3a5-deeb-4b86-a08f-832cb28e7b49
2023-10-08 05:37:26,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-078c6a72-6e41-480f-ad8c-016882a32818
2023-10-08 05:37:26,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ccba4775-6f34-4148-b752-3c9b34780c69
2023-10-08 05:37:26,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36269
2023-10-08 05:37:26,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36269
2023-10-08 05:37:26,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33905
2023-10-08 05:37:26,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,024 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,024 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k9spof9n
2023-10-08 05:37:26,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2c311f4-3331-4149-b875-cf35d5f7257a
2023-10-08 05:37:26,051 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38095
2023-10-08 05:37:26,052 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38095
2023-10-08 05:37:26,052 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41915
2023-10-08 05:37:26,052 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,052 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,052 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,052 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,052 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hnvrylnu
2023-10-08 05:37:26,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ccea6c1-f6d2-49db-ad10-f11b6c570bfc
2023-10-08 05:37:26,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33805
2023-10-08 05:37:26,057 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33805
2023-10-08 05:37:26,057 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41489
2023-10-08 05:37:26,057 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,057 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,057 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-amh20dtq
2023-10-08 05:37:26,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e18b150-fc65-46a3-b42a-2c0a23d7e03a
2023-10-08 05:37:26,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42061
2023-10-08 05:37:26,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42061
2023-10-08 05:37:26,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42109
2023-10-08 05:37:26,059 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,059 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,059 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,059 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tc0pxkky
2023-10-08 05:37:26,059 - distributed.worker - INFO - Starting Worker plugin PreImport-faa65132-02d4-4854-8c2b-cc2e81cc3022
2023-10-08 05:37:26,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d76d2f6d-2427-4785-9749-0de97dac413a
2023-10-08 05:37:26,060 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8755863-7572-488b-b99a-bae5871c41a3
2023-10-08 05:37:26,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35707
2023-10-08 05:37:26,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35707
2023-10-08 05:37:26,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34573
2023-10-08 05:37:26,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40255
2023-10-08 05:37:26,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40255
2023-10-08 05:37:26,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,062 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40109
2023-10-08 05:37:26,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,063 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,063 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,063 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:26,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-94wov7ti
2023-10-08 05:37:26,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:26,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r1r67p2r
2023-10-08 05:37:26,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b541602-f88f-4008-be71-a6dd364b8e28
2023-10-08 05:37:26,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e91721a0-ae46-480f-8fcf-38dffedc08f2
2023-10-08 05:37:26,076 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99afbe00-705b-48a3-9e55-de50c16cbc5f
2023-10-08 05:37:26,085 - distributed.worker - INFO - Starting Worker plugin PreImport-d0a846e1-5e9b-49da-b4ef-cfd13a0104bb
2023-10-08 05:37:26,085 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4884ac15-8c8a-4521-98a2-8f0a31f61f3e
2023-10-08 05:37:26,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec253204-b5ac-4ff2-8023-2affd8bdf092
2023-10-08 05:37:26,095 - distributed.worker - INFO - Starting Worker plugin PreImport-eeb59dc2-404a-4e3d-8c12-58a53a494672
2023-10-08 05:37:26,095 - distributed.worker - INFO - Starting Worker plugin PreImport-afd13e57-2e87-4e7b-9c93-397c7d9eea3b
2023-10-08 05:37:26,095 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,095 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37983', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37983
2023-10-08 05:37:26,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36888
2023-10-08 05:37:26,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,100 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,100 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a4b883d-1d68-4fd1-8f15-f0859f7fa43e
2023-10-08 05:37:26,101 - distributed.worker - INFO - Starting Worker plugin PreImport-df8f54bb-2bca-4c51-9ee4-66ae311397c7
2023-10-08 05:37:26,101 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32ec2a19-3506-48b0-9200-b97f33914f4a
2023-10-08 05:37:26,101 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,102 - distributed.worker - INFO - Starting Worker plugin PreImport-f6bcbfe5-d667-4326-8963-588af0dc773e
2023-10-08 05:37:26,102 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f0f8505-6ea2-406e-a4fe-dee9fbc8106a
2023-10-08 05:37:26,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,102 - distributed.worker - INFO - Starting Worker plugin PreImport-71096239-cd1e-4a62-8ee9-0f266833a233
2023-10-08 05:37:26,102 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,103 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,117 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35707', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,118 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35707
2023-10-08 05:37:26,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36892
2023-10-08 05:37:26,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,120 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,121 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,262 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36269', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36269
2023-10-08 05:37:26,263 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36922
2023-10-08 05:37:26,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36665', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36665
2023-10-08 05:37:26,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36908
2023-10-08 05:37:26,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,264 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,265 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38095', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,265 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38095
2023-10-08 05:37:26,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36928
2023-10-08 05:37:26,266 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,266 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33805', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,266 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,266 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33805
2023-10-08 05:37:26,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36934
2023-10-08 05:37:26,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,267 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42061', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,267 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,267 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,268 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42061
2023-10-08 05:37:26,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36930
2023-10-08 05:37:26,268 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40255', status: init, memory: 0, processing: 0>
2023-10-08 05:37:26,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,269 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40255
2023-10-08 05:37:26,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36946
2023-10-08 05:37:26,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,269 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,270 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,270 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,270 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:26,271 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:26,272 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:26,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:26,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,304 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,304 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,304 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,304 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:26,308 - distributed.scheduler - INFO - Remove client Client-bedddbad-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:26,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36862; closing.
2023-10-08 05:37:26,309 - distributed.scheduler - INFO - Remove client Client-bedddbad-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:26,309 - distributed.scheduler - INFO - Close client connection: Client-bedddbad-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:26,310 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39581'. Reason: nanny-close
2023-10-08 05:37:26,310 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39725'. Reason: nanny-close
2023-10-08 05:37:26,312 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,312 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40255. Reason: nanny-close
2023-10-08 05:37:26,312 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39859'. Reason: nanny-close
2023-10-08 05:37:26,312 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,312 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35707. Reason: nanny-close
2023-10-08 05:37:26,313 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36093'. Reason: nanny-close
2023-10-08 05:37:26,313 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,313 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36269. Reason: nanny-close
2023-10-08 05:37:26,313 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41033'. Reason: nanny-close
2023-10-08 05:37:26,313 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,313 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36665. Reason: nanny-close
2023-10-08 05:37:26,314 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42971'. Reason: nanny-close
2023-10-08 05:37:26,314 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,314 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,314 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36946; closing.
2023-10-08 05:37:26,314 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33805. Reason: nanny-close
2023-10-08 05:37:26,314 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37999'. Reason: nanny-close
2023-10-08 05:37:26,314 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.3148453')
2023-10-08 05:37:26,314 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,315 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42061. Reason: nanny-close
2023-10-08 05:37:26,315 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32955'. Reason: nanny-close
2023-10-08 05:37:26,315 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:26,315 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37983. Reason: nanny-close
2023-10-08 05:37:26,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,316 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38095. Reason: nanny-close
2023-10-08 05:37:26,316 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,316 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,316 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36908; closing.
2023-10-08 05:37:26,316 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36922; closing.
2023-10-08 05:37:26,316 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36892; closing.
2023-10-08 05:37:26,317 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,317 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,317 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,317 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36665', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.317578')
2023-10-08 05:37:26,317 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,318 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.3179562')
2023-10-08 05:37:26,318 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,318 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:26,318 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.318305')
2023-10-08 05:37:26,319 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,319 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,319 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36934; closing.
2023-10-08 05:37:26,319 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.3197775')
2023-10-08 05:37:26,319 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:26,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36930; closing.
2023-10-08 05:37:26,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36888; closing.
2023-10-08 05:37:26,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36928; closing.
2023-10-08 05:37:26,320 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42061', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.3208113')
2023-10-08 05:37:26,321 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.3211772')
2023-10-08 05:37:26,321 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743446.3215444')
2023-10-08 05:37:26,321 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:37:27,928 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:37:27,928 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:37:27,929 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:37:27,930 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:37:27,930 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-08 05:37:30,130 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:30,135 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43525 instead
  warnings.warn(
2023-10-08 05:37:30,139 - distributed.scheduler - INFO - State start
2023-10-08 05:37:30,178 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:30,179 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:37:30,180 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43525/status
2023-10-08 05:37:30,180 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:30,355 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36185'
2023-10-08 05:37:30,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38951'
2023-10-08 05:37:30,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39241'
2023-10-08 05:37:30,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35507'
2023-10-08 05:37:30,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40995'
2023-10-08 05:37:30,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44299'
2023-10-08 05:37:30,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42001'
2023-10-08 05:37:30,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39481'
2023-10-08 05:37:31,724 - distributed.scheduler - INFO - Receive client connection: Client-c49de23c-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:31,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56020
2023-10-08 05:37:32,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,260 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:32,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:32,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:32,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:35,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41891
2023-10-08 05:37:35,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41891
2023-10-08 05:37:35,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36667
2023-10-08 05:37:35,427 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,427 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,427 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,427 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,427 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i5plwsmu
2023-10-08 05:37:35,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1aa63f0-bccf-4e0c-9e40-7ef982d2ba7c
2023-10-08 05:37:35,473 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33609
2023-10-08 05:37:35,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33609
2023-10-08 05:37:35,474 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35177
2023-10-08 05:37:35,474 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,474 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,475 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,475 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,475 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ejpj855m
2023-10-08 05:37:35,475 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a499c5fd-6c09-45dc-8aa8-4d870cda500e
2023-10-08 05:37:35,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43175
2023-10-08 05:37:35,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43175
2023-10-08 05:37:35,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41299
2023-10-08 05:37:35,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,627 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,627 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,627 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,627 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2iw4ds9y
2023-10-08 05:37:35,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42033
2023-10-08 05:37:35,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42033
2023-10-08 05:37:35,628 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41675
2023-10-08 05:37:35,628 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,628 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,628 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52463a2d-9894-474d-b517-c512605b808c
2023-10-08 05:37:35,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zdtl3clj
2023-10-08 05:37:35,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e5fd6b86-9807-4439-a25c-0ecac27c8d64
2023-10-08 05:37:35,628 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41633
2023-10-08 05:37:35,629 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41633
2023-10-08 05:37:35,629 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45791
2023-10-08 05:37:35,629 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,629 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,629 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,629 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,629 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3nz9rsyu
2023-10-08 05:37:35,630 - distributed.worker - INFO - Starting Worker plugin PreImport-e49892e8-5684-44cd-9993-a18e28c45e8b
2023-10-08 05:37:35,630 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ef65af8-7005-4362-a3f0-188d6e70fd7f
2023-10-08 05:37:35,630 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f611884d-c52f-4236-9571-6ffac408ec04
2023-10-08 05:37:35,668 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42845
2023-10-08 05:37:35,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42845
2023-10-08 05:37:35,669 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45579
2023-10-08 05:37:35,669 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,669 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,669 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,669 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,669 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n2bco31o
2023-10-08 05:37:35,669 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71a72f2a-d687-48c3-8efd-e17f016fa34d
2023-10-08 05:37:35,669 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33703
2023-10-08 05:37:35,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33703
2023-10-08 05:37:35,670 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40405
2023-10-08 05:37:35,670 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,670 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,671 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,671 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,671 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jaq2cxup
2023-10-08 05:37:35,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d56eef06-740e-4646-9311-e4765a733df9
2023-10-08 05:37:35,683 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39153
2023-10-08 05:37:35,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39153
2023-10-08 05:37:35,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43303
2023-10-08 05:37:35,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,684 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,684 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:35,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:35,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_0kji0jv
2023-10-08 05:37:35,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b37d090f-bebb-4b33-97b5-d7d9df12ce31
2023-10-08 05:37:35,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e419eebc-be5f-4859-bf28-d0ad58436162
2023-10-08 05:37:35,942 - distributed.worker - INFO - Starting Worker plugin PreImport-6ba22043-705c-4f10-9de7-3fc166fde29a
2023-10-08 05:37:35,942 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,947 - distributed.worker - INFO - Starting Worker plugin PreImport-e5d72562-48c0-4c81-a16e-6e17b570ddce
2023-10-08 05:37:35,947 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b0c08e0-555c-4970-b777-4f817476a384
2023-10-08 05:37:35,949 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,950 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6d293c6-44e7-42b7-9a19-7ed436f8c099
2023-10-08 05:37:35,950 - distributed.worker - INFO - Starting Worker plugin PreImport-41b78963-7590-48f3-994f-9695d45f2343
2023-10-08 05:37:35,950 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,952 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7302bcb0-6c76-41e8-9cfa-93ba9e19a06a
2023-10-08 05:37:35,953 - distributed.worker - INFO - Starting Worker plugin PreImport-35340bca-b006-41e2-83e1-f91f3529e878
2023-10-08 05:37:35,953 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,961 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa7898a0-c44b-4ae9-b8c2-d0ea8c466e4b
2023-10-08 05:37:35,961 - distributed.worker - INFO - Starting Worker plugin PreImport-b4d1c5e0-9436-460f-bf9b-2d20c76a61c8
2023-10-08 05:37:35,962 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,963 - distributed.worker - INFO - Starting Worker plugin PreImport-a963091c-5ba3-4cfa-b4ea-7cb6031e8b34
2023-10-08 05:37:35,963 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee8c4f1a-5127-4286-a894-474f4303a57d
2023-10-08 05:37:35,964 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,965 - distributed.worker - INFO - Starting Worker plugin PreImport-29780f68-eaa0-4bee-8fb9-03d0bef9d714
2023-10-08 05:37:35,965 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-961a9b55-2fad-47be-b422-3f455c1aeec3
2023-10-08 05:37:35,965 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,980 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33609', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,982 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33609
2023-10-08 05:37:35,982 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56050
2023-10-08 05:37:35,983 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41891', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,983 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41891
2023-10-08 05:37:35,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56044
2023-10-08 05:37:35,984 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,984 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:35,986 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,986 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:35,989 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42033', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,989 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42033
2023-10-08 05:37:35,990 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56046
2023-10-08 05:37:35,990 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43175', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,991 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43175
2023-10-08 05:37:35,991 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56058
2023-10-08 05:37:35,991 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,992 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41633', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,992 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,992 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,992 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41633
2023-10-08 05:37:35,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56066
2023-10-08 05:37:35,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,994 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,994 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,994 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42845', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,994 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,994 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:35,994 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42845
2023-10-08 05:37:35,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56076
2023-10-08 05:37:35,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:35,996 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39153', status: init, memory: 0, processing: 0>
2023-10-08 05:37:35,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:35,996 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,996 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:35,996 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39153
2023-10-08 05:37:35,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56100
2023-10-08 05:37:35,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:35,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:35,999 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:35,999 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:36,000 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33703', status: init, memory: 0, processing: 0>
2023-10-08 05:37:36,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33703
2023-10-08 05:37:36,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56092
2023-10-08 05:37:36,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:36,002 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:36,003 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:36,003 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:36,005 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:36,109 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:36,122 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,122 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,122 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,122 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,123 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,123 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,123 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,123 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:36,129 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:36,131 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:36,133 - distributed.scheduler - INFO - Remove client Client-c49de23c-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:36,133 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56020; closing.
2023-10-08 05:37:36,134 - distributed.scheduler - INFO - Remove client Client-c49de23c-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:36,134 - distributed.scheduler - INFO - Close client connection: Client-c49de23c-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:36,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36185'. Reason: nanny-close
2023-10-08 05:37:36,136 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38951'. Reason: nanny-close
2023-10-08 05:37:36,137 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,137 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42033. Reason: nanny-close
2023-10-08 05:37:36,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39241'. Reason: nanny-close
2023-10-08 05:37:36,137 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,138 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33703. Reason: nanny-close
2023-10-08 05:37:36,138 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35507'. Reason: nanny-close
2023-10-08 05:37:36,138 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,138 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33609. Reason: nanny-close
2023-10-08 05:37:36,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40995'. Reason: nanny-close
2023-10-08 05:37:36,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41633. Reason: nanny-close
2023-10-08 05:37:36,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56046; closing.
2023-10-08 05:37:36,140 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44299'. Reason: nanny-close
2023-10-08 05:37:36,140 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,140 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.1404543')
2023-10-08 05:37:36,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43175. Reason: nanny-close
2023-10-08 05:37:36,140 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42001'. Reason: nanny-close
2023-10-08 05:37:36,141 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41891. Reason: nanny-close
2023-10-08 05:37:36,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,141 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56050; closing.
2023-10-08 05:37:36,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39481'. Reason: nanny-close
2023-10-08 05:37:36,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:36,142 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42845. Reason: nanny-close
2023-10-08 05:37:36,142 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,142 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.142815')
2023-10-08 05:37:36,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,143 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39153. Reason: nanny-close
2023-10-08 05:37:36,143 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,143 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56092; closing.
2023-10-08 05:37:36,144 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,144 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,144 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,145 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,145 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:36,146 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,144 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56050>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56050>: Stream is closed
2023-10-08 05:37:36,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56066; closing.
2023-10-08 05:37:36,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.1471817')
2023-10-08 05:37:36,148 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:36,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.148204')
2023-10-08 05:37:36,148 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56058; closing.
2023-10-08 05:37:36,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.1497455')
2023-10-08 05:37:36,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56044; closing.
2023-10-08 05:37:36,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56076; closing.
2023-10-08 05:37:36,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.151273')
2023-10-08 05:37:36,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42845', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.1517003')
2023-10-08 05:37:36,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56100; closing.
2023-10-08 05:37:36,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743456.152702')
2023-10-08 05:37:36,152 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:37:36,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56044>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-08 05:37:36,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56076>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-08 05:37:37,703 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:37:37,703 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:37:37,704 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:37:37,705 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:37:37,705 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-08 05:37:39,863 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:39,868 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-08 05:37:39,871 - distributed.scheduler - INFO - State start
2023-10-08 05:37:39,895 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:39,896 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:37:39,896 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-08 05:37:39,897 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:39,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40895'
2023-10-08 05:37:39,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37679'
2023-10-08 05:37:39,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45789'
2023-10-08 05:37:39,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36721'
2023-10-08 05:37:39,963 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36379'
2023-10-08 05:37:39,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33759'
2023-10-08 05:37:39,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41349'
2023-10-08 05:37:39,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37883'
2023-10-08 05:37:40,112 - distributed.scheduler - INFO - Receive client connection: Client-ca6e2479-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:40,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41710
2023-10-08 05:37:41,826 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:41,891 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:41,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:41,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:45,474 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37589
2023-10-08 05:37:45,475 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37589
2023-10-08 05:37:45,475 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40393
2023-10-08 05:37:45,475 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,475 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,475 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,475 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,475 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-39dgeqsf
2023-10-08 05:37:45,476 - distributed.worker - INFO - Starting Worker plugin PreImport-e254e657-a16b-4c45-8dff-7bf5c013a1c8
2023-10-08 05:37:45,476 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b2db767-079f-4f7d-aac1-1c0a24070d3d
2023-10-08 05:37:45,476 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4d86ecc-7d0a-40e6-844d-79ff328ac934
2023-10-08 05:37:45,483 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43293
2023-10-08 05:37:45,483 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43293
2023-10-08 05:37:45,484 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35729
2023-10-08 05:37:45,484 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,484 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,484 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,484 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,484 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p80xd_ge
2023-10-08 05:37:45,484 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5017a76-9341-4e2a-944c-0fac9ad419ae
2023-10-08 05:37:45,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43313
2023-10-08 05:37:45,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43313
2023-10-08 05:37:45,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45729
2023-10-08 05:37:45,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,500 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,500 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-elsyylqz
2023-10-08 05:37:45,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2ffae7c-07dd-4581-b7fa-26b4379adb06
2023-10-08 05:37:45,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44225
2023-10-08 05:37:45,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44225
2023-10-08 05:37:45,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45503
2023-10-08 05:37:45,518 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,518 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,518 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,518 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6r46ks5v
2023-10-08 05:37:45,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ce1089f-2535-48ff-bfe3-f00df7f0db02
2023-10-08 05:37:45,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39269
2023-10-08 05:37:45,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39269
2023-10-08 05:37:45,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44461
2023-10-08 05:37:45,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,519 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,519 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-359iksqq
2023-10-08 05:37:45,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f746c469-eb08-445f-9166-d0a3c738b80a
2023-10-08 05:37:45,523 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34361
2023-10-08 05:37:45,524 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34361
2023-10-08 05:37:45,524 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38405
2023-10-08 05:37:45,524 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,524 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,525 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,525 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e_fkha_w
2023-10-08 05:37:45,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d6294c1-ef2e-4952-ae40-b64c30ff7460
2023-10-08 05:37:45,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36323
2023-10-08 05:37:45,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36323
2023-10-08 05:37:45,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44843
2023-10-08 05:37:45,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,536 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,536 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-998pehma
2023-10-08 05:37:45,537 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbf8f00f-f972-433d-bc90-cc5ba8faa136
2023-10-08 05:37:45,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43345
2023-10-08 05:37:45,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43345
2023-10-08 05:37:45,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40795
2023-10-08 05:37:45,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,539 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,539 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:45,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:45,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-30mzs7_h
2023-10-08 05:37:45,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-766f4cdd-afbe-4e94-a394-df36a04269da
2023-10-08 05:37:45,745 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,745 - distributed.worker - INFO - Starting Worker plugin PreImport-9c76e58b-3843-424a-9b82-ab657a9e7454
2023-10-08 05:37:45,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-337bd08b-72f9-4a69-8ddb-38422c836c70
2023-10-08 05:37:45,745 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,754 - distributed.worker - INFO - Starting Worker plugin PreImport-1f295f2c-63cd-47ca-9351-9d672703c52b
2023-10-08 05:37:45,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5849ac23-a0f4-4ccf-b306-215db1842325
2023-10-08 05:37:45,755 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9199dfc-b56c-4137-9645-8760f4e16d22
2023-10-08 05:37:45,755 - distributed.worker - INFO - Starting Worker plugin PreImport-9c0a762e-962a-4c72-906f-4fd205b2feaf
2023-10-08 05:37:45,756 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f69458dc-b3db-428d-aa7e-b0eb06629b79
2023-10-08 05:37:45,763 - distributed.worker - INFO - Starting Worker plugin PreImport-ec53f3b9-2bf8-4cbf-8f0d-08053bc1d90e
2023-10-08 05:37:45,764 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,764 - distributed.worker - INFO - Starting Worker plugin PreImport-6d50b88c-39fe-41ab-8a0a-1fe8a9cee200
2023-10-08 05:37:45,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04d0ac0a-4201-4998-959d-41b14bd70bb6
2023-10-08 05:37:45,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dafc8515-b0ae-4e09-8695-8cddc4193ec3
2023-10-08 05:37:45,765 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,767 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7d1aba8-3cb0-481e-95ee-fb80de57d8b7
2023-10-08 05:37:45,767 - distributed.worker - INFO - Starting Worker plugin PreImport-8eddd66d-da5e-443f-b10f-b5fb65647784
2023-10-08 05:37:45,768 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,768 - distributed.worker - INFO - Starting Worker plugin PreImport-e00cc6cc-053f-4857-bfea-eea5e2754634
2023-10-08 05:37:45,769 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,775 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43293', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43293
2023-10-08 05:37:45,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41738
2023-10-08 05:37:45,779 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37589', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,780 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37589
2023-10-08 05:37:45,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41734
2023-10-08 05:37:45,780 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,780 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,780 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34361', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34361
2023-10-08 05:37:45,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41750
2023-10-08 05:37:45,781 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,781 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,783 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,783 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,790 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36323', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,790 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36323
2023-10-08 05:37:45,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41766
2023-10-08 05:37:45,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,792 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,792 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,797 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43313', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,797 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43313
2023-10-08 05:37:45,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41756
2023-10-08 05:37:45,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,800 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,800 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,804 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44225', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,804 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44225
2023-10-08 05:37:45,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41774
2023-10-08 05:37:45,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43345', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,806 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43345
2023-10-08 05:37:45,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41792
2023-10-08 05:37:45,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,807 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39269', status: init, memory: 0, processing: 0>
2023-10-08 05:37:45,807 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,807 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39269
2023-10-08 05:37:45,807 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41788
2023-10-08 05:37:45,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,808 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,808 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,809 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:45,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,810 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:45,810 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:45,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:45,844 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,844 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,844 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,844 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,845 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,845 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,845 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,845 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,856 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,856 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,856 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,856 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,856 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,856 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,857 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,857 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:37:45,864 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,865 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:37:45,868 - distributed.scheduler - INFO - Remove client Client-ca6e2479-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:45,868 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41710; closing.
2023-10-08 05:37:45,868 - distributed.scheduler - INFO - Remove client Client-ca6e2479-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:45,868 - distributed.scheduler - INFO - Close client connection: Client-ca6e2479-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:45,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45789'. Reason: nanny-close
2023-10-08 05:37:45,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36721'. Reason: nanny-close
2023-10-08 05:37:45,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43313. Reason: nanny-close
2023-10-08 05:37:45,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36379'. Reason: nanny-close
2023-10-08 05:37:45,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44225. Reason: nanny-close
2023-10-08 05:37:45,872 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33759'. Reason: nanny-close
2023-10-08 05:37:45,872 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,872 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36323. Reason: nanny-close
2023-10-08 05:37:45,872 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41349'. Reason: nanny-close
2023-10-08 05:37:45,872 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37589. Reason: nanny-close
2023-10-08 05:37:45,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37883'. Reason: nanny-close
2023-10-08 05:37:45,873 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43345. Reason: nanny-close
2023-10-08 05:37:45,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41756; closing.
2023-10-08 05:37:45,873 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40895'. Reason: nanny-close
2023-10-08 05:37:45,874 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,874 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.8741078')
2023-10-08 05:37:45,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,874 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39269. Reason: nanny-close
2023-10-08 05:37:45,874 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37679'. Reason: nanny-close
2023-10-08 05:37:45,874 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:45,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,874 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34361. Reason: nanny-close
2023-10-08 05:37:45,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41734; closing.
2023-10-08 05:37:45,875 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43293. Reason: nanny-close
2023-10-08 05:37:45,875 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,875 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,875 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37589', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.876176')
2023-10-08 05:37:45,876 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,876 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41774; closing.
2023-10-08 05:37:45,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41766; closing.
2023-10-08 05:37:45,877 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,877 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,877 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,877 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:45,878 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,879 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,877 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41734>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41734>: Stream is closed
2023-10-08 05:37:45,879 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44225', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.879551')
2023-10-08 05:37:45,879 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.879921')
2023-10-08 05:37:45,880 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41792; closing.
2023-10-08 05:37:45,880 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:45,881 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.8809602')
2023-10-08 05:37:45,881 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41750; closing.
2023-10-08 05:37:45,881 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41788; closing.
2023-10-08 05:37:45,882 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.8821795')
2023-10-08 05:37:45,882 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.8824584')
2023-10-08 05:37:45,882 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41738; closing.
2023-10-08 05:37:45,883 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743465.8832822')
2023-10-08 05:37:45,883 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:37:45,883 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41738>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-08 05:37:47,387 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:37:47,387 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:37:47,388 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:37:47,390 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:37:47,391 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-08 05:37:49,633 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:49,637 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38265 instead
  warnings.warn(
2023-10-08 05:37:49,642 - distributed.scheduler - INFO - State start
2023-10-08 05:37:49,665 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:49,666 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:37:49,667 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38265/status
2023-10-08 05:37:49,667 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:49,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46195'
2023-10-08 05:37:49,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44179'
2023-10-08 05:37:49,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42707'
2023-10-08 05:37:49,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43115'
2023-10-08 05:37:49,752 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36177'
2023-10-08 05:37:49,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43855'
2023-10-08 05:37:49,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44739'
2023-10-08 05:37:49,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45983'
2023-10-08 05:37:50,176 - distributed.scheduler - INFO - Receive client connection: Client-d0453c3f-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:50,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44506
2023-10-08 05:37:51,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,612 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,683 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:51,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:37:51,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:37:51,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:37:54,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38891
2023-10-08 05:37:54,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38891
2023-10-08 05:37:54,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45295
2023-10-08 05:37:54,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,543 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,543 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,543 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,543 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cd3u270w
2023-10-08 05:37:54,543 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e22e982-2f17-4a0d-a96a-f8cfde1fcfea
2023-10-08 05:37:54,719 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b73b6981-eb12-45dc-9eb2-08f14803851e
2023-10-08 05:37:54,720 - distributed.worker - INFO - Starting Worker plugin PreImport-61092275-d7ec-4c85-a616-438ae4f48dfa
2023-10-08 05:37:54,720 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44997
2023-10-08 05:37:54,735 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44997
2023-10-08 05:37:54,735 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34755
2023-10-08 05:37:54,735 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,735 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,735 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,735 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-73ptdkzi
2023-10-08 05:37:54,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9bccbd6a-a100-4d23-a041-e8ff65644674
2023-10-08 05:37:54,768 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38891', status: init, memory: 0, processing: 0>
2023-10-08 05:37:54,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38891
2023-10-08 05:37:54,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44518
2023-10-08 05:37:54,771 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:54,772 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,772 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:54,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42403
2023-10-08 05:37:54,803 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42403
2023-10-08 05:37:54,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36131
2023-10-08 05:37:54,803 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,803 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,803 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,804 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7o30jouj
2023-10-08 05:37:54,804 - distributed.worker - INFO - Starting Worker plugin PreImport-fbdb2b0e-253c-4e81-b74b-52331227f996
2023-10-08 05:37:54,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-76405a7a-75e6-4b5b-b4f0-8c90bde43dc7
2023-10-08 05:37:54,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0fd80548-a494-48c5-89d9-2be3080c6f77
2023-10-08 05:37:54,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37609
2023-10-08 05:37:54,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37609
2023-10-08 05:37:54,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41263
2023-10-08 05:37:54,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,830 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,830 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,830 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,830 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oawzctqh
2023-10-08 05:37:54,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4e65e75-1319-4a78-aacf-24ebc8e52c31
2023-10-08 05:37:54,831 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39245
2023-10-08 05:37:54,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39245
2023-10-08 05:37:54,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45153
2023-10-08 05:37:54,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,832 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,832 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n06k9z1r
2023-10-08 05:37:54,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3e70ca3-384d-49b5-be60-fd3c35f4b5ad
2023-10-08 05:37:54,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40213
2023-10-08 05:37:54,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40213
2023-10-08 05:37:54,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43669
2023-10-08 05:37:54,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38951
2023-10-08 05:37:54,838 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38951
2023-10-08 05:37:54,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40215
2023-10-08 05:37:54,838 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dm3l2gy9
2023-10-08 05:37:54,838 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,838 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8blhxfty
2023-10-08 05:37:54,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ebc6aa2-6a0a-4ad9-a00f-f5be53850e33
2023-10-08 05:37:54,839 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c983bf7d-70c1-496b-8fce-144739c6062b
2023-10-08 05:37:54,839 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39625
2023-10-08 05:37:54,840 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39625
2023-10-08 05:37:54,840 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42673
2023-10-08 05:37:54,840 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,840 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,841 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:37:54,841 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:37:54,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-36iy8_yo
2023-10-08 05:37:54,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f77bb03d-f2f5-492e-b006-61c1150da3dd
2023-10-08 05:37:54,965 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-493cdb3a-14bc-4492-acbb-9b2556a6d055
2023-10-08 05:37:54,965 - distributed.worker - INFO - Starting Worker plugin PreImport-19a8bcd3-ae2c-4cc4-acc4-a43e033bc95d
2023-10-08 05:37:54,966 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,987 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,987 - distributed.worker - INFO - Starting Worker plugin PreImport-73f6876d-3a79-42b5-b729-c054168cba22
2023-10-08 05:37:54,987 - distributed.worker - INFO - Starting Worker plugin PreImport-99ba8835-397e-4aa9-9d52-9585b31664bd
2023-10-08 05:37:54,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9304839b-9166-4cbe-98c3-d53fe5f98ada
2023-10-08 05:37:54,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16e244e2-8e1f-44e4-98ea-dceb42033e7f
2023-10-08 05:37:54,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38b52032-1fcf-49d1-96dc-1aff3bd04e68
2023-10-08 05:37:54,988 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,988 - distributed.worker - INFO - Starting Worker plugin PreImport-ea9a7db8-377b-49c6-80c0-f10848b37b0a
2023-10-08 05:37:54,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc9cf34e-5b09-4bd5-9a6a-2c3366deca1d
2023-10-08 05:37:54,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8cad14a3-2fcf-41cb-97be-24214fa510dd
2023-10-08 05:37:54,988 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,988 - distributed.worker - INFO - Starting Worker plugin PreImport-9bda2a7b-fc8d-42eb-b72d-07b1daf84d31
2023-10-08 05:37:54,988 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,988 - distributed.worker - INFO - Starting Worker plugin PreImport-45f32923-ac0e-4b90-93a5-88e1f0d5be52
2023-10-08 05:37:54,989 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,990 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44997', status: init, memory: 0, processing: 0>
2023-10-08 05:37:54,990 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44997
2023-10-08 05:37:54,990 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44528
2023-10-08 05:37:54,991 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:54,992 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,992 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:54,993 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:54,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39245', status: init, memory: 0, processing: 0>
2023-10-08 05:37:55,012 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39245
2023-10-08 05:37:55,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44554
2023-10-08 05:37:55,013 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42403', status: init, memory: 0, processing: 0>
2023-10-08 05:37:55,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:55,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42403
2023-10-08 05:37:55,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44538
2023-10-08 05:37:55,014 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:55,014 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:55,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:55,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39625', status: init, memory: 0, processing: 0>
2023-10-08 05:37:55,015 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:55,015 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:55,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39625
2023-10-08 05:37:55,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44570
2023-10-08 05:37:55,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:55,017 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:55,017 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:55,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38951', status: init, memory: 0, processing: 0>
2023-10-08 05:37:55,024 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38951
2023-10-08 05:37:55,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44586
2023-10-08 05:37:55,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40213', status: init, memory: 0, processing: 0>
2023-10-08 05:37:55,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40213
2023-10-08 05:37:55,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:55,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44600
2023-10-08 05:37:55,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:55,027 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:55,027 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:55,028 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37609', status: init, memory: 0, processing: 0>
2023-10-08 05:37:55,028 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:55,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:55,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37609
2023-10-08 05:37:55,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44572
2023-10-08 05:37:55,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,030 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:37:55,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,031 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:37:55,031 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:37:55,033 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:37:55,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,135 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,135 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,135 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,135 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:37:55,139 - distributed.scheduler - INFO - Remove client Client-d0453c3f-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:55,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44506; closing.
2023-10-08 05:37:55,140 - distributed.scheduler - INFO - Remove client Client-d0453c3f-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:55,140 - distributed.scheduler - INFO - Close client connection: Client-d0453c3f-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:55,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46195'. Reason: nanny-close
2023-10-08 05:37:55,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44179'. Reason: nanny-close
2023-10-08 05:37:55,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40213. Reason: nanny-close
2023-10-08 05:37:55,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42707'. Reason: nanny-close
2023-10-08 05:37:55,143 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37609. Reason: nanny-close
2023-10-08 05:37:55,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43115'. Reason: nanny-close
2023-10-08 05:37:55,143 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,144 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39625. Reason: nanny-close
2023-10-08 05:37:55,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36177'. Reason: nanny-close
2023-10-08 05:37:55,144 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,144 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42403. Reason: nanny-close
2023-10-08 05:37:55,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43855'. Reason: nanny-close
2023-10-08 05:37:55,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,145 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44600; closing.
2023-10-08 05:37:55,145 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38891. Reason: nanny-close
2023-10-08 05:37:55,145 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44739'. Reason: nanny-close
2023-10-08 05:37:55,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40213', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.1456652')
2023-10-08 05:37:55,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,146 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38951. Reason: nanny-close
2023-10-08 05:37:55,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45983'. Reason: nanny-close
2023-10-08 05:37:55,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:37:55,146 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44997. Reason: nanny-close
2023-10-08 05:37:55,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,147 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39245. Reason: nanny-close
2023-10-08 05:37:55,147 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44538; closing.
2023-10-08 05:37:55,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44570; closing.
2023-10-08 05:37:55,147 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44572; closing.
2023-10-08 05:37:55,148 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,148 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,148 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42403', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.148558')
2023-10-08 05:37:55,148 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.148956')
2023-10-08 05:37:55,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.1493318')
2023-10-08 05:37:55,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:37:55,150 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44518; closing.
2023-10-08 05:37:55,150 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.1510425')
2023-10-08 05:37:55,151 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,151 - distributed.nanny - INFO - Worker closed
2023-10-08 05:37:55,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44586; closing.
2023-10-08 05:37:55,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44528; closing.
2023-10-08 05:37:55,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44554; closing.
2023-10-08 05:37:55,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.152101')
2023-10-08 05:37:55,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44997', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.1524887')
2023-10-08 05:37:55,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743475.152882')
2023-10-08 05:37:55,153 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:37:56,659 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:37:56,659 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:37:56,660 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:37:56,661 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:37:56,661 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-08 05:37:58,738 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:58,742 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44143 instead
  warnings.warn(
2023-10-08 05:37:58,746 - distributed.scheduler - INFO - State start
2023-10-08 05:37:58,773 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:37:58,774 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:37:58,774 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44143/status
2023-10-08 05:37:58,774 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:37:58,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35985'
2023-10-08 05:37:59,125 - distributed.scheduler - INFO - Receive client connection: Client-d5c3deaa-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:37:59,137 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44672
2023-10-08 05:38:00,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:00,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:01,052 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:02,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37341
2023-10-08 05:38:02,638 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37341
2023-10-08 05:38:02,638 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-08 05:38:02,638 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:02,638 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:02,638 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:02,638 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-08 05:38:02,638 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sswrvitc
2023-10-08 05:38:02,639 - distributed.worker - INFO - Starting Worker plugin PreImport-d51782d8-951e-4131-a914-21f88c724695
2023-10-08 05:38:02,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41eda131-0338-4536-9b07-9c39f7982be1
2023-10-08 05:38:02,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-337ac116-327d-4759-a2f4-9ce7312f4148
2023-10-08 05:38:02,641 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:02,678 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37341', status: init, memory: 0, processing: 0>
2023-10-08 05:38:02,678 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37341
2023-10-08 05:38:02,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40582
2023-10-08 05:38:02,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:02,681 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:02,681 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:02,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:02,712 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:02,715 - distributed.scheduler - INFO - Remove client Client-d5c3deaa-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:02,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44672; closing.
2023-10-08 05:38:02,715 - distributed.scheduler - INFO - Remove client Client-d5c3deaa-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:02,716 - distributed.scheduler - INFO - Close client connection: Client-d5c3deaa-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:02,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35985'. Reason: nanny-close
2023-10-08 05:38:02,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:02,718 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37341. Reason: nanny-close
2023-10-08 05:38:02,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40582; closing.
2023-10-08 05:38:02,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:02,721 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37341', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743482.7216673')
2023-10-08 05:38:02,722 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:38:02,723 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:03,933 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:03,933 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:03,934 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:03,935 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:38:03,935 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-08 05:38:07,823 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:07,827 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40513 instead
  warnings.warn(
2023-10-08 05:38:07,831 - distributed.scheduler - INFO - State start
2023-10-08 05:38:07,852 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:07,853 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:38:07,854 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40513/status
2023-10-08 05:38:07,854 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:38:07,989 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41395'
2023-10-08 05:38:09,410 - distributed.scheduler - INFO - Receive client connection: Client-db27d55e-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:09,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40696
2023-10-08 05:38:09,471 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:09,471 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:09,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:10,901 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43623
2023-10-08 05:38:10,901 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43623
2023-10-08 05:38:10,901 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41219
2023-10-08 05:38:10,902 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:10,902 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:10,902 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:10,902 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-08 05:38:10,902 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xfwesapr
2023-10-08 05:38:10,902 - distributed.worker - INFO - Starting Worker plugin PreImport-b7165da4-0505-4ebe-b1b7-84903ed3d645
2023-10-08 05:38:10,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae10b1b1-51fb-48d0-a5a9-402f3de6745a
2023-10-08 05:38:10,904 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e5ed56af-4c22-46df-aa8c-8e4bbcb2f993
2023-10-08 05:38:10,905 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:10,937 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43623', status: init, memory: 0, processing: 0>
2023-10-08 05:38:10,938 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43623
2023-10-08 05:38:10,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39938
2023-10-08 05:38:10,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:10,940 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:10,940 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:10,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:10,996 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:10,998 - distributed.scheduler - INFO - Remove client Client-db27d55e-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:10,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40696; closing.
2023-10-08 05:38:10,999 - distributed.scheduler - INFO - Remove client Client-db27d55e-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:10,999 - distributed.scheduler - INFO - Close client connection: Client-db27d55e-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:11,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41395'. Reason: nanny-close
2023-10-08 05:38:11,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:11,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43623. Reason: nanny-close
2023-10-08 05:38:11,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:11,004 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39938; closing.
2023-10-08 05:38:11,004 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43623', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743491.0046756')
2023-10-08 05:38:11,004 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:38:11,005 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:12,166 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:12,167 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:12,167 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:12,168 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:38:12,168 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-08 05:38:14,131 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:14,135 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34567 instead
  warnings.warn(
2023-10-08 05:38:14,138 - distributed.scheduler - INFO - State start
2023-10-08 05:38:14,199 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:14,200 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:38:14,201 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34567/status
2023-10-08 05:38:14,201 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:38:18,212 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:39948'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39948>: Stream is closed
2023-10-08 05:38:18,459 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:18,460 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:18,460 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:18,461 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:38:18,461 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-08 05:38:20,538 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:20,542 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35189 instead
  warnings.warn(
2023-10-08 05:38:20,545 - distributed.scheduler - INFO - State start
2023-10-08 05:38:20,591 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:20,592 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-08 05:38:20,593 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35189/status
2023-10-08 05:38:20,593 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:38:20,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41401'
2023-10-08 05:38:21,507 - distributed.scheduler - INFO - Receive client connection: Client-e2b5facc-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:21,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43780
2023-10-08 05:38:22,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:22,249 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:22,254 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:23,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43207
2023-10-08 05:38:23,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43207
2023-10-08 05:38:23,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34321
2023-10-08 05:38:23,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-08 05:38:23,567 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:23,567 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:23,568 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-08 05:38:23,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-7myosxm_
2023-10-08 05:38:23,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34421f91-2bf4-489d-b910-ec47cc8f3c5a
2023-10-08 05:38:23,568 - distributed.worker - INFO - Starting Worker plugin PreImport-7c2aaf41-eb00-4ae9-a577-cbdf21d02974
2023-10-08 05:38:23,569 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7058c4f1-a616-4e4a-b419-2e044e2387ab
2023-10-08 05:38:23,569 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:23,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43207', status: init, memory: 0, processing: 0>
2023-10-08 05:38:23,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43207
2023-10-08 05:38:23,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43804
2023-10-08 05:38:23,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:23,595 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-08 05:38:23,595 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:23,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-08 05:38:23,635 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:23,637 - distributed.scheduler - INFO - Remove client Client-e2b5facc-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:23,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43780; closing.
2023-10-08 05:38:23,638 - distributed.scheduler - INFO - Remove client Client-e2b5facc-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:23,638 - distributed.scheduler - INFO - Close client connection: Client-e2b5facc-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:23,639 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41401'. Reason: nanny-close
2023-10-08 05:38:23,640 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:23,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43207. Reason: nanny-close
2023-10-08 05:38:23,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-08 05:38:23,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43804; closing.
2023-10-08 05:38:23,643 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43207', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743503.643289')
2023-10-08 05:38:23,643 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:38:23,644 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:24,706 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:24,706 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:24,707 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:24,708 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-08 05:38:24,708 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-08 05:38:26,859 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:26,865 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-08 05:38:26,868 - distributed.scheduler - INFO - State start
2023-10-08 05:38:26,894 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:26,895 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:38:26,896 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-08 05:38:26,896 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:38:26,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38333'
2023-10-08 05:38:26,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46093'
2023-10-08 05:38:26,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35947'
2023-10-08 05:38:26,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40835'
2023-10-08 05:38:27,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40989'
2023-10-08 05:38:27,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40371'
2023-10-08 05:38:27,022 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43193'
2023-10-08 05:38:27,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44497'
2023-10-08 05:38:28,112 - distributed.scheduler - INFO - Receive client connection: Client-e6749712-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:28,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51822
2023-10-08 05:38:28,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,843 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,843 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,850 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,883 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,887 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,887 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:28,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:28,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:28,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:31,849 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35213
2023-10-08 05:38:31,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35213
2023-10-08 05:38:31,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40685
2023-10-08 05:38:31,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,850 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,850 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,850 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pi_xjbwn
2023-10-08 05:38:31,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fc521ce-0c0e-419a-bf78-64c538d5a194
2023-10-08 05:38:31,857 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41323
2023-10-08 05:38:31,857 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41323
2023-10-08 05:38:31,857 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35909
2023-10-08 05:38:31,857 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,858 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,858 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,858 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,858 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tztprfdu
2023-10-08 05:38:31,858 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44927
2023-10-08 05:38:31,858 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0e8bae6-7e3b-47a8-a81c-c8788db8312c
2023-10-08 05:38:31,858 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44927
2023-10-08 05:38:31,858 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39497
2023-10-08 05:38:31,858 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,859 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,859 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,859 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,859 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-epwwd49n
2023-10-08 05:38:31,859 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7c07007-9647-4143-9a02-bd4d903c045a
2023-10-08 05:38:31,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37135
2023-10-08 05:38:31,867 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37135
2023-10-08 05:38:31,867 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37469
2023-10-08 05:38:31,867 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,867 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,867 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,868 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,868 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sggqytpu
2023-10-08 05:38:31,868 - distributed.worker - INFO - Starting Worker plugin PreImport-33513ad0-c17d-4293-b4b2-42e09cd6abf6
2023-10-08 05:38:31,868 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5861f7f-c641-4bd0-91b5-5f79acb28396
2023-10-08 05:38:31,868 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ff977b3-aef1-4c3a-98d0-eda8b588ee4c
2023-10-08 05:38:31,874 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33003
2023-10-08 05:38:31,875 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33003
2023-10-08 05:38:31,875 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37323
2023-10-08 05:38:31,875 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,875 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,875 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,875 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40251
2023-10-08 05:38:31,876 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,876 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40251
2023-10-08 05:38:31,876 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-758ri_g9
2023-10-08 05:38:31,876 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41595
2023-10-08 05:38:31,876 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,876 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,876 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,876 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,876 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lfm_bcag
2023-10-08 05:38:31,876 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81ac47fd-ecb9-4039-95f3-76e17ea87e6b
2023-10-08 05:38:31,877 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c9b788a-dd07-42c5-a44f-d98c351a8c42
2023-10-08 05:38:31,878 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35835
2023-10-08 05:38:31,879 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35835
2023-10-08 05:38:31,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34491
2023-10-08 05:38:31,879 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,879 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,879 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,879 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,879 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4hd3fya9
2023-10-08 05:38:31,880 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b0bf23a-a40e-4044-bcfa-c371ddca55c6
2023-10-08 05:38:31,881 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46469
2023-10-08 05:38:31,881 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46469
2023-10-08 05:38:31,882 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46421
2023-10-08 05:38:31,882 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:31,882 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:31,882 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:31,882 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-08 05:38:31,882 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i9rr2abg
2023-10-08 05:38:31,882 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db20e834-e8f4-4ee5-bbe2-c3ac324965b7
2023-10-08 05:38:32,200 - distributed.worker - INFO - Starting Worker plugin PreImport-b16933ed-6b7b-4245-9310-8e865669a0c2
2023-10-08 05:38:32,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67b430e9-87c0-4cd7-8535-f30091fcde37
2023-10-08 05:38:32,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb1406e9-b031-4aae-8686-c7a30bf1bf9b
2023-10-08 05:38:32,200 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin PreImport-04b1bdcc-6a9c-4f8d-a5d5-c6621cce2907
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a234079-7889-41c4-a2e5-d66bcfe7d872
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin PreImport-d7bec5e5-6292-4c91-a913-a3a7572ece57
2023-10-08 05:38:32,201 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,201 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin PreImport-8e01a208-c033-4cb9-89bc-9f3ffe5685f3
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d47eb54e-4607-43c2-a1c1-a4aeb0b6bcc9
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d579ec13-29d5-44fa-8eca-eab3d017a98d
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0265e6b-2de4-4e30-9295-bc3fa3d0cce0
2023-10-08 05:38:32,201 - distributed.worker - INFO - Starting Worker plugin PreImport-a07415ad-9bd9-4115-8b7e-0c32c1726e76
2023-10-08 05:38:32,202 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82f03c21-5e9e-4492-a5ea-b147e56c14db
2023-10-08 05:38:32,202 - distributed.worker - INFO - Starting Worker plugin PreImport-b02822ed-a880-4204-b3b0-96cada1717f9
2023-10-08 05:38:32,202 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,203 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,203 - distributed.worker - INFO - Starting Worker plugin PreImport-f0e9be72-97b5-4daf-85b0-6a5e1e473d3e
2023-10-08 05:38:32,204 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,204 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,228 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35213', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,231 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35213
2023-10-08 05:38:32,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52492
2023-10-08 05:38:32,232 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37135', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,233 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37135
2023-10-08 05:38:32,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52504
2023-10-08 05:38:32,233 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,233 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46469', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,233 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,234 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46469
2023-10-08 05:38:32,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52518
2023-10-08 05:38:32,234 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,235 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,235 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35835', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35835
2023-10-08 05:38:32,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52542
2023-10-08 05:38:32,236 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,236 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,237 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,237 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,239 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40251', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,239 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40251
2023-10-08 05:38:32,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52544
2023-10-08 05:38:32,241 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,241 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41323', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,242 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41323
2023-10-08 05:38:32,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52570
2023-10-08 05:38:32,242 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,242 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,243 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44927', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,244 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44927
2023-10-08 05:38:32,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52526
2023-10-08 05:38:32,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,245 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,245 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33003', status: init, memory: 0, processing: 0>
2023-10-08 05:38:32,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33003
2023-10-08 05:38:32,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52560
2023-10-08 05:38:32,247 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,247 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:32,249 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:32,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,249 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:32,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:32,356 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,356 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,356 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,357 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,357 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,357 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,357 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,358 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-08 05:38:32,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,371 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,371 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:32,375 - distributed.scheduler - INFO - Remove client Client-e6749712-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:32,375 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51822; closing.
2023-10-08 05:38:32,375 - distributed.scheduler - INFO - Remove client Client-e6749712-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:32,376 - distributed.scheduler - INFO - Close client connection: Client-e6749712-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:32,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38333'. Reason: nanny-close
2023-10-08 05:38:32,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46093'. Reason: nanny-close
2023-10-08 05:38:32,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40251. Reason: nanny-close
2023-10-08 05:38:32,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35947'. Reason: nanny-close
2023-10-08 05:38:32,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33003. Reason: nanny-close
2023-10-08 05:38:32,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40835'. Reason: nanny-close
2023-10-08 05:38:32,380 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35835. Reason: nanny-close
2023-10-08 05:38:32,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40989'. Reason: nanny-close
2023-10-08 05:38:32,381 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,381 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37135. Reason: nanny-close
2023-10-08 05:38:32,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40371'. Reason: nanny-close
2023-10-08 05:38:32,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,381 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52544; closing.
2023-10-08 05:38:32,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43193'. Reason: nanny-close
2023-10-08 05:38:32,381 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44927. Reason: nanny-close
2023-10-08 05:38:32,382 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40251', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.3820271')
2023-10-08 05:38:32,382 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,382 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41323. Reason: nanny-close
2023-10-08 05:38:32,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44497'. Reason: nanny-close
2023-10-08 05:38:32,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,382 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:32,382 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46469. Reason: nanny-close
2023-10-08 05:38:32,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,383 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35213. Reason: nanny-close
2023-10-08 05:38:32,383 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52504; closing.
2023-10-08 05:38:32,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52542; closing.
2023-10-08 05:38:32,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52560; closing.
2023-10-08 05:38:32,384 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,385 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37135', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.385343')
2023-10-08 05:38:32,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.3858137')
2023-10-08 05:38:32,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,386 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:32,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.3862717')
2023-10-08 05:38:32,387 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52526; closing.
2023-10-08 05:38:32,387 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,387 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.3881378')
2023-10-08 05:38:32,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52570; closing.
2023-10-08 05:38:32,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52518; closing.
2023-10-08 05:38:32,388 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:32,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52492; closing.
2023-10-08 05:38:32,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.389324')
2023-10-08 05:38:32,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.3897865')
2023-10-08 05:38:32,390 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35213', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743512.3901596')
2023-10-08 05:38:32,390 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:38:33,845 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:33,845 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:33,846 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:33,848 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:38:33,849 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-08 05:38:35,990 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:35,994 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41537 instead
  warnings.warn(
2023-10-08 05:38:35,998 - distributed.scheduler - INFO - State start
2023-10-08 05:38:36,018 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:36,019 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:38:36,020 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41537/status
2023-10-08 05:38:36,020 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:38:36,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34109'
2023-10-08 05:38:37,638 - distributed.scheduler - INFO - Receive client connection: Client-ebf13cfb-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:37,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52670
2023-10-08 05:38:37,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:37,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:37,742 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:38,661 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44909
2023-10-08 05:38:38,661 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44909
2023-10-08 05:38:38,661 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43847
2023-10-08 05:38:38,661 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:38,661 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:38,661 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:38,662 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-08 05:38:38,662 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ni6l2xi
2023-10-08 05:38:38,662 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a24dd157-c989-4b4e-848d-d5381a02df21
2023-10-08 05:38:38,752 - distributed.worker - INFO - Starting Worker plugin PreImport-55d58c14-a921-4f1b-8914-647366df8123
2023-10-08 05:38:38,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c583448-2ddf-4ef5-a105-2ef877470819
2023-10-08 05:38:38,752 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:38,782 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44909', status: init, memory: 0, processing: 0>
2023-10-08 05:38:38,783 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44909
2023-10-08 05:38:38,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52688
2023-10-08 05:38:38,785 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:38,786 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:38,786 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:38,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:38,879 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:38:38,883 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:38,884 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:38,887 - distributed.scheduler - INFO - Remove client Client-ebf13cfb-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:38,887 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52670; closing.
2023-10-08 05:38:38,887 - distributed.scheduler - INFO - Remove client Client-ebf13cfb-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:38,887 - distributed.scheduler - INFO - Close client connection: Client-ebf13cfb-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:38,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34109'. Reason: nanny-close
2023-10-08 05:38:38,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:38,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44909. Reason: nanny-close
2023-10-08 05:38:38,892 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52688; closing.
2023-10-08 05:38:38,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:38,893 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44909', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743518.893063')
2023-10-08 05:38:38,893 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:38:38,894 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:40,005 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:40,005 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:40,006 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:40,007 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:38:40,007 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-08 05:38:42,212 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:42,217 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35353 instead
  warnings.warn(
2023-10-08 05:38:42,221 - distributed.scheduler - INFO - State start
2023-10-08 05:38:42,289 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-08 05:38:42,290 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-08 05:38:42,291 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35353/status
2023-10-08 05:38:42,291 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-08 05:38:42,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46067'
2023-10-08 05:38:43,780 - distributed.scheduler - INFO - Receive client connection: Client-ef9cbfaf-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:43,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44780
2023-10-08 05:38:44,276 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-08 05:38:44,276 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-08 05:38:44,280 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-08 05:38:45,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42833
2023-10-08 05:38:45,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42833
2023-10-08 05:38:45,161 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40741
2023-10-08 05:38:45,161 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-08 05:38:45,161 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:45,161 - distributed.worker - INFO -               Threads:                          1
2023-10-08 05:38:45,161 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-08 05:38:45,161 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yq24xswm
2023-10-08 05:38:45,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af4b17a6-d203-4707-a86c-f6408231b17e
2023-10-08 05:38:45,268 - distributed.worker - INFO - Starting Worker plugin PreImport-8fae0c92-9b17-478e-8315-6917621652a1
2023-10-08 05:38:45,268 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ea4b77a-f980-45c8-a025-1810b462689f
2023-10-08 05:38:45,268 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:45,292 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42833', status: init, memory: 0, processing: 0>
2023-10-08 05:38:45,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42833
2023-10-08 05:38:45,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44798
2023-10-08 05:38:45,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-08 05:38:45,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-08 05:38:45,296 - distributed.worker - INFO - -------------------------------------------------
2023-10-08 05:38:45,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-08 05:38:45,331 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-08 05:38:45,335 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-08 05:38:45,339 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:45,342 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-08 05:38:45,344 - distributed.scheduler - INFO - Remove client Client-ef9cbfaf-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:45,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44780; closing.
2023-10-08 05:38:45,345 - distributed.scheduler - INFO - Remove client Client-ef9cbfaf-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:45,346 - distributed.scheduler - INFO - Close client connection: Client-ef9cbfaf-659c-11ee-b17e-d8c49764f6bb
2023-10-08 05:38:45,346 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46067'. Reason: nanny-close
2023-10-08 05:38:45,347 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-08 05:38:45,348 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42833. Reason: nanny-close
2023-10-08 05:38:45,350 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44798; closing.
2023-10-08 05:38:45,350 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-08 05:38:45,351 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696743525.3511415')
2023-10-08 05:38:45,351 - distributed.scheduler - INFO - Lost all workers
2023-10-08 05:38:45,352 - distributed.nanny - INFO - Worker closed
2023-10-08 05:38:46,564 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-08 05:38:46,564 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-08 05:38:46,565 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-08 05:38:46,566 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-08 05:38:46,567 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44209 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45045 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46313 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45957 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42221 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42921 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38821 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37575 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43077 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41525 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34867 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35205 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33637 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45941 instead
  warnings.warn(
[1696743852.155872] [dgx13:71072:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:59270) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34943 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34791 instead
  warnings.warn(
[1696743899.153427] [dgx13:71783:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:46113) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44643 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39013 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42041 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40569 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35355 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46751 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34093 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39543 instead
  warnings.warn(
[1696744091.304932] [dgx13:74565:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:54212) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46293 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-4404' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38361 instead
  warnings.warn(
[1696744128.988733] [dgx13:75344:0]            sock.c:470  UCX  ERROR bind(fd=156 addr=0.0.0.0:56748) failed: Address already in use
[1696744129.131420] [dgx13:75344:0]            sock.c:470  UCX  ERROR bind(fd=163 addr=0.0.0.0:49064) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33217 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34859 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42071 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40965 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40453 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40651 instead
  warnings.warn(
2023-10-08 05:51:07,661 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,667 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,669 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,674 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:45955'.
2023-10-08 05:51:07,673 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34470 remote=tcp://127.0.0.1:46049>: Stream is closed
2023-10-08 05:51:07,677 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fbe1a04e7f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,677 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,679 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:39599', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-08 05:51:07,679 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,680 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f98338497f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,681 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46613'.
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,681 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:46613'. Shutting down.
2023-10-08 05:51:07,684 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7c8d8ce820>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:07,690 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:43137'.
2023-10-08 05:51:07,691 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:43137'. Shutting down.
2023-10-08 05:51:07,693 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fdf5dbc67f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-08 05:51:09,680 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-08 05:51:09,682 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-08 05:51:09,687 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-08 05:51:09,696 - distributed.nanny - ERROR - Worker process died unexpectedly
