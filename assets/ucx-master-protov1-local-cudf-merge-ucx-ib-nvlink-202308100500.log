2023-08-10 06:11:59,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:11:59,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,042 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,043 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,067 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,067 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:00,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:00,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1691647932.396699] [dgx13:69455:0]    ib_mlx5dv_md.c:389  UCX  ERROR mlx5_3: LRU push returned Unsupported operation
[dgx13:69455:0:69455]        rndv.c:165  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  69455) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7ff9c80e9ced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7ff9c80e78a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7ff9c80e7a3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x72c14) [0x7ff9c8191c14]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7ff9c8169e3f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7ff9c81a5ddd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x707) [0x7ff9c81ab3a7]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7ff9c81ac02f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7ff9c825cec2]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55da8453fdc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55da8453e1a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55da84524d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55da8451e27a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55da8452fc05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55da845203cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55da8451e27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55da8452fc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55da845203cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55da8454470e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55da84525923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55da8454470e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55da84525923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55da8454470e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55da84525923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55da8454470e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55da84525923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55da8454470e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55da84525923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55da8454470e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7ff9da4e92fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7ff9da4e9b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55da845282bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55da844db817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55da84526f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55da84524d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55da8451f81b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55da8451f81b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55da8451f81b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55da8451f81b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55da8451e27a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55da8452fc05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55da84523fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55da8451e27a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55da8453d935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55da8453e104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55da84604fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55da845282bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55da845231bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55da8453dc72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55da845231bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55da8451f81b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55da8451e27a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55da8452fc05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55da8451f81b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55da8452fef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55da8451f568]
=================================
2023-08-10 06:12:12,660 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #032] ep: 0x7f334d5c6240, tag: 0x21834217bae510a6, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #032] ep: 0x7f334d5c6240, tag: 0x21834217bae510a6, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:12,670 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42796 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7f2f9512e440, tag: 0x17257f49c6025d72, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:12,671 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f2f9512e200, tag: 0x4dcac76e9c8a623a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f2f9512e200, tag: 0x4dcac76e9c8a623a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:12,671 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:36630 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7ff020668440, tag: 0x871a69cf01d68ac0, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:12,673 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7ff020668180, tag: 0xa78b0b4c4757e08a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7ff020668180, tag: 0xa78b0b4c4757e08a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:12,679 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:39063 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7fddf08db440, tag: 0x57da1b9e7922cb4d, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:12,679 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51937 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7f3e9e0b6440, tag: 0xdf6d5771f88817fb, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:12,680 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35380 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #043] ep: 0x7fda843cd100, tag: 0x516f7bba2e85a1d7, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:12,681 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:50718 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7f6e30a2a400, tag: 0xf9e1516cd9155e5f, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:12,681 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #039] ep: 0x7f3e9e0b6140, tag: 0x3fe166bf6de6417e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #039] ep: 0x7f3e9e0b6140, tag: 0x3fe166bf6de6417e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-10 06:12:12,681 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7fda843cd180, tag: 0xb5753b2d7bd912e3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7fda843cd180, tag: 0xb5753b2d7bd912e3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:12,682 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f6e30a2a180, tag: 0x935c74acfd95d9d3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f6e30a2a180, tag: 0x935c74acfd95d9d3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:12,682 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7fddf08db240, tag: 0x985ace51e1eb0de9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7fddf08db240, tag: 0x985ace51e1eb0de9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:12,693 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44539 -> ucx://127.0.0.1:50419
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7f334d5c6440, tag: 0x6732c3f35308d350, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:13,172 - distributed.nanny - WARNING - Restarting worker
[1691647934.312060] [dgx13:69466:0]    ib_mlx5dv_md.c:389  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:69466:0:69466]        rndv.c:165  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  69466) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fdac01a3ced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7fdac01a18a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7fdac01a1a3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x72c14) [0x7fdac024bc14]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7fdac0223e3f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7fdac025fddd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x707) [0x7fdac02653a7]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7fdac026602f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7fdac0316ec2]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55eab3a70dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55eab3a6f1a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55eab3a55d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55eab3a4f27a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55eab3a60c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55eab3a513cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55eab3a4f27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55eab3a60c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55eab3a513cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55eab3a7570e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55eab3a56923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55eab3a7570e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55eab3a56923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55eab3a7570e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55eab3a56923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55eab3a7570e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55eab3a56923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55eab3a7570e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55eab3a56923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55eab3a7570e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fdae05b32fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7fdae05b3b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55eab3a592bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55eab3a0c817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55eab3a57f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55eab3a55d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55eab3a5081b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55eab3a5081b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55eab3a5081b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55eab3a5081b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55eab3a4f27a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55eab3a60c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55eab3a54fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55eab3a4f27a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55eab3a6e935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55eab3a6f104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55eab3b35fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55eab3a592bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55eab3a541bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55eab3a6ec72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55eab3a541bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55eab3a5081b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55eab3a4f27a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55eab3a60c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55eab3a5081b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55eab3a60ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55eab3a50568]
=================================
[1691647934.355192] [dgx13:69471:0]    ib_mlx5dv_md.c:389  UCX  ERROR mlx5_0: LRU push returned Unsupported operation
[dgx13:69471:0:69471]        rndv.c:165  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  69471) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f6e39807ced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f6e398058a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7f6e39805a3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x72c14) [0x7f6e398afc14]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f6e39887e3f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f6e398c3ddd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x707) [0x7f6e398c93a7]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f6e398ca02f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7f6e3997aec2]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x555e28f52dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x555e28f511a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x555e28f37d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555e28f3127a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555e28f42c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x555e28f333cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555e28f3127a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555e28f42c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x555e28f333cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555e28f5770e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x555e28f38923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555e28f5770e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x555e28f38923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555e28f5770e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x555e28f38923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555e28f5770e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x555e28f38923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555e28f5770e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x555e28f38923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555e28f5770e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f6e5dc022fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f6e5dc02b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x555e28f3b2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x555e28eee817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x555e28f39f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x555e28f37d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555e28f3281b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555e28f3281b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555e28f3281b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555e28f3281b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555e28f3127a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555e28f42c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x555e28f36fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555e28f3127a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x555e28f50935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x555e28f51104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x555e29017fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x555e28f3b2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x555e28f361bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x555e28f50c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x555e28f361bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555e28f3281b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555e28f3127a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555e28f42c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555e28f3281b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555e28f42ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x555e28f32568]
=================================
2023-08-10 06:12:14,603 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35380
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #033] ep: 0x7f2f9512e100, tag: 0x75c68b4965fb5732, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #033] ep: 0x7f2f9512e100, tag: 0x75c68b4965fb5732, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:14,603 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35380
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #048] ep: 0x7f3e9e0b61c0, tag: 0xb8588f0798cf6fe2, nbytes: 100031032, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #048] ep: 0x7f3e9e0b61c0, tag: 0xb8588f0798cf6fe2, nbytes: 100031032, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-10 06:12:14,603 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35380
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #050] ep: 0x7fddf08db200, tag: 0x5b579b7325b02e97, nbytes: 100006856, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #050] ep: 0x7fddf08db200, tag: 0x5b579b7325b02e97, nbytes: 100006856, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-10 06:12:14,603 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35380
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #041] ep: 0x7f334d5c6100, tag: 0x4574ef5cb4fbb45a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #041] ep: 0x7f334d5c6100, tag: 0x4574ef5cb4fbb45a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:14,605 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35380
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #043] ep: 0x7ff0206681c0, tag: 0xd192f6d02d91d9b3, nbytes: 99977344, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #043] ep: 0x7ff0206681c0, tag: 0xd192f6d02d91d9b3, nbytes: 99977344, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-10 06:12:14,740 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:36630 -> ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #037] ep: 0x7ff020668300, tag: 0x464bb0958533aef4, nbytes: 100000984, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:14,740 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51937 -> ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #044] ep: 0x7f3e9e0b6300, tag: 0x861157b77185bf6, nbytes: 99974576, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:14,741 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #042] ep: 0x7ff020668280, tag: 0x9893b062b8b8b492, nbytes: 100005496, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #042] ep: 0x7ff020668280, tag: 0x9893b062b8b8b492, nbytes: 100005496, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-10 06:12:14,741 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #051] ep: 0x7f3e9e0b6200, tag: 0xb8ae3e0bf7333ef0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #051] ep: 0x7f3e9e0b6200, tag: 0xb8ae3e0bf7333ef0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:14,749 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42796 -> ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #037] ep: 0x7f2f9512e380, tag: 0x50e8744317774fb1, nbytes: 100007192, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:14,749 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44539 -> ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #038] ep: 0x7f334d5c6400, tag: 0x4a9fada97b93702d, nbytes: 99971912, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:14,750 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #033] ep: 0x7f2f9512e240, tag: 0xf4902af575c9f50e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #033] ep: 0x7f2f9512e240, tag: 0xf4902af575c9f50e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:14,750 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #033] ep: 0x7f334d5c61c0, tag: 0x82e7d86d618f861e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #033] ep: 0x7f334d5c61c0, tag: 0x82e7d86d618f861e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:14,758 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #046] ep: 0x7fddf08db140, tag: 0xee726ac0296c7dfc, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #046] ep: 0x7fddf08db140, tag: 0xee726ac0296c7dfc, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:14,775 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:14,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:14,964 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:39063 -> ucx://127.0.0.1:50718
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #045] ep: 0x7fddf08db3c0, tag: 0xf456761bf64c8c38, nbytes: 99986032, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:15,311 - distributed.nanny - WARNING - Restarting worker
2023-08-10 06:12:15,407 - distributed.nanny - WARNING - Restarting worker
2023-08-10 06:12:15,411 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7feb7b
args:      (                key  shuffle   payload  _partitions
0         500000000        1  48863339            1
1         500000001        6  82208751            6
2         500000002        5  23172095            5
3         500000003        3  83618300            3
4         500000004        7  32090160            7
...             ...      ...       ...          ...
99999995  599999995        4   2417575            4
99999996  599999996        3  79206659            3
99999997  599999997        4  89655348            4
99999998  599999998        0  82540937            0
99999999  599999999        1  42812172            1

[100000000 rows x 4 columns], '_partitions', 0, 8, 8, True, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-10 06:12:16,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:16,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1691647936.951752] [dgx13:69458:0]           ib_md.c:303  UCX  ERROR ibv_reg_mr(address=0x7f2b7a000000, length=17039818752, access=0xf) failed: Bad address
[1691647936.951780] [dgx13:69458:0]          ucp_mm.c:63   UCX  ERROR failed to register address 0x7f2b7a000000 (cuda) length 17039818752 on md[9]=mlx5_3: Input/output error (md supports: host|cuda)
[1691647936.951804] [dgx13:69458:0]     ucp_request.c:508  UCX  ERROR failed to register user buffer datatype 0x8 address 0x7f2ba9af0800 len 100000000: Input/output error
[dgx13:69458:0:69458]        rndv.c:544  Assertion `status == UCS_OK' failed
==== backtrace (tid:  69458) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f2fc405fced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f2fc405d8a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7f2fc405da3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x70679) [0x7f2fc1f05679]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_progress_rma_get_zcopy+0x4b) [0x7f2fc1f05d6b]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_receive+0x30c) [0x7f2fc1f0a37c]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_rndv_process_rts+0x110) [0x7f2fc1f1ce60]
 7  /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0(+0x3c975) [0x7f2fc1de9975]
 8  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f2fc1edc52a]
 9  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f2fc1f9117a]
10  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5609b78f9b08]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5609b78ea112]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5609b78e327a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5609b78f4c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
15  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5609b790970e]
16  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f2fe42ed2fe]
17  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5609b78ed2bc]
18  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5609b78a0817]
19  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5609b78ebf83]
20  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5609b78e9d36]
21  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
29  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5609b78e327a]
30  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5609b78f4c05]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5609b78e8fa7]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5609b78e327a]
33  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5609b7902935]
34  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5609b7903104]
35  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5609b79c9fc8]
36  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5609b78ed2bc]
37  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5609b78e81bb]
38  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
39  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5609b7902c72]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5609b78e81bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5609b78e327a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5609b78f4c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5609b78e481b]
46  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5609b78f4ef3]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5609b78e4568]
48  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5609b78e327a]
49  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5609b78f4c05]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5609b78e53cb]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5609b78e327a]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5609b78e2f07]
53  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5609b78e2eb9]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5609b79938bb]
55  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5609b79c1adc]
56  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5609b79bdc24]
57  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5609b79b57ed]
58  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5609b79b56bd]
59  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5609b79b48a2]
60  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x39) [0x5609b7987359]
61  /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f3063430083]
=================================
2023-08-10 06:12:17,091 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:17,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:17,221 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #075] ep: 0x7fddf08db180, tag: 0xf8898e82acb2afcb, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #075] ep: 0x7fddf08db180, tag: 0xf8898e82acb2afcb, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:17,239 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:44539 -> ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #059] ep: 0x7f334d5c63c0, tag: 0xdce0563d84cf7be0, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:17,240 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #054] ep: 0x7f334d5c6180, tag: 0xabc2cd953b8bbb80, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #054] ep: 0x7f334d5c6180, tag: 0xabc2cd953b8bbb80, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:17,241 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51937 -> ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #051] ep: 0x7f3e9e0b62c0, tag: 0x8f8033d95584bc7b, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:17,242 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #072] ep: 0x7f3e9e0b6240, tag: 0x8a64f44171e1b8c0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #072] ep: 0x7f3e9e0b6240, tag: 0x8a64f44171e1b8c0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-10 06:12:17,241 - tornado.application - ERROR - Exception in callback <bound method ActiveMemoryManagerExtension.run_once of <distributed.active_memory_manager.ActiveMemoryManagerExtension object at 0x7f963ed3d070>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 758, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/active_memory_manager.py", line 174, in run_once
    self._enact_suggestions()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/active_memory_manager.py", line 415, in _enact_suggestions
    self.scheduler.request_remove_replicas(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 7866, in request_remove_replicas
    self.stream_comms[addr].send(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 156, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <UCX (closed) Scheduler connection to worker local=ucx://127.0.0.1:58124 remote=ucx://127.0.0.1:58124> already closed.
2023-08-10 06:12:17,254 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:39063 -> ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #069] ep: 0x7fddf08db2c0, tag: 0xed21b480097e0a82, nbytes: 50000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:17,265 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-08-10 06:12:17,460 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:36630 -> ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #059] ep: 0x7ff0206682c0, tag: 0xbb8a3e4a09457728, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-10 06:12:17,461 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42796
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #064] ep: 0x7ff020668140, tag: 0xf3e31c060930ca00, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #064] ep: 0x7ff020668140, tag: 0xf3e31c060930ca00, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-10 06:12:17,751 - distributed.nanny - WARNING - Restarting worker
2023-08-10 06:12:19,221 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-10 06:12:19,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-10 06:12:23,899 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 0)
Function:  <dask.layers.CallableLazyImport object at 0x7f3a21
args:      ([                key   payload
73281     812345427  45277053
60994     808649942  31681538
73296     811818954  24094631
61011     839351319  20233865
78913     863129679  21393690
...             ...       ...
99983277  836417341  70852122
99983284  811251114  21163645
99983338  853057426  66301261
99983353  855345577  79451882
99983359  703544055  51781847

[12497168 rows x 2 columns],                 key   payload
61093     904409618  91081047
18517     961099256  40529834
61094     618357716  31457000
18526     935949663  48791914
61104     928466525  52188885
...             ...       ...
99991538  616886783  91291471
99991539  624932498  55247498
99991545  928403125  74500411
99991549  902389830  12325791
99991550   21630389  81947996

[12502889 rows x 2 columns],                  key   payload
64161     1039826010  68584853
64181      135605584  36720834
64184     1066359877  83383601
22598     1039649842  52227979
64185     1007695682    987566
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-10 06:12:24,040 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7f3a21
args:      ([                key   payload
73280     301460787  70613113
60996        638881  94159723
73284     610721114  47604585
61006     840414166  67454898
78916     109127426  57385069
...             ...       ...
99983285  862715121  58168324
99983332  843178853  22921353
99983341  844743323  77771821
99983343  211666324  30818046
99983346  812208598  85330982

[12498632 rows x 2 columns],                 key   payload
61089     922123419  78639904
18506     962143832  26703394
61090     911569532  97260955
18518     963413943   3303405
61092     423891129  87221568
...             ...       ...
99991413  913440848  20044448
99991414   19301861  71908656
99991415   13416895  67772195
99991524  966048814  82593727
99991535  219072430  20665764

[12502237 rows x 2 columns],                  key   payload
64160      527670230  64125225
64169     1011627031  57807775
64179     1035148720  26589523
22599      127942445  76160936
22622     1044662546   8758588
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-10 06:12:24,165 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7f3a21
args:      ([                key   payload
73288     867535414  70257925
60992     609464869  20458817
73295     844698880  21924434
61000     805355153  78114130
78915     849627411  40313860
...             ...       ...
99983292  102806362  10149108
99983329  838835460  43373290
99983333  838828323    699895
99983355  603858001  73042166
99983356  837331194  59245184

[12497796 rows x 2 columns],                 key   payload
61097     907671199  43775168
18504     963677263  69615153
61099     966519292  60482917
18508     919430276  88851481
61113     118145577  65841357
...             ...       ...
99991409  952949866  53875641
99991533  615422913  25729278
99991541  915487977  32328020
99991546  936884122  93504207
99991551  905870856  50725659

[12497151 rows x 2 columns],                  key   payload
64164     1000948007  79756417
64170     1028967211  35342595
64190      427955283  35500029
22601      637236447    845906
22610     1032978875  21174110
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-10 06:12:24,322 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-08-10 06:12:24,322 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-08-10 06:12:24,347 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-10 06:12:24,348 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-10 06:12:24,361 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-10 06:12:24,361 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-10 06:12:24,382 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7feb78
args:      ([                key   payload
73286     805849067  25623663
60993     311695253  95771693
73301     822906196  13500935
61003     831454222  47334416
78912     803942758  47263199
...             ...       ...
99983344     989238  95922743
99983345  865254519  71175825
99983349  812046354   6805508
99983351  853386382   8903194
99983358  824520016  74819146

[12500893 rows x 2 columns],                 key   payload
61095     916268094   1346802
18502     314430664  38321365
61102     939131412   1504504
18520     944399332  71806168
61107     114691777  56353383
...             ...       ...
99991395  957908684   3929853
99991406  908245174  15012571
99991419  915984923  36647761
99991528  724869995  93436443
99991542  962320433  13864109

[12495890 rows x 2 columns],                  key   payload
64166     1063609259  22674754
64172     1029406627  36204070
64173     1036726367  56351810
22607      434325593  83887075
64187     1059225565  80665175
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-10 06:12:24,670 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-f1e9396f54451b9cde4495a9a287c08c', 5)
Function:  subgraph_callable-ee5ddd85-1108-4118-b095-7cc60ca7
args:      (               key   payload
shuffle                     
0           107073  83041393
0           152681  19597058
0           140379  17128811
0           147154  52167383
0           189769  51029016
...            ...       ...
7        799704065  69042551
7        799977248  33231686
7        799937502  82033076
7        799774738   8432142
7        799896469   7659986

[99993166 rows x 2 columns],                  key   payload
73283      824884967  95369104
61001      807259252  11342855
73289      822336168  19969669
61007      206476241  96255862
78921      846419244  94739659
...              ...       ...
99962238    96148430  78754824
99962274  1535946265  55991662
99962277  1514640336  99580523
99962280  1500171124  64461986
99962283  1535043396  71376538

[99997899 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-10 06:12:25,800 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-10 06:12:25,800 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
