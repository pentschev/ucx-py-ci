============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-26 06:37:35,963 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:35,968 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41621 instead
  warnings.warn(
2024-01-26 06:37:35,972 - distributed.scheduler - INFO - State start
2024-01-26 06:37:35,995 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:35,995 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-26 06:37:35,996 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41621/status
2024-01-26 06:37:35,996 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:37:36,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44005'
2024-01-26 06:37:36,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32985'
2024-01-26 06:37:36,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33295'
2024-01-26 06:37:36,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44019'
2024-01-26 06:37:37,548 - distributed.scheduler - INFO - Receive client connection: Client-634b39f5-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:37,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54516
2024-01-26 06:37:38,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:38,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:38,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:38,071 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45017
2024-01-26 06:37:38,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45017
2024-01-26 06:37:38,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42279
2024-01-26 06:37:38,071 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,071 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,071 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:37:38,071 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:37:38,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-14hkw93h
2024-01-26 06:37:38,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b0556bb-ad41-4683-a508-5a8cdd64d88a
2024-01-26 06:37:38,072 - distributed.worker - INFO - Starting Worker plugin PreImport-6f66eb3c-36e4-4865-820a-124533ea07cf
2024-01-26 06:37:38,072 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56753e10-3bb3-4153-ae30-2f5c8ad0a3bd
2024-01-26 06:37:38,072 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:38,094 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:38,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:38,098 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38013
2024-01-26 06:37:38,099 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38013
2024-01-26 06:37:38,099 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43083
2024-01-26 06:37:38,099 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,099 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:37:38,099 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:37:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-hme0edsy
2024-01-26 06:37:38,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0316177-12e6-4b8d-b58a-facc3ff5eb74
2024-01-26 06:37:38,099 - distributed.worker - INFO - Starting Worker plugin PreImport-5f32d78a-e97e-435e-8a87-b25bb09a9755
2024-01-26 06:37:38,100 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7df786fd-5bd5-4fa3-9217-615d35cac2fa
2024-01-26 06:37:38,100 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:38,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:38,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:38,113 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44443
2024-01-26 06:37:38,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44443
2024-01-26 06:37:38,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43901
2024-01-26 06:37:38,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,113 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,113 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:37:38,113 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:37:38,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-wk7erlrf
2024-01-26 06:37:38,113 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6156197a-d840-4e47-aa2a-ab207ca3c14c
2024-01-26 06:37:38,114 - distributed.worker - INFO - Starting Worker plugin PreImport-ce2e08a3-4234-4b2a-8dd0-822f3961a89e
2024-01-26 06:37:38,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14130f7f-99cf-482c-a274-34b462796c5a
2024-01-26 06:37:38,114 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,123 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45017', status: init, memory: 0, processing: 0>
2024-01-26 06:37:38,124 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45017
2024-01-26 06:37:38,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54536
2024-01-26 06:37:38,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:38,126 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,126 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:37:38,168 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38013', status: init, memory: 0, processing: 0>
2024-01-26 06:37:38,169 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38013
2024-01-26 06:37:38,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54544
2024-01-26 06:37:38,169 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:38,170 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,170 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:37:38,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44443', status: init, memory: 0, processing: 0>
2024-01-26 06:37:38,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44443
2024-01-26 06:37:38,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54556
2024-01-26 06:37:38,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:38,183 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,183 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:37:38,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:38,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:38,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:38,387 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32893
2024-01-26 06:37:38,387 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32893
2024-01-26 06:37:38,387 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46557
2024-01-26 06:37:38,387 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,387 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,387 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:37:38,388 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:37:38,388 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-vuijro3u
2024-01-26 06:37:38,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9456427f-6f71-4a50-875b-afee9bbd0cbb
2024-01-26 06:37:38,388 - distributed.worker - INFO - Starting Worker plugin PreImport-ac3a28cb-e8ca-45f4-8128-023123800f79
2024-01-26 06:37:38,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3b78639-921e-4da9-9198-a064617d4345
2024-01-26 06:37:38,388 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,437 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32893', status: init, memory: 0, processing: 0>
2024-01-26 06:37:38,438 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32893
2024-01-26 06:37:38,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54558
2024-01-26 06:37:38,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:38,439 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:37:38,439 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:38,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:37:38,486 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:37:38,486 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:37:38,486 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:37:38,486 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:37:38,491 - distributed.scheduler - INFO - Remove client Client-634b39f5-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:38,492 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54516; closing.
2024-01-26 06:37:38,492 - distributed.scheduler - INFO - Remove client Client-634b39f5-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:38,493 - distributed.scheduler - INFO - Close client connection: Client-634b39f5-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:38,493 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44005'. Reason: nanny-close
2024-01-26 06:37:38,494 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:38,494 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32985'. Reason: nanny-close
2024-01-26 06:37:38,495 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:38,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33295'. Reason: nanny-close
2024-01-26 06:37:38,495 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38013. Reason: nanny-close
2024-01-26 06:37:38,495 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:38,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44019'. Reason: nanny-close
2024-01-26 06:37:38,495 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45017. Reason: nanny-close
2024-01-26 06:37:38,496 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:38,496 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32893. Reason: nanny-close
2024-01-26 06:37:38,496 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44443. Reason: nanny-close
2024-01-26 06:37:38,497 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:37:38,497 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54544; closing.
2024-01-26 06:37:38,497 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:37:38,497 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38013', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251058.4976525')
2024-01-26 06:37:38,497 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:37:38,498 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:38,498 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:38,498 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:37:38,498 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:38,500 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:38,500 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54536; closing.
2024-01-26 06:37:38,501 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45017', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251058.5016632')
2024-01-26 06:37:38,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54558; closing.
2024-01-26 06:37:38,502 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251058.5025477')
2024-01-26 06:37:38,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54556; closing.
2024-01-26 06:37:38,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44443', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251058.5032525')
2024-01-26 06:37:38,503 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:37:39,259 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:37:39,259 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:37:39,260 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:37:39,261 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-26 06:37:39,261 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-26 06:37:41,488 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:41,493 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45247 instead
  warnings.warn(
2024-01-26 06:37:41,497 - distributed.scheduler - INFO - State start
2024-01-26 06:37:41,519 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:41,520 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:37:41,521 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45247/status
2024-01-26 06:37:41,521 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:37:41,735 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34251'
2024-01-26 06:37:41,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39477'
2024-01-26 06:37:41,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46387'
2024-01-26 06:37:41,771 - distributed.scheduler - INFO - Receive client connection: Client-6690db28-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:41,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37235'
2024-01-26 06:37:41,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37515'
2024-01-26 06:37:41,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34502
2024-01-26 06:37:41,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37113'
2024-01-26 06:37:41,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45283'
2024-01-26 06:37:41,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33319'
2024-01-26 06:37:43,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,635 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42175
2024-01-26 06:37:43,635 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42175
2024-01-26 06:37:43,635 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33803
2024-01-26 06:37:43,635 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,635 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,635 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wjjw80s6
2024-01-26 06:37:43,635 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9ec0138-0a6f-4544-b5c9-ae9d35c34e88
2024-01-26 06:37:43,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc88d11b-e27f-4e0e-8234-e4bbb674e294
2024-01-26 06:37:43,651 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,656 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34101
2024-01-26 06:37:43,656 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34101
2024-01-26 06:37:43,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41313
2024-01-26 06:37:43,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,657 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,657 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-id_r3bsw
2024-01-26 06:37:43,657 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d53d971-6918-4831-98f7-75422358f533
2024-01-26 06:37:43,658 - distributed.worker - INFO - Starting Worker plugin PreImport-11df0ec7-2f7b-4e8f-9bba-a7805dea3eb3
2024-01-26 06:37:43,659 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cec61201-89d4-479e-be39-e6cb36ab273d
2024-01-26 06:37:43,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34857
2024-01-26 06:37:43,664 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34857
2024-01-26 06:37:43,664 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33471
2024-01-26 06:37:43,664 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,664 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,664 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,664 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,664 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bvnbafpp
2024-01-26 06:37:43,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-afc01e24-90f7-4f8f-8844-8658a94d7417
2024-01-26 06:37:43,665 - distributed.worker - INFO - Starting Worker plugin PreImport-51a984e9-9b7a-4e40-97b7-dc672b7d8ee9
2024-01-26 06:37:43,665 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4fd9a56-e065-4540-8ebe-fdb32b5110e6
2024-01-26 06:37:43,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,917 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46137
2024-01-26 06:37:43,917 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46137
2024-01-26 06:37:43,917 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45599
2024-01-26 06:37:43,917 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,917 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,917 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,918 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tt7wyn2p
2024-01-26 06:37:43,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe6db9b1-105d-4041-a149-a0a5757e35a3
2024-01-26 06:37:43,918 - distributed.worker - INFO - Starting Worker plugin PreImport-f4d656f0-b3fa-4fcc-8cdc-14271a871844
2024-01-26 06:37:43,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,918 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07d55de4-6cf0-4f7a-9862-762496a72480
2024-01-26 06:37:43,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,930 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,931 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33233
2024-01-26 06:37:43,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33233
2024-01-26 06:37:43,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33153
2024-01-26 06:37:43,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,933 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,933 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-htpfdxze
2024-01-26 06:37:43,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40147
2024-01-26 06:37:43,934 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40147
2024-01-26 06:37:43,934 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44571
2024-01-26 06:37:43,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f36d7361-7193-4880-8bb3-b3d9bf77e093
2024-01-26 06:37:43,934 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:43,934 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,934 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:43,934 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-28a67i2w
2024-01-26 06:37:43,935 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6537831c-2e90-408b-8024-f9359cc9ebd1
2024-01-26 06:37:43,935 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7770d41-8f51-4668-9c92-8195589672d5
2024-01-26 06:37:43,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,942 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45459
2024-01-26 06:37:43,942 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45459
2024-01-26 06:37:43,942 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42395
2024-01-26 06:37:43,942 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,942 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,942 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,942 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gvo_ken6
2024-01-26 06:37:43,943 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1eed38ac-4985-45f8-94ea-efdf400d63a1
2024-01-26 06:37:43,949 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:43,952 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37715
2024-01-26 06:37:43,952 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37715
2024-01-26 06:37:43,952 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33497
2024-01-26 06:37:43,952 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:43,952 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:43,952 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:43,953 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:43,953 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fstrstq3
2024-01-26 06:37:43,953 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65bc646e-a860-43a1-a22a-36d0f518df38
2024-01-26 06:37:44,682 - distributed.worker - INFO - Starting Worker plugin PreImport-8fdcbd34-a880-4ad5-9519-e33e313ec85a
2024-01-26 06:37:44,683 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:44,718 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42175', status: init, memory: 0, processing: 0>
2024-01-26 06:37:44,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42175
2024-01-26 06:37:44,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34522
2024-01-26 06:37:44,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:44,723 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:44,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:44,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,440 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,472 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34101', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,472 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34101
2024-01-26 06:37:45,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34524
2024-01-26 06:37:45,474 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,475 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,476 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,572 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,605 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34857', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34857
2024-01-26 06:37:45,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34534
2024-01-26 06:37:45,607 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,608 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,608 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,638 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,659 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46137', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,659 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46137
2024-01-26 06:37:45,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34536
2024-01-26 06:37:45,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,661 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,661 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,683 - distributed.worker - INFO - Starting Worker plugin PreImport-b3372e2d-2af8-4ae3-914c-1dfaf3e52b30
2024-01-26 06:37:45,683 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,696 - distributed.worker - INFO - Starting Worker plugin PreImport-6caeed9e-2728-44e9-a428-e4b64ea88c1d
2024-01-26 06:37:45,696 - distributed.worker - INFO - Starting Worker plugin PreImport-36bc4a3b-3733-49e6-b44a-a5c3ebd2ca88
2024-01-26 06:37:45,697 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91c06aa5-07ff-40db-97fe-fab0c0243f14
2024-01-26 06:37:45,697 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49a61eb0-e347-42be-80a1-bdcdc2aa2f11
2024-01-26 06:37:45,697 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,702 - distributed.worker - INFO - Starting Worker plugin PreImport-68ff64cd-a063-4c36-af80-8f6733889d99
2024-01-26 06:37:45,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1a88867-2bab-44ae-8f14-62ae300a4dfd
2024-01-26 06:37:45,703 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,710 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40147', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,711 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40147
2024-01-26 06:37:45,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34540
2024-01-26 06:37:45,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,713 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,713 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,720 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33233', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33233
2024-01-26 06:37:45,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34548
2024-01-26 06:37:45,721 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,722 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37715', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,722 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37715
2024-01-26 06:37:45,722 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34554
2024-01-26 06:37:45,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,732 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45459', status: init, memory: 0, processing: 0>
2024-01-26 06:37:45,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45459
2024-01-26 06:37:45,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34558
2024-01-26 06:37:45,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:45,735 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:45,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:45,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:45,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:45,774 - distributed.scheduler - INFO - Remove client Client-6690db28-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:45,774 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34502; closing.
2024-01-26 06:37:45,775 - distributed.scheduler - INFO - Remove client Client-6690db28-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:45,775 - distributed.scheduler - INFO - Close client connection: Client-6690db28-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:45,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34251'. Reason: nanny-close
2024-01-26 06:37:45,776 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39477'. Reason: nanny-close
2024-01-26 06:37:45,777 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,777 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46387'. Reason: nanny-close
2024-01-26 06:37:45,777 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42175. Reason: nanny-close
2024-01-26 06:37:45,777 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,777 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37235'. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34857. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37515'. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37715. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37113'. Reason: nanny-close
2024-01-26 06:37:45,778 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46137. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45283'. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34101. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33319'. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40147. Reason: nanny-close
2024-01-26 06:37:45,779 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:45,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34522; closing.
2024-01-26 06:37:45,780 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33233. Reason: nanny-close
2024-01-26 06:37:45,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,780 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7804282')
2024-01-26 06:37:45,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,780 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45459. Reason: nanny-close
2024-01-26 06:37:45,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34536; closing.
2024-01-26 06:37:45,781 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,781 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,781 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7816207')
2024-01-26 06:37:45,781 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,781 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,781 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,782 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34554; closing.
2024-01-26 06:37:45,782 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,782 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,783 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7830122')
2024-01-26 06:37:45,783 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,783 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,783 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34534; closing.
2024-01-26 06:37:45,783 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,784 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:45,785 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:45,784 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34536>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:37:45,786 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.786288')
2024-01-26 06:37:45,786 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34524; closing.
2024-01-26 06:37:45,786 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34540; closing.
2024-01-26 06:37:45,787 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34548; closing.
2024-01-26 06:37:45,787 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7875276')
2024-01-26 06:37:45,787 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7879145')
2024-01-26 06:37:45,788 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7882738')
2024-01-26 06:37:45,788 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34558; closing.
2024-01-26 06:37:45,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45459', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251065.7890441')
2024-01-26 06:37:45,789 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:37:46,842 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:37:46,842 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:37:46,843 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:37:46,844 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:37:46,845 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-26 06:37:49,170 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:49,175 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34529 instead
  warnings.warn(
2024-01-26 06:37:49,179 - distributed.scheduler - INFO - State start
2024-01-26 06:37:49,205 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:49,206 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:37:49,206 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34529/status
2024-01-26 06:37:49,207 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:37:49,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42987'
2024-01-26 06:37:49,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39531'
2024-01-26 06:37:49,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36021'
2024-01-26 06:37:49,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44735'
2024-01-26 06:37:49,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35893'
2024-01-26 06:37:49,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46619'
2024-01-26 06:37:49,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34169'
2024-01-26 06:37:49,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34367'
2024-01-26 06:37:49,543 - distributed.scheduler - INFO - Receive client connection: Client-6b1c9574-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:49,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34714
2024-01-26 06:37:51,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,251 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39353
2024-01-26 06:37:51,251 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39353
2024-01-26 06:37:51,251 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39845
2024-01-26 06:37:51,251 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,251 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,251 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,251 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,251 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t4n0y4dq
2024-01-26 06:37:51,251 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db8c837c-a3bb-4d1b-bc78-d6a68f36be89
2024-01-26 06:37:51,252 - distributed.worker - INFO - Starting Worker plugin PreImport-9cc81b04-f861-4b86-a975-393af9860c62
2024-01-26 06:37:51,252 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c1d8691-f566-4649-b089-0fb1240faf50
2024-01-26 06:37:51,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,303 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42385
2024-01-26 06:37:51,303 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42385
2024-01-26 06:37:51,303 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38307
2024-01-26 06:37:51,303 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,303 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,303 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,303 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,303 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oovnbydx
2024-01-26 06:37:51,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20b73f81-94bc-41e8-bf7e-d47de55bfcf2
2024-01-26 06:37:51,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46767
2024-01-26 06:37:51,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46767
2024-01-26 06:37:51,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35131
2024-01-26 06:37:51,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,304 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,304 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,304 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_qknby58
2024-01-26 06:37:51,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69ef8a67-b36c-4e99-a100-88285ffb2f4e
2024-01-26 06:37:51,305 - distributed.worker - INFO - Starting Worker plugin PreImport-9392edbe-cf5c-4b1b-98f0-a466047fad48
2024-01-26 06:37:51,305 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ddbc5a7-8fcf-45d7-8435-64688afe8e77
2024-01-26 06:37:51,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,308 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42921
2024-01-26 06:37:51,308 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42921
2024-01-26 06:37:51,308 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42561
2024-01-26 06:37:51,308 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,308 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,308 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,308 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ml41fsh4
2024-01-26 06:37:51,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17e426a3-a75c-405f-aeef-b4f5ffb61f79
2024-01-26 06:37:51,309 - distributed.worker - INFO - Starting Worker plugin PreImport-46c93988-1789-4253-bff6-10d5934abd34
2024-01-26 06:37:51,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee6c4d6a-edf5-42ef-a695-2148e53d6dcb
2024-01-26 06:37:51,310 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,311 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41453
2024-01-26 06:37:51,311 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41453
2024-01-26 06:37:51,311 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40997
2024-01-26 06:37:51,311 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,311 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,312 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,312 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zss9k71m
2024-01-26 06:37:51,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c028e753-46b0-4cb8-9a2c-9cb146884254
2024-01-26 06:37:51,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9436deb-68fc-4f80-92bb-67eb8723ca2a
2024-01-26 06:37:51,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,317 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,318 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43945
2024-01-26 06:37:51,318 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43945
2024-01-26 06:37:51,318 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36371
2024-01-26 06:37:51,318 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,318 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,318 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qzkhshjh
2024-01-26 06:37:51,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ef6fa38-c611-4429-9483-65980a4d6e51
2024-01-26 06:37:51,323 - distributed.worker - INFO - Starting Worker plugin PreImport-ee6da292-cbf6-4e9a-b160-68ad0154d2d1
2024-01-26 06:37:51,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e1a85cd-3cb6-451b-b616-2ea2aab43a7e
2024-01-26 06:37:51,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:51,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:51,520 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,521 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37711
2024-01-26 06:37:51,521 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:51,521 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37711
2024-01-26 06:37:51,521 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36933
2024-01-26 06:37:51,521 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,521 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,521 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,521 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,522 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mr4yegcw
2024-01-26 06:37:51,522 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a8f40e5-423b-4dbb-838c-514bbe25049b
2024-01-26 06:37:51,522 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38675
2024-01-26 06:37:51,522 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38675
2024-01-26 06:37:51,522 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39309
2024-01-26 06:37:51,522 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:51,522 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:51,522 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:51,522 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:51,522 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4d4c4zad
2024-01-26 06:37:51,522 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2dc1bc8-013a-492c-a5d0-e15cce01e57d
2024-01-26 06:37:51,523 - distributed.worker - INFO - Starting Worker plugin PreImport-2c3c325e-83bf-474c-baa5-f7332729d2bf
2024-01-26 06:37:51,523 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c348b0f3-16a9-4503-b613-4f84d6c7226a
2024-01-26 06:37:52,957 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:52,990 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39353', status: init, memory: 0, processing: 0>
2024-01-26 06:37:52,994 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39353
2024-01-26 06:37:52,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42088
2024-01-26 06:37:52,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:52,996 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:52,996 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:52,999 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,192 - distributed.worker - INFO - Starting Worker plugin PreImport-ed44463c-4998-41d8-947f-3ffe21a6351a
2024-01-26 06:37:53,193 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,200 - distributed.worker - INFO - Starting Worker plugin PreImport-d828408b-37f9-48b6-be6a-ed01c165e5c1
2024-01-26 06:37:53,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa197e01-3d86-4ffd-be6c-7a6d0d835300
2024-01-26 06:37:53,201 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,206 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41453', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41453
2024-01-26 06:37:53,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42096
2024-01-26 06:37:53,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,216 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,225 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,234 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42921', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42921
2024-01-26 06:37:53,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42118
2024-01-26 06:37:53,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,236 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42385', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,236 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,236 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42385
2024-01-26 06:37:53,237 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42110
2024-01-26 06:37:53,238 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,239 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,239 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,245 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46767', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46767
2024-01-26 06:37:53,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42124
2024-01-26 06:37:53,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,247 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,247 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,283 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,287 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,292 - distributed.worker - INFO - Starting Worker plugin PreImport-6d01bc82-b2ec-46cb-806a-bf7640f05431
2024-01-26 06:37:53,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b996146-556a-46f2-9c3c-51cfcb0aca8a
2024-01-26 06:37:53,294 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,308 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38675', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,309 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38675
2024-01-26 06:37:53,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42146
2024-01-26 06:37:53,310 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,311 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,313 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,315 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43945', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,316 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43945
2024-01-26 06:37:53,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42134
2024-01-26 06:37:53,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37711', status: init, memory: 0, processing: 0>
2024-01-26 06:37:53,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37711
2024-01-26 06:37:53,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42156
2024-01-26 06:37:53,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:53,328 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:53,328 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:53,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,370 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:37:53,374 - distributed.scheduler - INFO - Remove client Client-6b1c9574-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:53,375 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34714; closing.
2024-01-26 06:37:53,375 - distributed.scheduler - INFO - Remove client Client-6b1c9574-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:53,375 - distributed.scheduler - INFO - Close client connection: Client-6b1c9574-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:53,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42987'. Reason: nanny-close
2024-01-26 06:37:53,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39531'. Reason: nanny-close
2024-01-26 06:37:53,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36021'. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42385. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44735'. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39353. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35893'. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41453. Reason: nanny-close
2024-01-26 06:37:53,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46619'. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42921. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34169'. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37711. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34367'. Reason: nanny-close
2024-01-26 06:37:53,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43945. Reason: nanny-close
2024-01-26 06:37:53,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42110; closing.
2024-01-26 06:37:53,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,380 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:37:53,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46767. Reason: nanny-close
2024-01-26 06:37:53,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,380 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3806107')
2024-01-26 06:37:53,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38675. Reason: nanny-close
2024-01-26 06:37:53,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42118; closing.
2024-01-26 06:37:53,381 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42921', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.381835')
2024-01-26 06:37:53,381 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,381 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42096; closing.
2024-01-26 06:37:53,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,382 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42088; closing.
2024-01-26 06:37:53,382 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:37:53,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3832788')
2024-01-26 06:37:53,383 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39353', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3836317')
2024-01-26 06:37:53,383 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,384 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,384 - distributed.nanny - INFO - Worker closed
2024-01-26 06:37:53,384 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42118>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:37:53,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42156; closing.
2024-01-26 06:37:53,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42134; closing.
2024-01-26 06:37:53,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42124; closing.
2024-01-26 06:37:53,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3873117')
2024-01-26 06:37:53,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43945', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3876839')
2024-01-26 06:37:53,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3880303')
2024-01-26 06:37:53,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42146; closing.
2024-01-26 06:37:53,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251073.3887298')
2024-01-26 06:37:53,388 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:37:54,242 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:37:54,242 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:37:54,243 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:37:54,244 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:37:54,244 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-26 06:37:56,574 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:56,579 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33635 instead
  warnings.warn(
2024-01-26 06:37:56,583 - distributed.scheduler - INFO - State start
2024-01-26 06:37:56,605 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:37:56,606 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:37:56,606 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33635/status
2024-01-26 06:37:56,607 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:37:56,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38853'
2024-01-26 06:37:56,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39289'
2024-01-26 06:37:56,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34403'
2024-01-26 06:37:56,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46845'
2024-01-26 06:37:56,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42741'
2024-01-26 06:37:57,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42181'
2024-01-26 06:37:57,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36777'
2024-01-26 06:37:57,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35977'
2024-01-26 06:37:58,113 - distributed.scheduler - INFO - Receive client connection: Client-6f8cb850-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:37:58,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42330
2024-01-26 06:37:58,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:58,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:58,791 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:58,792 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37973
2024-01-26 06:37:58,792 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37973
2024-01-26 06:37:58,792 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44813
2024-01-26 06:37:58,792 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:58,792 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:58,792 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:58,792 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:58,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aohfjpo2
2024-01-26 06:37:58,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-493efa30-fafe-457a-9eb4-265874eddbb0
2024-01-26 06:37:58,793 - distributed.worker - INFO - Starting Worker plugin PreImport-7b620699-ad86-40d1-a5a0-5af0564be7ea
2024-01-26 06:37:58,793 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a41c273-11ba-4ced-a27a-d77fd1461a57
2024-01-26 06:37:59,025 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,025 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,030 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,030 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45403
2024-01-26 06:37:59,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45403
2024-01-26 06:37:59,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44405
2024-01-26 06:37:59,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,031 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,031 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9bgb61zb
2024-01-26 06:37:59,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4cd5cfe-22a5-4a8a-ae33-fbf45e65c52c
2024-01-26 06:37:59,031 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8fe7f4b1-c696-4a4d-a64e-cdf774bb4d50
2024-01-26 06:37:59,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,033 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,034 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38435
2024-01-26 06:37:59,034 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38435
2024-01-26 06:37:59,034 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37515
2024-01-26 06:37:59,034 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,034 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,034 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,034 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-roomp54d
2024-01-26 06:37:59,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae5b1e37-c829-4722-a862-190229ca1e0c
2024-01-26 06:37:59,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45417
2024-01-26 06:37:59,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45417
2024-01-26 06:37:59,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41581
2024-01-26 06:37:59,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,040 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,040 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d598dll8
2024-01-26 06:37:59,040 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af8e71e7-1213-4942-baa3-bb41ef3987ac
2024-01-26 06:37:59,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43963
2024-01-26 06:37:59,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43963
2024-01-26 06:37:59,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44721
2024-01-26 06:37:59,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,043 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,043 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,043 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j9u8_sb9
2024-01-26 06:37:59,043 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14c5fb99-3034-438f-81d8-a4aaeefc4c8f
2024-01-26 06:37:59,043 - distributed.worker - INFO - Starting Worker plugin PreImport-2174888d-569a-4762-81f0-24eee2eb1cea
2024-01-26 06:37:59,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5af85aae-6cf9-4a22-a4ad-a7b2912aac0a
2024-01-26 06:37:59,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:37:59,043 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,043 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:37:59,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43911
2024-01-26 06:37:59,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43911
2024-01-26 06:37:59,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36891
2024-01-26 06:37:59,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,046 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z9q8mkkn
2024-01-26 06:37:59,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-814c1b94-03ab-4b46-ac1f-0575da23e204
2024-01-26 06:37:59,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef013984-d556-4bdc-b0f3-b3b0a7c0687c
2024-01-26 06:37:59,048 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37377
2024-01-26 06:37:59,048 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37377
2024-01-26 06:37:59,048 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34395
2024-01-26 06:37:59,048 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,048 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,048 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,048 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,048 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-37lw449l
2024-01-26 06:37:59,049 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ec73e39-2092-48da-b37a-a2e21d38df64
2024-01-26 06:37:59,049 - distributed.worker - INFO - Starting Worker plugin PreImport-0cc5900b-af61-4bd2-a41b-fbf5974d1149
2024-01-26 06:37:59,049 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88caeabe-d03f-473b-9b63-574269eccb6f
2024-01-26 06:37:59,054 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:37:59,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35349
2024-01-26 06:37:59,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35349
2024-01-26 06:37:59,056 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34893
2024-01-26 06:37:59,056 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,056 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,056 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:37:59,056 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:37:59,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nk1mulzy
2024-01-26 06:37:59,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b471c65e-bca5-484f-a379-91913a94cd52
2024-01-26 06:37:59,310 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,342 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37973', status: init, memory: 0, processing: 0>
2024-01-26 06:37:59,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37973
2024-01-26 06:37:59,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42344
2024-01-26 06:37:59,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:37:59,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:37:59,348 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:37:59,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,844 - distributed.worker - INFO - Starting Worker plugin PreImport-cc04df28-55d5-43b3-aeaa-0996c8bc1ba4
2024-01-26 06:38:00,844 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef14e4ac-1c4b-457d-8c06-8ec7a6c510d1
2024-01-26 06:38:00,845 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,860 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38435', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,869 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38435
2024-01-26 06:38:00,869 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48652
2024-01-26 06:38:00,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,883 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43963', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,883 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43963
2024-01-26 06:38:00,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48660
2024-01-26 06:38:00,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,885 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,887 - distributed.worker - INFO - Starting Worker plugin PreImport-5e9d5f7c-b00d-4be6-8b04-ccee450938b0
2024-01-26 06:38:00,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-373b44dc-f1b8-4ad9-b447-56bf8b156449
2024-01-26 06:38:00,888 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,890 - distributed.worker - INFO - Starting Worker plugin PreImport-79fac688-2a00-44fe-9d8b-cfc8d3b65df7
2024-01-26 06:38:00,891 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcaa7ccd-a30c-454d-9dbe-a8337e4736c7
2024-01-26 06:38:00,891 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,892 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,894 - distributed.worker - INFO - Starting Worker plugin PreImport-1fef0d96-3544-49cb-84e3-6707b80988cd
2024-01-26 06:38:00,896 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,897 - distributed.worker - INFO - Starting Worker plugin PreImport-38d648b3-5132-4869-9f27-9f58bb3442bb
2024-01-26 06:38:00,899 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45417', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45417
2024-01-26 06:38:00,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48666
2024-01-26 06:38:00,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,912 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37377', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37377
2024-01-26 06:38:00,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48672
2024-01-26 06:38:00,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,918 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,918 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,920 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35349', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,921 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35349
2024-01-26 06:38:00,921 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48684
2024-01-26 06:38:00,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,923 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,923 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,929 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43911', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,929 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43911
2024-01-26 06:38:00,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48692
2024-01-26 06:38:00,930 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45403', status: init, memory: 0, processing: 0>
2024-01-26 06:38:00,931 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45403
2024-01-26 06:38:00,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48696
2024-01-26 06:38:00,931 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,932 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,932 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:00,933 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:00,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:00,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,986 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,987 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:00,998 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,998 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,998 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,998 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,998 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,998 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,999 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:00,999 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:01,007 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:01,009 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:01,011 - distributed.scheduler - INFO - Remove client Client-6f8cb850-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:01,011 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42330; closing.
2024-01-26 06:38:01,012 - distributed.scheduler - INFO - Remove client Client-6f8cb850-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:01,012 - distributed.scheduler - INFO - Close client connection: Client-6f8cb850-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:01,013 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38853'. Reason: nanny-close
2024-01-26 06:38:01,013 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,013 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39289'. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34403'. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45403. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46845'. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37973. Reason: nanny-close
2024-01-26 06:38:01,014 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42741'. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35349. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42181'. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43963. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36777'. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37377. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35977'. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43911. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,016 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45417. Reason: nanny-close
2024-01-26 06:38:01,016 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,017 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38435. Reason: nanny-close
2024-01-26 06:38:01,017 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48696; closing.
2024-01-26 06:38:01,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,017 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42344; closing.
2024-01-26 06:38:01,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,017 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45403', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.0177896')
2024-01-26 06:38:01,018 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,018 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,018 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.018424')
2024-01-26 06:38:01,018 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,018 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,018 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,018 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,018 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48672; closing.
2024-01-26 06:38:01,018 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:01,018 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48660; closing.
2024-01-26 06:38:01,019 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,019 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,019 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.0196238')
2024-01-26 06:38:01,019 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,020 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.0200074')
2024-01-26 06:38:01,020 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,020 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:01,020 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48684; closing.
2024-01-26 06:38:01,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48692; closing.
2024-01-26 06:38:01,021 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.0215065')
2024-01-26 06:38:01,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48666; closing.
2024-01-26 06:38:01,022 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48672>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:38:01,024 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48660>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:38:01,024 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43911', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.024463')
2024-01-26 06:38:01,024 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45417', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.0248392')
2024-01-26 06:38:01,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48652; closing.
2024-01-26 06:38:01,025 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251081.0254993')
2024-01-26 06:38:01,025 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:01,978 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:01,979 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:01,979 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:01,981 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:01,981 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-26 06:38:04,377 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:04,382 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:38:04,386 - distributed.scheduler - INFO - State start
2024-01-26 06:38:04,409 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:04,410 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:04,410 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-26 06:38:04,411 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:04,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33511'
2024-01-26 06:38:04,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36911'
2024-01-26 06:38:04,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40543'
2024-01-26 06:38:04,668 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38163'
2024-01-26 06:38:04,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35437'
2024-01-26 06:38:04,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43137'
2024-01-26 06:38:04,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41513'
2024-01-26 06:38:04,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42633'
2024-01-26 06:38:05,220 - distributed.scheduler - INFO - Receive client connection: Client-74219295-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:05,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48854
2024-01-26 06:38:06,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,587 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,587 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,590 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38351
2024-01-26 06:38:06,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38351
2024-01-26 06:38:06,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45251
2024-01-26 06:38:06,591 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,591 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,591 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,591 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_lysc2i
2024-01-26 06:38:06,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-318d0e88-857e-42aa-b76f-76b65e5afe6f
2024-01-26 06:38:06,591 - distributed.worker - INFO - Starting Worker plugin PreImport-fb6aef2b-2cfc-404b-965b-87b06876790a
2024-01-26 06:38:06,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42b2ed82-cc3c-4881-b21c-3e0f6e034972
2024-01-26 06:38:06,591 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45371
2024-01-26 06:38:06,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45371
2024-01-26 06:38:06,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35097
2024-01-26 06:38:06,592 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,593 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,593 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,593 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ty7s7kaf
2024-01-26 06:38:06,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c831a5f-edcf-41f1-b121-b56dd735c868
2024-01-26 06:38:06,593 - distributed.worker - INFO - Starting Worker plugin PreImport-f72261f0-99aa-474b-b715-2f29dacaee04
2024-01-26 06:38:06,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,593 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f521f4dc-8e16-442c-b901-31b7b31d32ea
2024-01-26 06:38:06,593 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36105
2024-01-26 06:38:06,593 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36105
2024-01-26 06:38:06,593 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40343
2024-01-26 06:38:06,594 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,594 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,594 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,594 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,594 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zn71vue4
2024-01-26 06:38:06,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-646a8962-e1fa-4652-a518-c6c97fde6e0b
2024-01-26 06:38:06,595 - distributed.worker - INFO - Starting Worker plugin PreImport-e8d13b29-2d37-4e45-bc7f-c00723d489a6
2024-01-26 06:38:06,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc88ad7b-0fb0-47ef-9f27-3bc860e3b978
2024-01-26 06:38:06,596 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,596 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,598 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45621
2024-01-26 06:38:06,598 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45621
2024-01-26 06:38:06,598 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34047
2024-01-26 06:38:06,598 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,598 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-17py9hk3
2024-01-26 06:38:06,599 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-278f2ca1-abff-46ad-be21-4e8119909e35
2024-01-26 06:38:06,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38785
2024-01-26 06:38:06,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38785
2024-01-26 06:38:06,601 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39197
2024-01-26 06:38:06,601 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,602 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yluw_l8f
2024-01-26 06:38:06,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9891ed0-a344-402e-85b5-433c6bb104f8
2024-01-26 06:38:06,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea763fcf-c8d7-457b-97b1-73afca09d372
2024-01-26 06:38:06,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a370b42b-2e1a-4409-ba04-80a99da0d8fb
2024-01-26 06:38:06,696 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,704 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35289
2024-01-26 06:38:06,704 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35289
2024-01-26 06:38:06,704 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40615
2024-01-26 06:38:06,704 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,704 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,704 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,704 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,704 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s8e5r9cs
2024-01-26 06:38:06,704 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efdd8d89-d12a-4e7a-b689-08729c8dfe2a
2024-01-26 06:38:06,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,720 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,720 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44363
2024-01-26 06:38:06,720 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44363
2024-01-26 06:38:06,721 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39617
2024-01-26 06:38:06,721 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,721 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,721 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,721 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wl19dk32
2024-01-26 06:38:06,721 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b0ad7f9-f9c2-4464-872a-1664afcf91ae
2024-01-26 06:38:06,726 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:06,726 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:06,730 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:06,731 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36615
2024-01-26 06:38:06,731 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36615
2024-01-26 06:38:06,731 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39471
2024-01-26 06:38:06,731 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:06,731 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:06,731 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:06,731 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:06,732 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-igeq6475
2024-01-26 06:38:06,732 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92ab1941-c215-4843-aeff-e9e1c39ad3d4
2024-01-26 06:38:08,551 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,576 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38351', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,578 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38351
2024-01-26 06:38:08,578 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48872
2024-01-26 06:38:08,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,580 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,580 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,655 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,663 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,687 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36105', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,687 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36105
2024-01-26 06:38:08,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48884
2024-01-26 06:38:08,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,689 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,690 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,691 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,693 - distributed.worker - INFO - Starting Worker plugin PreImport-d5ba4044-8dda-4351-8c24-b448be1b68ed
2024-01-26 06:38:08,693 - distributed.worker - INFO - Starting Worker plugin PreImport-c3bcfa62-59f6-4cf3-b4fa-e321ee96f1d7
2024-01-26 06:38:08,694 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,695 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45371', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45371
2024-01-26 06:38:08,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48886
2024-01-26 06:38:08,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,722 - distributed.worker - INFO - Starting Worker plugin PreImport-9ec52b6a-59d5-4b1e-88c6-40ad3a32c327
2024-01-26 06:38:08,723 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a777842-e0dc-44bf-8dc2-13c8613e26f5
2024-01-26 06:38:08,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,724 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45621', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,725 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45621
2024-01-26 06:38:08,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48898
2024-01-26 06:38:08,726 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38785', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,726 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38785
2024-01-26 06:38:08,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48900
2024-01-26 06:38:08,727 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,728 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,728 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,729 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,729 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,733 - distributed.worker - INFO - Starting Worker plugin PreImport-50ce2233-1e49-4982-a702-bb8b59b83500
2024-01-26 06:38:08,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ecd1a6a6-714a-47c7-b2a3-594ddf98f017
2024-01-26 06:38:08,735 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,735 - distributed.worker - INFO - Starting Worker plugin PreImport-58c4cb60-0be7-4c51-9da5-47414196bc2c
2024-01-26 06:38:08,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-271b7168-bb5e-4ac0-beff-7ba3d3c6793e
2024-01-26 06:38:08,737 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,748 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35289', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35289
2024-01-26 06:38:08,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48914
2024-01-26 06:38:08,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,750 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,750 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,767 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36615', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,768 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36615
2024-01-26 06:38:08,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48918
2024-01-26 06:38:08,769 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44363', status: init, memory: 0, processing: 0>
2024-01-26 06:38:08,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44363
2024-01-26 06:38:08,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48922
2024-01-26 06:38:08,769 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,770 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,770 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,771 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:08,772 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:08,772 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:08,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:08,820 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,820 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:08,845 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,847 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:08,849 - distributed.scheduler - INFO - Remove client Client-74219295-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:08,850 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48854; closing.
2024-01-26 06:38:08,850 - distributed.scheduler - INFO - Remove client Client-74219295-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:08,850 - distributed.scheduler - INFO - Close client connection: Client-74219295-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:08,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33511'. Reason: nanny-close
2024-01-26 06:38:08,852 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,852 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36911'. Reason: nanny-close
2024-01-26 06:38:08,852 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,852 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40543'. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38785. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38163'. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36105. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35437'. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35289. Reason: nanny-close
2024-01-26 06:38:08,853 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43137'. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38351. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41513'. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45371. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,854 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42633'. Reason: nanny-close
2024-01-26 06:38:08,855 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:08,855 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45621. Reason: nanny-close
2024-01-26 06:38:08,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48914; closing.
2024-01-26 06:38:08,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,855 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36615. Reason: nanny-close
2024-01-26 06:38:08,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48884; closing.
2024-01-26 06:38:08,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,856 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8561146')
2024-01-26 06:38:08,856 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44363. Reason: nanny-close
2024-01-26 06:38:08,856 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,856 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,856 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8568563')
2024-01-26 06:38:08,856 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,857 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,857 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,857 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,857 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48900; closing.
2024-01-26 06:38:08,857 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,858 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48872; closing.
2024-01-26 06:38:08,858 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:08,858 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,858 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8586512')
2024-01-26 06:38:08,859 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,859 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,860 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:08,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38351', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8602295')
2024-01-26 06:38:08,860 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48886; closing.
2024-01-26 06:38:08,861 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48872>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48872>: Stream is closed
2024-01-26 06:38:08,863 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48900>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:38:08,864 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45371', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.864443')
2024-01-26 06:38:08,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48898; closing.
2024-01-26 06:38:08,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48918; closing.
2024-01-26 06:38:08,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48922; closing.
2024-01-26 06:38:08,865 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8659105')
2024-01-26 06:38:08,866 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8663929')
2024-01-26 06:38:08,866 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251088.8668838')
2024-01-26 06:38:08,867 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:09,817 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:09,817 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:09,818 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:09,819 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:09,819 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-26 06:38:12,108 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:12,112 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:38:12,116 - distributed.scheduler - INFO - State start
2024-01-26 06:38:12,138 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:12,139 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:12,139 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-26 06:38:12,140 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:12,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44667'
2024-01-26 06:38:12,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33143'
2024-01-26 06:38:12,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45649'
2024-01-26 06:38:12,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41847'
2024-01-26 06:38:12,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40817'
2024-01-26 06:38:12,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35645'
2024-01-26 06:38:12,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41079'
2024-01-26 06:38:12,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43809'
2024-01-26 06:38:12,849 - distributed.scheduler - INFO - Receive client connection: Client-78c578fe-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:12,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51328
2024-01-26 06:38:14,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,229 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33433
2024-01-26 06:38:14,229 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33433
2024-01-26 06:38:14,229 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41997
2024-01-26 06:38:14,229 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,229 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,229 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,229 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,230 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rm77ue9u
2024-01-26 06:38:14,230 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9dabf102-b9f5-4b1d-8a59-15c9c0c1ce70
2024-01-26 06:38:14,231 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5cb840f-c73e-4614-8bbe-2e7603efddfd
2024-01-26 06:38:14,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36685
2024-01-26 06:38:14,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36685
2024-01-26 06:38:14,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41707
2024-01-26 06:38:14,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,289 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,289 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ya6fc9w3
2024-01-26 06:38:14,289 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de6606f5-36ef-4700-9077-f7930467bedd
2024-01-26 06:38:14,291 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,291 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,295 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,296 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43549
2024-01-26 06:38:14,296 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43549
2024-01-26 06:38:14,296 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42887
2024-01-26 06:38:14,296 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,296 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,296 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,296 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,296 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_tmdjniu
2024-01-26 06:38:14,296 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37888702-b95e-44a8-801c-1d9c930fbb4e
2024-01-26 06:38:14,297 - distributed.worker - INFO - Starting Worker plugin PreImport-3f2e5d8c-f8ea-4c80-b1ca-705ec6726519
2024-01-26 06:38:14,297 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be97c10d-f60c-45f0-ae2f-6f6faca830da
2024-01-26 06:38:14,497 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,497 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,501 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,502 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41649
2024-01-26 06:38:14,502 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41649
2024-01-26 06:38:14,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38293
2024-01-26 06:38:14,502 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,502 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,502 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,503 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,503 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ighf0_ua
2024-01-26 06:38:14,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5edf471-9150-45eb-97af-bbb23e2281b0
2024-01-26 06:38:14,503 - distributed.worker - INFO - Starting Worker plugin PreImport-3ff9b55b-6a8e-4a31-8962-1ceb0bc890d4
2024-01-26 06:38:14,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce2d77ee-3381-4499-8c80-b32379b266f6
2024-01-26 06:38:14,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38197
2024-01-26 06:38:14,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38197
2024-01-26 06:38:14,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44077
2024-01-26 06:38:14,506 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,506 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,506 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,506 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ivscyq7
2024-01-26 06:38:14,507 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b38a87af-b550-4986-b9ac-3cdb220a58e6
2024-01-26 06:38:14,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,514 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,515 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43241
2024-01-26 06:38:14,515 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43241
2024-01-26 06:38:14,516 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39067
2024-01-26 06:38:14,516 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,516 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,516 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,516 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,516 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6n702z7k
2024-01-26 06:38:14,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f64d3c5-9462-4801-9558-b7d15993ee5b
2024-01-26 06:38:14,516 - distributed.worker - INFO - Starting Worker plugin PreImport-b727b86c-f81a-4f29-9d27-b8c4540f3dea
2024-01-26 06:38:14,517 - distributed.worker - INFO - Starting Worker plugin RMMSetup-438c40c0-6e0d-4faa-a185-5d956ecbfaf3
2024-01-26 06:38:14,518 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,518 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:14,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:14,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38465
2024-01-26 06:38:14,531 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38465
2024-01-26 06:38:14,531 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42243
2024-01-26 06:38:14,531 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,531 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,531 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,531 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ka869kmt
2024-01-26 06:38:14,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-374800b5-9b51-407c-aa4c-736875e41bfe
2024-01-26 06:38:14,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c9624d9-bf00-4a0f-8576-61e8e3c2af70
2024-01-26 06:38:14,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:14,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34159
2024-01-26 06:38:14,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34159
2024-01-26 06:38:14,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33137
2024-01-26 06:38:14,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:14,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:14,535 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:14,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:14,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-82gyc51n
2024-01-26 06:38:14,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59770fc3-1e62-4df0-8f96-b1a2e274a27f
2024-01-26 06:38:15,074 - distributed.worker - INFO - Starting Worker plugin PreImport-981143b5-a7b7-4878-b4ce-887f7addf397
2024-01-26 06:38:15,075 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:15,104 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33433', status: init, memory: 0, processing: 0>
2024-01-26 06:38:15,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33433
2024-01-26 06:38:15,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51342
2024-01-26 06:38:15,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:15,109 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:15,109 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:15,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,201 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,209 - distributed.worker - INFO - Starting Worker plugin PreImport-86cacba4-9dd4-4fbb-83ab-8ed2157366ec
2024-01-26 06:38:16,210 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af9563fa-f6b2-49ab-8f5f-a8de54b72ebe
2024-01-26 06:38:16,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,217 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,233 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43549', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,234 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43549
2024-01-26 06:38:16,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51362
2024-01-26 06:38:16,235 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36685', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36685
2024-01-26 06:38:16,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51364
2024-01-26 06:38:16,236 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,236 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,237 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41649', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,237 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,238 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41649
2024-01-26 06:38:16,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51370
2024-01-26 06:38:16,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,238 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,239 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,239 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,241 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,269 - distributed.worker - INFO - Starting Worker plugin PreImport-fe578f53-94f9-4f28-bf52-c7aba620d205
2024-01-26 06:38:16,270 - distributed.worker - INFO - Starting Worker plugin PreImport-ac3273a4-46eb-40e2-97d9-fec83585b022
2024-01-26 06:38:16,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bf4ff84-f22a-40f6-9396-c197c8c4d752
2024-01-26 06:38:16,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38197', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,292 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38197
2024-01-26 06:38:16,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51386
2024-01-26 06:38:16,292 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,293 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,293 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,294 - distributed.worker - INFO - Starting Worker plugin PreImport-4586c661-5bbe-4a73-953a-3e06b5ed131c
2024-01-26 06:38:16,295 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf0ddbcd-c0cc-4b1b-ad93-75c5be29c300
2024-01-26 06:38:16,295 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,300 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38465', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,300 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38465
2024-01-26 06:38:16,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51388
2024-01-26 06:38:16,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,302 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,302 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,317 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34159', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,318 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34159
2024-01-26 06:38:16,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51390
2024-01-26 06:38:16,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,320 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,320 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43241', status: init, memory: 0, processing: 0>
2024-01-26 06:38:16,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43241
2024-01-26 06:38:16,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51402
2024-01-26 06:38:16,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:16,328 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:16,328 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:16,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:16,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:16,380 - distributed.scheduler - INFO - Remove client Client-78c578fe-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:16,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51328; closing.
2024-01-26 06:38:16,380 - distributed.scheduler - INFO - Remove client Client-78c578fe-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:16,381 - distributed.scheduler - INFO - Close client connection: Client-78c578fe-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:16,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44667'. Reason: nanny-close
2024-01-26 06:38:16,382 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33143'. Reason: nanny-close
2024-01-26 06:38:16,382 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45649'. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33433. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41847'. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43241. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40817'. Reason: nanny-close
2024-01-26 06:38:16,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36685. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35645'. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41649. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41079'. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43549. Reason: nanny-close
2024-01-26 06:38:16,384 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43809'. Reason: nanny-close
2024-01-26 06:38:16,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38465. Reason: nanny-close
2024-01-26 06:38:16,385 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:16,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38197. Reason: nanny-close
2024-01-26 06:38:16,385 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51342; closing.
2024-01-26 06:38:16,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33433', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.3857813')
2024-01-26 06:38:16,386 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,386 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,386 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34159. Reason: nanny-close
2024-01-26 06:38:16,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51364; closing.
2024-01-26 06:38:16,387 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,387 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,387 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.387718')
2024-01-26 06:38:16,387 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51402; closing.
2024-01-26 06:38:16,388 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:16,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51370; closing.
2024-01-26 06:38:16,388 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,388 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,388 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.389341')
2024-01-26 06:38:16,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51362; closing.
2024-01-26 06:38:16,389 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:16,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41649', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.3898761')
2024-01-26 06:38:16,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51388; closing.
2024-01-26 06:38:16,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51386; closing.
2024-01-26 06:38:16,390 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43549', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.3908985')
2024-01-26 06:38:16,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38465', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.3912487')
2024-01-26 06:38:16,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.3916314')
2024-01-26 06:38:16,392 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51390; closing.
2024-01-26 06:38:16,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34159', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251096.3927383')
2024-01-26 06:38:16,393 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:17,247 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:17,247 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:17,248 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:17,249 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:17,249 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-26 06:38:19,506 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:19,510 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:38:19,514 - distributed.scheduler - INFO - State start
2024-01-26 06:38:19,536 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:19,537 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:19,538 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-26 06:38:19,538 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:19,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38681'
2024-01-26 06:38:21,421 - distributed.scheduler - INFO - Receive client connection: Client-7d34ed27-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:21,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57792
2024-01-26 06:38:21,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:21,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:22,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:22,131 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40899
2024-01-26 06:38:22,132 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40899
2024-01-26 06:38:22,132 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-26 06:38:22,132 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:22,132 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:22,132 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:22,132 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:38:22,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qp5_4pc0
2024-01-26 06:38:22,132 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b491d5b-b930-4476-8e67-9b02eec2138a
2024-01-26 06:38:22,132 - distributed.worker - INFO - Starting Worker plugin PreImport-e684e514-af9f-453e-86d0-0bd4f97bba1e
2024-01-26 06:38:22,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-517ed43d-55ce-47e6-ba31-d587e640b841
2024-01-26 06:38:22,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:22,189 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40899', status: init, memory: 0, processing: 0>
2024-01-26 06:38:22,190 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40899
2024-01-26 06:38:22,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57796
2024-01-26 06:38:22,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:22,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:22,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:22,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:22,252 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:22,254 - distributed.scheduler - INFO - Remove client Client-7d34ed27-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:22,254 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57792; closing.
2024-01-26 06:38:22,255 - distributed.scheduler - INFO - Remove client Client-7d34ed27-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:22,255 - distributed.scheduler - INFO - Close client connection: Client-7d34ed27-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:22,256 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38681'. Reason: nanny-close
2024-01-26 06:38:22,256 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:22,257 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40899. Reason: nanny-close
2024-01-26 06:38:22,259 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:22,259 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57796; closing.
2024-01-26 06:38:22,259 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40899', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251102.2596436')
2024-01-26 06:38:22,259 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:22,260 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:22,871 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:22,871 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:22,873 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:22,875 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:22,875 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-26 06:38:27,431 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:27,435 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:38:27,439 - distributed.scheduler - INFO - State start
2024-01-26 06:38:27,462 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:27,463 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:27,464 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-26 06:38:27,464 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:27,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36067'
2024-01-26 06:38:29,396 - distributed.scheduler - INFO - Receive client connection: Client-81e73472-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:29,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57870
2024-01-26 06:38:29,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:29,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:30,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:30,080 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43693
2024-01-26 06:38:30,080 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43693
2024-01-26 06:38:30,080 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34287
2024-01-26 06:38:30,080 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:30,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:30,080 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:30,080 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:38:30,081 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nvkrjd5b
2024-01-26 06:38:30,081 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6cdcdfd-fdbb-48ff-a74d-2882fefbb40d
2024-01-26 06:38:30,081 - distributed.worker - INFO - Starting Worker plugin PreImport-aa780780-3624-4218-8ed9-8b5173ee88d5
2024-01-26 06:38:30,082 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae843db4-17fd-44c1-a9d4-c7c814dba09a
2024-01-26 06:38:30,082 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:30,138 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43693', status: init, memory: 0, processing: 0>
2024-01-26 06:38:30,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43693
2024-01-26 06:38:30,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40762
2024-01-26 06:38:30,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:30,142 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:30,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:30,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:30,236 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:30,239 - distributed.scheduler - INFO - Remove client Client-81e73472-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:30,239 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57870; closing.
2024-01-26 06:38:30,240 - distributed.scheduler - INFO - Remove client Client-81e73472-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:30,240 - distributed.scheduler - INFO - Close client connection: Client-81e73472-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:30,241 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36067'. Reason: nanny-close
2024-01-26 06:38:30,241 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:30,242 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43693. Reason: nanny-close
2024-01-26 06:38:30,244 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:30,244 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40762; closing.
2024-01-26 06:38:30,245 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43693', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251110.2452464')
2024-01-26 06:38:30,245 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:30,246 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:30,906 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:30,906 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:30,907 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:30,908 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:30,908 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-26 06:38:33,179 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:33,184 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37891 instead
  warnings.warn(
2024-01-26 06:38:33,188 - distributed.scheduler - INFO - State start
2024-01-26 06:38:33,210 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:33,211 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:33,211 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37891/status
2024-01-26 06:38:33,211 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:36,020 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40768'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40768>: Stream is closed
2024-01-26 06:38:36,300 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:36,300 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:36,300 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:36,301 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:36,302 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-26 06:38:38,544 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:38,549 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41581 instead
  warnings.warn(
2024-01-26 06:38:38,553 - distributed.scheduler - INFO - State start
2024-01-26 06:38:38,576 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:38,577 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-26 06:38:38,578 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41581/status
2024-01-26 06:38:38,578 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:38,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44935'
2024-01-26 06:38:40,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:40,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:40,632 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:40,633 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34145
2024-01-26 06:38:40,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34145
2024-01-26 06:38:40,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40363
2024-01-26 06:38:40,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:38:40,633 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:40,633 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:40,633 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:38:40,633 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-rkh_fjxn
2024-01-26 06:38:40,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06bf76e4-6b33-431d-a07d-38c5e646adcc
2024-01-26 06:38:40,634 - distributed.worker - INFO - Starting Worker plugin PreImport-e3dbcb71-707b-4a53-a99c-ef66eedf0f2a
2024-01-26 06:38:40,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a16fb23-49f3-4ca5-a13f-53c36bf9d81f
2024-01-26 06:38:40,634 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:40,682 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34145', status: init, memory: 0, processing: 0>
2024-01-26 06:38:40,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34145
2024-01-26 06:38:40,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41402
2024-01-26 06:38:40,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:40,696 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:38:40,697 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:40,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:38:42,561 - distributed.scheduler - INFO - Receive client connection: Client-88913819-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:42,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41428
2024-01-26 06:38:42,568 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:42,573 - distributed.scheduler - INFO - Remove client Client-88913819-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:42,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41428; closing.
2024-01-26 06:38:42,574 - distributed.scheduler - INFO - Remove client Client-88913819-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:42,574 - distributed.scheduler - INFO - Close client connection: Client-88913819-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:42,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44935'. Reason: nanny-close
2024-01-26 06:38:42,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:42,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34145. Reason: nanny-close
2024-01-26 06:38:42,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:38:42,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41402; closing.
2024-01-26 06:38:42,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34145', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251122.5786448')
2024-01-26 06:38:42,578 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:42,579 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:43,190 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:43,190 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:43,191 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:43,192 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-26 06:38:43,192 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-26 06:38:45,421 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:45,426 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34441 instead
  warnings.warn(
2024-01-26 06:38:45,430 - distributed.scheduler - INFO - State start
2024-01-26 06:38:45,452 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:45,453 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:45,454 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34441/status
2024-01-26 06:38:45,454 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:45,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46057'
2024-01-26 06:38:45,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36677'
2024-01-26 06:38:45,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43925'
2024-01-26 06:38:45,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36657'
2024-01-26 06:38:45,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44765'
2024-01-26 06:38:45,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40715'
2024-01-26 06:38:45,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37141'
2024-01-26 06:38:45,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34131'
2024-01-26 06:38:46,496 - distributed.scheduler - INFO - Receive client connection: Client-8cab086c-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:46,507 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47206
2024-01-26 06:38:47,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38135
2024-01-26 06:38:47,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38135
2024-01-26 06:38:47,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40537
2024-01-26 06:38:47,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,602 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-visvxpjy
2024-01-26 06:38:47,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d72407c-7802-44b5-aafe-bacab498dac1
2024-01-26 06:38:47,603 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,603 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42077
2024-01-26 06:38:47,604 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42077
2024-01-26 06:38:47,604 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35061
2024-01-26 06:38:47,604 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,604 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,604 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,604 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,604 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oto9y938
2024-01-26 06:38:47,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-900b9e3f-a9ce-4fc5-bd9a-557975583281
2024-01-26 06:38:47,604 - distributed.worker - INFO - Starting Worker plugin PreImport-a4111917-80f1-48e0-bed4-f6942fda8c7b
2024-01-26 06:38:47,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58ae1a9a-189c-4c68-a3b9-320b48d1d554
2024-01-26 06:38:47,632 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37023
2024-01-26 06:38:47,637 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37023
2024-01-26 06:38:47,637 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37865
2024-01-26 06:38:47,637 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,637 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,637 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,637 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,637 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ec6n3qds
2024-01-26 06:38:47,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4515af43-542e-420a-b5ce-ecf4f089fa8f
2024-01-26 06:38:47,638 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d737ef40-ac6c-42d2-ba1e-0eeb4e942f76
2024-01-26 06:38:47,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41795
2024-01-26 06:38:47,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41795
2024-01-26 06:38:47,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36365
2024-01-26 06:38:47,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,646 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,646 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q9wfett4
2024-01-26 06:38:47,646 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9350337b-0cc4-450b-81f4-21ed5517de47
2024-01-26 06:38:47,646 - distributed.worker - INFO - Starting Worker plugin PreImport-29bdb448-42c8-488d-935f-f164bb531f2e
2024-01-26 06:38:47,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6db12cb6-46da-4137-8413-05e671027f65
2024-01-26 06:38:47,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36585
2024-01-26 06:38:47,648 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36585
2024-01-26 06:38:47,648 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34727
2024-01-26 06:38:47,648 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,648 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,648 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,648 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mt6tlvtg
2024-01-26 06:38:47,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6811cbe-037e-4306-8681-abe048395fb7
2024-01-26 06:38:47,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,670 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43301
2024-01-26 06:38:47,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43301
2024-01-26 06:38:47,670 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40629
2024-01-26 06:38:47,670 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,670 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,670 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,670 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,671 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4zymnbsm
2024-01-26 06:38:47,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eef750a0-4c7a-433b-bbca-1022a573370c
2024-01-26 06:38:47,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-567fa1b1-95ea-416b-9073-bcfd3bfac23e
2024-01-26 06:38:47,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:47,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:47,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:47,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46795
2024-01-26 06:38:47,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46795
2024-01-26 06:38:47,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43927
2024-01-26 06:38:47,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:47,679 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:47,679 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:47,679 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:47,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-80ynfelp
2024-01-26 06:38:47,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d20f7b9-ef4b-4bee-8aa0-84135762943c
2024-01-26 06:38:48,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:48,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:48,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:48,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44595
2024-01-26 06:38:48,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44595
2024-01-26 06:38:48,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41877
2024-01-26 06:38:48,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:48,198 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:48,198 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:48,198 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:38:48,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-korboaom
2024-01-26 06:38:48,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b617d38d-9714-4a32-a469-a397435c8cdf
2024-01-26 06:38:48,198 - distributed.worker - INFO - Starting Worker plugin PreImport-02a46165-ee8b-4808-ae9a-c957611539bf
2024-01-26 06:38:48,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db93d6e1-c7c1-449c-867d-76401ffa27ac
2024-01-26 06:38:49,506 - distributed.worker - INFO - Starting Worker plugin PreImport-66a27282-7138-462a-abd2-1d726e685dd8
2024-01-26 06:38:49,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5aef961d-2b28-41b2-9abd-742e38681e92
2024-01-26 06:38:49,507 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,527 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38135', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,529 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38135
2024-01-26 06:38:49,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47230
2024-01-26 06:38:49,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,531 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,531 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,716 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,726 - distributed.worker - INFO - Starting Worker plugin PreImport-d0c45f70-8184-4bad-8c11-8fd183fdd58b
2024-01-26 06:38:49,727 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,746 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42077', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,747 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42077
2024-01-26 06:38:49,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47234
2024-01-26 06:38:49,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,749 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,755 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37023', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37023
2024-01-26 06:38:49,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47248
2024-01-26 06:38:49,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,757 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,757 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,803 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,819 - distributed.worker - INFO - Starting Worker plugin PreImport-f50641c3-86b5-44fe-b755-b3b7bd53ecee
2024-01-26 06:38:49,819 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0abd964a-37ef-4f4a-8ac6-d48f47457400
2024-01-26 06:38:49,819 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,822 - distributed.worker - INFO - Starting Worker plugin PreImport-80056053-8b27-45a7-badf-b06965d6b648
2024-01-26 06:38:49,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70f0d72a-78ee-465b-959d-c25d645d465a
2024-01-26 06:38:49,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,832 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41795', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41795
2024-01-26 06:38:49,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47254
2024-01-26 06:38:49,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,834 - distributed.worker - INFO - Starting Worker plugin PreImport-51d97713-06bd-45e6-8e07-69fb3f304403
2024-01-26 06:38:49,835 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,835 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,835 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,841 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46795', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,841 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46795
2024-01-26 06:38:49,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47258
2024-01-26 06:38:49,842 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36585', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,843 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36585
2024-01-26 06:38:49,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47260
2024-01-26 06:38:49,844 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,844 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,844 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,845 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,866 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43301', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,866 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43301
2024-01-26 06:38:49,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47276
2024-01-26 06:38:49,868 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,868 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,869 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:49,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44595', status: init, memory: 0, processing: 0>
2024-01-26 06:38:49,954 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44595
2024-01-26 06:38:49,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35040
2024-01-26 06:38:49,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:49,956 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:49,956 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:49,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:50,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,031 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,031 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,031 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,031 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,031 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:38:50,044 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,045 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:50,049 - distributed.scheduler - INFO - Remove client Client-8cab086c-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:50,049 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47206; closing.
2024-01-26 06:38:50,050 - distributed.scheduler - INFO - Remove client Client-8cab086c-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:50,051 - distributed.scheduler - INFO - Close client connection: Client-8cab086c-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:50,051 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46057'. Reason: nanny-close
2024-01-26 06:38:50,051 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36677'. Reason: nanny-close
2024-01-26 06:38:50,052 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43925'. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37023. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36657'. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42077. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44765'. Reason: nanny-close
2024-01-26 06:38:50,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38135. Reason: nanny-close
2024-01-26 06:38:50,054 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40715'. Reason: nanny-close
2024-01-26 06:38:50,054 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44595. Reason: nanny-close
2024-01-26 06:38:50,054 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37141'. Reason: nanny-close
2024-01-26 06:38:50,054 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41795. Reason: nanny-close
2024-01-26 06:38:50,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,055 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47248; closing.
2024-01-26 06:38:50,055 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34131'. Reason: nanny-close
2024-01-26 06:38:50,055 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43301. Reason: nanny-close
2024-01-26 06:38:50,055 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.055615')
2024-01-26 06:38:50,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:50,055 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36585. Reason: nanny-close
2024-01-26 06:38:50,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,056 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35040; closing.
2024-01-26 06:38:50,056 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46795. Reason: nanny-close
2024-01-26 06:38:50,056 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47230; closing.
2024-01-26 06:38:50,057 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,057 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,057 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,057 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,057 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,057 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0577185')
2024-01-26 06:38:50,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,058 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38135', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0580866')
2024-01-26 06:38:50,058 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47234; closing.
2024-01-26 06:38:50,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:50,058 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,059 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,059 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,060 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:50,059 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35040>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35040>: Stream is closed
2024-01-26 06:38:50,060 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0606973')
2024-01-26 06:38:50,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47254; closing.
2024-01-26 06:38:50,061 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0616298')
2024-01-26 06:38:50,062 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47276; closing.
2024-01-26 06:38:50,062 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47260; closing.
2024-01-26 06:38:50,062 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43301', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0628998')
2024-01-26 06:38:50,063 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36585', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0634112')
2024-01-26 06:38:50,063 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47258; closing.
2024-01-26 06:38:50,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251130.0644124')
2024-01-26 06:38:50,064 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:51,017 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:51,017 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:51,018 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:51,019 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:51,020 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-26 06:38:53,280 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:53,285 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39389 instead
  warnings.warn(
2024-01-26 06:38:53,289 - distributed.scheduler - INFO - State start
2024-01-26 06:38:53,313 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:53,314 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:53,315 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39389/status
2024-01-26 06:38:53,315 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:53,332 - distributed.scheduler - INFO - Receive client connection: Client-91543a83-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:53,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35150
2024-01-26 06:38:53,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38737'
2024-01-26 06:38:55,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:38:55,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:38:55,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:38:55,313 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35133
2024-01-26 06:38:55,313 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35133
2024-01-26 06:38:55,313 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36413
2024-01-26 06:38:55,313 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:38:55,313 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:55,313 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:38:55,313 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:38:55,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fy5pggil
2024-01-26 06:38:55,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fdc0bc2-99a1-47a8-957c-22cc6bb72b11
2024-01-26 06:38:55,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d09be188-0039-4e7a-afbe-7fbaf2881750
2024-01-26 06:38:55,603 - distributed.worker - INFO - Starting Worker plugin PreImport-9b3a3e09-799c-4674-820d-8a49905e1c35
2024-01-26 06:38:55,605 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:55,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35133', status: init, memory: 0, processing: 0>
2024-01-26 06:38:55,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35133
2024-01-26 06:38:55,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35178
2024-01-26 06:38:55,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:38:55,830 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:38:55,830 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:38:55,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:38:55,898 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:38:55,904 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:55,906 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:38:55,909 - distributed.scheduler - INFO - Remove client Client-91543a83-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:55,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35150; closing.
2024-01-26 06:38:55,909 - distributed.scheduler - INFO - Remove client Client-91543a83-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:55,909 - distributed.scheduler - INFO - Close client connection: Client-91543a83-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:38:55,910 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38737'. Reason: nanny-close
2024-01-26 06:38:55,911 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:38:55,912 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35133. Reason: nanny-close
2024-01-26 06:38:55,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35178; closing.
2024-01-26 06:38:55,914 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:38:55,915 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35133', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251135.9150708')
2024-01-26 06:38:55,915 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:38:55,916 - distributed.nanny - INFO - Worker closed
2024-01-26 06:38:56,525 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:38:56,526 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:38:56,526 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:38:56,528 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:38:56,528 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-26 06:38:58,866 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:58,871 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:38:58,874 - distributed.scheduler - INFO - State start
2024-01-26 06:38:58,896 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:38:58,897 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:38:58,898 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-26 06:38:58,898 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:38:58,963 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40577'
2024-01-26 06:39:00,321 - distributed.scheduler - INFO - Receive client connection: Client-94a4e940-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:39:00,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37126
2024-01-26 06:39:00,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:39:00,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:39:00,788 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:39:00,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39791
2024-01-26 06:39:00,789 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39791
2024-01-26 06:39:00,789 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44063
2024-01-26 06:39:00,789 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:39:00,789 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:39:00,789 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:39:00,789 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:39:00,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n5qbatm0
2024-01-26 06:39:00,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7631cff7-37bd-4589-b783-ccd6615b5781
2024-01-26 06:39:01,067 - distributed.worker - INFO - Starting Worker plugin PreImport-e840e112-e50b-4ea4-918b-f47f7b78bf23
2024-01-26 06:39:01,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02f33c75-306b-4ebe-a3a7-82089cc50961
2024-01-26 06:39:01,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:39:01,140 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39791', status: init, memory: 0, processing: 0>
2024-01-26 06:39:01,141 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39791
2024-01-26 06:39:01,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37136
2024-01-26 06:39:01,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:39:01,143 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:39:01,144 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:39:01,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:39:01,184 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-26 06:39:01,190 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:39:01,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:39:01,196 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:39:01,199 - distributed.scheduler - INFO - Remove client Client-94a4e940-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:39:01,199 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37126; closing.
2024-01-26 06:39:01,200 - distributed.scheduler - INFO - Remove client Client-94a4e940-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:39:01,200 - distributed.scheduler - INFO - Close client connection: Client-94a4e940-bc15-11ee-b0ca-d8c49764f6bb
2024-01-26 06:39:01,201 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40577'. Reason: nanny-close
2024-01-26 06:39:01,201 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:39:01,202 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39791. Reason: nanny-close
2024-01-26 06:39:01,204 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37136; closing.
2024-01-26 06:39:01,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:39:01,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706251141.205114')
2024-01-26 06:39:01,205 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:39:01,206 - distributed.nanny - INFO - Worker closed
2024-01-26 06:39:01,866 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:39:01,866 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:39:01,867 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:39:01,868 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:39:01,868 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41399 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40181 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44161 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44261 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34533 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38033 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] [1706251330.027231] [dgx13:68789:0]            sock.c:470  UCX  ERROR bind(fd=155 addr=0.0.0.0:37652) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46807 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44363 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43511 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36599 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33515 instead
  warnings.warn(
[1706251554.910558] [dgx13:73121:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:43633) failed: Address already in use
2024-01-26 06:46:19,002 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #120] ep: 0x7f0be95e30c0, tag: 0x610f43f15f48ecd0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #120] ep: 0x7f0be95e30c0, tag: 0x610f43f15f48ecd0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44843 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35811 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33605 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35581 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42175 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44429 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36613 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40399 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34175 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36723 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42585 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36233 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37047 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35721 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43483 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40727 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41773 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44615 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44975 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35335 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44543 instead
  warnings.warn(
2024-01-26 06:57:32,330 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-28b09e34599ee244278b2c641a307a68', 0)
Function:  subgraph_callable-9689b827-f9c0-47a6-a904-8907a6c8
args:      (   key  payload1
0    0         0
1    1         1
2    2         2
3    3         3
4    4         4,    key  payload2
0    7         3
1    9         4
2    5         5
3    0         6
4    3         7
5    8         8
6    4         9, 'from_pandas-492229f08d0dfffad7ce5052624e8563', 'from_pandas-24c5280a81cff902213a17da836069b3')
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-26 06:57:32,356 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('merge_chunk-28b09e34599ee244278b2c641a307a68', 1))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
Process SpawnProcess-60:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 281, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/sorting.py", line 46, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/core.py", line 214, in set_index
    return super().set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5551, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 262, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 61, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/multi.py", line 290, in merge_chunk
    out = lhs.merge(rhs, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 4149, in merge
    return merge_cls(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/join/join.py", line 279, in perform_merge
    left_rows, right_rows = self._gather_maps(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/join/join.py", line 205, in _gather_maps
    maps = self._joiner(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "join.pyx", line 30, in cudf._lib.join.join
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40353 instead
  warnings.warn(
2024-01-26 06:57:41,927 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:41,933 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:40491'.
2024-01-26 06:57:41,933 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:40491'. Shutting down.
2024-01-26 06:57:41,936 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7fee68ebb0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:42,805 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-d76574e686c5267f482ce1d69074ee4a', 1)
Function:  subgraph_callable-6525cfe0-3f25-45c1-9f80-fd322be6
args:      (    key  payload1
7     7         7
8     8         8
9     9         9
10   10        10
11   11        11
12   12        12
13   13        13,     key  payload2
0    16         6
1     9         7
2    17         8
3     7         9
4     8        10
5    19        11
6    10        12
7     1        13
8    18        14
9     6        15
10    5        16
11    4        17
12   11        18
13   12        19, 'from_pandas-53de5882071cc2a9776511eabcdf4500', 'from_pandas-6f7a1f58c9f0f89911a703f2fb560f26')
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-26 06:57:42,844 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('merge_chunk-d76574e686c5267f482ce1d69074ee4a', 0))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
Process SpawnProcess-61:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 281, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/sorting.py", line 46, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/core.py", line 214, in set_index
    return super().set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5551, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 262, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 61, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/multi.py", line 290, in merge_chunk
    out = lhs.merge(rhs, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 4149, in merge
    return merge_cls(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/join/join.py", line 279, in perform_merge
    left_rows, right_rows = self._gather_maps(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/join/join.py", line 205, in _gather_maps
    maps = self._joiner(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "join.pyx", line 30, in cudf._lib.join.join
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46265 instead
  warnings.warn(
2024-01-26 06:57:55,574 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:55,580 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:35889'.
2024-01-26 06:57:55,581 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:35889'. Shutting down.
2024-01-26 06:57:55,585 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f5161482be0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:55,609 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:55,614 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:55,617 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:34597'.
2024-01-26 06:57:55,617 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:34597'. Shutting down.
2024-01-26 06:57:55,620 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44341'.
2024-01-26 06:57:55,621 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:44341'. Shutting down.
2024-01-26 06:57:55,621 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f564787abe0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:55,624 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f710d5c3be0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-01-26 06:57:57,624 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-26 06:57:58,701 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 3)
Function:  subgraph_callable-d198b353-31ef-4924-8c90-69cdd207
args:      (    key  payload1
24   24        24
25   25        25
26   26        26
27   27        27
28   28        28
29   29        29
30   30        30
31   31        31, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-26 06:57:58,778 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 0)
Function:  subgraph_callable-d198b353-31ef-4924-8c90-69cdd207
args:      (   key  payload1
0    0         0
1    1         1
2    2         2
3    3         3
4    4         4
5    5         5
6    6         6
7    7         7, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-26 06:57:58,830 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 4)
Function:  subgraph_callable-d198b353-31ef-4924-8c90-69cdd207
args:      (    key  payload1
32   32        32
33   33        33
34   34        34
35   35        35
36   36        36
37   37        37
38   38        38
39   39        39, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-26 06:57:58,886 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 2)
Function:  subgraph_callable-d198b353-31ef-4924-8c90-69cdd207
args:      (    key  payload1
16   16        16
17   17        17
18   18        18
19   19        19
20   20        20
21   21        21
22   22        22
23   23        23, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-26 06:57:58,932 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 1)
Function:  subgraph_callable-d198b353-31ef-4924-8c90-69cdd207
args:      (    key  payload1
8     8         8
9     9         9
10   10        10
11   11        11
12   12        12
13   13        13
14   14        14
15   15        15, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
