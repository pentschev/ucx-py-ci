============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-03 05:40:10,699 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:10,704 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39779 instead
  warnings.warn(
2023-05-03 05:40:10,708 - distributed.scheduler - INFO - State start
2023-05-03 05:40:10,731 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:10,732 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-03 05:40:10,732 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39779/status
2023-05-03 05:40:11,030 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42919'
2023-05-03 05:40:11,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33319'
2023-05-03 05:40:11,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45899'
2023-05-03 05:40:11,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45725'
2023-05-03 05:40:11,151 - distributed.scheduler - INFO - Receive client connection: Client-f7155dc0-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:11,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56886
2023-05-03 05:40:12,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:12,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:12,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:12,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:12,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:12,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:12,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:12,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:12,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:12,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:12,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:12,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-03 05:40:12,984 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41355
2023-05-03 05:40:12,984 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41355
2023-05-03 05:40:12,985 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35263
2023-05-03 05:40:12,985 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-03 05:40:12,985 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:12,985 - distributed.worker - INFO -               Threads:                          4
2023-05-03 05:40:12,985 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-03 05:40:12,985 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i7n__wrw
2023-05-03 05:40:12,985 - distributed.worker - INFO - Starting Worker plugin PreImport-a90d2128-20bd-4e81-bce6-bf196d16cced
2023-05-03 05:40:12,985 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2d9e0c3-ecbd-4d55-afc5-404d03a31229
2023-05-03 05:40:12,985 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68258ee1-28ce-43e1-84e2-dcc2561f620e
2023-05-03 05:40:12,985 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,008 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41355', status: init, memory: 0, processing: 0>
2023-05-03 05:40:13,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41355
2023-05-03 05:40:13,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56912
2023-05-03 05:40:13,013 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,013 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-03 05:40:13,667 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37119
2023-05-03 05:40:13,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37119
2023-05-03 05:40:13,668 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38829
2023-05-03 05:40:13,668 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,668 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,668 - distributed.worker - INFO -               Threads:                          4
2023-05-03 05:40:13,668 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-03 05:40:13,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o6307vbo
2023-05-03 05:40:13,668 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42025
2023-05-03 05:40:13,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42025
2023-05-03 05:40:13,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e0f54d6-551b-4654-a6a4-6cc1f9cce4cb
2023-05-03 05:40:13,668 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41075
2023-05-03 05:40:13,668 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,668 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-368f28d1-b4db-4c44-802c-b7e4c58c389b
2023-05-03 05:40:13,668 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,669 - distributed.worker - INFO -               Threads:                          4
2023-05-03 05:40:13,669 - distributed.worker - INFO - Starting Worker plugin PreImport-f343f797-7397-4f7c-bd37-342062d73abb
2023-05-03 05:40:13,669 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-03 05:40:13,669 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c9ts1shp
2023-05-03 05:40:13,669 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,669 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4768b30-5db6-4996-bbbd-2392f5f2adfd
2023-05-03 05:40:13,669 - distributed.worker - INFO - Starting Worker plugin PreImport-3a7d628d-0138-44e4-ba29-60fe4370e3ee
2023-05-03 05:40:13,669 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d78cfaf-e649-45b3-a33d-df9e872fec47
2023-05-03 05:40:13,670 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,691 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37119', status: init, memory: 0, processing: 0>
2023-05-03 05:40:13,692 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37119
2023-05-03 05:40:13,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56916
2023-05-03 05:40:13,693 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,693 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-03 05:40:13,697 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42025', status: init, memory: 0, processing: 0>
2023-05-03 05:40:13,698 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42025
2023-05-03 05:40:13,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56920
2023-05-03 05:40:13,699 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,699 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-03 05:40:13,755 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38011
2023-05-03 05:40:13,755 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38011
2023-05-03 05:40:13,755 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34795
2023-05-03 05:40:13,755 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,755 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,755 - distributed.worker - INFO -               Threads:                          4
2023-05-03 05:40:13,755 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-03 05:40:13,755 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pc1m46jx
2023-05-03 05:40:13,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d42cccab-53da-4709-b94b-88ffb184e1ff
2023-05-03 05:40:13,756 - distributed.worker - INFO - Starting Worker plugin PreImport-e72ed42f-c706-4eb9-94f8-029caf6f204f
2023-05-03 05:40:13,756 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab98b41a-5889-446c-b99c-ef7739bb4942
2023-05-03 05:40:13,756 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,776 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38011', status: init, memory: 0, processing: 0>
2023-05-03 05:40:13,777 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38011
2023-05-03 05:40:13,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56934
2023-05-03 05:40:13,778 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-03 05:40:13,778 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:13,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-03 05:40:13,841 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-03 05:40:13,841 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-03 05:40:13,841 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-03 05:40:13,842 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-03 05:40:13,846 - distributed.scheduler - INFO - Remove client Client-f7155dc0-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:13,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56886; closing.
2023-05-03 05:40:13,847 - distributed.scheduler - INFO - Remove client Client-f7155dc0-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:13,847 - distributed.scheduler - INFO - Close client connection: Client-f7155dc0-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:13,848 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33319'. Reason: nanny-close
2023-05-03 05:40:13,849 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:13,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42919'. Reason: nanny-close
2023-05-03 05:40:13,850 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:13,850 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37119. Reason: nanny-close
2023-05-03 05:40:13,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45899'. Reason: nanny-close
2023-05-03 05:40:13,851 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:13,851 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42025. Reason: nanny-close
2023-05-03 05:40:13,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45725'. Reason: nanny-close
2023-05-03 05:40:13,852 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:13,852 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-03 05:40:13,852 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56916; closing.
2023-05-03 05:40:13,852 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38011. Reason: nanny-close
2023-05-03 05:40:13,852 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37119', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:13,853 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37119
2023-05-03 05:40:13,853 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41355. Reason: nanny-close
2023-05-03 05:40:13,853 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-03 05:40:13,853 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:13,854 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56920; closing.
2023-05-03 05:40:13,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37119
2023-05-03 05:40:13,854 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-03 05:40:13,854 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42025', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:13,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42025
2023-05-03 05:40:13,854 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:13,855 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37119
2023-05-03 05:40:13,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56934; closing.
2023-05-03 05:40:13,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-03 05:40:13,855 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38011', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:13,855 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:13,855 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38011
2023-05-03 05:40:13,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56912; closing.
2023-05-03 05:40:13,856 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41355', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:13,856 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41355
2023-05-03 05:40:13,856 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:13,856 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:40:15,016 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:40:15,016 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:40:15,017 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:40:15,018 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-03 05:40:15,018 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-03 05:40:17,308 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:17,312 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43265 instead
  warnings.warn(
2023-05-03 05:40:17,317 - distributed.scheduler - INFO - State start
2023-05-03 05:40:17,338 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:17,339 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:40:17,340 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43265/status
2023-05-03 05:40:17,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33897'
2023-05-03 05:40:17,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37403'
2023-05-03 05:40:17,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43819'
2023-05-03 05:40:17,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36661'
2023-05-03 05:40:17,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39953'
2023-05-03 05:40:17,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45009'
2023-05-03 05:40:17,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38353'
2023-05-03 05:40:17,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42145'
2023-05-03 05:40:17,794 - distributed.scheduler - INFO - Receive client connection: Client-faffbfa6-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:17,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40976
2023-05-03 05:40:18,141 - distributed.scheduler - INFO - Receive client connection: Client-fb3b0b0e-e974-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:18,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41050
2023-05-03 05:40:19,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:21,584 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33037
2023-05-03 05:40:21,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33037
2023-05-03 05:40:21,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34993
2023-05-03 05:40:21,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:21,585 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:21,585 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:21,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:21,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8no9hzt2
2023-05-03 05:40:21,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15940f01-f738-4a0c-8acc-68d2cfb599a5
2023-05-03 05:40:21,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37933
2023-05-03 05:40:21,589 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37933
2023-05-03 05:40:21,589 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41619
2023-05-03 05:40:21,589 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:21,589 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:21,589 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:21,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:21,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8grzurwr
2023-05-03 05:40:21,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7a23f16-34c0-4013-a42d-ff65055b37b1
2023-05-03 05:40:21,590 - distributed.worker - INFO - Starting Worker plugin RMMSetup-023064ad-d585-4acf-91c3-0a4c7732139c
2023-05-03 05:40:21,895 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62649a53-4724-45d5-bdf6-7e0e0c68ac7d
2023-05-03 05:40:21,896 - distributed.worker - INFO - Starting Worker plugin PreImport-95f61aea-5d37-452f-904b-594800122c1a
2023-05-03 05:40:21,896 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:21,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33037', status: init, memory: 0, processing: 0>
2023-05-03 05:40:21,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33037
2023-05-03 05:40:21,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41072
2023-05-03 05:40:21,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:21,958 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:21,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:22,012 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46325
2023-05-03 05:40:22,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46325
2023-05-03 05:40:22,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35419
2023-05-03 05:40:22,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,013 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,013 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:22,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:22,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mgxub17u
2023-05-03 05:40:22,014 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35bb2b34-6dbe-400e-a8bd-04bdd7d3e928
2023-05-03 05:40:22,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34865
2023-05-03 05:40:22,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34865
2023-05-03 05:40:22,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44393
2023-05-03 05:40:22,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,035 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,035 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:22,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:22,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-43l37qp2
2023-05-03 05:40:22,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5321cdc4-1539-4d7e-8ada-5354ed863410
2023-05-03 05:40:22,036 - distributed.worker - INFO - Starting Worker plugin PreImport-a762f38e-3700-4455-a0c5-25fc6448fd45
2023-05-03 05:40:22,037 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd2e6f2c-aec1-40f4-ae5a-486117363329
2023-05-03 05:40:22,334 - distributed.worker - INFO - Starting Worker plugin PreImport-a1e9fcc1-fe02-47be-b679-3936247e350f
2023-05-03 05:40:22,335 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,372 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41849
2023-05-03 05:40:22,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41849
2023-05-03 05:40:22,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36923
2023-05-03 05:40:22,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,373 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,373 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:22,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:22,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s5bd2mpg
2023-05-03 05:40:22,373 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c1552c1-31a0-42a7-9f6b-3c819ca543b1
2023-05-03 05:40:22,374 - distributed.worker - INFO - Starting Worker plugin PreImport-2a17a2a9-bf01-40df-87fb-0f7c94da949c
2023-05-03 05:40:22,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a13b7ce-5702-43b8-a68f-10564b413287
2023-05-03 05:40:22,377 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37933', status: init, memory: 0, processing: 0>
2023-05-03 05:40:22,378 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37933
2023-05-03 05:40:22,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41080
2023-05-03 05:40:22,378 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,378 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:22,482 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33505
2023-05-03 05:40:22,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33505
2023-05-03 05:40:22,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34265
2023-05-03 05:40:22,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,500 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,500 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:22,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:22,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8jpje5hb
2023-05-03 05:40:22,501 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea943aeb-4fbe-4159-b713-43c0e388ab82
2023-05-03 05:40:22,501 - distributed.worker - INFO - Starting Worker plugin PreImport-4b37daa9-708c-4c2f-beaf-3646eb491d87
2023-05-03 05:40:22,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34eff198-e4e3-4da2-af89-41ecb7c6bec6
2023-05-03 05:40:22,532 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34865', status: init, memory: 0, processing: 0>
2023-05-03 05:40:22,533 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34865
2023-05-03 05:40:22,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41082
2023-05-03 05:40:22,534 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,534 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:22,765 - distributed.worker - INFO - Starting Worker plugin PreImport-59f47ba7-a47e-4199-b74c-b4989795bd1d
2023-05-03 05:40:22,765 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5d7cc34-b5f5-4b9b-9e16-b3998504bee5
2023-05-03 05:40:22,766 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,766 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,804 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33505', status: init, memory: 0, processing: 0>
2023-05-03 05:40:22,805 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33505
2023-05-03 05:40:22,805 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41094
2023-05-03 05:40:22,805 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,805 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,807 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46325', status: init, memory: 0, processing: 0>
2023-05-03 05:40:22,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:22,808 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46325
2023-05-03 05:40:22,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41108
2023-05-03 05:40:22,808 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,808 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:22,928 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,973 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41849', status: init, memory: 0, processing: 0>
2023-05-03 05:40:22,974 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41849
2023-05-03 05:40:22,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41122
2023-05-03 05:40:22,974 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:22,974 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:22,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:23,144 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37269
2023-05-03 05:40:23,144 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37269
2023-05-03 05:40:23,144 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44493
2023-05-03 05:40:23,144 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:23,144 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:23,144 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:23,144 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:23,145 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vjk77z6h
2023-05-03 05:40:23,145 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2afa850-4ff1-46de-8757-aa990f0692d8
2023-05-03 05:40:23,145 - distributed.worker - INFO - Starting Worker plugin PreImport-a1876252-9997-4168-8fc4-56dff14e62ec
2023-05-03 05:40:23,145 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd90771f-aebd-43e8-956f-19ae2fa118de
2023-05-03 05:40:23,582 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:23,613 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37269', status: init, memory: 0, processing: 0>
2023-05-03 05:40:23,614 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37269
2023-05-03 05:40:23,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41142
2023-05-03 05:40:23,614 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:23,614 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:23,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,149 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44929
2023-05-03 05:40:24,149 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44929
2023-05-03 05:40:24,149 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39249
2023-05-03 05:40:24,149 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,149 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,149 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,149 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,149 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wnh_llws
2023-05-03 05:40:24,150 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d4671d4-3e1a-4740-956a-782b46b62e19
2023-05-03 05:40:24,449 - distributed.worker - INFO - Starting Worker plugin PreImport-36cc8f00-be35-4316-a238-4bfd7a2db6df
2023-05-03 05:40:24,449 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f9b3430-2d8f-42b5-ac27-15e01ac7820b
2023-05-03 05:40:24,449 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,491 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44929', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44929
2023-05-03 05:40:24,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40764
2023-05-03 05:40:24,492 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,493 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,493 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43213', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,494 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43213
2023-05-03 05:40:24,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40762
2023-05-03 05:40:24,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35647', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35647
2023-05-03 05:40:24,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40766
2023-05-03 05:40:24,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45745', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45745
2023-05-03 05:40:24,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40788
2023-05-03 05:40:24,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41311', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41311
2023-05-03 05:40:24,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40802
2023-05-03 05:40:24,724 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45331', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,725 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45331
2023-05-03 05:40:24,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40806
2023-05-03 05:40:24,748 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44077', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44077
2023-05-03 05:40:24,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40820
2023-05-03 05:40:24,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46263', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46263
2023-05-03 05:40:24,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40834
2023-05-03 05:40:24,757 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42745', status: init, memory: 0, processing: 0>
2023-05-03 05:40:24,757 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42745
2023-05-03 05:40:24,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40840
2023-05-03 05:40:33,841 - distributed.scheduler - INFO - Remove client Client-faffbfa6-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:33,841 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40976; closing.
2023-05-03 05:40:33,843 - distributed.scheduler - INFO - Remove client Client-faffbfa6-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:33,844 - distributed.scheduler - INFO - Close client connection: Client-faffbfa6-e974-11ed-b865-d8c49764f6bb
2023-05-03 05:40:33,845 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33897'. Reason: nanny-close
2023-05-03 05:40:33,845 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,846 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37403'. Reason: nanny-close
2023-05-03 05:40:33,847 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,847 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36661'. Reason: nanny-close
2023-05-03 05:40:33,847 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33037. Reason: nanny-close
2023-05-03 05:40:33,847 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,848 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34865. Reason: nanny-close
2023-05-03 05:40:33,848 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39953'. Reason: nanny-close
2023-05-03 05:40:33,848 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45009'. Reason: nanny-close
2023-05-03 05:40:33,849 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38353'. Reason: nanny-close
2023-05-03 05:40:33,849 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44929. Reason: nanny-close
2023-05-03 05:40:33,850 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,850 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41072; closing.
2023-05-03 05:40:33,850 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37269. Reason: nanny-close
2023-05-03 05:40:33,850 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,850 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42145'. Reason: nanny-close
2023-05-03 05:40:33,850 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33037', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,850 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,850 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41849. Reason: nanny-close
2023-05-03 05:40:33,850 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,850 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43819'. Reason: nanny-close
2023-05-03 05:40:33,851 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,851 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46325. Reason: nanny-close
2023-05-03 05:40:33,851 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33505. Reason: nanny-close
2023-05-03 05:40:33,851 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,852 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41082; closing.
2023-05-03 05:40:33,852 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,852 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,852 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,852 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37933. Reason: nanny-close
2023-05-03 05:40:33,853 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,853 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,854 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,854 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,854 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,854 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34865', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,854 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,854 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,855 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,855 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41142; closing.
2023-05-03 05:40:33,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40764; closing.
2023-05-03 05:40:33,856 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,857 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,857 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37269', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,857 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,857 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44929', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,857 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41122; closing.
2023-05-03 05:40:33,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41094; closing.
2023-05-03 05:40:33,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41108; closing.
2023-05-03 05:40:33,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41080; closing.
2023-05-03 05:40:33,859 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41849', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,859 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33505', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,860 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46325', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,860 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,861 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37933', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,861 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,958 - distributed.scheduler - INFO - Remove client Client-fb3b0b0e-e974-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:33,958 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41050; closing.
2023-05-03 05:40:33,959 - distributed.scheduler - INFO - Remove client Client-fb3b0b0e-e974-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:33,959 - distributed.scheduler - INFO - Close client connection: Client-fb3b0b0e-e974-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:33,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40820; closing.
2023-05-03 05:40:33,964 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44077', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,964 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,966 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40802; closing.
2023-05-03 05:40:33,967 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41311', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,967 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41311
2023-05-03 05:40:33,967 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40766; closing.
2023-05-03 05:40:33,968 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35647', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,968 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35647
2023-05-03 05:40:33,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40788; closing.
2023-05-03 05:40:33,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40834; closing.
2023-05-03 05:40:33,969 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45745', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,969 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45745
2023-05-03 05:40:33,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46263', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,970 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46263
2023-05-03 05:40:33,970 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40840; closing.
2023-05-03 05:40:33,971 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42745', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,971 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42745
2023-05-03 05:40:33,971 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40806; closing.
2023-05-03 05:40:33,972 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45331', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,972 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45331
2023-05-03 05:40:33,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40762; closing.
2023-05-03 05:40:33,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43213', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:33,973 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43213
2023-05-03 05:40:33,973 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:40:33,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40762>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-05-03 05:40:35,513 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:40:35,514 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:40:35,514 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:40:35,516 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:40:35,517 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-03 05:40:38,106 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:38,110 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42203 instead
  warnings.warn(
2023-05-03 05:40:38,115 - distributed.scheduler - INFO - State start
2023-05-03 05:40:38,137 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:38,138 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:40:38,138 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:40:38,139 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-03 05:40:38,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39435'
2023-05-03 05:40:38,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44151'
2023-05-03 05:40:38,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43731'
2023-05-03 05:40:38,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44729'
2023-05-03 05:40:38,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43911'
2023-05-03 05:40:38,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46653'
2023-05-03 05:40:38,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44017'
2023-05-03 05:40:38,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40335'
2023-05-03 05:40:40,297 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:40,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,384 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,454 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,464 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,464 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,465 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,471 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:43,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39435'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44151'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43731'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44729'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43911'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46653'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44017'. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40335'. Reason: nanny-close
2023-05-03 05:40:43,925 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34177
2023-05-03 05:40:43,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34177
2023-05-03 05:40:43,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36813
2023-05-03 05:40:43,926 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,926 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,926 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:43,926 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:43,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-44ttw4zl
2023-05-03 05:40:43,926 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e0affcf-b97f-46e6-81eb-2be8cc6ac3d4
2023-05-03 05:40:43,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6593869a-5cdc-442c-bfe0-2038924ee073
2023-05-03 05:40:43,946 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45061
2023-05-03 05:40:43,946 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45061
2023-05-03 05:40:43,946 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43711
2023-05-03 05:40:43,946 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,946 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,946 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:43,946 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:43,946 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m8mte7vt
2023-05-03 05:40:43,947 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87f76233-fa96-43a2-9801-a6a0151a9a2a
2023-05-03 05:40:43,964 - distributed.worker - INFO - Starting Worker plugin PreImport-f56e0874-ba4c-4cf9-be65-da1a63c14a61
2023-05-03 05:40:43,964 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,966 - distributed.worker - INFO - Starting Worker plugin PreImport-bbdea397-16c5-461c-9087-3892e4770fd2
2023-05-03 05:40:43,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-829a0ce6-5d5a-4e08-ae92-0bfece8e0b52
2023-05-03 05:40:43,966 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,992 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,992 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,010 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,010 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,042 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,043 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,044 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45061. Reason: nanny-close
2023-05-03 05:40:44,045 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34177. Reason: nanny-close
2023-05-03 05:40:44,047 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,048 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,049 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,049 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,346 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38801
2023-05-03 05:40:44,347 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38801
2023-05-03 05:40:44,347 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34627
2023-05-03 05:40:44,347 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,347 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,347 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:44,347 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:44,347 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dm8gj5hi
2023-05-03 05:40:44,347 - distributed.worker - INFO - Starting Worker plugin PreImport-7635a461-3203-40a4-bdf0-654aa3e42f8d
2023-05-03 05:40:44,347 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62021534-1035-407f-b8fd-a1667ed174a4
2023-05-03 05:40:44,348 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43394db5-fc7d-4a62-9f1d-c40469f089aa
2023-05-03 05:40:44,375 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41677
2023-05-03 05:40:44,375 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41677
2023-05-03 05:40:44,375 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38593
2023-05-03 05:40:44,375 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,375 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,375 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:44,375 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:44,375 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sj9v1vk0
2023-05-03 05:40:44,376 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec071454-e091-4be9-bfb5-5c1f49a6aa79
2023-05-03 05:40:44,376 - distributed.worker - INFO - Starting Worker plugin PreImport-171d5ea7-d047-452c-92a8-119a532d311d
2023-05-03 05:40:44,376 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8668f57-c451-4de7-bb10-882227c6d9fd
2023-05-03 05:40:44,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45369
2023-05-03 05:40:44,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45369
2023-05-03 05:40:44,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38855
2023-05-03 05:40:44,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,417 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,417 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:44,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:44,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mmu4pkc6
2023-05-03 05:40:44,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-315625ab-65f7-4418-ae76-fee682d57135
2023-05-03 05:40:44,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f3f1342-7767-421a-8405-cb91f8acee4f
2023-05-03 05:40:44,477 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33277
2023-05-03 05:40:44,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33277
2023-05-03 05:40:44,478 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44883
2023-05-03 05:40:44,478 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,478 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,478 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:44,478 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:44,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oiu59ebw
2023-05-03 05:40:44,478 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b89fa0bd-1c92-4cfb-b886-c639dbef78b0
2023-05-03 05:40:44,480 - distributed.worker - INFO - Starting Worker plugin PreImport-0fca7b30-c558-4ee4-a945-1f566427d1f2
2023-05-03 05:40:44,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1942e15-f7a0-4334-bdc4-d9ba7d4bcb66
2023-05-03 05:40:44,515 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40789
2023-05-03 05:40:44,515 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40789
2023-05-03 05:40:44,515 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33981
2023-05-03 05:40:44,515 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,515 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,515 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:44,515 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:44,515 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y6b08oih
2023-05-03 05:40:44,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74077a7e-32cd-4ca7-a7b5-eff01f350124
2023-05-03 05:40:44,516 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1efd1fbd-af45-4b3e-8401-60197f11ce11
2023-05-03 05:40:44,517 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33661
2023-05-03 05:40:44,540 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33661
2023-05-03 05:40:44,540 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45251
2023-05-03 05:40:44,540 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,540 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,540 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:44,541 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:44,541 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-toq9deeh
2023-05-03 05:40:44,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c5f59d6-de46-42b4-870a-a566bf0926ad
2023-05-03 05:40:44,542 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,542 - distributed.worker - INFO - Starting Worker plugin PreImport-8c0b7eef-e7be-476f-b80f-1982a7f995c0
2023-05-03 05:40:44,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9b5ac49-1fad-4eed-a874-032947b98c99
2023-05-03 05:40:44,542 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,561 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,562 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,581 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,581 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,586 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,611 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,611 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,612 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,612 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38801. Reason: nanny-close
2023-05-03 05:40:44,612 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41677. Reason: nanny-close
2023-05-03 05:40:44,613 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33277. Reason: nanny-close
2023-05-03 05:40:44,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,615 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,615 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,616 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,616 - distributed.worker - INFO - Starting Worker plugin PreImport-070c0b38-2665-4499-9cea-5027f4ddb8d3
2023-05-03 05:40:44,616 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,617 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,622 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,624 - distributed.worker - INFO - Starting Worker plugin PreImport-41423ef5-f9d4-4448-bd7f-05f6561c5334
2023-05-03 05:40:44,624 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,647 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,647 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,653 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,653 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,654 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:44,654 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:44,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:44,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,663 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:44,663 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40789. Reason: nanny-close
2023-05-03 05:40:44,664 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33661. Reason: nanny-close
2023-05-03 05:40:44,664 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45369. Reason: nanny-close
2023-05-03 05:40:44,665 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,666 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,666 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:44,667 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,668 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:44,668 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:45,699 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-03 05:40:45,896 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48238 parent=47941 started daemon>
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-03 05:40:48,167 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:48,172 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43991 instead
  warnings.warn(
2023-05-03 05:40:48,176 - distributed.scheduler - INFO - State start
2023-05-03 05:40:48,197 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:48,198 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:40:48,199 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:40:48,199 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-03 05:40:48,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40493'
2023-05-03 05:40:48,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42663'
2023-05-03 05:40:48,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33265'
2023-05-03 05:40:48,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43769'
2023-05-03 05:40:48,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43095'
2023-05-03 05:40:48,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45949'
2023-05-03 05:40:48,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40473'
2023-05-03 05:40:48,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36009'
2023-05-03 05:40:50,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,289 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,289 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,331 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,344 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:52,961 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44691
2023-05-03 05:40:52,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44691
2023-05-03 05:40:52,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41851
2023-05-03 05:40:52,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:52,961 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:52,961 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:52,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:52,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u9_3z5h9
2023-05-03 05:40:52,962 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28cdb6b8-4dbf-48fc-a611-2c186311eebf
2023-05-03 05:40:52,962 - distributed.worker - INFO - Starting Worker plugin PreImport-3821179d-8ded-4821-bfc1-42c9106e71bb
2023-05-03 05:40:52,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-710f3191-6c58-42f0-a0d4-03fd7781e9b4
2023-05-03 05:40:53,295 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43193
2023-05-03 05:40:53,295 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43193
2023-05-03 05:40:53,295 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32795
2023-05-03 05:40:53,295 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,296 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,296 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,296 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,296 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ukqg_bke
2023-05-03 05:40:53,296 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f3a4b1e-3c75-44a7-a2ae-f328dcb044db
2023-05-03 05:40:53,296 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55231c9b-0cec-4603-8483-b254e189cf55
2023-05-03 05:40:53,338 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,540 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46661
2023-05-03 05:40:53,540 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46661
2023-05-03 05:40:53,540 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33737
2023-05-03 05:40:53,540 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,540 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,540 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,540 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,541 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hp0sh6ms
2023-05-03 05:40:53,541 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9bc14fd-17e9-46a2-acac-02f1e97ee5fa
2023-05-03 05:40:53,541 - distributed.worker - INFO - Starting Worker plugin PreImport-affbe048-6631-4487-a31d-44ddcba1d68d
2023-05-03 05:40:53,541 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62dc66ad-02a2-401a-b02b-29c5f01aba8a
2023-05-03 05:40:53,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,570 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:53,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44617
2023-05-03 05:40:53,745 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44617
2023-05-03 05:40:53,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40905
2023-05-03 05:40:53,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,745 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,745 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lzria0gl
2023-05-03 05:40:53,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-851ebd40-eeaa-45b7-a5d9-d6c182f5fde8
2023-05-03 05:40:53,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44549
2023-05-03 05:40:53,767 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44549
2023-05-03 05:40:53,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39947
2023-05-03 05:40:53,767 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,768 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,768 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,768 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,768 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p9733817
2023-05-03 05:40:53,769 - distributed.worker - INFO - Starting Worker plugin PreImport-3cef6888-05a2-4622-bfc1-5a08158fbee3
2023-05-03 05:40:53,769 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40b32619-4817-4dcd-8393-99ff3feb3c49
2023-05-03 05:40:53,886 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38359
2023-05-03 05:40:53,887 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38359
2023-05-03 05:40:53,887 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39273
2023-05-03 05:40:53,887 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,887 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,887 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,887 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,887 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gmv8plg6
2023-05-03 05:40:53,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c862007-9181-4e6c-b278-99c2454a9640
2023-05-03 05:40:53,888 - distributed.worker - INFO - Starting Worker plugin PreImport-60eab34f-5622-457e-913d-8bcd0da3d451
2023-05-03 05:40:53,888 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84d0cd27-f9f7-4087-b0a6-ef94e7e0000f
2023-05-03 05:40:54,168 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34695
2023-05-03 05:40:54,168 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34695
2023-05-03 05:40:54,168 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39263
2023-05-03 05:40:54,168 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,168 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,169 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:54,169 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:54,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8bdpr7f2
2023-05-03 05:40:54,170 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d006928-6310-4ba9-9c55-81627e566e37
2023-05-03 05:40:54,171 - distributed.worker - INFO - Starting Worker plugin PreImport-1567f292-d9ac-4c23-9631-e612c05932f0
2023-05-03 05:40:54,171 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8fe359f-6920-49de-aa71-750a4dc3fc2d
2023-05-03 05:40:54,287 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41489
2023-05-03 05:40:54,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41489
2023-05-03 05:40:54,288 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35393
2023-05-03 05:40:54,288 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,289 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,289 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:54,289 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:54,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jjtt42x1
2023-05-03 05:40:54,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca18fb17-329c-4867-9761-50cdcb3bc27d
2023-05-03 05:40:54,334 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,364 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,364 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,370 - distributed.worker - INFO - Starting Worker plugin PreImport-4c67796d-15a6-4f2a-ad3c-faf60119588d
2023-05-03 05:40:54,370 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6af6809a-4b05-4663-a50b-78e5f27653bf
2023-05-03 05:40:54,371 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,388 - distributed.worker - INFO - Starting Worker plugin PreImport-a8dbe4db-9ace-4619-b71e-5c79b1bc1512
2023-05-03 05:40:54,388 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,409 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-869827a9-4a06-4a61-b573-fe2af55f03d4
2023-05-03 05:40:54,409 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,409 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,419 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,423 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,456 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,456 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,458 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,459 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,481 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,553 - distributed.worker - INFO - Starting Worker plugin PreImport-f61b82e1-5a49-431e-82a4-2eaf32007e67
2023-05-03 05:40:54,554 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8edf735e-3297-47f6-bb92-f0305dc83734
2023-05-03 05:40:54,554 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,554 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,554 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,588 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,588 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:05,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43769'. Reason: nanny-close
2023-05-03 05:41:05,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,700 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40493'. Reason: nanny-close
2023-05-03 05:41:05,701 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,701 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42663'. Reason: nanny-close
2023-05-03 05:41:05,701 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44549. Reason: nanny-close
2023-05-03 05:41:05,701 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,702 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33265'. Reason: nanny-close
2023-05-03 05:41:05,702 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44691. Reason: nanny-close
2023-05-03 05:41:05,702 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,702 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43095'. Reason: nanny-close
2023-05-03 05:41:05,702 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41489. Reason: nanny-close
2023-05-03 05:41:05,702 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45949'. Reason: nanny-close
2023-05-03 05:41:05,703 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38359. Reason: nanny-close
2023-05-03 05:41:05,703 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40473'. Reason: nanny-close
2023-05-03 05:41:05,703 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,703 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,703 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46661. Reason: nanny-close
2023-05-03 05:41:05,704 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,704 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44617. Reason: nanny-close
2023-05-03 05:41:05,704 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36009'. Reason: nanny-close
2023-05-03 05:41:05,705 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,705 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,705 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,705 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34695. Reason: nanny-close
2023-05-03 05:41:05,705 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,706 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,706 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43193. Reason: nanny-close
2023-05-03 05:41:05,706 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,706 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,707 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,708 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,708 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,708 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,708 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,709 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,710 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging Fatal Python error: init_import_site: Failed to import the site module
Python runtime state: initialized
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site.py", line 73, in <module>
    import os
  File "/opt/conda/envs/gdf/lib/python3.9/os.py", line 27, in <module>
    import stat as st
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 978, in get_code
  File "<frozen importlib._bootstrap_external>", line 647, in _compile_bytecode
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 1, in <module>
    from dask import config, datasets
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/datasets.py", line 3, in <module>
    from dask.utils import import_required
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 25, in <module>
    import tlz as toolz
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tlz/__init__.py", line 9, in <module>
    from . import _build_tlz
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tlz/_build_tlz.py", line 3, in <module>
    import toolz
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/toolz/__init__.py", line 3, in <module>
    from .functoolz import *
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/toolz/functoolz.py", line 1048, in <module>
    from . import _signatures as _sigs
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 941, in get_code
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-03 05:41:10,068 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:10,073 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45805 instead
  warnings.warn(
2023-05-03 05:41:10,076 - distributed.scheduler - INFO - State start
2023-05-03 05:41:10,097 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:10,098 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:10,099 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45805/status
2023-05-03 05:41:10,168 - distributed.scheduler - INFO - Receive client connection: Client-1a7baf58-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:10,187 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34022
2023-05-03 05:41:10,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38607'
2023-05-03 05:41:12,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,298 - distributed.scheduler - INFO - Receive client connection: Client-1a78b651-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:12,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34130
2023-05-03 05:41:12,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:14,712 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44567
2023-05-03 05:41:14,713 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44567
2023-05-03 05:41:14,713 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-05-03 05:41:14,713 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,713 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,713 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,713 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:14,713 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s_xt5quz
2023-05-03 05:41:14,714 - distributed.worker - INFO - Starting Worker plugin PreImport-10b4b55f-96a5-493d-801c-b18ffd23a93d
2023-05-03 05:41:14,714 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba40678a-f81d-4ef0-9eff-1eea31521a99
2023-05-03 05:41:14,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d2d2c52-02e3-4ff0-9b64-0eb4a5fbfc9e
2023-05-03 05:41:14,714 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,728 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46527', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46527
2023-05-03 05:41:14,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52914
2023-05-03 05:41:14,745 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44567', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,746 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44567
2023-05-03 05:41:14,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52916
2023-05-03 05:41:14,746 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,746 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,804 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34127', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,805 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34127
2023-05-03 05:41:14,805 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52926
2023-05-03 05:41:14,876 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45763', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45763
2023-05-03 05:41:14,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52938
2023-05-03 05:41:14,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45287', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45287
2023-05-03 05:41:14,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52944
2023-05-03 05:41:14,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38651', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38651
2023-05-03 05:41:14,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52960
2023-05-03 05:41:14,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39307', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39307
2023-05-03 05:41:14,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52958
2023-05-03 05:41:14,931 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38681', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,932 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38681
2023-05-03 05:41:14,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52970
2023-05-03 05:41:14,963 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36767', status: init, memory: 0, processing: 0>
2023-05-03 05:41:14,964 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36767
2023-05-03 05:41:14,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52984
2023-05-03 05:41:14,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,982 - distributed.scheduler - INFO - Remove client Client-1a78b651-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:14,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34130; closing.
2023-05-03 05:41:14,982 - distributed.scheduler - INFO - Remove client Client-1a78b651-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:14,983 - distributed.scheduler - INFO - Close client connection: Client-1a78b651-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:14,988 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52926; closing.
2023-05-03 05:41:14,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34127', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,988 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52944; closing.
2023-05-03 05:41:14,990 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,991 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45287', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,991 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45287
2023-05-03 05:41:14,991 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52960; closing.
2023-05-03 05:41:14,992 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38651', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,992 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38651
2023-05-03 05:41:14,992 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52970; closing.
2023-05-03 05:41:14,993 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38681', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,993 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38681
2023-05-03 05:41:14,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52958; closing.
2023-05-03 05:41:14,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52938; closing.
2023-05-03 05:41:14,994 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39307', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,994 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39307
2023-05-03 05:41:14,994 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45763', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,994 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45763
2023-05-03 05:41:14,994 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52914; closing.
2023-05-03 05:41:14,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46527', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,995 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46527
2023-05-03 05:41:14,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52984; closing.
2023-05-03 05:41:14,996 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36767', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:14,996 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36767
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45287
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38651
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38681
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39307
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45763
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46527
2023-05-03 05:41:14,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36767
2023-05-03 05:41:15,007 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:15,010 - distributed.scheduler - INFO - Remove client Client-1a7baf58-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:15,010 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34022; closing.
2023-05-03 05:41:15,010 - distributed.scheduler - INFO - Remove client Client-1a7baf58-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:15,011 - distributed.scheduler - INFO - Close client connection: Client-1a7baf58-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:15,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38607'. Reason: nanny-close
2023-05-03 05:41:15,013 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:15,015 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44567. Reason: nanny-close
2023-05-03 05:41:15,017 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52916; closing.
2023-05-03 05:41:15,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:15,017 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44567', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:15,017 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44567
2023-05-03 05:41:15,017 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:15,018 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:16,280 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:16,281 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:16,281 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:16,282 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:16,283 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-03 05:41:20,926 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:20,930 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43097 instead
  warnings.warn(
2023-05-03 05:41:20,934 - distributed.scheduler - INFO - State start
2023-05-03 05:41:20,957 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:20,958 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:20,958 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:20,959 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-03 05:41:21,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45681'
2023-05-03 05:41:21,916 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45681'. Reason: nanny-close
2023-05-03 05:41:22,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:22,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:23,165 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:24,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36109
2023-05-03 05:41:24,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36109
2023-05-03 05:41:24,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35487
2023-05-03 05:41:24,020 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:24,020 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:24,020 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:24,020 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:24,020 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wk_5hppk
2023-05-03 05:41:24,021 - distributed.worker - INFO - Starting Worker plugin PreImport-09168105-89c4-4834-a1ec-091fc1dd4d1f
2023-05-03 05:41:24,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11981a0e-4930-40c1-9d59-0b062306a657
2023-05-03 05:41:24,022 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bffd4aa6-26dd-4a4e-ade5-2b0b4520e60a
2023-05-03 05:41:24,022 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:27,849 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:27,849 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:27,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:27,888 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:27,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36109. Reason: nanny-close
2023-05-03 05:41:27,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:27,893 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:28,594 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-03 05:41:28,815 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48991 parent=48809 started daemon>
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-03 05:41:31,018 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:31,023 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40237 instead
  warnings.warn(
2023-05-03 05:41:31,027 - distributed.scheduler - INFO - State start
2023-05-03 05:41:31,050 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:31,051 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:31,051 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:31,052 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-03 05:41:37,168 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:37,173 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42529 instead
  warnings.warn(
2023-05-03 05:41:37,177 - distributed.scheduler - INFO - State start
2023-05-03 05:41:37,199 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:37,200 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-03 05:41:37,201 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42529/status
2023-05-03 05:41:37,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38919'
2023-05-03 05:41:38,965 - distributed.scheduler - INFO - Receive client connection: Client-2aa4847d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:38,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45636
2023-05-03 05:41:39,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:39,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:39,054 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:39,710 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41327
2023-05-03 05:41:39,710 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41327
2023-05-03 05:41:39,710 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33929
2023-05-03 05:41:39,710 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-03 05:41:39,710 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:39,710 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:39,711 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:39,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rt9fg65_
2023-05-03 05:41:39,711 - distributed.worker - INFO - Starting Worker plugin PreImport-029dc107-3395-419e-8dac-01f5ef3d00f1
2023-05-03 05:41:39,711 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a31ca350-475a-4652-98f7-ecfdd2c0d8c2
2023-05-03 05:41:39,711 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa00c743-0d15-4858-ad4b-9fa18dda2577
2023-05-03 05:41:39,711 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:39,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41327', status: init, memory: 0, processing: 0>
2023-05-03 05:41:39,743 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41327
2023-05-03 05:41:39,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45658
2023-05-03 05:41:39,744 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-03 05:41:39,744 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:39,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-03 05:41:39,807 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:39,811 - distributed.scheduler - INFO - Remove client Client-2aa4847d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:39,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45636; closing.
2023-05-03 05:41:39,812 - distributed.scheduler - INFO - Remove client Client-2aa4847d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:39,812 - distributed.scheduler - INFO - Close client connection: Client-2aa4847d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:39,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38919'. Reason: nanny-close
2023-05-03 05:41:39,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:39,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41327. Reason: nanny-close
2023-05-03 05:41:39,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45658; closing.
2023-05-03 05:41:39,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-03 05:41:39,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41327', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:39,818 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41327
2023-05-03 05:41:39,818 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:39,818 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:40,275 - distributed.scheduler - INFO - Receive client connection: Client-2d87d10b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:40,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45666
2023-05-03 05:41:40,881 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:40,882 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:40,882 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:40,883 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-03 05:41:40,883 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-05-03 05:41:43,166 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:43,171 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36973 instead
  warnings.warn(
2023-05-03 05:41:43,175 - distributed.scheduler - INFO - State start
2023-05-03 05:41:43,200 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:43,201 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:43,201 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36973/status
2023-05-03 05:41:43,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40795'
2023-05-03 05:41:43,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41027'
2023-05-03 05:41:43,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40735'
2023-05-03 05:41:43,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43965'
2023-05-03 05:41:43,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39313'
2023-05-03 05:41:43,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38759'
2023-05-03 05:41:43,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42513'
2023-05-03 05:41:43,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38135'
2023-05-03 05:41:45,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,281 - distributed.scheduler - INFO - Receive client connection: Client-2e34dae6-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:45,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:45,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:45,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41924
2023-05-03 05:41:45,332 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:45,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:46,657 - distributed.scheduler - INFO - Receive client connection: Client-3155cc9b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:46,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41948
2023-05-03 05:41:47,132 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41533
2023-05-03 05:41:47,132 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41533
2023-05-03 05:41:47,132 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42455
2023-05-03 05:41:47,132 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,132 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,132 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,132 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ip49t2uz
2023-05-03 05:41:47,133 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-078aac48-6c38-45c3-8797-9166a6bf03f3
2023-05-03 05:41:47,133 - distributed.worker - INFO - Starting Worker plugin PreImport-da4a1cbd-cea1-4922-877d-fcecdb60d21f
2023-05-03 05:41:47,133 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08321687-5d56-41d0-a739-7063e8810588
2023-05-03 05:41:47,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38301
2023-05-03 05:41:47,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38301
2023-05-03 05:41:47,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41647
2023-05-03 05:41:47,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,273 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,273 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ckhru26s
2023-05-03 05:41:47,274 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c848658-d95b-4e67-acdb-0dd2509c6837
2023-05-03 05:41:47,311 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43553
2023-05-03 05:41:47,329 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43553
2023-05-03 05:41:47,329 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41677
2023-05-03 05:41:47,329 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,329 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,329 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,329 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7ex9flpg
2023-05-03 05:41:47,330 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65ceba3b-a45d-49a6-8468-ef26a8eba857
2023-05-03 05:41:47,330 - distributed.worker - INFO - Starting Worker plugin PreImport-0c232613-d59c-4e7f-bcf0-0c9987333e4e
2023-05-03 05:41:47,331 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be1023f9-5fb8-4461-94ff-1e8f001e244d
2023-05-03 05:41:47,335 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35025
2023-05-03 05:41:47,335 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35025
2023-05-03 05:41:47,335 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35475
2023-05-03 05:41:47,335 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,335 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,336 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,336 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,336 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1uwb3v9e
2023-05-03 05:41:47,337 - distributed.worker - INFO - Starting Worker plugin PreImport-1503191c-1bfb-4e9e-a9e8-41a937a7dfe7
2023-05-03 05:41:47,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e8bae8e-eaaf-43f4-84ba-a4c8c835f5d3
2023-05-03 05:41:47,342 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41533', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41533
2023-05-03 05:41:47,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41966
2023-05-03 05:41:47,345 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,345 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,556 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45571
2023-05-03 05:41:47,556 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45571
2023-05-03 05:41:47,556 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39321
2023-05-03 05:41:47,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,557 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,557 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qwhu3i7a
2023-05-03 05:41:47,557 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02ad96ec-9acf-4db7-86b7-0c454caab306
2023-05-03 05:41:47,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40573
2023-05-03 05:41:47,571 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40573
2023-05-03 05:41:47,571 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46649
2023-05-03 05:41:47,571 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,571 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,571 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,571 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,571 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yl7x7ao7
2023-05-03 05:41:47,572 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc04efb9-cf18-46c6-a0b3-eef87b6b6afe
2023-05-03 05:41:47,573 - distributed.worker - INFO - Starting Worker plugin PreImport-8f3a5116-1c72-4f47-af86-8285eefd578c
2023-05-03 05:41:47,573 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c95f9f47-b1e6-40d0-a0cc-4906ae1ad17d
2023-05-03 05:41:47,597 - distributed.worker - INFO - Starting Worker plugin PreImport-95e54ab5-720c-410a-a619-55384ec00bd8
2023-05-03 05:41:47,598 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54c0d917-d561-44f4-9be3-168a534b3bd4
2023-05-03 05:41:47,598 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,600 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd07d814-e83d-4e6c-a0de-a510eca20b3f
2023-05-03 05:41:47,601 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,636 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38301', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,637 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38301
2023-05-03 05:41:47,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41970
2023-05-03 05:41:47,638 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35025', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,638 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,638 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35025
2023-05-03 05:41:47,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41974
2023-05-03 05:41:47,639 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,639 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,641 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,642 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43553', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,643 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43553
2023-05-03 05:41:47,643 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41968
2023-05-03 05:41:47,643 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,643 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39483
2023-05-03 05:41:47,644 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39483
2023-05-03 05:41:47,644 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42707
2023-05-03 05:41:47,644 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,644 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,644 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,644 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-64q7uvn_
2023-05-03 05:41:47,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-824c9256-7d08-4956-a8f7-58060b0e3288
2023-05-03 05:41:47,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33d007d8-a442-4581-af65-5705f046f894
2023-05-03 05:41:47,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,653 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40069
2023-05-03 05:41:47,654 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40069
2023-05-03 05:41:47,654 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39163
2023-05-03 05:41:47,654 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,654 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,654 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:47,654 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:47,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1blfsyb5
2023-05-03 05:41:47,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1db8121-e22e-4171-b1ff-15b2cef092ee
2023-05-03 05:41:47,655 - distributed.worker - INFO - Starting Worker plugin PreImport-a4c1f023-e985-4492-9a2c-e65c2b9d0c92
2023-05-03 05:41:47,655 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e67463e-ab08-472b-b3e9-63ee23e4374b
2023-05-03 05:41:47,776 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,791 - distributed.worker - INFO - Starting Worker plugin PreImport-2b2f23e1-d3f9-414c-a195-3c9a5764cf61
2023-05-03 05:41:47,791 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,791 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,796 - distributed.worker - INFO - Starting Worker plugin PreImport-8cdf81ba-b6ce-434e-b15c-f2d9d783c0d6
2023-05-03 05:41:47,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c6d89fc-daac-4cfc-802a-744741a58f2d
2023-05-03 05:41:47,797 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,814 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40573', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,815 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40573
2023-05-03 05:41:47,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41982
2023-05-03 05:41:47,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,815 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39483', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39483
2023-05-03 05:41:47,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42002
2023-05-03 05:41:47,830 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,831 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40069', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40069
2023-05-03 05:41:47,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41986
2023-05-03 05:41:47,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,833 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,836 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45571', status: init, memory: 0, processing: 0>
2023-05-03 05:41:47,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45571
2023-05-03 05:41:47,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42006
2023-05-03 05:41:47,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:47,838 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:47,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:47,902 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,903 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,903 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,903 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,903 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,903 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,904 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,905 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,906 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,906 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,906 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,906 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:47,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,930 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,930 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,930 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:47,936 - distributed.scheduler - INFO - Remove client Client-2e34dae6-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:47,936 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41924; closing.
2023-05-03 05:41:47,937 - distributed.scheduler - INFO - Remove client Client-2e34dae6-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:47,937 - distributed.scheduler - INFO - Remove client Client-3155cc9b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:47,937 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41948; closing.
2023-05-03 05:41:47,938 - distributed.scheduler - INFO - Remove client Client-3155cc9b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:47,938 - distributed.scheduler - INFO - Close client connection: Client-2e34dae6-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:47,939 - distributed.scheduler - INFO - Close client connection: Client-3155cc9b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:47,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41027'. Reason: nanny-close
2023-05-03 05:41:47,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,940 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39313'. Reason: nanny-close
2023-05-03 05:41:47,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40795'. Reason: nanny-close
2023-05-03 05:41:47,941 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35025. Reason: nanny-close
2023-05-03 05:41:47,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40735'. Reason: nanny-close
2023-05-03 05:41:47,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43553. Reason: nanny-close
2023-05-03 05:41:47,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43965'. Reason: nanny-close
2023-05-03 05:41:47,943 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45571. Reason: nanny-close
2023-05-03 05:41:47,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38759'. Reason: nanny-close
2023-05-03 05:41:47,943 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,943 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41974; closing.
2023-05-03 05:41:47,943 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40069. Reason: nanny-close
2023-05-03 05:41:47,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,943 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35025', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,944 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35025
2023-05-03 05:41:47,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42513'. Reason: nanny-close
2023-05-03 05:41:47,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41533. Reason: nanny-close
2023-05-03 05:41:47,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38135'. Reason: nanny-close
2023-05-03 05:41:47,944 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38301. Reason: nanny-close
2023-05-03 05:41:47,944 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:47,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,945 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35025
2023-05-03 05:41:47,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40573. Reason: nanny-close
2023-05-03 05:41:47,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41968; closing.
2023-05-03 05:41:47,945 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35025
2023-05-03 05:41:47,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43553', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,946 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43553
2023-05-03 05:41:47,946 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,946 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35025
2023-05-03 05:41:47,946 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39483. Reason: nanny-close
2023-05-03 05:41:47,946 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42006; closing.
2023-05-03 05:41:47,946 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,946 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35025
2023-05-03 05:41:47,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41986; closing.
2023-05-03 05:41:47,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,947 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35025
2023-05-03 05:41:47,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45571', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,947 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45571
2023-05-03 05:41:47,947 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40069', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,948 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40069
2023-05-03 05:41:47,948 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,948 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41966; closing.
2023-05-03 05:41:47,948 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:47,948 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41533', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,948 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41533
2023-05-03 05:41:47,948 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41970; closing.
2023-05-03 05:41:47,949 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38301', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,949 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38301
2023-05-03 05:41:47,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41982; closing.
2023-05-03 05:41:47,950 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:47,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42002; closing.
2023-05-03 05:41:47,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40573', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,950 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40573
2023-05-03 05:41:47,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39483', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:47,951 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39483
2023-05-03 05:41:47,951 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:48,493 - distributed.scheduler - INFO - Receive client connection: Client-326de0ee-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:48,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42008
2023-05-03 05:41:49,809 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:49,809 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:49,809 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:49,811 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:49,812 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-03 05:41:52,075 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:52,080 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45819 instead
  warnings.warn(
2023-05-03 05:41:52,084 - distributed.scheduler - INFO - State start
2023-05-03 05:41:52,104 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:52,105 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:52,106 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:52,106 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-03 05:41:52,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36529'
2023-05-03 05:41:53,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36529'. Reason: nanny-close
2023-05-03 05:41:53,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:53,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:53,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:54,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36681
2023-05-03 05:41:54,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36681
2023-05-03 05:41:54,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39099
2023-05-03 05:41:54,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:54,787 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:54,787 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:54,788 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:54,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kdeg00rv
2023-05-03 05:41:54,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dde009ff-fe8b-4468-9b23-743863d08f5d
2023-05-03 05:41:54,788 - distributed.worker - INFO - Starting Worker plugin PreImport-d5eefcfe-6810-4731-be96-246ffc4b8a56
2023-05-03 05:41:54,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-932177d8-21d2-4d33-b4b9-9775d85af187
2023-05-03 05:41:54,897 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:58,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:58,026 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:58,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:58,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:58,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36681. Reason: nanny-close
2023-05-03 05:41:58,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:58,039 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:58,699 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-03 05:41:58,925 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49838 parent=49656 started daemon>
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 3, in <module>
    from dask.base import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 27, in <module>
    from dask import config, local
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/local.py", line 120, in <module>
    from dask.order import order
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1423, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1395, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1522, in find_spec
  File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 1, in <module>
    from dask.cli import run_cli
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 3, in <module>
    import click
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/__init__.py", line 7, in <module>
    from .core import Argument as Argument
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 12, in <module>
    from gettext import gettext as _
  File "/opt/conda/envs/gdf/lib/python3.9/gettext.py", line 73, in <module>
    _token_pattern = re.compile(r"""
  File "/opt/conda/envs/gdf/lib/python3.9/re.py", line 252, in compile
    return _compile(pattern, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/re.py", line 304, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/sre_compile.py", line 788, in compile
    p = sre_parse.parse(p, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/sre_parse.py", line 955, in parse
    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)
  File "/opt/conda/envs/gdf/lib/python3.9/sre_parse.py", line 444, in _parse_sub
    itemsappend(_parse(source, state, verbose, nested + 1,
  File "/opt/conda/envs/gdf/lib/python3.9/sre_parse.py", line 512, in _parse
    sourceget()
  File "/opt/conda/envs/gdf/lib/python3.9/sre_parse.py", line 255, in get
    def get(self):
KeyboardInterrupt
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35691 instead
  warnings.warn(
2023-05-03 05:42:09,353 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,353 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,440 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:09,515 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:09,515 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45931 instead
  warnings.warn(
2023-05-03 05:42:18,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,871 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:18,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:18,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39285 instead
  warnings.warn(
2023-05-03 05:42:28,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:28,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:28,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:28,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:28,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:28,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:28,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:28,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:29,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:29,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41967 instead
  warnings.warn(
2023-05-03 05:42:40,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,228 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,228 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:40,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:40,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35861 instead
  warnings.warn(
2023-05-03 05:42:53,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:53,465 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:53,465 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42821 instead
  warnings.warn(
2023-05-03 05:43:06,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,241 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,241 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,276 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,276 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:06,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:06,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38357 instead
  warnings.warn(
2023-05-03 05:43:17,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,812 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,812 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:17,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:17,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38493 instead
  warnings.warn(
2023-05-03 05:43:28,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,603 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:28,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:28,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36967 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46583 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41169 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38193 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33139 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43923 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38773 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-03 05:45:07,396 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-03 05:45:07,403 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fbd36e33e20>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-03 05:45:09,407 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
