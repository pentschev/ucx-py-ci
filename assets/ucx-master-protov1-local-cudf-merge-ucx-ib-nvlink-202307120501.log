2023-07-12 06:02:33,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:33,449 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:33,449 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1689141764.863029] [dgx13:74759:0]    ib_mlx5dv_md.c:416  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:74759:0:74759]        rndv.c:165  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  74759) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fc30c45bced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7fc30c4598a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7fc30c459a3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x72c14) [0x7fc30c503c14]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7fc30c4dbe3f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7fc30c517ddd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x707) [0x7fc30c51d3a7]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7fc30c51e02f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7fc30c5ceec2]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55c0b70ffdc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55c0b70fe1a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55c0b70e4d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c0b70de27a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c0b70efc05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55c0b70e03cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c0b70de27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c0b70efc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55c0b70e03cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c0b710470e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c0b70e5923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c0b710470e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c0b70e5923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c0b710470e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c0b70e5923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c0b710470e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c0b70e5923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c0b710470e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c0b70e5923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c0b710470e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fc31e8262fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7fc31e826b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c0b70e82bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55c0b709b817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55c0b70e6f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55c0b70e4d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c0b70df81b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c0b70df81b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c0b70df81b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c0b70df81b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c0b70de27a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c0b70efc05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55c0b70e3fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c0b70de27a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55c0b70fd935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55c0b70fe104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55c0b71c4fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c0b70e82bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c0b70e31bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55c0b70fdc72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c0b70e31bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c0b70df81b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c0b70de27a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c0b70efc05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c0b70df81b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c0b70efef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55c0b70df568]
=================================
2023-07-12 06:02:45,102 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34003
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #030] ep: 0x7ff0be585200, tag: 0x6e19fb1fce4a49bc, nbytes: 799843992, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #030] ep: 0x7ff0be585200, tag: 0x6e19fb1fce4a49bc, nbytes: 799843992, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:45,103 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34003
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #030] ep: 0x7fae9ce99180, tag: 0xd1d4e4b290e3a7e4, nbytes: 799946864, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #030] ep: 0x7fae9ce99180, tag: 0xd1d4e4b290e3a7e4, nbytes: 799946864, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:45,554 - distributed.nanny - WARNING - Restarting worker
[1689141766.978517] [dgx13:74762:0]    ib_mlx5dv_md.c:416  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:74762:0:74762]        rndv.c:165  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  74762) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f8b787cdced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f8b787cb8a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7f8b787cba3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x72c14) [0x7f8b78875c14]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f8b7884de3f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f8b78889ddd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x707) [0x7f8b7888f3a7]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f8b7889002f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7f8b78940ec2]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x56253b10cdc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x56253b10b1a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x56253b0f1d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56253b0eb27a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56253b0fcc05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x56253b0ed3cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56253b0eb27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56253b0fcc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x56253b0ed3cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56253b11170e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56253b0f2923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56253b11170e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56253b0f2923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56253b11170e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56253b0f2923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56253b11170e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56253b0f2923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56253b11170e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56253b0f2923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56253b11170e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f8b8abb32fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f8b8abb3b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x56253b0f52bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x56253b0a8817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x56253b0f3f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x56253b0f1d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56253b0ec81b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56253b0ec81b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56253b0ec81b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56253b0ec81b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56253b0eb27a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56253b0fcc05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x56253b0f0fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56253b0eb27a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x56253b10a935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x56253b10b104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x56253b1d1fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x56253b0f52bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x56253b0f01bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x56253b10ac72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x56253b0f01bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56253b0ec81b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56253b0eb27a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56253b0fcc05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56253b0ec81b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56253b0fcef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x56253b0ec568]
=================================
2023-07-12 06:02:47,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:47,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-12 06:02:47,247 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #042] ep: 0x7f860ced91c0, tag: 0x1216075c46282adf, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #042] ep: 0x7f860ced91c0, tag: 0x1216075c46282adf, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:47,258 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:46640 -> ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #031] ep: 0x7f1c95ad0300, tag: 0x31cb1e96d219ff21, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-07-12 06:02:47,259 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7f1c95ad0100, tag: 0x56c1df1983ba87ca, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7f1c95ad0100, tag: 0x56c1df1983ba87ca, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:47,264 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:59546 -> ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #036] ep: 0x7fae9ce99440, tag: 0xe50213d25a8fa4f0, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-07-12 06:02:47,265 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41808 -> ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7ff0be585340, tag: 0x4a2a6ad1a52c53f6, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-07-12 06:02:47,264 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:55399 -> ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7f3d4d9072c0, tag: 0x344a1231f1aaabd0, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-07-12 06:02:47,264 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49672 -> ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #030] ep: 0x7f31809df380, tag: 0x8e2bed562942adef, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-07-12 06:02:47,265 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #037] ep: 0x7fae9ce99280, tag: 0x44397b0c8e2181b1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #037] ep: 0x7fae9ce99280, tag: 0x44397b0c8e2181b1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:47,267 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #037] ep: 0x7f31809df1c0, tag: 0x6e91805ae907dfd2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #037] ep: 0x7f31809df1c0, tag: 0x6e91805ae907dfd2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:47,266 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7ff0be585180, tag: 0xceda86b9b07262ff, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7ff0be585180, tag: 0xceda86b9b07262ff, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-07-12 06:02:47,267 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #035] ep: 0x7f3d4d907100, tag: 0x424c29f36066f652, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #035] ep: 0x7f3d4d907100, tag: 0x424c29f36066f652, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:47,270 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:37863 -> ucx://127.0.0.1:55494
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #029] ep: 0x7f860ced9380, tag: 0xd03a38282f87442e, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-07-12 06:02:47,741 - distributed.nanny - WARNING - Restarting worker
2023-07-12 06:02:49,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:49,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1689141769.348625] [dgx13:74750:0]    ib_mlx5dv_md.c:416  UCX  ERROR mlx5_2: LRU push returned Unsupported operation
[dgx13:74750:0:74750]        rndv.c:165  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  74750) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f3d746cdced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f3d746cb8a1]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a3c) [0x7f3d746cba3c]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x72c14) [0x7f3d74775c14]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f3d7474de3f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f3d74789ddd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x707) [0x7f3d7478f3a7]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f3d7479002f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6cec2) [0x7f3d74840ec2]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x561e99734dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x561e997331a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x561e99719d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561e9971327a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561e99724c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x561e997153cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561e9971327a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561e99724c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x561e997153cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561e9973970e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561e9971a923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561e9973970e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561e9971a923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561e9973970e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561e9971a923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561e9973970e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561e9971a923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561e9973970e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x561e9971a923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561e9973970e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f3d86a8f2fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f3d86a8fb4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x561e9971d2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x561e996d0817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x561e9971bf83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x561e99719d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561e9971481b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561e9971481b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561e9971481b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561e9971481b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561e9971327a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561e99724c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x561e99718fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561e9971327a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x561e99732935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x561e99733104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x561e997f9fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x561e9971d2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x561e997181bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x561e99732c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x561e997181bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561e9971481b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561e9971327a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561e99724c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561e9971481b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561e99724ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x561e99714568]
=================================
2023-07-12 06:02:49,428 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-07-12 06:02:49,429 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-07-12 06:02:49,528 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-e8f5b501103064749b1dc12a417fcfa9', 2)
Function:  <dask.layers.CallableLazyImport object at 0x7feb96
args:      ([               key   payload
shuffle                     
0            71494  12038660
0           157962  87737078
0            50551  75131134
0           109548  80241391
0            78639  67056011
...            ...       ...
0        799913907  85722463
0        799966009  31196440
0        799797408  66771564
0        799863365  83522708
0        799865468  40601268

[12497244 rows x 2 columns],                key   payload
shuffle                     
1           244315  70915519
1           187024  92241479
1           237208  37450898
1            45202  34398741
1            88151  98265310
...            ...       ...
1        799983593  92148950
1        799854810  31280003
1        799862278  92404445
1        799833122  97760809
1        799935334  13216006

[12500558 rows x 2 columns],                key   payload
shuffle                     
2             8387  84927110
2           106234  11261401
2            70331  12852544
2           161869  10881699
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-07-12 06:02:49,528 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-e8f5b501103064749b1dc12a417fcfa9', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7f80bb
args:      ([               key   payload
shuffle                     
0           102852  87398985
0            75027  12041057
0           199490  65241524
0             8463  93488224
0           244762   3686313
...            ...       ...
0        799985615  37582139
0        799863520  97881370
0        799734384  72165713
0        799776364  92043612
0        799936945  72284163

[12502296 rows x 2 columns],                key   payload
shuffle                     
1           197364  76232368
1           268469  23099487
1            30212  39670522
1           161866  25361139
1           120389  39161376
...            ...       ...
1        799814910  17112745
1        799868948  97026382
1        799938849  64677099
1        799944043  89687103
1        799834745  91103891

[12499115 rows x 2 columns],                key   payload
shuffle                     
2            34884   3350562
2           189959  91393579
2            67752  95714981
2           198769  95828727
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-07-12 06:02:49,596 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55399
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #090] ep: 0x7f31809df140, tag: 0x1212160d5a298c2a, nbytes: 799946864, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #090] ep: 0x7f31809df140, tag: 0x1212160d5a298c2a, nbytes: 799946864, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-07-12 06:02:49,652 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-05e9aa9abcd7f265b9fd146a7815955d', 3)
Function:  subgraph_callable-b8f007f3-2ed4-4ddd-be59-fadb19a6
args:      (               key   payload
shuffle                     
0            10127  48963054
0           305737  96580188
0           797095  51328607
0           170147  85855200
0           985779  95712977
...            ...       ...
7        799888528   4109200
7        799980075  82550766
7        799886395  59618219
7        799953659  98939001
7        799949382  87408279

[100002012 rows x 2 columns],                  key   payload
163392     846892109  23331900
15078      806575511  26322254
163410       9843820   5937483
15090      805891782  70914589
163417     607004105   9857040
...              ...       ...
99998181    99781977  29237933
99998190   690349202  30466804
99998201  1503697920  60728262
99998125  1513221776   1201132
99998139  1544375144  26435445

[100005738 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-07-12 06:02:49,693 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7fec30
args:      (                 key   payload
0         1218863339  99051695
1         1252208751  78953234
2          652278035  81293845
3         1253618300  74974697
4         1202090160  62919800
...              ...       ...
99999995    50213713  72068972
99999996  1249206659  33160153
99999997  1259655348  40095643
99999998  1252540937  33956372
99999999  1212812172  90324512

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-07-12 06:02:49,735 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7f8150
args:      (                 key   payload
0         1418863339  99051695
1         1452208751  78953234
2          677278035  81293845
3         1453618300  74974697
4         1402090160  62919800
...              ...       ...
99999995    75213713  72068972
99999996  1449206659  33160153
99999997  1459655348  40095643
99999998  1452540937  33956372
99999999  1412812172  90324512

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-07-12 06:02:49,753 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 6)
Function:  generate_chunk
args:      (6, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-07-12 06:02:50,020 - distributed.nanny - WARNING - Restarting worker
2023-07-12 06:02:50,316 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55399
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2907, in get_data_from_worker
    await comm.write("OK")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 289, in write
    raise CommClosedError("Endpoint is closed -- unable to send message")
distributed.comm.core.CommClosedError: Endpoint is closed -- unable to send message
2023-07-12 06:02:51,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-12 06:02:51,532 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
