============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-09 06:33:03,586 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:03,591 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35789 instead
  warnings.warn(
2024-01-09 06:33:03,595 - distributed.scheduler - INFO - State start
2024-01-09 06:33:03,729 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:03,730 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-09 06:33:03,731 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35789/status
2024-01-09 06:33:03,731 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:33:04,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33401'
2024-01-09 06:33:04,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42851'
2024-01-09 06:33:04,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37539'
2024-01-09 06:33:04,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33623'
2024-01-09 06:33:04,668 - distributed.scheduler - INFO - Receive client connection: Client-efe93daf-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:04,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47438
2024-01-09 06:33:06,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:06,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:06,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:06,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:06,024 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:06,024 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:06,025 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41101
2024-01-09 06:33:06,025 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41101
2024-01-09 06:33:06,025 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40899
2024-01-09 06:33:06,025 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40899
2024-01-09 06:33:06,025 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45393
2024-01-09 06:33:06,025 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:33:06,025 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42897
2024-01-09 06:33:06,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,025 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:33:06,025 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:33:06,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,025 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:33:06,025 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:33:06,025 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qdbkxzgo
2024-01-09 06:33:06,025 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:33:06,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-aq1zx_xl
2024-01-09 06:33:06,026 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1a8371f-1d29-4927-853b-d4cb404368f5
2024-01-09 06:33:06,026 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08d684ca-dd11-40e9-a989-a3839e71c74b
2024-01-09 06:33:06,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:06,026 - distributed.worker - INFO - Starting Worker plugin PreImport-8907cf35-0872-4820-967b-01fe61b7b703
2024-01-09 06:33:06,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:06,026 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8fed7f7-d639-4daf-ae2a-d4f9583bd0e0
2024-01-09 06:33:06,026 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:06,026 - distributed.worker - INFO - Starting Worker plugin PreImport-19ec76dd-cf60-4825-9575-541270b34660
2024-01-09 06:33:06,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:06,026 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4a10514-544e-46c8-bbe3-46e5235711b8
2024-01-09 06:33:06,026 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,030 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:06,030 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:06,030 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40745
2024-01-09 06:33:06,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40745
2024-01-09 06:33:06,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41415
2024-01-09 06:33:06,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:33:06,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,031 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45311
2024-01-09 06:33:06,031 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:33:06,031 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:33:06,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45311
2024-01-09 06:33:06,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-5tb5npal
2024-01-09 06:33:06,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39763
2024-01-09 06:33:06,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:33:06,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,031 - distributed.worker - INFO - Starting Worker plugin PreImport-a7134151-fc82-4717-bdfb-ef975966d279
2024-01-09 06:33:06,031 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:33:06,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8427a9f6-1d1b-47b7-8344-e5ca8a3641ef
2024-01-09 06:33:06,031 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:33:06,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8emrvc2a
2024-01-09 06:33:06,031 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66cf8dac-0124-4c3d-b859-2815127ceb79
2024-01-09 06:33:06,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42126e49-34fa-4de2-b6fa-c309ca8e9068
2024-01-09 06:33:06,032 - distributed.worker - INFO - Starting Worker plugin PreImport-57ac395b-5afd-409a-a3f1-bafc3cafd081
2024-01-09 06:33:06,032 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:06,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f51c898-a164-429b-92b0-7d6da2269399
2024-01-09 06:33:06,032 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:07,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45311', status: init, memory: 0, processing: 0>
2024-01-09 06:33:07,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45311
2024-01-09 06:33:07,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47488
2024-01-09 06:33:07,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:07,500 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:33:07,500 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:07,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:33:07,503 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40745', status: init, memory: 0, processing: 0>
2024-01-09 06:33:07,504 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40745
2024-01-09 06:33:07,504 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47480
2024-01-09 06:33:07,506 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:07,507 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:33:07,507 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:07,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:33:07,526 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40899', status: init, memory: 0, processing: 0>
2024-01-09 06:33:07,527 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40899
2024-01-09 06:33:07,527 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47474
2024-01-09 06:33:07,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:07,529 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:33:07,530 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:07,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:33:07,572 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41101', status: init, memory: 0, processing: 0>
2024-01-09 06:33:07,572 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41101
2024-01-09 06:33:07,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47458
2024-01-09 06:33:07,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:07,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:33:07,576 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:07,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:33:07,615 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:33:07,615 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:33:07,615 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:33:07,615 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:33:07,620 - distributed.scheduler - INFO - Remove client Client-efe93daf-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:07,620 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47438; closing.
2024-01-09 06:33:07,621 - distributed.scheduler - INFO - Remove client Client-efe93daf-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:07,621 - distributed.scheduler - INFO - Close client connection: Client-efe93daf-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:07,622 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33401'. Reason: nanny-close
2024-01-09 06:33:07,623 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:07,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42851'. Reason: nanny-close
2024-01-09 06:33:07,624 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:07,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37539'. Reason: nanny-close
2024-01-09 06:33:07,624 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41101. Reason: nanny-close
2024-01-09 06:33:07,624 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:07,624 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45311. Reason: nanny-close
2024-01-09 06:33:07,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33623'. Reason: nanny-close
2024-01-09 06:33:07,625 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:07,625 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40745. Reason: nanny-close
2024-01-09 06:33:07,626 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40899. Reason: nanny-close
2024-01-09 06:33:07,626 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:33:07,626 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47458; closing.
2024-01-09 06:33:07,626 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:33:07,626 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781987.626746')
2024-01-09 06:33:07,627 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:33:07,627 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:07,627 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47488; closing.
2024-01-09 06:33:07,628 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:07,628 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:33:07,629 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45311', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781987.6291316')
2024-01-09 06:33:07,629 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:07,629 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47480; closing.
2024-01-09 06:33:07,630 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781987.6307337')
2024-01-09 06:33:07,630 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:07,631 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47474; closing.
2024-01-09 06:33:07,631 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40899', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781987.6317449')
2024-01-09 06:33:07,632 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:33:08,538 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:33:08,539 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:33:08,540 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:33:08,541 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-09 06:33:08,541 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-09 06:33:11,072 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:11,081 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:33:11,088 - distributed.scheduler - INFO - State start
2024-01-09 06:33:11,133 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:11,135 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:33:11,137 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:33:11,138 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:33:11,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44987'
2024-01-09 06:33:11,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41877'
2024-01-09 06:33:11,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34303'
2024-01-09 06:33:11,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45437'
2024-01-09 06:33:11,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39565'
2024-01-09 06:33:11,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43975'
2024-01-09 06:33:11,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41319'
2024-01-09 06:33:11,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45061'
2024-01-09 06:33:12,112 - distributed.scheduler - INFO - Receive client connection: Client-f47fe4d4-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:12,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51530
2024-01-09 06:33:13,875 - distributed.scheduler - INFO - Receive client connection: Client-f448db92-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:13,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51540
2024-01-09 06:33:13,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,912 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,912 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,918 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,918 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,918 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35865
2024-01-09 06:33:13,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35865
2024-01-09 06:33:13,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45873
2024-01-09 06:33:13,918 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32963
2024-01-09 06:33:13,919 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,919 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32963
2024-01-09 06:33:13,919 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,919 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37023
2024-01-09 06:33:13,919 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rmg_vwge
2024-01-09 06:33:13,919 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,919 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,919 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,919 - distributed.worker - INFO - Starting Worker plugin PreImport-318f4494-43e8-4b0d-a911-82b51b620fd3
2024-01-09 06:33:13,919 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yyas1awt
2024-01-09 06:33:13,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-490ccc53-2dd6-48ce-b57c-ba1931fe6b00
2024-01-09 06:33:13,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bd48553-641f-488d-87ad-72554384c34c
2024-01-09 06:33:13,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42715
2024-01-09 06:33:13,919 - distributed.worker - INFO - Starting Worker plugin PreImport-37ed44c8-4ea4-4be4-b88e-a61783aeebca
2024-01-09 06:33:13,919 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42715
2024-01-09 06:33:13,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67ef7ba3-0e63-4bdd-870c-aa39afb12a8d
2024-01-09 06:33:13,919 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42941
2024-01-09 06:33:13,919 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,920 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,920 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,920 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ei5eqw0r
2024-01-09 06:33:13,920 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ed28852-1d6f-40a1-a614-aa5c56853dc9
2024-01-09 06:33:13,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,920 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92899d5e-2452-4209-add4-d18352a04d9c
2024-01-09 06:33:13,921 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46033
2024-01-09 06:33:13,921 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46033
2024-01-09 06:33:13,921 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40113
2024-01-09 06:33:13,921 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,921 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,921 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,921 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0vy005tc
2024-01-09 06:33:13,921 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39573
2024-01-09 06:33:13,921 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39573
2024-01-09 06:33:13,921 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42241
2024-01-09 06:33:13,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c63ebcc-8396-4334-a84e-1c9613d1ac9b
2024-01-09 06:33:13,921 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,922 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,922 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,922 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,922 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-je7mxhoe
2024-01-09 06:33:13,922 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0334b5dc-5e8d-4666-9847-4e536329bd6a
2024-01-09 06:33:13,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44985
2024-01-09 06:33:13,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44985
2024-01-09 06:33:13,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42837
2024-01-09 06:33:13,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,929 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xku1qzae
2024-01-09 06:33:13,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33929
2024-01-09 06:33:13,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33929
2024-01-09 06:33:13,929 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f623edd0-4d8e-4615-9e06-ff76873f91cf
2024-01-09 06:33:13,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32941
2024-01-09 06:33:13,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,930 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a6j5uk04
2024-01-09 06:33:13,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1dc0128-d12f-4393-b084-391493ec5204
2024-01-09 06:33:13,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:13,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:13,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:13,937 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41779
2024-01-09 06:33:13,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41779
2024-01-09 06:33:13,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41213
2024-01-09 06:33:13,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:13,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:13,938 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:13,938 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:13,938 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bpzy1gpz
2024-01-09 06:33:13,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f513195-ad45-4bed-8f2c-89a45ab85470
2024-01-09 06:33:18,728 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea436b4e-ff51-42d7-a6a2-af65c82ae0d4
2024-01-09 06:33:18,729 - distributed.worker - INFO - Starting Worker plugin PreImport-c2bb0d4e-0127-4c9d-aea7-e72039d696b8
2024-01-09 06:33:18,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,762 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39573', status: init, memory: 0, processing: 0>
2024-01-09 06:33:18,763 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39573
2024-01-09 06:33:18,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51566
2024-01-09 06:33:18,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:18,766 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:18,766 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:18,845 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,860 - distributed.worker - INFO - Starting Worker plugin PreImport-9373bbcd-ca93-44c2-bc0e-07d8d1c63c00
2024-01-09 06:33:18,863 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff80c366-ae8d-4720-991e-cf89f6f3bbb5
2024-01-09 06:33:18,864 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32963', status: init, memory: 0, processing: 0>
2024-01-09 06:33:18,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32963
2024-01-09 06:33:18,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51572
2024-01-09 06:33:18,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:18,870 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:18,870 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:18,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f4177aa-b711-4555-a84f-1c2906b680f4
2024-01-09 06:33:18,876 - distributed.worker - INFO - Starting Worker plugin PreImport-d49ee4a5-8883-4dfe-a778-70662261d1d6
2024-01-09 06:33:18,877 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42715', status: init, memory: 0, processing: 0>
2024-01-09 06:33:18,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42715
2024-01-09 06:33:18,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51574
2024-01-09 06:33:18,896 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:18,897 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:18,897 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:18,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33929', status: init, memory: 0, processing: 0>
2024-01-09 06:33:18,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33929
2024-01-09 06:33:18,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51584
2024-01-09 06:33:18,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:18,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:18,912 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:18,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:19,021 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,031 - distributed.worker - INFO - Starting Worker plugin PreImport-c99622d2-5926-479c-9375-df30e5f0f18c
2024-01-09 06:33:19,032 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81de8b99-8db9-4496-a0a5-603c2d4ac3f9
2024-01-09 06:33:19,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,056 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35865', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,057 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35865
2024-01-09 06:33:19,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51600
2024-01-09 06:33:19,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:19,060 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:19,060 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:19,066 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46033', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,066 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46033
2024-01-09 06:33:19,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51612
2024-01-09 06:33:19,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa98c054-3754-4f98-974d-54063ffc2d61
2024-01-09 06:33:19,068 - distributed.worker - INFO - Starting Worker plugin PreImport-ed76996f-5f9f-4662-a059-38a338826cc2
2024-01-09 06:33:19,068 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:19,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:19,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:19,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c9b6fb6-4641-435f-8131-1c87dd3e956c
2024-01-09 06:33:19,081 - distributed.worker - INFO - Starting Worker plugin PreImport-35a9afeb-1094-4741-a701-aafd3358626f
2024-01-09 06:33:19,081 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,089 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44985', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,090 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44985
2024-01-09 06:33:19,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51622
2024-01-09 06:33:19,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:19,091 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:19,091 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:19,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41779', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41779
2024-01-09 06:33:19,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51630
2024-01-09 06:33:19,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:19,105 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:19,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:19,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:19,127 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:33:19,132 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36217', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,133 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36217
2024-01-09 06:33:19,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51634
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,136 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:33:19,141 - distributed.scheduler - INFO - Remove client Client-f448db92-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:19,141 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51540; closing.
2024-01-09 06:33:19,144 - distributed.scheduler - INFO - Remove client Client-f448db92-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:19,145 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36987', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44987'. Reason: nanny-close
2024-01-09 06:33:19,146 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36987
2024-01-09 06:33:19,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51648
2024-01-09 06:33:19,146 - distributed.scheduler - INFO - Remove client Client-f47fe4d4-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:19,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51530; closing.
2024-01-09 06:33:19,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41877'. Reason: nanny-close
2024-01-09 06:33:19,147 - distributed.scheduler - INFO - Remove client Client-f47fe4d4-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:19,147 - distributed.scheduler - INFO - Close client connection: Client-f448db92-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:19,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,147 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41779. Reason: nanny-close
2024-01-09 06:33:19,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34303'. Reason: nanny-close
2024-01-09 06:33:19,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,148 - distributed.scheduler - INFO - Close client connection: Client-f47fe4d4-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:19,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45437'. Reason: nanny-close
2024-01-09 06:33:19,148 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42715. Reason: nanny-close
2024-01-09 06:33:19,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39565'. Reason: nanny-close
2024-01-09 06:33:19,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33929. Reason: nanny-close
2024-01-09 06:33:19,149 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51630; closing.
2024-01-09 06:33:19,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43975'. Reason: nanny-close
2024-01-09 06:33:19,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41779', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1495352')
2024-01-09 06:33:19,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35865. Reason: nanny-close
2024-01-09 06:33:19,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32963. Reason: nanny-close
2024-01-09 06:33:19,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41319'. Reason: nanny-close
2024-01-09 06:33:19,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,150 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44985. Reason: nanny-close
2024-01-09 06:33:19,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45061'. Reason: nanny-close
2024-01-09 06:33:19,150 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:19,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51574; closing.
2024-01-09 06:33:19,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39573. Reason: nanny-close
2024-01-09 06:33:19,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,152 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1526153')
2024-01-09 06:33:19,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46033. Reason: nanny-close
2024-01-09 06:33:19,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51584; closing.
2024-01-09 06:33:19,153 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,153 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,154 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1541536')
2024-01-09 06:33:19,154 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51600; closing.
2024-01-09 06:33:19,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51572; closing.
2024-01-09 06:33:19,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:19,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51622; closing.
2024-01-09 06:33:19,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41561', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41561
2024-01-09 06:33:19,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51656
2024-01-09 06:33:19,156 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,156 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:19,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35865', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1566617')
2024-01-09 06:33:19,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1570704')
2024-01-09 06:33:19,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44985', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1574426')
2024-01-09 06:33:19,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51566; closing.
2024-01-09 06:33:19,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51612; closing.
2024-01-09 06:33:19,159 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.159237')
2024-01-09 06:33:19,159 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1596415')
2024-01-09 06:33:19,160 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51566>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-09 06:33:19,162 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51612>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-09 06:33:19,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44953', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,170 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44953
2024-01-09 06:33:19,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51664
2024-01-09 06:33:19,178 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41497', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,178 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41497
2024-01-09 06:33:19,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51668
2024-01-09 06:33:19,183 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51656; closing.
2024-01-09 06:33:19,184 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41561', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.18397')
2024-01-09 06:33:19,184 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51664; closing.
2024-01-09 06:33:19,185 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51634; closing.
2024-01-09 06:33:19,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44953', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1856627')
2024-01-09 06:33:19,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51648; closing.
2024-01-09 06:33:19,186 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36217', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.186405')
2024-01-09 06:33:19,186 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36987', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.1867697')
2024-01-09 06:33:19,196 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36299', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,197 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36299
2024-01-09 06:33:19,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51682
2024-01-09 06:33:19,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39325', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39325
2024-01-09 06:33:19,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51678
2024-01-09 06:33:19,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36851', status: init, memory: 0, processing: 0>
2024-01-09 06:33:19,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36851
2024-01-09 06:33:19,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51696
2024-01-09 06:33:19,235 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51668; closing.
2024-01-09 06:33:19,236 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41497', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.2359266')
2024-01-09 06:33:19,236 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51682; closing.
2024-01-09 06:33:19,237 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36299', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.2373757')
2024-01-09 06:33:19,237 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51696; closing.
2024-01-09 06:33:19,238 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.238133')
2024-01-09 06:33:19,244 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51678; closing.
2024-01-09 06:33:19,244 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39325', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781999.2448995')
2024-01-09 06:33:19,245 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:33:20,412 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:33:20,413 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:33:20,414 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:33:20,415 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:33:20,416 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-09 06:33:22,863 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:22,868 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:33:22,872 - distributed.scheduler - INFO - State start
2024-01-09 06:33:22,895 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:22,896 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:33:22,897 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:33:22,897 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:33:23,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39345'
2024-01-09 06:33:23,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42349'
2024-01-09 06:33:23,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43273'
2024-01-09 06:33:23,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37719'
2024-01-09 06:33:23,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39225'
2024-01-09 06:33:23,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36125'
2024-01-09 06:33:23,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44035'
2024-01-09 06:33:23,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45085'
2024-01-09 06:33:23,379 - distributed.scheduler - INFO - Receive client connection: Client-fb56ae6e-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:23,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50662
2024-01-09 06:33:23,422 - distributed.scheduler - INFO - Receive client connection: Client-fb77c5ad-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:23,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50676
2024-01-09 06:33:25,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,087 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42027
2024-01-09 06:33:25,088 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42027
2024-01-09 06:33:25,088 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37737
2024-01-09 06:33:25,088 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,089 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-12e47ora
2024-01-09 06:33:25,089 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,089 - distributed.worker - INFO - Starting Worker plugin PreImport-c1ba6b68-5162-4a25-bf8b-2a851b52d986
2024-01-09 06:33:25,089 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb0fc6c3-9121-49fb-8d4e-35d2659bf7d7
2024-01-09 06:33:25,090 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb8158c4-efce-4d1e-a54e-695875706eab
2024-01-09 06:33:25,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35049
2024-01-09 06:33:25,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35049
2024-01-09 06:33:25,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44747
2024-01-09 06:33:25,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,095 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,095 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6yeh2diy
2024-01-09 06:33:25,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07b266a3-2d05-4d48-a043-8e422d112e4e
2024-01-09 06:33:25,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,114 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46087
2024-01-09 06:33:25,114 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46087
2024-01-09 06:33:25,114 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33345
2024-01-09 06:33:25,114 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,114 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,114 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,114 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,115 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t181558b
2024-01-09 06:33:25,115 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae3949a8-94c6-4207-8670-21223252ecb6
2024-01-09 06:33:25,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,133 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41781
2024-01-09 06:33:25,133 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41781
2024-01-09 06:33:25,134 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38411
2024-01-09 06:33:25,134 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,134 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,134 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,134 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,134 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vymarofw
2024-01-09 06:33:25,134 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56ea83ff-26f9-4124-9c8c-4fad587c319e
2024-01-09 06:33:25,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,172 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35121
2024-01-09 06:33:25,172 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35121
2024-01-09 06:33:25,172 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44029
2024-01-09 06:33:25,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,172 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,173 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,173 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kwveoyxm
2024-01-09 06:33:25,173 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e01d87a-dc09-4443-9c38-e87475d4792d
2024-01-09 06:33:25,178 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,178 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,183 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,184 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35115
2024-01-09 06:33:25,184 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35115
2024-01-09 06:33:25,184 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39599
2024-01-09 06:33:25,184 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,184 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,184 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,184 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,184 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-72q8486m
2024-01-09 06:33:25,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20200645-2bfa-44cf-9278-45db4eb138fa
2024-01-09 06:33:25,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:25,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:25,200 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,201 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44059
2024-01-09 06:33:25,201 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44059
2024-01-09 06:33:25,201 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41395
2024-01-09 06:33:25,201 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,201 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,201 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:25,201 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,201 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,201 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mjbyg88e
2024-01-09 06:33:25,202 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b0d6a25-7e41-42c9-896a-14faf7fec1bc
2024-01-09 06:33:25,202 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36363
2024-01-09 06:33:25,202 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36363
2024-01-09 06:33:25,202 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46787
2024-01-09 06:33:25,202 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:25,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:25,203 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:25,203 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:25,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ed72qt_2
2024-01-09 06:33:25,203 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc728ab5-893e-4720-8871-8ce352dbdd87
2024-01-09 06:33:29,642 - distributed.worker - INFO - Starting Worker plugin PreImport-176d488d-b912-405c-b8f1-591f9e27a723
2024-01-09 06:33:29,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40af80d6-8017-4eb9-8e3f-6704fb374256
2024-01-09 06:33:29,644 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,680 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35121', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,681 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35121
2024-01-09 06:33:29,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50750
2024-01-09 06:33:29,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,684 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3db39fe7-5369-49bc-9913-44522711d88a
2024-01-09 06:33:29,744 - distributed.worker - INFO - Starting Worker plugin PreImport-f8c87152-7675-437f-b933-95766da35fd0
2024-01-09 06:33:29,744 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,764 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,767 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44059', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,767 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44059
2024-01-09 06:33:29,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50766
2024-01-09 06:33:29,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,769 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,769 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42027', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,795 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42027
2024-01-09 06:33:29,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50772
2024-01-09 06:33:29,797 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,797 - distributed.worker - INFO - Starting Worker plugin PreImport-8ca0ed2b-28fe-4408-9ba7-a449620e1517
2024-01-09 06:33:29,798 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e64d911f-2362-4b02-baef-141b952a479e
2024-01-09 06:33:29,798 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,798 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,819 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35115', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35115
2024-01-09 06:33:29,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50780
2024-01-09 06:33:29,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,821 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a455c19-32f8-4c96-a6e1-fb6fd8992ee8
2024-01-09 06:33:29,826 - distributed.worker - INFO - Starting Worker plugin PreImport-40b82df5-ae7c-4f30-b85f-ba43288e8d32
2024-01-09 06:33:29,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f1a0ee0-eca4-452c-9d9b-6b1c94ef4677
2024-01-09 06:33:29,831 - distributed.worker - INFO - Starting Worker plugin PreImport-2dcb3d53-2865-4432-90dc-7a92e70980fc
2024-01-09 06:33:29,831 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9bd2184d-499d-43d5-ad2b-430f64a777f9
2024-01-09 06:33:29,832 - distributed.worker - INFO - Starting Worker plugin PreImport-26468091-d296-43c6-88b5-0294a9fdd0c0
2024-01-09 06:33:29,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,833 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,839 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-674e9c16-d755-4e7f-8fb1-e732d152eb1c
2024-01-09 06:33:29,839 - distributed.worker - INFO - Starting Worker plugin PreImport-126fbbde-b29d-49fc-914a-487599e58d19
2024-01-09 06:33:29,840 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35049', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,855 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35049
2024-01-09 06:33:29,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50794
2024-01-09 06:33:29,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,857 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,857 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36363', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36363
2024-01-09 06:33:29,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50814
2024-01-09 06:33:29,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,863 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,873 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46087', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,874 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46087
2024-01-09 06:33:29,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50810
2024-01-09 06:33:29,874 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41781', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,875 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41781
2024-01-09 06:33:29,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50796
2024-01-09 06:33:29,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,876 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,877 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:29,878 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:29,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:29,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:29,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33763', status: init, memory: 0, processing: 0>
2024-01-09 06:33:29,923 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33763
2024-01-09 06:33:29,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50830
2024-01-09 06:33:30,042 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41489', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,042 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41489
2024-01-09 06:33:30,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33066
2024-01-09 06:33:30,044 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34065', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,045 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34065
2024-01-09 06:33:30,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33058
2024-01-09 06:33:30,050 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41757', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,051 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41757
2024-01-09 06:33:30,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33072
2024-01-09 06:33:30,075 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46291', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,076 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46291
2024-01-09 06:33:30,076 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33076
2024-01-09 06:33:30,084 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33205', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,085 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33205
2024-01-09 06:33:30,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33086
2024-01-09 06:33:30,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36587', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36587
2024-01-09 06:33:30,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33088
2024-01-09 06:33:30,103 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37465', status: init, memory: 0, processing: 0>
2024-01-09 06:33:30,104 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37465
2024-01-09 06:33:30,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33100
2024-01-09 06:33:39,473 - distributed.scheduler - INFO - Remove client Client-fb77c5ad-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:39,474 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50676; closing.
2024-01-09 06:33:39,474 - distributed.scheduler - INFO - Remove client Client-fb77c5ad-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:39,475 - distributed.scheduler - INFO - Remove client Client-fb56ae6e-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:39,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50662; closing.
2024-01-09 06:33:39,475 - distributed.scheduler - INFO - Remove client Client-fb56ae6e-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:39,475 - distributed.scheduler - INFO - Close client connection: Client-fb77c5ad-aeb8-11ee-b596-d8c49764f6bb
2024-01-09 06:33:39,476 - distributed.scheduler - INFO - Close client connection: Client-fb56ae6e-aeb8-11ee-b9fd-d8c49764f6bb
2024-01-09 06:33:39,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39345'. Reason: nanny-close
2024-01-09 06:33:39,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42349'. Reason: nanny-close
2024-01-09 06:33:39,478 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43273'. Reason: nanny-close
2024-01-09 06:33:39,479 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35049. Reason: nanny-close
2024-01-09 06:33:39,479 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37719'. Reason: nanny-close
2024-01-09 06:33:39,479 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35115. Reason: nanny-close
2024-01-09 06:33:39,479 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39225'. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41781. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36125'. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42027. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44035'. Reason: nanny-close
2024-01-09 06:33:39,480 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36363. Reason: nanny-close
2024-01-09 06:33:39,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45085'. Reason: nanny-close
2024-01-09 06:33:39,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,481 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44059. Reason: nanny-close
2024-01-09 06:33:39,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:33:39,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33066; closing.
2024-01-09 06:33:39,482 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50794; closing.
2024-01-09 06:33:39,482 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,482 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,482 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46087. Reason: nanny-close
2024-01-09 06:33:39,482 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,482 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,483 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41489', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4830596')
2024-01-09 06:33:39,483 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,482 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35121. Reason: nanny-close
2024-01-09 06:33:39,483 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,483 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50830; closing.
2024-01-09 06:33:39,484 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4841478')
2024-01-09 06:33:39,484 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33088; closing.
2024-01-09 06:33:39,484 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,484 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50780; closing.
2024-01-09 06:33:39,485 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,485 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,485 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:39,486 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33763', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.486831')
2024-01-09 06:33:39,487 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4872832')
2024-01-09 06:33:39,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50796; closing.
2024-01-09 06:33:39,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.487908')
2024-01-09 06:33:39,488 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:39,488 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50772; closing.
2024-01-09 06:33:39,488 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33076; closing.
2024-01-09 06:33:39,489 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33100; closing.
2024-01-09 06:33:39,491 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4911723')
2024-01-09 06:33:39,491 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42027', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4915826')
2024-01-09 06:33:39,492 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.491951')
2024-01-09 06:33:39,492 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50814; closing.
2024-01-09 06:33:39,492 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33086; closing.
2024-01-09 06:33:39,492 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50766; closing.
2024-01-09 06:33:39,492 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37465', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.492839')
2024-01-09 06:33:39,493 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33072; closing.
2024-01-09 06:33:39,494 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50830>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-09 06:33:39,496 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50780>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-09 06:33:39,497 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33088>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-09 06:33:39,498 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.497958')
2024-01-09 06:33:39,498 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33205', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4984145')
2024-01-09 06:33:39,498 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44059', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4987698')
2024-01-09 06:33:39,499 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.4991448')
2024-01-09 06:33:39,499 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33058; closing.
2024-01-09 06:33:39,499 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50810; closing.
2024-01-09 06:33:39,499 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50750; closing.
2024-01-09 06:33:39,500 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34065', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.5003572')
2024-01-09 06:33:39,500 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.500707')
2024-01-09 06:33:39,501 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782019.5010564')
2024-01-09 06:33:39,501 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:33:41,796 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:33:41,796 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:33:41,797 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:33:41,799 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:33:41,800 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-09 06:33:44,702 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:44,707 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37577 instead
  warnings.warn(
2024-01-09 06:33:44,711 - distributed.scheduler - INFO - State start
2024-01-09 06:33:44,735 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:33:44,736 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:33:44,736 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:33:44,737 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:33:44,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35691'
2024-01-09 06:33:44,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34157'
2024-01-09 06:33:44,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41491'
2024-01-09 06:33:45,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38571'
2024-01-09 06:33:45,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37085'
2024-01-09 06:33:45,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46145'
2024-01-09 06:33:45,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43199'
2024-01-09 06:33:45,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40837'
2024-01-09 06:33:47,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,078 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,079 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34793
2024-01-09 06:33:47,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34793
2024-01-09 06:33:47,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45009
2024-01-09 06:33:47,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,079 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,079 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,080 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qk2d5ii1
2024-01-09 06:33:47,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e7b9be5-d244-4004-b526-64cc3e24610f
2024-01-09 06:33:47,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,081 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39111
2024-01-09 06:33:47,081 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39111
2024-01-09 06:33:47,081 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41813
2024-01-09 06:33:47,081 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,081 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,081 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,082 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,082 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jl4g9y4y
2024-01-09 06:33:47,082 - distributed.worker - INFO - Starting Worker plugin PreImport-44289f30-cd3b-426a-975b-a8a599621984
2024-01-09 06:33:47,082 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e156c5e6-4f7b-4c46-be6e-77ed36fcdbe1
2024-01-09 06:33:47,082 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04504be4-6726-45cf-8d2f-ea6f28174dab
2024-01-09 06:33:47,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,089 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,090 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44535
2024-01-09 06:33:47,090 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44535
2024-01-09 06:33:47,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45609
2024-01-09 06:33:47,090 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,090 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,090 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,090 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,090 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t9tmyivl
2024-01-09 06:33:47,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f1b6606-90b2-4ba8-ba2d-2b611b1e3190
2024-01-09 06:33:47,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37953
2024-01-09 06:33:47,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37953
2024-01-09 06:33:47,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34305
2024-01-09 06:33:47,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,094 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42553
2024-01-09 06:33:47,094 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43427
2024-01-09 06:33:47,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42553
2024-01-09 06:33:47,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43427
2024-01-09 06:33:47,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44589
2024-01-09 06:33:47,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45139
2024-01-09 06:33:47,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rud8hbej
2024-01-09 06:33:47,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,094 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,094 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,094 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,094 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-svobeh7q
2024-01-09 06:33:47,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e85xl7hl
2024-01-09 06:33:47,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b83df99a-f7e4-4fc4-b0fa-6404599710fa
2024-01-09 06:33:47,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-480a7b4e-54b1-4179-926f-08c4cdaec204
2024-01-09 06:33:47,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38da41cf-5b19-45b1-8e9e-1e32b8a30b5e
2024-01-09 06:33:47,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40289
2024-01-09 06:33:47,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40289
2024-01-09 06:33:47,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33303
2024-01-09 06:33:47,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,305 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,305 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,305 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1cfx87i0
2024-01-09 06:33:47,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1bfa36a-9023-40e9-8c01-5e9dce44560a
2024-01-09 06:33:47,305 - distributed.worker - INFO - Starting Worker plugin PreImport-5b925b6d-94a8-4117-a39b-7346cf005eda
2024-01-09 06:33:47,306 - distributed.worker - INFO - Starting Worker plugin RMMSetup-727bac49-0dc6-4cb8-aec1-0e93514f632e
2024-01-09 06:33:47,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:33:47,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:33:47,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:33:47,313 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39759
2024-01-09 06:33:47,313 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39759
2024-01-09 06:33:47,313 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33263
2024-01-09 06:33:47,313 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:33:47,313 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:47,314 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:33:47,314 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:33:47,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aleo_vzl
2024-01-09 06:33:47,314 - distributed.worker - INFO - Starting Worker plugin PreImport-eeb19f0e-f7c4-46e7-8f15-b1c6b0a83c8b
2024-01-09 06:33:47,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4eaab8b-a29b-423b-8a56-862f754a4b54
2024-01-09 06:33:47,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2efd872-c8bd-4d9d-9675-553006fde487
2024-01-09 06:33:50,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:50,991 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40c72904-a901-4934-9609-f47073149cd5
2024-01-09 06:33:50,992 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee4f433c-4dc0-4cc7-923b-0b7a7779e6db
2024-01-09 06:33:50,993 - distributed.worker - INFO - Starting Worker plugin PreImport-bee5c1f7-c375-4d0e-bf7a-0f9b035ca6eb
2024-01-09 06:33:50,993 - distributed.worker - INFO - Starting Worker plugin PreImport-e8d36812-e56e-4b9c-8c96-d90e04864dd5
2024-01-09 06:33:50,993 - distributed.worker - INFO - Starting Worker plugin PreImport-39921654-bbb7-42b7-b1b4-d550aaa44edf
2024-01-09 06:33:50,993 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:50,993 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c0c2839-832f-46d4-8724-d9a6eb6ae388
2024-01-09 06:33:50,994 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:50,994 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:50,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:51,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3409d125-babe-453a-aaae-2254ed9d7789
2024-01-09 06:33:51,203 - distributed.worker - INFO - Starting Worker plugin PreImport-9d94c38e-97a7-4eaa-8183-cdde9042fe66
2024-01-09 06:33:51,203 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:51,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:51,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00eb95c5-2746-43a7-95b9-8202921413ba
2024-01-09 06:33:51,315 - distributed.worker - INFO - Starting Worker plugin PreImport-68364f28-fbc3-4821-a323-9fa9604a3191
2024-01-09 06:33:51,315 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:53,818 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:53,819 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:53,819 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:53,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:53,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35691'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:33:53,832 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:33:53,833 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44535. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:33:53,836 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:53,838 - distributed.nanny - INFO - Worker closed
2024-01-09 06:33:54,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:33:54,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:33:54,067 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:33:54,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:33:54,087 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37085'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:33:54,087 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:33:54,088 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43427. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:33:54,090 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:33:54,092 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:55484 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:33:54,178 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65168 parent=64971 started daemon>
2024-01-09 06:33:54,178 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65164 parent=64971 started daemon>
2024-01-09 06:33:54,178 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65160 parent=64971 started daemon>
2024-01-09 06:33:54,178 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65156 parent=64971 started daemon>
2024-01-09 06:33:54,179 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65151 parent=64971 started daemon>
2024-01-09 06:33:54,179 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65146 parent=64971 started daemon>
2024-01-09 06:33:54,179 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65141 parent=64971 started daemon>
2024-01-09 06:33:54,302 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65141 exit status was already read will report exitcode 255
2024-01-09 06:33:54,334 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65156 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-09 06:34:03,220 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:34:03,226 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:34:03,231 - distributed.scheduler - INFO - State start
2024-01-09 06:34:03,232 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qk2d5ii1', purging
2024-01-09 06:34:03,233 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rud8hbej', purging
2024-01-09 06:34:03,234 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-svobeh7q', purging
2024-01-09 06:34:03,234 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1cfx87i0', purging
2024-01-09 06:34:03,234 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-aleo_vzl', purging
2024-01-09 06:34:03,235 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jl4g9y4y', purging
2024-01-09 06:34:03,592 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:34:03,593 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:34:03,594 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:34:03,595 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:34:12,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34295'
2024-01-09 06:34:12,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43105'
2024-01-09 06:34:12,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37005'
2024-01-09 06:34:14,700 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:14,700 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:14,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:14,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39269
2024-01-09 06:34:14,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39269
2024-01-09 06:34:14,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44091
2024-01-09 06:34:14,707 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:14,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:14,707 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:14,707 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:14,707 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zlvuwzmt
2024-01-09 06:34:14,708 - distributed.worker - INFO - Starting Worker plugin PreImport-47dcba8d-4647-40e5-a923-47dbf84124d1
2024-01-09 06:34:14,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29c9e478-b306-4489-aaa5-b20a133a760d
2024-01-09 06:34:14,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-386ce979-7b6a-442c-9ae8-d9af874b32ae
2024-01-09 06:34:14,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:14,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:14,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:14,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:14,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:14,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:14,739 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40291
2024-01-09 06:34:14,739 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40291
2024-01-09 06:34:14,739 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37747
2024-01-09 06:34:14,739 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:14,739 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:14,739 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:14,739 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:14,739 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z7k0z2bn
2024-01-09 06:34:14,739 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33443
2024-01-09 06:34:14,740 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33443
2024-01-09 06:34:14,740 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37911
2024-01-09 06:34:14,740 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:14,740 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f6d7a3a-b3e7-4967-b427-fa60bb4fe579
2024-01-09 06:34:14,740 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:14,740 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:14,740 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:14,740 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tgx2m82b
2024-01-09 06:34:14,740 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acbe0945-1a08-4b9b-afba-8faeb67eb711
2024-01-09 06:34:15,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44027'
2024-01-09 06:34:16,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39759'
2024-01-09 06:34:17,130 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41827'
2024-01-09 06:34:17,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:34:17,168 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:34:17,168 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:34:17,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12d42d48-7467-420f-9bd8-4ffda2e08d96
2024-01-09 06:34:17,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b9867cc-2444-41dc-87fd-c95b26c906f2
2024-01-09 06:34:17,198 - distributed.worker - INFO - Starting Worker plugin PreImport-c31c6f1d-89d7-427f-9797-8e0b7d2d92da
2024-01-09 06:34:17,198 - distributed.worker - INFO - Starting Worker plugin PreImport-a21ad1e9-2d2d-4a58-92d2-78431c4c7139
2024-01-09 06:34:17,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:34:17,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:34:17,225 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:34:17,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:34:17,227 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:34:17,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:34:17,454 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40291. Reason: scheduler-close
2024-01-09 06:34:17,454 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33443. Reason: scheduler-close
2024-01-09 06:34:17,454 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39269. Reason: scheduler-close
2024-01-09 06:34:17,455 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56216 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56216 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-09 06:34:17,455 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56206 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56206 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-09 06:34:17,459 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:37005'. Reason: scheduler-close
2024-01-09 06:34:17,456 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56198 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56198 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-09 06:34:17,460 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:43105'. Reason: scheduler-close
2024-01-09 06:34:17,461 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:34295'. Reason: scheduler-close
2024-01-09 06:34:17,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2024-01-09 06:34:17,462 - distributed.nanny - INFO - Worker closed
2024-01-09 06:34:17,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2024-01-09 06:34:17,462 - distributed.nanny - INFO - Worker closed
2024-01-09 06:34:17,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2024-01-09 06:34:17,464 - distributed.nanny - INFO - Worker closed
2024-01-09 06:34:17,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:17,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:17,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:17,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33033
2024-01-09 06:34:17,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33033
2024-01-09 06:34:17,678 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40169
2024-01-09 06:34:17,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:17,678 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:17,678 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:17,678 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:17,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-05ktjkht
2024-01-09 06:34:17,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2807d3b-915d-4616-988f-6df55b1ae370
2024-01-09 06:34:18,018 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:18,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:18,020 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80c04fd8-f648-498a-93e0-813567d1ff9c
2024-01-09 06:34:18,021 - distributed.worker - INFO - Starting Worker plugin PreImport-080f6a6b-c1d7-457e-98fa-eb15bd32ec9c
2024-01-09 06:34:18,021 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:18,022 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:18,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44303
2024-01-09 06:34:18,023 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44303
2024-01-09 06:34:18,023 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45855
2024-01-09 06:34:18,023 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:18,023 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:18,023 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:18,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:18,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iil_5k4m
2024-01-09 06:34:18,024 - distributed.worker - INFO - Starting Worker plugin PreImport-14b23cd3-5780-411e-bb50-e2b5f3a521ee
2024-01-09 06:34:18,024 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f6bbbad-1073-4b70-8cc9-44262afc663e
2024-01-09 06:34:18,024 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8096e70b-f2d3-4ab4-8045-0f01d30e33a1
2024-01-09 06:34:18,398 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:19,013 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:19,013 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:19,018 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:19,019 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42039
2024-01-09 06:34:19,019 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42039
2024-01-09 06:34:19,019 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37909
2024-01-09 06:34:19,019 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:19,019 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:19,019 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:19,019 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:19,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aaul6i5a
2024-01-09 06:34:19,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17bb2c66-c60b-4f38-a171-7aca63e1289c
2024-01-09 06:34:19,339 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d3ad854-0f8a-4812-b3bb-a5f24f672b5e
2024-01-09 06:34:19,339 - distributed.worker - INFO - Starting Worker plugin PreImport-12a62880-58bd-4ff3-9f1b-fdfe84dd349e
2024-01-09 06:34:19,339 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:19,464 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-09 06:34:19,464 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-09 06:34:19,466 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-09 06:34:19,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37005'. Reason: nanny-close-gracefully
2024-01-09 06:34:19,865 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43105'. Reason: nanny-close-gracefully
2024-01-09 06:34:19,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:34:19,923 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:34:19,923 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:19,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:34:19,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41827'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:34:19,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:34:19,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42039. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-09 06:34:19,941 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:34:19,942 - distributed.nanny - INFO - Worker closed
2024-01-09 06:34:19,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34295'. Reason: nanny-close-gracefully
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:56202 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:34:20,261 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65430 parent=65230 started daemon>
2024-01-09 06:34:20,261 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65422 parent=65230 started daemon>
2024-01-09 06:34:20,355 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65430 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-09 06:34:28,154 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:34:28,160 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:34:28,164 - distributed.scheduler - INFO - State start
2024-01-09 06:34:28,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-05ktjkht', purging
2024-01-09 06:34:28,166 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-iil_5k4m', purging
2024-01-09 06:34:28,189 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:34:28,190 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:34:28,190 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:34:28,191 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:34:28,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33979'
2024-01-09 06:34:28,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35821'
2024-01-09 06:34:28,351 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41849'
2024-01-09 06:34:28,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46755'
2024-01-09 06:34:28,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44775'
2024-01-09 06:34:28,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40983'
2024-01-09 06:34:28,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39543'
2024-01-09 06:34:28,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36979'
2024-01-09 06:34:30,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,263 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43373
2024-01-09 06:34:30,263 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33035
2024-01-09 06:34:30,263 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33027
2024-01-09 06:34:30,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43373
2024-01-09 06:34:30,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33035
2024-01-09 06:34:30,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33027
2024-01-09 06:34:30,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34885
2024-01-09 06:34:30,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42941
2024-01-09 06:34:30,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41269
2024-01-09 06:34:30,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,264 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,264 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,264 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o2tt38ho
2024-01-09 06:34:30,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5rgtku59
2024-01-09 06:34:30,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f3b_qn6z
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec175549-8642-4ca7-9d5b-401e32ab7441
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c391ea58-caa4-4cd1-9981-5a4f916ca785
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin PreImport-e0d3185a-12ba-49e7-b974-3aaf8fff0fc5
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin PreImport-55d8b37d-57f4-4f47-ae7a-da3d74ea29db
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba298747-aa10-4f8c-8725-933f723bb8c0
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-778f3671-c41b-4218-ab11-88c7b6db7976
2024-01-09 06:34:30,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-790d838e-485f-4a5f-8a84-82ce97c2e034
2024-01-09 06:34:30,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,287 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45633
2024-01-09 06:34:30,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45633
2024-01-09 06:34:30,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35575
2024-01-09 06:34:30,288 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39503
2024-01-09 06:34:30,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35575
2024-01-09 06:34:30,288 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,288 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42191
2024-01-09 06:34:30,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,289 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,289 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,289 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6wuu6xor
2024-01-09 06:34:30,289 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_rebv8v2
2024-01-09 06:34:30,289 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e48c346c-ef8d-4b51-9d71-846f0a478db0
2024-01-09 06:34:30,289 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb1b4ef8-5cee-4250-bc27-9e533e7ab91f
2024-01-09 06:34:30,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,352 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34903
2024-01-09 06:34:30,352 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34903
2024-01-09 06:34:30,352 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34531
2024-01-09 06:34:30,352 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,352 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,352 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,352 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,352 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ylbireoo
2024-01-09 06:34:30,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4f858f3-0341-4d75-8f71-f9635a908667
2024-01-09 06:34:30,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:34:30,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:34:30,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,368 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34649
2024-01-09 06:34:30,368 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34649
2024-01-09 06:34:30,368 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44877
2024-01-09 06:34:30,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,369 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,369 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zb665gfp
2024-01-09 06:34:30,369 - distributed.worker - INFO - Starting Worker plugin PreImport-cb3417d7-a4cb-4206-b16f-b320f6cc1b28
2024-01-09 06:34:30,369 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0cdb1960-1003-438f-8e54-587a7d7598cc
2024-01-09 06:34:30,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:34:30,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e44491a-4815-4cd3-86ba-809cad13487d
2024-01-09 06:34:30,370 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43705
2024-01-09 06:34:30,370 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43705
2024-01-09 06:34:30,370 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46555
2024-01-09 06:34:30,370 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:34:30,370 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:30,370 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:34:30,370 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:34:30,370 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5qkn6nf7
2024-01-09 06:34:30,371 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff96a8a5-487f-440a-9306-3bbf19321631
2024-01-09 06:34:32,380 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3dccdcd-aa40-4b7d-b01b-15737e6be969
2024-01-09 06:34:32,462 - distributed.worker - INFO - Starting Worker plugin PreImport-cee38f2e-5158-4890-82e4-3e2d8ffbd181
2024-01-09 06:34:32,464 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,464 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,492 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d368b772-f8f1-4ae9-927b-6618555c2f0a
2024-01-09 06:34:32,493 - distributed.worker - INFO - Starting Worker plugin PreImport-35ab9396-bde9-4186-954b-46f506c162cd
2024-01-09 06:34:32,493 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,496 - distributed.worker - INFO - Starting Worker plugin PreImport-5b6e4d79-4278-43e0-aaab-bf7415c63761
2024-01-09 06:34:32,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39f2a374-77b9-4e7f-addd-1d518d67aa9d
2024-01-09 06:34:32,497 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,555 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0de341d7-8580-4c78-a647-838d87d7dd79
2024-01-09 06:34:32,556 - distributed.worker - INFO - Starting Worker plugin PreImport-15bcd45c-c867-4eec-bfd8-23e6a4055904
2024-01-09 06:34:32,557 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,578 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:32,599 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e50c8fb3-36c5-4967-b0a7-ee79130d7e72
2024-01-09 06:34:32,599 - distributed.worker - INFO - Starting Worker plugin PreImport-5b12a5e2-ce7f-48a6-8354-0356455a6017
2024-01-09 06:34:32,600 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:34:58,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33979'. Reason: nanny-close
2024-01-09 06:34:58,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35821'. Reason: nanny-close
2024-01-09 06:34:58,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41849'. Reason: nanny-close
2024-01-09 06:34:58,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46755'. Reason: nanny-close
2024-01-09 06:34:58,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44775'. Reason: nanny-close
2024-01-09 06:34:58,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40983'. Reason: nanny-close
2024-01-09 06:34:58,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39543'. Reason: nanny-close
2024-01-09 06:34:58,785 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36979'. Reason: nanny-close
2024-01-09 06:35:02,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,465 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,466 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,493 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,498 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,558 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,581 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:02,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-09 06:35:31,046 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:31,051 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39595 instead
  warnings.warn(
2024-01-09 06:35:31,056 - distributed.scheduler - INFO - State start
2024-01-09 06:35:31,057 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5rgtku59', purging
2024-01-09 06:35:31,058 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ylbireoo', purging
2024-01-09 06:35:31,058 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6wuu6xor', purging
2024-01-09 06:35:31,058 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zb665gfp', purging
2024-01-09 06:35:31,059 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_rebv8v2', purging
2024-01-09 06:35:31,059 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o2tt38ho', purging
2024-01-09 06:35:31,059 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f3b_qn6z', purging
2024-01-09 06:35:31,059 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5qkn6nf7', purging
2024-01-09 06:35:31,082 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:31,082 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:35:31,083 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39595/status
2024-01-09 06:35:31,083 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:35:31,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37305'
2024-01-09 06:35:31,315 - distributed.scheduler - INFO - Receive client connection: Client-47c84ec5-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:31,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46462
2024-01-09 06:35:33,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:33,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:33,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:33,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38713
2024-01-09 06:35:33,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38713
2024-01-09 06:35:33,698 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-09 06:35:33,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:33,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:33,699 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:33,699 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:35:33,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kbhe8l8t
2024-01-09 06:35:33,699 - distributed.worker - INFO - Starting Worker plugin PreImport-62ccef2c-6cac-4c5b-9f8d-2dbe0354e808
2024-01-09 06:35:33,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67822d47-6c96-4b89-b08a-eb69cd078ba0
2024-01-09 06:35:33,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e79ca9a-63c5-4f20-bf01-3c656cbb14a7
2024-01-09 06:35:33,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:33,753 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38713', status: init, memory: 0, processing: 0>
2024-01-09 06:35:33,754 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38713
2024-01-09 06:35:33,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46486
2024-01-09 06:35:33,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:35:33,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:35:33,756 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:33,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:35:33,779 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:35:33,782 - distributed.scheduler - INFO - Remove client Client-47c84ec5-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:33,782 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46462; closing.
2024-01-09 06:35:33,782 - distributed.scheduler - INFO - Remove client Client-47c84ec5-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:33,783 - distributed.scheduler - INFO - Close client connection: Client-47c84ec5-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:33,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37305'. Reason: nanny-close
2024-01-09 06:35:33,784 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:35:33,785 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38713. Reason: nanny-close
2024-01-09 06:35:33,787 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:35:33,787 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46486; closing.
2024-01-09 06:35:33,787 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38713', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782133.7874968')
2024-01-09 06:35:33,787 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:35:33,788 - distributed.nanny - INFO - Worker closed
2024-01-09 06:35:34,399 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:35:34,399 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:35:34,400 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:35:34,401 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:35:34,401 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-09 06:35:38,724 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:38,729 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32865 instead
  warnings.warn(
2024-01-09 06:35:38,733 - distributed.scheduler - INFO - State start
2024-01-09 06:35:38,754 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:38,754 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:35:38,755 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32865/status
2024-01-09 06:35:38,755 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:35:39,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35843'
2024-01-09 06:35:39,353 - distributed.scheduler - INFO - Receive client connection: Client-4c6ccdca-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:39,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46598
2024-01-09 06:35:41,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:41,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:41,903 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:41,904 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36625
2024-01-09 06:35:41,904 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36625
2024-01-09 06:35:41,904 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34291
2024-01-09 06:35:41,904 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:41,905 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:41,905 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:41,905 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:35:41,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8leaz4kd
2024-01-09 06:35:41,905 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ff7775f-4c33-4e20-a232-d2872ff97436
2024-01-09 06:35:41,905 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81f57ed3-a223-4a55-9f66-c8a6f8138d01
2024-01-09 06:35:41,905 - distributed.worker - INFO - Starting Worker plugin PreImport-881850c4-2b59-4364-87e1-9f01be46d0f1
2024-01-09 06:35:41,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:42,377 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36625', status: init, memory: 0, processing: 0>
2024-01-09 06:35:42,378 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36625
2024-01-09 06:35:42,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40528
2024-01-09 06:35:42,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:35:42,380 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:35:42,381 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:42,382 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:35:42,470 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:35:42,474 - distributed.scheduler - INFO - Remove client Client-4c6ccdca-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:42,474 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46598; closing.
2024-01-09 06:35:42,474 - distributed.scheduler - INFO - Remove client Client-4c6ccdca-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:42,475 - distributed.scheduler - INFO - Close client connection: Client-4c6ccdca-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:42,476 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35843'. Reason: nanny-close
2024-01-09 06:35:42,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:35:42,477 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36625. Reason: nanny-close
2024-01-09 06:35:42,479 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:35:42,479 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40528; closing.
2024-01-09 06:35:42,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782142.4801838')
2024-01-09 06:35:42,480 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:35:42,481 - distributed.nanny - INFO - Worker closed
2024-01-09 06:35:43,091 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:35:43,091 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:35:43,092 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:35:43,093 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:35:43,094 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-09 06:35:45,387 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:45,391 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:35:45,395 - distributed.scheduler - INFO - State start
2024-01-09 06:35:45,417 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:45,418 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:35:45,419 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:35:45,419 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:35:47,906 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40538'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40538>: Stream is closed
2024-01-09 06:35:48,240 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:35:48,241 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:35:48,241 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:35:48,242 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:35:48,242 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-09 06:35:50,365 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:50,369 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42425 instead
  warnings.warn(
2024-01-09 06:35:50,373 - distributed.scheduler - INFO - State start
2024-01-09 06:35:50,396 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:50,397 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-09 06:35:50,397 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42425/status
2024-01-09 06:35:50,398 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:35:50,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36551'
2024-01-09 06:35:51,211 - distributed.scheduler - INFO - Receive client connection: Client-5364d7a1-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:51,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55248
2024-01-09 06:35:52,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:52,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:52,409 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:52,410 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34639
2024-01-09 06:35:52,410 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34639
2024-01-09 06:35:52,411 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44567
2024-01-09 06:35:52,411 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:35:52,411 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:52,411 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:52,411 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:35:52,411 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-5llwf6fy
2024-01-09 06:35:52,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6c12dd6-e6b9-440f-87e2-568dc87d6eb6
2024-01-09 06:35:52,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b520a0b7-9bee-4357-b4ed-fa4de59af674
2024-01-09 06:35:52,411 - distributed.worker - INFO - Starting Worker plugin PreImport-57af1cef-ab54-4e40-893a-eb897704ed2b
2024-01-09 06:35:52,411 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:53,117 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34639', status: init, memory: 0, processing: 0>
2024-01-09 06:35:53,118 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34639
2024-01-09 06:35:53,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55266
2024-01-09 06:35:53,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:35:53,120 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:35:53,120 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:53,121 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:35:53,199 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:35:53,202 - distributed.scheduler - INFO - Remove client Client-5364d7a1-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:53,202 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55248; closing.
2024-01-09 06:35:53,202 - distributed.scheduler - INFO - Remove client Client-5364d7a1-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:53,202 - distributed.scheduler - INFO - Close client connection: Client-5364d7a1-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:53,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36551'. Reason: nanny-close
2024-01-09 06:35:53,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:35:53,205 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34639. Reason: nanny-close
2024-01-09 06:35:53,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:35:53,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55266; closing.
2024-01-09 06:35:53,207 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782153.2069657')
2024-01-09 06:35:53,207 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:35:53,207 - distributed.nanny - INFO - Worker closed
2024-01-09 06:35:53,819 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:35:53,819 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:35:53,820 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:35:53,821 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-09 06:35:53,821 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-09 06:35:56,138 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:56,143 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43697 instead
  warnings.warn(
2024-01-09 06:35:56,147 - distributed.scheduler - INFO - State start
2024-01-09 06:35:56,170 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:35:56,171 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:35:56,171 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43697/status
2024-01-09 06:35:56,172 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:35:56,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38655'
2024-01-09 06:35:56,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44895'
2024-01-09 06:35:56,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41317'
2024-01-09 06:35:56,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38301'
2024-01-09 06:35:56,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34371'
2024-01-09 06:35:56,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39345'
2024-01-09 06:35:56,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33553'
2024-01-09 06:35:56,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33495'
2024-01-09 06:35:56,736 - distributed.scheduler - INFO - Receive client connection: Client-56bdfe81-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:35:56,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37086
2024-01-09 06:35:58,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,349 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45655
2024-01-09 06:35:58,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45655
2024-01-09 06:35:58,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33905
2024-01-09 06:35:58,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,350 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,350 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z5rwhsxy
2024-01-09 06:35:58,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1d8e5c8-9cc5-4436-bc1a-e2aa2cfe0890
2024-01-09 06:35:58,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,361 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,361 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41117
2024-01-09 06:35:58,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45713
2024-01-09 06:35:58,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41117
2024-01-09 06:35:58,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45713
2024-01-09 06:35:58,362 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41821
2024-01-09 06:35:58,362 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43805
2024-01-09 06:35:58,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,362 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,362 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,362 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,362 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-djd7fp2y
2024-01-09 06:35:58,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n3jbvjjn
2024-01-09 06:35:58,363 - distributed.worker - INFO - Starting Worker plugin RMMSetup-060f2e45-0b8f-4ab6-b75c-0f292d7c064f
2024-01-09 06:35:58,363 - distributed.worker - INFO - Starting Worker plugin PreImport-1ee14d78-2315-4275-abd8-91d1ada801ab
2024-01-09 06:35:58,363 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1eff2889-3a6c-4ef0-a571-9c3e5e712924
2024-01-09 06:35:58,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e98ec0fc-e9e1-4502-9802-c51d23e77a05
2024-01-09 06:35:58,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,392 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41733
2024-01-09 06:35:58,393 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41733
2024-01-09 06:35:58,393 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40643
2024-01-09 06:35:58,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,393 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,393 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j59e9xzx
2024-01-09 06:35:58,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a6c2940-fd47-4679-886e-f8ba652552a6
2024-01-09 06:35:58,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,401 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36901
2024-01-09 06:35:58,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36901
2024-01-09 06:35:58,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36725
2024-01-09 06:35:58,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,402 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,402 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,403 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hc7716rl
2024-01-09 06:35:58,403 - distributed.worker - INFO - Starting Worker plugin PreImport-dbbb71c3-2991-4777-a640-c0751c58a058
2024-01-09 06:35:58,403 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f7b2b2d-9dce-4678-b35c-eb142a1a8905
2024-01-09 06:35:58,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-804a0837-9278-4b16-8d13-d2df3cfe753e
2024-01-09 06:35:58,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,405 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33273
2024-01-09 06:35:58,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33273
2024-01-09 06:35:58,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45779
2024-01-09 06:35:58,405 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,406 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,406 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4v6azd7f
2024-01-09 06:35:58,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22f00b80-b207-41a9-a9e7-6d192aae4d1c
2024-01-09 06:35:58,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41233
2024-01-09 06:35:58,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41233
2024-01-09 06:35:58,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46669
2024-01-09 06:35:58,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,406 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,406 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-71p65rtm
2024-01-09 06:35:58,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b592cc39-d421-4604-ad8b-e398237eab7a
2024-01-09 06:35:58,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:35:58,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:35:58,436 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:35:58,436 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43249
2024-01-09 06:35:58,437 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43249
2024-01-09 06:35:58,437 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33743
2024-01-09 06:35:58,437 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:35:58,437 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:35:58,437 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:35:58,437 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:35:58,437 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5zchkg2a
2024-01-09 06:35:58,437 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1c6b4ed-f22c-4cd6-bd40-674ec2314c32
2024-01-09 06:36:00,465 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6e2cc58-49cf-41c5-902c-afc3bec793cc
2024-01-09 06:36:00,466 - distributed.worker - INFO - Starting Worker plugin PreImport-719007ef-2667-4eeb-84e2-b32a55019286
2024-01-09 06:36:00,467 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,489 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41117', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,491 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41117
2024-01-09 06:36:00,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53018
2024-01-09 06:36:00,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,493 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,493 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,528 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ad33cd4-ea05-47ec-ba79-2c73bfd6253b
2024-01-09 06:36:00,529 - distributed.worker - INFO - Starting Worker plugin PreImport-e6454075-a69e-4def-9602-4563d633c9fa
2024-01-09 06:36:00,530 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,553 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45655', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45655
2024-01-09 06:36:00,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53022
2024-01-09 06:36:00,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,555 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,555 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,557 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,593 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45713', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45713
2024-01-09 06:36:00,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53030
2024-01-09 06:36:00,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,596 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,616 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38ac84ad-5585-48e0-963b-febee1e1be43
2024-01-09 06:36:00,617 - distributed.worker - INFO - Starting Worker plugin PreImport-4cbd376a-5f3b-4b6e-ae4d-77fe4af63f56
2024-01-09 06:36:00,618 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb1f0fcc-967a-4d64-8b72-22f0a8d16ee2
2024-01-09 06:36:00,638 - distributed.worker - INFO - Starting Worker plugin PreImport-da62fce1-1cc6-4ce7-8b23-ffbed7da0494
2024-01-09 06:36:00,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,657 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33273', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33273
2024-01-09 06:36:00,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53036
2024-01-09 06:36:00,657 - distributed.worker - INFO - Starting Worker plugin PreImport-7adbfbde-4f90-4700-88ba-e9a3bdaa78a7
2024-01-09 06:36:00,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,660 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa1af8b2-d8bf-487b-875b-632bc85154f5
2024-01-09 06:36:00,660 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,661 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,661 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,663 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41233', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,663 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,664 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41233
2024-01-09 06:36:00,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53044
2024-01-09 06:36:00,665 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,666 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,666 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,667 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2692b07b-ad48-4d2d-851e-9c00cd784802
2024-01-09 06:36:00,667 - distributed.worker - INFO - Starting Worker plugin PreImport-9d821945-4594-41f0-b766-f25f55a52081
2024-01-09 06:36:00,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,668 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,688 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41733', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,689 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41733
2024-01-09 06:36:00,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53052
2024-01-09 06:36:00,690 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,691 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,691 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43249', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,704 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43249
2024-01-09 06:36:00,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53078
2024-01-09 06:36:00,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36901', status: init, memory: 0, processing: 0>
2024-01-09 06:36:00,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36901
2024-01-09 06:36:00,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53062
2024-01-09 06:36:00,706 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,707 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:00,708 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:00,708 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:00,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:00,781 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,783 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:36:00,796 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,796 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,796 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,796 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,796 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,797 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,797 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,797 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:00,801 - distributed.scheduler - INFO - Remove client Client-56bdfe81-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:00,801 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37086; closing.
2024-01-09 06:36:00,801 - distributed.scheduler - INFO - Remove client Client-56bdfe81-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:00,802 - distributed.scheduler - INFO - Close client connection: Client-56bdfe81-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:00,802 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38655'. Reason: nanny-close
2024-01-09 06:36:00,803 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,803 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44895'. Reason: nanny-close
2024-01-09 06:36:00,804 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,804 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41317'. Reason: nanny-close
2024-01-09 06:36:00,804 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41733. Reason: nanny-close
2024-01-09 06:36:00,804 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,804 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38301'. Reason: nanny-close
2024-01-09 06:36:00,804 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45655. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34371'. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45713. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39345'. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33273. Reason: nanny-close
2024-01-09 06:36:00,805 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,806 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33553'. Reason: nanny-close
2024-01-09 06:36:00,806 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41117. Reason: nanny-close
2024-01-09 06:36:00,806 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53052; closing.
2024-01-09 06:36:00,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,806 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,806 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33495'. Reason: nanny-close
2024-01-09 06:36:00,806 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.806518')
2024-01-09 06:36:00,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,806 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41233. Reason: nanny-close
2024-01-09 06:36:00,806 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:00,807 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36901. Reason: nanny-close
2024-01-09 06:36:00,807 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,807 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,807 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,808 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,808 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43249. Reason: nanny-close
2024-01-09 06:36:00,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53022; closing.
2024-01-09 06:36:00,808 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,808 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,809 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,809 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.8094952')
2024-01-09 06:36:00,809 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53030; closing.
2024-01-09 06:36:00,809 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,810 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53018; closing.
2024-01-09 06:36:00,810 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,810 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53036; closing.
2024-01-09 06:36:00,810 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,810 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,810 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45713', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.810916')
2024-01-09 06:36:00,811 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.8112433')
2024-01-09 06:36:00,811 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33273', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.8115277')
2024-01-09 06:36:00,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53044; closing.
2024-01-09 06:36:00,811 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:00,812 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,812 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.812528')
2024-01-09 06:36:00,812 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53062; closing.
2024-01-09 06:36:00,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36901', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.8135781')
2024-01-09 06:36:00,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53078; closing.
2024-01-09 06:36:00,814 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:00,814 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782160.8144362')
2024-01-09 06:36:00,814 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:36:00,814 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53078>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-09 06:36:01,919 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:36:01,919 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:36:01,920 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:36:01,921 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:36:01,922 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-09 06:36:04,138 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:36:04,143 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46319 instead
  warnings.warn(
2024-01-09 06:36:04,147 - distributed.scheduler - INFO - State start
2024-01-09 06:36:04,170 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:36:04,171 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:36:04,171 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46319/status
2024-01-09 06:36:04,171 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:36:04,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40935'
2024-01-09 06:36:05,332 - distributed.scheduler - INFO - Receive client connection: Client-5b8ccc94-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:05,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53174
2024-01-09 06:36:06,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:36:06,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:36:06,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:36:06,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34037
2024-01-09 06:36:06,649 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34037
2024-01-09 06:36:06,649 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42337
2024-01-09 06:36:06,649 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:36:06,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:06,649 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:36:06,649 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:36:06,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fmo8qc14
2024-01-09 06:36:06,649 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8066d6c-00a9-4d5e-a913-77bf32e80df8
2024-01-09 06:36:08,067 - distributed.worker - INFO - Starting Worker plugin PreImport-acc8affb-2e56-4676-9232-13dc2a2b564f
2024-01-09 06:36:08,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b635e427-fefb-4745-b376-0b37449b6fe1
2024-01-09 06:36:08,068 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:08,146 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34037', status: init, memory: 0, processing: 0>
2024-01-09 06:36:08,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34037
2024-01-09 06:36:08,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53196
2024-01-09 06:36:08,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:08,150 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:08,151 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:08,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:08,234 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-09 06:36:08,240 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:08,241 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:08,244 - distributed.scheduler - INFO - Remove client Client-5b8ccc94-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:08,245 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53174; closing.
2024-01-09 06:36:08,245 - distributed.scheduler - INFO - Remove client Client-5b8ccc94-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:08,245 - distributed.scheduler - INFO - Close client connection: Client-5b8ccc94-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:08,246 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40935'. Reason: nanny-close
2024-01-09 06:36:08,247 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:08,249 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34037. Reason: nanny-close
2024-01-09 06:36:08,251 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53196; closing.
2024-01-09 06:36:08,251 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:08,252 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782168.2520857')
2024-01-09 06:36:08,252 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:36:08,253 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:09,062 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:36:09,062 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:36:09,063 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:36:09,064 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:36:09,064 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-09 06:36:11,361 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:36:11,366 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:36:11,369 - distributed.scheduler - INFO - State start
2024-01-09 06:36:11,391 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:36:11,392 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:36:11,392 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:36:11,393 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:36:11,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35367'
2024-01-09 06:36:12,567 - distributed.scheduler - INFO - Receive client connection: Client-5fcd82c0-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:12,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46318
2024-01-09 06:36:13,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:36:13,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:36:13,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:36:13,648 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45177
2024-01-09 06:36:13,648 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45177
2024-01-09 06:36:13,648 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46641
2024-01-09 06:36:13,648 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:36:13,648 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:13,648 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:36:13,648 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:36:13,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2u0po6_x
2024-01-09 06:36:13,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8843dc90-3095-41c5-97ed-58f240cb941d
2024-01-09 06:36:14,030 - distributed.worker - INFO - Starting Worker plugin PreImport-40d8190d-363c-4ca9-bbc5-90f8e82d4790
2024-01-09 06:36:14,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b5b9b9d-cea3-4c0f-a95f-db40e3f9ed56
2024-01-09 06:36:14,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:14,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45177', status: init, memory: 0, processing: 0>
2024-01-09 06:36:14,092 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45177
2024-01-09 06:36:14,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46334
2024-01-09 06:36:14,093 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:36:14,094 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:36:14,094 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:36:14,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:36:14,170 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-09 06:36:14,174 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-09 06:36:14,178 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:14,180 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:36:14,182 - distributed.scheduler - INFO - Remove client Client-5fcd82c0-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:14,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46318; closing.
2024-01-09 06:36:14,182 - distributed.scheduler - INFO - Remove client Client-5fcd82c0-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:14,183 - distributed.scheduler - INFO - Close client connection: Client-5fcd82c0-aeb9-11ee-b9fd-d8c49764f6bb
2024-01-09 06:36:14,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35367'. Reason: nanny-close
2024-01-09 06:36:14,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:36:14,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45177. Reason: nanny-close
2024-01-09 06:36:14,186 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:36:14,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46334; closing.
2024-01-09 06:36:14,187 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45177', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704782174.1870565')
2024-01-09 06:36:14,187 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:36:14,188 - distributed.nanny - INFO - Worker closed
2024-01-09 06:36:14,949 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:36:14,950 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:36:14,951 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:36:14,952 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:36:14,953 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40651 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40429 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33749 instead
  warnings.warn(
2024-01-09 06:37:24,639 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:47965 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f0d3400c2c0 error: Connection reset by remote peer')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f0d3400c2c0 error: Connection reset by remote peer')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39721 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40655 instead
  warnings.warn(
2024-01-09 06:38:17,777 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-09 06:38:17,780 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-09 06:38:17,780 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-09 06:38:17,781 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45999 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34291 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35485 instead
  warnings.warn(
[1704782329.418523] [dgx13:70118:0]            sock.c:481  UCX  ERROR bind(fd=179 addr=0.0.0.0:37398) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33725 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33197 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39615 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44083 instead
  warnings.warn(
2024-01-09 06:39:41,054 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 406, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-09 06:39:41,055 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39463 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40315 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36841 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40161 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-5732' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35935 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43997 instead
  warnings.warn(
[1704782586.704591] [dgx13:74770:0]            sock.c:481  UCX  ERROR bind(fd=159 addr=0.0.0.0:35314) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45081 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35025 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45133 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39453 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44633 instead
  warnings.warn(
[1704782723.401768] [dgx13:76691:0]            sock.c:481  UCX  ERROR bind(fd=150 addr=0.0.0.0:57414) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38307 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42653 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33339 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42183 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44501 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37551 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46223 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39997 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44009 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46759 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39477 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46055 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] [1704783157.930190] [dgx13:83467:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:48320) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36839 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38353 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39453 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36089 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35853 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39871 instead
  warnings.warn(
[1704783391.807801] [dgx13:86436:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:33483) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40579 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36153 instead
  warnings.warn(
[1704783452.310186] [dgx13:86958:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:42144) failed: Address already in use
2024-01-09 06:57:36,202 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1704783456.922083] [dgx13:86958] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
[1704783456.922230] [dgx13:86958] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39171 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42439 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34955 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41233 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44923 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34007 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers 2024-01-09 07:00:01,820 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:49836 remote=tcp://127.0.0.1:35601>: Stream is closed
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44665 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42383 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34433 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46755 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35221 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46013 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704783699.958597] [dgx13:89860:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:60178) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704783701.833773] [dgx13:63997:0]            sock.c:481  UCX  ERROR bind(fd=243 addr=0.0.0.0:51298) failed: Address already in use
2024-01-09 07:01:47,843 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-09 07:01:47,845 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 439, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 445, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1704783709.268390] [dgx13:63997:0]            sock.c:481  UCX  ERROR bind(fd=245 addr=0.0.0.0:37472) failed: Address already in use
[1704783709.294176] [dgx13:63997:0]            sock.c:481  UCX  ERROR bind(fd=252 addr=0.0.0.0:45337) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-09 07:02:14,826 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:14,826 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:14,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:14,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:14,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:14,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:14,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:14,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:15,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:15,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:15,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:15,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:15,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:15,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:15,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:15,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:15,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,511 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35605
2024-01-09 07:02:15,511 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35605
2024-01-09 07:02:15,511 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42307
2024-01-09 07:02:15,511 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,511 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,511 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sxl1ka4z
2024-01-09 07:02:15,511 - distributed.worker - INFO - Starting Worker plugin PreImport-e0cfd91f-1970-4b53-b48a-a1c3bbd1069b
2024-01-09 07:02:15,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e99006db-afdc-4c89-aa8f-18232fc1bc03
2024-01-09 07:02:15,512 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e6b8dc7-bceb-4703-aa02-b36fed999dea
2024-01-09 07:02:15,512 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34411
2024-01-09 07:02:15,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34411
2024-01-09 07:02:15,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43049
2024-01-09 07:02:15,552 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,552 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_bxlf1zs
2024-01-09 07:02:15,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b705202-5bf1-4f2e-8791-d81c8a29d82d
2024-01-09 07:02:15,553 - distributed.worker - INFO - Starting Worker plugin PreImport-64baa011-9abd-4c50-ab9f-48fb35018480
2024-01-09 07:02:15,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d714d62-5878-4810-81aa-aa7d40ce079f
2024-01-09 07:02:15,553 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,582 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,582 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,587 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43855
2024-01-09 07:02:15,587 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43855
2024-01-09 07:02:15,587 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41045
2024-01-09 07:02:15,587 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,587 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,587 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-brm_faox
2024-01-09 07:02:15,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed7584a1-b4f9-4be9-8798-fc7a84810494
2024-01-09 07:02:15,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-236e3f44-7a51-4549-9179-4c0085fa6de8
2024-01-09 07:02:15,588 - distributed.worker - INFO - Starting Worker plugin PreImport-ecd25afa-c2d3-4c90-8de2-2882094bd723
2024-01-09 07:02:15,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,611 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,612 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40177
2024-01-09 07:02:15,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40177
2024-01-09 07:02:15,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37507
2024-01-09 07:02:15,612 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,612 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,612 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ywckxv0
2024-01-09 07:02:15,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30acbcff-91d8-4360-b25e-5c4258038191
2024-01-09 07:02:15,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ad91cf9-d82b-411c-9fb8-68d6f216230b
2024-01-09 07:02:15,613 - distributed.worker - INFO - Starting Worker plugin PreImport-17175bee-0e03-488a-a7ab-9809c125a481
2024-01-09 07:02:15,613 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,617 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,617 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43125
2024-01-09 07:02:15,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43125
2024-01-09 07:02:15,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41979
2024-01-09 07:02:15,634 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,634 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,634 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u1ozjjnq
2024-01-09 07:02:15,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46b50055-6a0a-49fd-8bd2-525667a34270
2024-01-09 07:02:15,634 - distributed.worker - INFO - Starting Worker plugin PreImport-d0f9bb26-1fdd-4b84-9aa8-e833214a64de
2024-01-09 07:02:15,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b097e09-c957-4823-b68e-061ab3d14632
2024-01-09 07:02:15,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,658 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,704 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,704 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,717 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,718 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,718 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,718 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,719 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44103
2024-01-09 07:02:15,719 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44103
2024-01-09 07:02:15,719 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44905
2024-01-09 07:02:15,720 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,720 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,720 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,720 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z3pcsxne
2024-01-09 07:02:15,720 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ddf207b-1d02-4bd3-b103-627d07f2f0af
2024-01-09 07:02:15,720 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33392fd1-34dd-41cc-8744-a67eefeb82db
2024-01-09 07:02:15,720 - distributed.worker - INFO - Starting Worker plugin PreImport-474d29dc-79ef-4d28-8078-0879013787ac
2024-01-09 07:02:15,721 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:15,739 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34343
2024-01-09 07:02:15,739 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34343
2024-01-09 07:02:15,739 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35653
2024-01-09 07:02:15,739 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,739 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,739 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,739 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c4ri5j_5
2024-01-09 07:02:15,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43009
2024-01-09 07:02:15,740 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43009
2024-01-09 07:02:15,740 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37823
2024-01-09 07:02:15,740 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,740 - distributed.worker - INFO - Starting Worker plugin PreImport-35cd34f4-7a7b-40a0-8bea-99a27476b91d
2024-01-09 07:02:15,740 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,740 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:15,740 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7965f93-f98e-45d3-90ea-3c438628ffcc
2024-01-09 07:02:15,740 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ukuekjo
2024-01-09 07:02:15,740 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21259181-7082-48db-8c5b-16520e7ba743
2024-01-09 07:02:15,740 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,740 - distributed.worker - INFO - Starting Worker plugin PreImport-963be070-3ae6-4118-99e4-31cc93ca1fd7
2024-01-09 07:02:15,741 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8dde8080-e179-44d5-a77b-9a8909075b6b
2024-01-09 07:02:15,741 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ed8f9a2-3cb3-486c-afe9-0e0cb33863a5
2024-01-09 07:02:15,741 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,807 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,807 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,859 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,859 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 07:02:15,862 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42975
2024-01-09 07:02:15,862 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:15,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42975
2024-01-09 07:02:15,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 07:02:15,922 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34411. Reason: nanny-close
2024-01-09 07:02:15,922 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35605. Reason: nanny-close
2024-01-09 07:02:15,923 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40177. Reason: nanny-close
2024-01-09 07:02:15,924 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43855. Reason: nanny-close
2024-01-09 07:02:15,924 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43125. Reason: nanny-close
2024-01-09 07:02:15,925 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34343. Reason: nanny-close
2024-01-09 07:02:15,925 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,925 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43009. Reason: nanny-close
2024-01-09 07:02:15,926 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44103. Reason: nanny-close
2024-01-09 07:02:15,926 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,927 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,927 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,927 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,928 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:42975 has been closed.
2024-01-09 07:02:15,929 - distributed.nanny - INFO - Worker closed
2024-01-09 07:02:15,930 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-09 07:02:50,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 07:02:50,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 07:02:50,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 07:02:50,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43319
2024-01-09 07:02:50,618 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43319
2024-01-09 07:02:50,618 - distributed.worker - INFO -           Worker name:                          0
2024-01-09 07:02:50,618 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40105
2024-01-09 07:02:50,618 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44329
2024-01-09 07:02:50,618 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 07:02:50,618 - distributed.worker - INFO -               Threads:                          1
2024-01-09 07:02:50,618 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 07:02:50,619 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n621zg1w
2024-01-09 07:02:50,619 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60c064c2-124d-4716-a824-7e98ee14362a
2024-01-09 07:02:50,619 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79aceaef-b644-45dc-997e-ef6a4fa749db
2024-01-09 07:02:50,619 - distributed.worker - INFO - Starting Worker plugin PreImport-e43715ac-87fd-438a-b7fb-6d31f3de9c58
2024-01-09 07:02:50,623 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-09 07:02:50,624 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43319. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-09 07:02:50,624 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-09 07:02:50,625 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 9 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
