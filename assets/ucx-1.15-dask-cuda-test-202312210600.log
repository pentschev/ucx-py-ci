============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-21 06:39:42,884 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:39:42,889 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44237 instead
  warnings.warn(
2023-12-21 06:39:42,893 - distributed.scheduler - INFO - State start
2023-12-21 06:39:42,919 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:39:42,921 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-21 06:39:42,921 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44237/status
2023-12-21 06:39:42,921 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:39:43,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38399'
2023-12-21 06:39:43,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43667'
2023-12-21 06:39:43,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41623'
2023-12-21 06:39:43,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34021'
2023-12-21 06:39:43,489 - distributed.scheduler - INFO - Receive client connection: Client-b81a697e-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:43,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46122
2023-12-21 06:39:44,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:44,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:44,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:44,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:44,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:44,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:44,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:44,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:44,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:44,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:44,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:45,002 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-21 06:39:45,031 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35257
2023-12-21 06:39:45,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35257
2023-12-21 06:39:45,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42993
2023-12-21 06:39:45,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-21 06:39:45,031 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:45,031 - distributed.worker - INFO -               Threads:                          4
2023-12-21 06:39:45,031 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-21 06:39:45,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mopjgfce
2023-12-21 06:39:45,031 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61143bc0-4b2e-49d1-a1ce-e1d1cb907715
2023-12-21 06:39:45,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f74e3bc-41df-4ae1-b425-d0e92838fe5b
2023-12-21 06:39:45,032 - distributed.worker - INFO - Starting Worker plugin PreImport-8ff64c39-6f9f-4fe3-afab-6763fa726218
2023-12-21 06:39:45,032 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:45,898 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35257', status: init, memory: 0, processing: 0>
2023-12-21 06:39:45,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35257
2023-12-21 06:39:45,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46138
2023-12-21 06:39:45,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:45,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-21 06:39:45,901 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:45,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-21 06:39:46,425 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37341
2023-12-21 06:39:46,425 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37341
2023-12-21 06:39:46,425 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34911
2023-12-21 06:39:46,425 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-21 06:39:46,425 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,425 - distributed.worker - INFO -               Threads:                          4
2023-12-21 06:39:46,426 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-21 06:39:46,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-08xnz3a1
2023-12-21 06:39:46,426 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66123742-164d-4204-86bf-12f464539fff
2023-12-21 06:39:46,426 - distributed.worker - INFO - Starting Worker plugin PreImport-f6e44478-8a0c-4117-bf59-60b0a03884f4
2023-12-21 06:39:46,426 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b2d0b2e-4bca-4923-98e5-79729e582c76
2023-12-21 06:39:46,426 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,437 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38909
2023-12-21 06:39:46,437 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38909
2023-12-21 06:39:46,437 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44279
2023-12-21 06:39:46,437 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-21 06:39:46,438 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,438 - distributed.worker - INFO -               Threads:                          4
2023-12-21 06:39:46,438 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-21 06:39:46,438 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-sv1brmt_
2023-12-21 06:39:46,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-927e9aab-6fb7-4c6a-902c-47566d2710f3
2023-12-21 06:39:46,439 - distributed.worker - INFO - Starting Worker plugin PreImport-24b3f30c-b490-4af7-a8a4-5f1b5d43438c
2023-12-21 06:39:46,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7432b9d-5f5e-4d67-a1f3-108f47ad3ad1
2023-12-21 06:39:46,439 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41757
2023-12-21 06:39:46,440 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41757
2023-12-21 06:39:46,440 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37357
2023-12-21 06:39:46,440 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-21 06:39:46,440 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,440 - distributed.worker - INFO -               Threads:                          4
2023-12-21 06:39:46,441 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-21 06:39:46,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-blkwt5qt
2023-12-21 06:39:46,441 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1bdb54a4-1a20-4347-a7d6-3e5b5f302cff
2023-12-21 06:39:46,441 - distributed.worker - INFO - Starting Worker plugin PreImport-e4acb575-34e6-4f9f-94b7-8883fbc5ecb3
2023-12-21 06:39:46,442 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2011910-8e75-4492-84fe-b90459690dc8
2023-12-21 06:39:46,442 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,444 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,453 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37341', status: init, memory: 0, processing: 0>
2023-12-21 06:39:46,453 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37341
2023-12-21 06:39:46,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46154
2023-12-21 06:39:46,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:46,455 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-21 06:39:46,455 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-21 06:39:46,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38909', status: init, memory: 0, processing: 0>
2023-12-21 06:39:46,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38909
2023-12-21 06:39:46,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46164
2023-12-21 06:39:46,480 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41757', status: init, memory: 0, processing: 0>
2023-12-21 06:39:46,480 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41757
2023-12-21 06:39:46,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46156
2023-12-21 06:39:46,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:46,481 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-21 06:39:46,482 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:46,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-21 06:39:46,484 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:46,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-21 06:39:46,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-21 06:39:46,556 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-21 06:39:46,556 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-21 06:39:46,557 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-21 06:39:46,557 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-21 06:39:46,562 - distributed.scheduler - INFO - Remove client Client-b81a697e-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:46,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46122; closing.
2023-12-21 06:39:46,562 - distributed.scheduler - INFO - Remove client Client-b81a697e-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:46,563 - distributed.scheduler - INFO - Close client connection: Client-b81a697e-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:46,563 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38399'. Reason: nanny-close
2023-12-21 06:39:46,564 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:46,565 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43667'. Reason: nanny-close
2023-12-21 06:39:46,565 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:46,565 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41757. Reason: nanny-close
2023-12-21 06:39:46,565 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37341. Reason: nanny-close
2023-12-21 06:39:46,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46156; closing.
2023-12-21 06:39:46,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-21 06:39:46,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-21 06:39:46,567 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140786.56792')
2023-12-21 06:39:46,568 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46154; closing.
2023-12-21 06:39:46,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37341', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140786.5688365')
2023-12-21 06:39:46,569 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:46,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41623'. Reason: nanny-close
2023-12-21 06:39:46,569 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:46,569 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:46,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34021'. Reason: nanny-close
2023-12-21 06:39:46,570 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:46,569 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:46154>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-21 06:39:46,570 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38909. Reason: nanny-close
2023-12-21 06:39:46,571 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35257. Reason: nanny-close
2023-12-21 06:39:46,572 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46164; closing.
2023-12-21 06:39:46,572 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-21 06:39:46,572 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-21 06:39:46,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38909', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140786.5729797')
2023-12-21 06:39:46,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46138; closing.
2023-12-21 06:39:46,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35257', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140786.5737062')
2023-12-21 06:39:46,573 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:39:46,574 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:46,574 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:47,781 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:39:47,781 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:39:47,781 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:39:47,782 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-21 06:39:47,783 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-21 06:39:50,031 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:39:50,036 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33153 instead
  warnings.warn(
2023-12-21 06:39:50,040 - distributed.scheduler - INFO - State start
2023-12-21 06:39:50,063 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:39:50,064 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:39:50,065 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33153/status
2023-12-21 06:39:50,065 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:39:50,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34813'
2023-12-21 06:39:50,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43887'
2023-12-21 06:39:50,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40197'
2023-12-21 06:39:50,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42051'
2023-12-21 06:39:50,161 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40005'
2023-12-21 06:39:50,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40619'
2023-12-21 06:39:50,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44245'
2023-12-21 06:39:50,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38925'
2023-12-21 06:39:50,589 - distributed.scheduler - INFO - Receive client connection: Client-bc50dfc4-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:50,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37220
2023-12-21 06:39:52,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,042 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,042 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:52,153 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:39:52,153 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:39:52,157 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:39:56,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40533
2023-12-21 06:39:56,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40533
2023-12-21 06:39:56,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41781
2023-12-21 06:39:56,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,419 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,419 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hq0p0iv6
2023-12-21 06:39:56,419 - distributed.worker - INFO - Starting Worker plugin PreImport-e14fe185-4f06-48b0-8ae9-e846d8150580
2023-12-21 06:39:56,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef55c638-5b9d-4741-a43d-c0b880178c91
2023-12-21 06:39:56,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45313
2023-12-21 06:39:56,434 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45313
2023-12-21 06:39:56,434 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43255
2023-12-21 06:39:56,434 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,435 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,435 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,435 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,435 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nlhofen9
2023-12-21 06:39:56,435 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b489c20-6698-45ed-88ad-aa832d564ba6
2023-12-21 06:39:56,437 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34967
2023-12-21 06:39:56,438 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34967
2023-12-21 06:39:56,438 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44699
2023-12-21 06:39:56,438 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,438 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,438 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,438 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,438 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-os172nv_
2023-12-21 06:39:56,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9997a045-ea86-4bce-aa9e-0917330deb82
2023-12-21 06:39:56,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3875246e-234d-42d3-9ad6-b96526ad3f99
2023-12-21 06:39:56,452 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37813
2023-12-21 06:39:56,452 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37813
2023-12-21 06:39:56,452 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34819
2023-12-21 06:39:56,453 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,453 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,453 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,453 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ixwl_smg
2023-12-21 06:39:56,453 - distributed.worker - INFO - Starting Worker plugin RMMSetup-983086fa-a095-47e4-a69a-78dc92002c87
2023-12-21 06:39:56,503 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35327
2023-12-21 06:39:56,504 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35327
2023-12-21 06:39:56,504 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46191
2023-12-21 06:39:56,503 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41765
2023-12-21 06:39:56,504 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,504 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41765
2023-12-21 06:39:56,504 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,504 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38123
2023-12-21 06:39:56,504 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,504 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,504 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,504 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,504 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-orz_pgpg
2023-12-21 06:39:56,504 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,504 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,504 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2skrvtgd
2023-12-21 06:39:56,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfad0c39-a82d-4593-b6a7-3b513fa8c79e
2023-12-21 06:39:56,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abf6f899-69ac-401d-a45a-5461003462c6
2023-12-21 06:39:56,505 - distributed.worker - INFO - Starting Worker plugin PreImport-0ccaed5a-10ce-426b-8fcc-71f903f111f8
2023-12-21 06:39:56,505 - distributed.worker - INFO - Starting Worker plugin PreImport-50ee2595-9e73-4b72-9f51-3b4949c55d3e
2023-12-21 06:39:56,505 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa2fb0ac-f524-481e-b640-ce421f05c319
2023-12-21 06:39:56,505 - distributed.worker - INFO - Starting Worker plugin RMMSetup-109947f0-132a-41fd-80bc-088effe82538
2023-12-21 06:39:56,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41431
2023-12-21 06:39:56,514 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40635
2023-12-21 06:39:56,515 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41431
2023-12-21 06:39:56,515 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40635
2023-12-21 06:39:56,515 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37493
2023-12-21 06:39:56,515 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41631
2023-12-21 06:39:56,515 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,515 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,515 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,515 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,515 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,516 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,515 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:39:56,516 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-88j6dmov
2023-12-21 06:39:56,516 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:39:56,516 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v4e2vs_0
2023-12-21 06:39:56,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d76dc79e-7a81-4f99-95ef-1f4380ac89ac
2023-12-21 06:39:56,517 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ee30f3f-b824-4c12-b539-5d791c71d05f
2023-12-21 06:39:56,517 - distributed.worker - INFO - Starting Worker plugin PreImport-ad7483f8-ae03-4b2b-a048-5072e621cffc
2023-12-21 06:39:56,517 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b34d3c5f-db98-411f-a26b-d9fd9aa11208
2023-12-21 06:39:56,518 - distributed.worker - INFO - Starting Worker plugin PreImport-87ebe882-426f-4be2-91f0-8b049af176b2
2023-12-21 06:39:56,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d2b4413-fa91-4840-826c-51583a153262
2023-12-21 06:39:56,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0bd6a5ba-92fb-42f6-b14b-c165ab1c70ba
2023-12-21 06:39:56,579 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,582 - distributed.worker - INFO - Starting Worker plugin PreImport-d6e568b0-bbd6-45ed-b376-fb0f20dcb0e3
2023-12-21 06:39:56,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9cd41a44-b839-4db0-9cf6-e818e8d2c4ee
2023-12-21 06:39:56,582 - distributed.worker - INFO - Starting Worker plugin PreImport-087e480f-b4d2-4cf4-855f-6370567727d6
2023-12-21 06:39:56,583 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,583 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,586 - distributed.worker - INFO - Starting Worker plugin PreImport-0e22cf71-951e-45e5-8914-e04ac75aa08a
2023-12-21 06:39:56,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b07b287-31da-4ca3-8e65-ba884acc2228
2023-12-21 06:39:56,588 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,600 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,600 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40533', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40533
2023-12-21 06:39:56,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37244
2023-12-21 06:39:56,609 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,609 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,610 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34967', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,610 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,610 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,610 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34967
2023-12-21 06:39:56,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37254
2023-12-21 06:39:56,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,612 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,612 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45313', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,614 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45313
2023-12-21 06:39:56,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37266
2023-12-21 06:39:56,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,617 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,617 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,617 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37813', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,625 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37813
2023-12-21 06:39:56,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37280
2023-12-21 06:39:56,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41765', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41765
2023-12-21 06:39:56,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37284
2023-12-21 06:39:56,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,628 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35327', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,628 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35327
2023-12-21 06:39:56,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37298
2023-12-21 06:39:56,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,628 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,629 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,629 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,630 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,630 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,640 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41431', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,640 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41431
2023-12-21 06:39:56,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37302
2023-12-21 06:39:56,641 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40635', status: init, memory: 0, processing: 0>
2023-12-21 06:39:56,642 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40635
2023-12-21 06:39:56,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37318
2023-12-21 06:39:56,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,643 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,643 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,643 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:39:56,644 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:39:56,644 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:39:56,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:39:56,667 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,667 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,668 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,668 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,668 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,668 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,668 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,668 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:39:56,675 - distributed.scheduler - INFO - Remove client Client-bc50dfc4-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:56,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37220; closing.
2023-12-21 06:39:56,675 - distributed.scheduler - INFO - Remove client Client-bc50dfc4-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:56,675 - distributed.scheduler - INFO - Close client connection: Client-bc50dfc4-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:39:56,676 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34813'. Reason: nanny-close
2023-12-21 06:39:56,677 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43887'. Reason: nanny-close
2023-12-21 06:39:56,678 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40533. Reason: nanny-close
2023-12-21 06:39:56,678 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,679 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40197'. Reason: nanny-close
2023-12-21 06:39:56,679 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,679 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35327. Reason: nanny-close
2023-12-21 06:39:56,679 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42051'. Reason: nanny-close
2023-12-21 06:39:56,680 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,680 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45313. Reason: nanny-close
2023-12-21 06:39:56,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40005'. Reason: nanny-close
2023-12-21 06:39:56,680 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,680 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,681 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37813. Reason: nanny-close
2023-12-21 06:39:56,681 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37244; closing.
2023-12-21 06:39:56,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40619'. Reason: nanny-close
2023-12-21 06:39:56,681 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41765. Reason: nanny-close
2023-12-21 06:39:56,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,681 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40533', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.6813416')
2023-12-21 06:39:56,681 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44245'. Reason: nanny-close
2023-12-21 06:39:56,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,682 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34967. Reason: nanny-close
2023-12-21 06:39:56,682 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38925'. Reason: nanny-close
2023-12-21 06:39:56,682 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:39:56,682 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41431. Reason: nanny-close
2023-12-21 06:39:56,682 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,683 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,683 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,683 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37298; closing.
2023-12-21 06:39:56,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40635. Reason: nanny-close
2023-12-21 06:39:56,683 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,684 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.684025')
2023-12-21 06:39:56,684 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,684 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37266; closing.
2023-12-21 06:39:56,684 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,684 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,685 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.684998')
2023-12-21 06:39:56,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37284; closing.
2023-12-21 06:39:56,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37280; closing.
2023-12-21 06:39:56,685 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,685 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,686 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:39:56,686 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.6862407')
2023-12-21 06:39:56,686 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37813', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.6865249')
2023-12-21 06:39:56,686 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37254; closing.
2023-12-21 06:39:56,687 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,687 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.68755')
2023-12-21 06:39:56,687 - distributed.nanny - INFO - Worker closed
2023-12-21 06:39:56,687 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37302; closing.
2023-12-21 06:39:56,688 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37318; closing.
2023-12-21 06:39:56,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41431', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.688549')
2023-12-21 06:39:56,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140796.688927')
2023-12-21 06:39:56,689 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:39:58,395 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:39:58,396 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:39:58,397 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:39:58,398 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:39:58,398 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-21 06:40:00,891 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:00,897 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43271 instead
  warnings.warn(
2023-12-21 06:40:00,901 - distributed.scheduler - INFO - State start
2023-12-21 06:40:00,927 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:00,928 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:40:00,929 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43271/status
2023-12-21 06:40:00,930 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:40:00,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41371'
2023-12-21 06:40:00,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42793'
2023-12-21 06:40:00,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33507'
2023-12-21 06:40:01,001 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35273'
2023-12-21 06:40:01,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44323'
2023-12-21 06:40:01,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39241'
2023-12-21 06:40:01,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39625'
2023-12-21 06:40:01,041 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44283'
2023-12-21 06:40:02,608 - distributed.scheduler - INFO - Receive client connection: Client-c2c008dc-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:02,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43012
2023-12-21 06:40:02,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,886 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,926 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:02,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:02,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:02,991 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:05,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45785
2023-12-21 06:40:05,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45785
2023-12-21 06:40:05,619 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36173
2023-12-21 06:40:05,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,620 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,620 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzo8f8aw
2023-12-21 06:40:05,620 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b801e28c-5e4c-4d7a-bb02-bbbf30442a57
2023-12-21 06:40:05,621 - distributed.worker - INFO - Starting Worker plugin PreImport-71c971d1-6745-4f01-803d-e0dab830dce4
2023-12-21 06:40:05,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0f9e1f5-a1e6-4686-bcba-0db04178b99a
2023-12-21 06:40:05,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33709
2023-12-21 06:40:05,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33709
2023-12-21 06:40:05,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37639
2023-12-21 06:40:05,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,650 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,650 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-artidiz_
2023-12-21 06:40:05,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb972cf8-9406-449d-b739-00fe82e8a015
2023-12-21 06:40:05,651 - distributed.worker - INFO - Starting Worker plugin PreImport-961ece4d-87f4-4485-8944-4e2c8d1949d9
2023-12-21 06:40:05,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f5afe344-1af8-4073-b3bc-a3a85efeff86
2023-12-21 06:40:05,655 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44475
2023-12-21 06:40:05,656 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44475
2023-12-21 06:40:05,656 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38345
2023-12-21 06:40:05,656 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,656 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,656 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,656 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,656 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u9emi4jd
2023-12-21 06:40:05,657 - distributed.worker - INFO - Starting Worker plugin PreImport-3b34f0fd-06d1-49b3-b401-aaef1558fc33
2023-12-21 06:40:05,657 - distributed.worker - INFO - Starting Worker plugin RMMSetup-95de4f18-c23d-4369-b1e3-77c6de41cfde
2023-12-21 06:40:05,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44321
2023-12-21 06:40:05,659 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44321
2023-12-21 06:40:05,659 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36903
2023-12-21 06:40:05,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44319
2023-12-21 06:40:05,660 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44319
2023-12-21 06:40:05,660 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,660 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,660 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41539
2023-12-21 06:40:05,660 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,660 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,660 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,660 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,660 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nczrqfdq
2023-12-21 06:40:05,660 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ralhoj0
2023-12-21 06:40:05,660 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9166b264-1fb9-4885-a6e0-371fc4fbe7ee
2023-12-21 06:40:05,661 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc274d23-cbe4-4e8e-ab0f-bc84e5b8c7a7
2023-12-21 06:40:05,661 - distributed.worker - INFO - Starting Worker plugin PreImport-4fcf1c0a-9887-4313-a98a-03ae00506a4f
2023-12-21 06:40:05,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-549c6855-c03a-45bd-9235-617e7e6f069e
2023-12-21 06:40:05,661 - distributed.worker - INFO - Starting Worker plugin PreImport-6e56e542-d62f-4a11-9293-a771575b7294
2023-12-21 06:40:05,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed509a7c-7c85-4096-87e7-7e1415b9f6db
2023-12-21 06:40:05,679 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45555
2023-12-21 06:40:05,680 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45555
2023-12-21 06:40:05,680 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43167
2023-12-21 06:40:05,680 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,680 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,681 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,681 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yxfnp3b9
2023-12-21 06:40:05,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bf066fa-54c7-479c-8b33-39f5fcf17f23
2023-12-21 06:40:05,683 - distributed.worker - INFO - Starting Worker plugin PreImport-e5abdefc-9d27-4f8c-ac09-549bda8d1bd0
2023-12-21 06:40:05,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e909382c-ec12-46f4-a509-8ce75f97b244
2023-12-21 06:40:05,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45457
2023-12-21 06:40:05,685 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45457
2023-12-21 06:40:05,685 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43473
2023-12-21 06:40:05,685 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,685 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,685 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v78abnht
2023-12-21 06:40:05,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85b447a4-6a9b-4e33-846a-0233fd4f0823
2023-12-21 06:40:05,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-201b6e9d-38c3-4699-8f3d-034ac90fa7ef
2023-12-21 06:40:05,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33943
2023-12-21 06:40:05,689 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33943
2023-12-21 06:40:05,689 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42159
2023-12-21 06:40:05,689 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,689 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,689 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:05,690 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:05,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_gbjm_yz
2023-12-21 06:40:05,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86202d24-059c-4c21-83f4-29b70f17aa0c
2023-12-21 06:40:05,701 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,703 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88095338-a35c-4a8c-84b7-2c0efc4a4b55
2023-12-21 06:40:05,705 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,705 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,705 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,711 - distributed.worker - INFO - Starting Worker plugin PreImport-18697ecf-0f51-4f3a-8df7-d40648b767e8
2023-12-21 06:40:05,711 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,711 - distributed.worker - INFO - Starting Worker plugin PreImport-2ca85e21-e547-45aa-a36f-c488ed0f9960
2023-12-21 06:40:05,712 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08bb5a1b-30ef-46de-8ac3-e9969a6ebbc1
2023-12-21 06:40:05,712 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,713 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,729 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33709', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,730 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33709
2023-12-21 06:40:05,730 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43030
2023-12-21 06:40:05,731 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,732 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,732 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,733 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44319', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,733 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44319
2023-12-21 06:40:05,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43044
2023-12-21 06:40:05,734 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44321', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,735 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44321
2023-12-21 06:40:05,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43040
2023-12-21 06:40:05,735 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,735 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,736 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,737 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,738 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33943', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,739 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33943
2023-12-21 06:40:05,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43064
2023-12-21 06:40:05,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,739 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45785', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,740 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45785
2023-12-21 06:40:05,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43026
2023-12-21 06:40:05,740 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,740 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44475', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,741 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44475
2023-12-21 06:40:05,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43050
2023-12-21 06:40:05,743 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,743 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,745 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,745 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,747 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45555', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,748 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45555
2023-12-21 06:40:05,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43090
2023-12-21 06:40:05,748 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45457', status: init, memory: 0, processing: 0>
2023-12-21 06:40:05,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45457
2023-12-21 06:40:05,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43074
2023-12-21 06:40:05,749 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,750 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,750 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:05,752 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:05,752 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:05,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:05,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,791 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,792 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,792 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,792 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,793 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:05,797 - distributed.scheduler - INFO - Remove client Client-c2c008dc-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:05,797 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43012; closing.
2023-12-21 06:40:05,798 - distributed.scheduler - INFO - Remove client Client-c2c008dc-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:05,798 - distributed.scheduler - INFO - Close client connection: Client-c2c008dc-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:05,799 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41371'. Reason: nanny-close
2023-12-21 06:40:05,799 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,800 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42793'. Reason: nanny-close
2023-12-21 06:40:05,800 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,800 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44475. Reason: nanny-close
2023-12-21 06:40:05,800 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33507'. Reason: nanny-close
2023-12-21 06:40:05,800 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,801 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45457. Reason: nanny-close
2023-12-21 06:40:05,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35273'. Reason: nanny-close
2023-12-21 06:40:05,801 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,801 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33709. Reason: nanny-close
2023-12-21 06:40:05,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44323'. Reason: nanny-close
2023-12-21 06:40:05,802 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,802 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44319. Reason: nanny-close
2023-12-21 06:40:05,802 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39241'. Reason: nanny-close
2023-12-21 06:40:05,802 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,802 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43050; closing.
2023-12-21 06:40:05,802 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,802 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45785. Reason: nanny-close
2023-12-21 06:40:05,803 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44475', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.8031304')
2023-12-21 06:40:05,803 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39625'. Reason: nanny-close
2023-12-21 06:40:05,803 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,803 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,803 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,803 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45555. Reason: nanny-close
2023-12-21 06:40:05,803 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44283'. Reason: nanny-close
2023-12-21 06:40:05,803 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:05,804 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44321. Reason: nanny-close
2023-12-21 06:40:05,804 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,804 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43030; closing.
2023-12-21 06:40:05,804 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,804 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,804 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43074; closing.
2023-12-21 06:40:05,805 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33943. Reason: nanny-close
2023-12-21 06:40:05,805 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,805 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33709', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.805695')
2023-12-21 06:40:05,805 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,805 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,806 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45457', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.8060997')
2023-12-21 06:40:05,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,806 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43044; closing.
2023-12-21 06:40:05,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:05,807 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44319', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.8070126')
2023-12-21 06:40:05,807 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43026; closing.
2023-12-21 06:40:05,807 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,807 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,807 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,808 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.8080177')
2023-12-21 06:40:05,808 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:05,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43090; closing.
2023-12-21 06:40:05,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43040; closing.
2023-12-21 06:40:05,809 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45555', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.809029')
2023-12-21 06:40:05,809 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.809432')
2023-12-21 06:40:05,809 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43064; closing.
2023-12-21 06:40:05,810 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140805.8101857')
2023-12-21 06:40:05,810 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:40:07,417 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:40:07,417 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:40:07,418 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:40:07,419 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:40:07,420 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-21 06:40:09,594 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:09,598 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-21 06:40:09,601 - distributed.scheduler - INFO - State start
2023-12-21 06:40:09,622 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:09,623 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:40:09,623 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-21 06:40:09,624 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:40:10,083 - distributed.scheduler - INFO - Receive client connection: Client-c80f5996-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:10,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59928
2023-12-21 06:40:10,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33463'
2023-12-21 06:40:10,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45297'
2023-12-21 06:40:10,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42217'
2023-12-21 06:40:10,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45139'
2023-12-21 06:40:10,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38089'
2023-12-21 06:40:10,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33697'
2023-12-21 06:40:10,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46345'
2023-12-21 06:40:10,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40785'
2023-12-21 06:40:11,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:11,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:11,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:11,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:11,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:11,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:11,995 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:11,995 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:11,995 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:12,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:12,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:12,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:12,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:12,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:12,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:12,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:12,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:12,069 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:12,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:12,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:12,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:12,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:12,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:12,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:14,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45119
2023-12-21 06:40:14,514 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45119
2023-12-21 06:40:14,514 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34549
2023-12-21 06:40:14,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,514 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,514 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,514 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,514 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rn4cyacn
2023-12-21 06:40:14,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e90d1bf6-40a0-4b36-8cea-bab55c94abc9
2023-12-21 06:40:14,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44633
2023-12-21 06:40:14,531 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44633
2023-12-21 06:40:14,531 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44509
2023-12-21 06:40:14,531 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,531 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,531 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,531 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qljj20ic
2023-12-21 06:40:14,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3733ec9d-57c8-4a42-87fc-897bb5528060
2023-12-21 06:40:14,532 - distributed.worker - INFO - Starting Worker plugin PreImport-31845cd9-f2f7-4847-841b-e2e7cc91362c
2023-12-21 06:40:14,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07562bca-ed16-417a-803e-e5c1a335e34b
2023-12-21 06:40:14,569 - distributed.worker - INFO - Starting Worker plugin PreImport-d8774b9a-e640-44a6-bf4b-e3e2f850857e
2023-12-21 06:40:14,569 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-830fe3aa-c318-4d00-b2bf-9e32a12bd0fa
2023-12-21 06:40:14,569 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,572 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45119', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45119
2023-12-21 06:40:14,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60020
2023-12-21 06:40:14,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44633', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44633
2023-12-21 06:40:14,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60024
2023-12-21 06:40:14,599 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,599 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,600 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,601 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,624 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34237
2023-12-21 06:40:14,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34237
2023-12-21 06:40:14,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35695
2023-12-21 06:40:14,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,625 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,625 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,625 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,625 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e9ziwozt
2023-12-21 06:40:14,626 - distributed.worker - INFO - Starting Worker plugin PreImport-dcfbfabd-03e2-4a1e-9dc3-d86540af0892
2023-12-21 06:40:14,625 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44235
2023-12-21 06:40:14,626 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44235
2023-12-21 06:40:14,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32838d93-2ccd-4add-bae3-2767e68a5c3a
2023-12-21 06:40:14,626 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33077
2023-12-21 06:40:14,626 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,626 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,626 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,626 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ar1kkx1v
2023-12-21 06:40:14,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5b54ebe-ee00-43c5-a403-20b8ce91f07f
2023-12-21 06:40:14,627 - distributed.worker - INFO - Starting Worker plugin PreImport-486a55b9-d0ad-4955-9740-f217ffcd985a
2023-12-21 06:40:14,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56870bf3-ba93-4ba7-98f3-534c999eabb0
2023-12-21 06:40:14,696 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36067
2023-12-21 06:40:14,697 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36067
2023-12-21 06:40:14,697 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44015
2023-12-21 06:40:14,697 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,697 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,697 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,697 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jsj7d80c
2023-12-21 06:40:14,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea1b0118-0dcf-464c-a13b-33f2b763b417
2023-12-21 06:40:14,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38833
2023-12-21 06:40:14,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38833
2023-12-21 06:40:14,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32907
2023-12-21 06:40:14,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,698 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,698 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ua84fcf
2023-12-21 06:40:14,699 - distributed.worker - INFO - Starting Worker plugin PreImport-2416779a-7254-4d58-bced-858bb9682466
2023-12-21 06:40:14,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d97e6b0f-2b4d-436d-9b96-873ac84e980e
2023-12-21 06:40:14,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e079cd5-2d10-4b16-910e-4d5d9632f1f2
2023-12-21 06:40:14,699 - distributed.worker - INFO - Starting Worker plugin PreImport-4300ae3b-0295-4661-b8fc-bea7f0a920c0
2023-12-21 06:40:14,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-979d3a3f-edcc-4227-812a-05673a32c7d4
2023-12-21 06:40:14,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44527
2023-12-21 06:40:14,701 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44527
2023-12-21 06:40:14,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40975
2023-12-21 06:40:14,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41287
2023-12-21 06:40:14,701 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40975
2023-12-21 06:40:14,701 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46363
2023-12-21 06:40:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,701 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,701 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,701 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,701 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:14,701 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-719y3wp0
2023-12-21 06:40:14,701 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:14,701 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_0qozjbs
2023-12-21 06:40:14,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-994c3b0d-ae81-4931-be8b-b8264da3b3cb
2023-12-21 06:40:14,702 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4d66640-3885-4f9c-b3d6-910c65182191
2023-12-21 06:40:14,702 - distributed.worker - INFO - Starting Worker plugin RMMSetup-adf16132-f661-4bea-9d84-fd6cc7930b55
2023-12-21 06:40:14,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b517c10-9ae1-45cd-939d-02dd57a024b5
2023-12-21 06:40:14,827 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,829 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,835 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,835 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,836 - distributed.worker - INFO - Starting Worker plugin PreImport-40da5845-4634-4f85-b898-ffcf395666ce
2023-12-21 06:40:14,836 - distributed.worker - INFO - Starting Worker plugin PreImport-a5210227-a8c8-48ca-87ce-5f0297d5a411
2023-12-21 06:40:14,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c936b545-b19b-40c6-be80-43ac44666485
2023-12-21 06:40:14,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40975', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40975
2023-12-21 06:40:14,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60084
2023-12-21 06:40:14,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38833', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38833
2023-12-21 06:40:14,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60060
2023-12-21 06:40:14,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,862 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,863 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44235', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44235
2023-12-21 06:40:14,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60046
2023-12-21 06:40:14,869 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36067', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,870 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36067
2023-12-21 06:40:14,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60076
2023-12-21 06:40:14,870 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,870 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34237', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,871 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34237
2023-12-21 06:40:14,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60032
2023-12-21 06:40:14,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,872 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,872 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,873 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44527', status: init, memory: 0, processing: 0>
2023-12-21 06:40:14,873 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44527
2023-12-21 06:40:14,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60100
2023-12-21 06:40:14,874 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,874 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:14,876 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:14,876 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:14,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:14,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,921 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,921 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,921 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,921 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,921 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:14,932 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,932 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,932 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,932 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,933 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,933 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,933 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,933 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:14,939 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:14,941 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:14,943 - distributed.scheduler - INFO - Remove client Client-c80f5996-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:14,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59928; closing.
2023-12-21 06:40:14,944 - distributed.scheduler - INFO - Remove client Client-c80f5996-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:14,944 - distributed.scheduler - INFO - Close client connection: Client-c80f5996-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:14,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33463'. Reason: nanny-close
2023-12-21 06:40:14,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,946 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45297'. Reason: nanny-close
2023-12-21 06:40:14,946 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,947 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34237. Reason: nanny-close
2023-12-21 06:40:14,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42217'. Reason: nanny-close
2023-12-21 06:40:14,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45139'. Reason: nanny-close
2023-12-21 06:40:14,948 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44235. Reason: nanny-close
2023-12-21 06:40:14,948 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,948 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45119. Reason: nanny-close
2023-12-21 06:40:14,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38089'. Reason: nanny-close
2023-12-21 06:40:14,949 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,949 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40975. Reason: nanny-close
2023-12-21 06:40:14,949 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33697'. Reason: nanny-close
2023-12-21 06:40:14,949 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,949 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36067. Reason: nanny-close
2023-12-21 06:40:14,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46345'. Reason: nanny-close
2023-12-21 06:40:14,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60032; closing.
2023-12-21 06:40:14,950 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,950 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44527. Reason: nanny-close
2023-12-21 06:40:14,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34237', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9507387')
2023-12-21 06:40:14,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40785'. Reason: nanny-close
2023-12-21 06:40:14,951 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:14,951 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,951 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38833. Reason: nanny-close
2023-12-21 06:40:14,951 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60020; closing.
2023-12-21 06:40:14,951 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44633. Reason: nanny-close
2023-12-21 06:40:14,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9519024')
2023-12-21 06:40:14,952 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,952 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,952 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60046; closing.
2023-12-21 06:40:14,952 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,952 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,952 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,952 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,953 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44235', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9532695')
2023-12-21 06:40:14,953 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60084; closing.
2023-12-21 06:40:14,953 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,954 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,954 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:14,954 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,955 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,954 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60020>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-21 06:40:14,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60076; closing.
2023-12-21 06:40:14,955 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:14,956 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9560332')
2023-12-21 06:40:14,956 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9567654')
2023-12-21 06:40:14,957 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60100; closing.
2023-12-21 06:40:14,957 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60060; closing.
2023-12-21 06:40:14,957 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9578264')
2023-12-21 06:40:14,958 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.9582644')
2023-12-21 06:40:14,958 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60024; closing.
2023-12-21 06:40:14,959 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140814.959029')
2023-12-21 06:40:14,959 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:40:16,613 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:40:16,613 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:40:16,614 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:40:16,615 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:40:16,615 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-21 06:40:18,777 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:18,781 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39093 instead
  warnings.warn(
2023-12-21 06:40:18,785 - distributed.scheduler - INFO - State start
2023-12-21 06:40:18,809 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:18,810 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:40:18,810 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39093/status
2023-12-21 06:40:18,811 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:40:18,861 - distributed.scheduler - INFO - Receive client connection: Client-cd831682-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:18,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60182
2023-12-21 06:40:19,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45899'
2023-12-21 06:40:19,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35869'
2023-12-21 06:40:19,063 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44117'
2023-12-21 06:40:19,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32913'
2023-12-21 06:40:19,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36803'
2023-12-21 06:40:19,089 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46357'
2023-12-21 06:40:19,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39907'
2023-12-21 06:40:19,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41037'
2023-12-21 06:40:21,475 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,500 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,501 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:21,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:21,509 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:21,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:24,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40721
2023-12-21 06:40:24,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40721
2023-12-21 06:40:24,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42137
2023-12-21 06:40:24,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,627 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,627 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,627 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,627 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3k6ba3cq
2023-12-21 06:40:24,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9626605-4b19-4ac9-a7fa-da1dd4caf545
2023-12-21 06:40:24,628 - distributed.worker - INFO - Starting Worker plugin PreImport-ca92cacf-c94e-4a51-bdd3-3f0377ae122d
2023-12-21 06:40:24,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3523bfb6-cb8e-41a1-8e83-4dc7c46fece3
2023-12-21 06:40:24,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35611
2023-12-21 06:40:24,640 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35611
2023-12-21 06:40:24,640 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34073
2023-12-21 06:40:24,640 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,640 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,640 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,640 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hs0hf_ll
2023-12-21 06:40:24,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4aa30ad-cc7d-4d73-8237-071bb97cb424
2023-12-21 06:40:24,641 - distributed.worker - INFO - Starting Worker plugin PreImport-8210df74-42bd-4628-ba79-16e641e2fd08
2023-12-21 06:40:24,642 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e094dc5c-9f02-4b84-aca5-085033369c9d
2023-12-21 06:40:24,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34119
2023-12-21 06:40:24,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34119
2023-12-21 06:40:24,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36823
2023-12-21 06:40:24,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,645 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,645 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1rjdmmew
2023-12-21 06:40:24,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cabb8e9-12cc-46f1-8182-e9be79c1c5e9
2023-12-21 06:40:24,646 - distributed.worker - INFO - Starting Worker plugin PreImport-a4dff1d9-a8be-4ca0-ae61-a5f4c5dee473
2023-12-21 06:40:24,647 - distributed.worker - INFO - Starting Worker plugin RMMSetup-027e60bf-53c2-4d4b-8c77-6764979dc015
2023-12-21 06:40:24,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36533
2023-12-21 06:40:24,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36533
2023-12-21 06:40:24,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41471
2023-12-21 06:40:24,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,650 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,650 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c_ntrdoa
2023-12-21 06:40:24,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b54af6c-746a-4e96-a6f1-0b97d8dd2006
2023-12-21 06:40:24,651 - distributed.worker - INFO - Starting Worker plugin PreImport-31521bb7-4bd0-4b16-b07d-bf0831e159f9
2023-12-21 06:40:24,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87261b5b-d4a6-473e-be70-78555c2e28e9
2023-12-21 06:40:24,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43911
2023-12-21 06:40:24,652 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43911
2023-12-21 06:40:24,652 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46263
2023-12-21 06:40:24,652 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,652 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,652 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,653 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4loh_qqn
2023-12-21 06:40:24,653 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9886be7-e925-423e-8dde-2296d70962c7
2023-12-21 06:40:24,653 - distributed.worker - INFO - Starting Worker plugin PreImport-265d9481-568d-412c-b6e3-f02b0d44baed
2023-12-21 06:40:24,653 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24fefa07-1bc6-49a9-a68e-4986c791f1d4
2023-12-21 06:40:24,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37249
2023-12-21 06:40:24,657 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37249
2023-12-21 06:40:24,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37701
2023-12-21 06:40:24,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,657 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,657 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0k7npxqc
2023-12-21 06:40:24,658 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3afeeb45-674f-4094-9893-f20967b2bda6
2023-12-21 06:40:24,665 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36809
2023-12-21 06:40:24,665 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36809
2023-12-21 06:40:24,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38263
2023-12-21 06:40:24,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,665 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,666 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,666 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ieo55e_
2023-12-21 06:40:24,666 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a07c21b2-0db0-42c0-af4a-cf0042ddb309
2023-12-21 06:40:24,667 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acd9c9c9-dc9f-44e1-a820-123ad1291e1b
2023-12-21 06:40:24,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41561
2023-12-21 06:40:24,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41561
2023-12-21 06:40:24,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40813
2023-12-21 06:40:24,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:24,774 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:24,774 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:24,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:24,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9qj6vyj7
2023-12-21 06:40:24,775 - distributed.worker - INFO - Starting Worker plugin PreImport-bf93527b-7644-4160-a566-f84f54a58fdb
2023-12-21 06:40:24,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08976b00-2144-4c8c-9df8-8bd24e80392c
2023-12-21 06:40:25,220 - distributed.worker - INFO - Starting Worker plugin PreImport-082568a0-9944-4ff3-b45d-971e4536a387
2023-12-21 06:40:25,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9312f640-c416-46ec-a8e1-7065066fd1f4
2023-12-21 06:40:25,221 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,223 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,237 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37249', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,249 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37249
2023-12-21 06:40:25,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34604
2023-12-21 06:40:25,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40721', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,249 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,250 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40721
2023-12-21 06:40:25,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34618
2023-12-21 06:40:25,250 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,250 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,250 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,251 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,252 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,252 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,262 - distributed.worker - INFO - Starting Worker plugin PreImport-03709ab2-f7cf-49b5-a547-695be11f44f2
2023-12-21 06:40:25,263 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,266 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34119', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,267 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34119
2023-12-21 06:40:25,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34620
2023-12-21 06:40:25,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,269 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,269 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36533', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,277 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36533
2023-12-21 06:40:25,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34634
2023-12-21 06:40:25,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,278 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43911', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,279 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43911
2023-12-21 06:40:25,279 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,279 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34636
2023-12-21 06:40:25,279 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,281 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,281 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,282 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35611', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,282 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35611
2023-12-21 06:40:25,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34644
2023-12-21 06:40:25,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,285 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,285 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,294 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36809', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36809
2023-12-21 06:40:25,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34660
2023-12-21 06:40:25,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,296 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,297 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,297 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d40dca52-56ff-41ca-a99f-4252b194063c
2023-12-21 06:40:25,308 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41561', status: init, memory: 0, processing: 0>
2023-12-21 06:40:25,339 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41561
2023-12-21 06:40:25,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34662
2023-12-21 06:40:25,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:25,341 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:25,341 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:25,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:25,383 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,384 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,396 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,397 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:40:25,404 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,405 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:25,408 - distributed.scheduler - INFO - Remove client Client-cd831682-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:25,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60182; closing.
2023-12-21 06:40:25,408 - distributed.scheduler - INFO - Remove client Client-cd831682-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:25,409 - distributed.scheduler - INFO - Close client connection: Client-cd831682-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:25,409 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45899'. Reason: nanny-close
2023-12-21 06:40:25,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35869'. Reason: nanny-close
2023-12-21 06:40:25,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41561. Reason: nanny-close
2023-12-21 06:40:25,411 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44117'. Reason: nanny-close
2023-12-21 06:40:25,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36809. Reason: nanny-close
2023-12-21 06:40:25,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32913'. Reason: nanny-close
2023-12-21 06:40:25,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43911. Reason: nanny-close
2023-12-21 06:40:25,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36803'. Reason: nanny-close
2023-12-21 06:40:25,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40721. Reason: nanny-close
2023-12-21 06:40:25,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46357'. Reason: nanny-close
2023-12-21 06:40:25,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35611. Reason: nanny-close
2023-12-21 06:40:25,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34662; closing.
2023-12-21 06:40:25,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39907'. Reason: nanny-close
2023-12-21 06:40:25,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41561', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4149868')
2023-12-21 06:40:25,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34119. Reason: nanny-close
2023-12-21 06:40:25,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41037'. Reason: nanny-close
2023-12-21 06:40:25,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:25,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36533. Reason: nanny-close
2023-12-21 06:40:25,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34636; closing.
2023-12-21 06:40:25,415 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37249. Reason: nanny-close
2023-12-21 06:40:25,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,416 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43911', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4166334')
2023-12-21 06:40:25,416 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34660; closing.
2023-12-21 06:40:25,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:25,418 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,418 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,418 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,418 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36809', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4187405')
2023-12-21 06:40:25,419 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34618; closing.
2023-12-21 06:40:25,419 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:25,420 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34636>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-21 06:40:25,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4220643')
2023-12-21 06:40:25,422 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34644; closing.
2023-12-21 06:40:25,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4230676')
2023-12-21 06:40:25,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34620; closing.
2023-12-21 06:40:25,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34634; closing.
2023-12-21 06:40:25,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34604; closing.
2023-12-21 06:40:25,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.424315')
2023-12-21 06:40:25,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36533', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4246495')
2023-12-21 06:40:25,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140825.4250722')
2023-12-21 06:40:25,425 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:40:26,977 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:40:26,977 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:40:26,978 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:40:26,979 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:40:26,980 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-21 06:40:29,193 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:29,198 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35963 instead
  warnings.warn(
2023-12-21 06:40:29,203 - distributed.scheduler - INFO - State start
2023-12-21 06:40:29,277 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:29,278 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:40:29,279 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35963/status
2023-12-21 06:40:29,279 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:40:29,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45249'
2023-12-21 06:40:29,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36951'
2023-12-21 06:40:29,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41717'
2023-12-21 06:40:29,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36891'
2023-12-21 06:40:29,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38797'
2023-12-21 06:40:29,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46717'
2023-12-21 06:40:29,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33597'
2023-12-21 06:40:29,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39225'
2023-12-21 06:40:31,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:31,370 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:31,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:31,680 - distributed.scheduler - INFO - Receive client connection: Client-d3b4c7f9-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:31,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49434
2023-12-21 06:40:34,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44873
2023-12-21 06:40:34,036 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44873
2023-12-21 06:40:34,036 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33927
2023-12-21 06:40:34,036 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,036 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,037 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,037 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p2wdzz2i
2023-12-21 06:40:34,037 - distributed.worker - INFO - Starting Worker plugin PreImport-b31a815a-4fb0-4801-bb07-9d498e6f1c1d
2023-12-21 06:40:34,037 - distributed.worker - INFO - Starting Worker plugin RMMSetup-898ff2ab-b5b5-4e36-9565-bc318a7c83d6
2023-12-21 06:40:34,049 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d80bb30-40d2-4621-a43b-3711efc6d166
2023-12-21 06:40:34,050 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44873', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44873
2023-12-21 06:40:34,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49450
2023-12-21 06:40:34,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,086 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,086 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46395
2023-12-21 06:40:34,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46395
2023-12-21 06:40:34,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33385
2023-12-21 06:40:34,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,094 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,094 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fuihh8co
2023-12-21 06:40:34,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39467
2023-12-21 06:40:34,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-302a2e67-7b7d-4924-8334-a9c0b3ec90e4
2023-12-21 06:40:34,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39467
2023-12-21 06:40:34,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35555
2023-12-21 06:40:34,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,095 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bb65aba-3bcf-4647-9f76-80458588e6a2
2023-12-21 06:40:34,095 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v0clks7t
2023-12-21 06:40:34,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da917dd7-42a9-4a74-a596-587ea7908ddb
2023-12-21 06:40:34,096 - distributed.worker - INFO - Starting Worker plugin PreImport-5bc280b8-b591-4bd6-940e-a796fd1ad1f3
2023-12-21 06:40:34,097 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5dc9bb0-785d-48b2-9244-0dfa106ab46b
2023-12-21 06:40:34,117 - distributed.worker - INFO - Starting Worker plugin PreImport-01b8ddd3-8614-48f5-afc4-d9a736e48460
2023-12-21 06:40:34,118 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,120 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,146 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39467', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,147 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39467
2023-12-21 06:40:34,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49468
2023-12-21 06:40:34,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,148 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,149 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46395', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,149 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46395
2023-12-21 06:40:34,149 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49452
2023-12-21 06:40:34,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,151 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,151 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46309
2023-12-21 06:40:34,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46309
2023-12-21 06:40:34,205 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44697
2023-12-21 06:40:34,205 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,205 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,205 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,205 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,205 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r1vbpzao
2023-12-21 06:40:34,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-839ece04-cac3-4464-930a-2da63a0dbe38
2023-12-21 06:40:34,206 - distributed.worker - INFO - Starting Worker plugin PreImport-92669699-0133-4c82-9549-cff7700b9252
2023-12-21 06:40:34,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-221c4ca5-96d8-43e1-9c42-8c47075b4181
2023-12-21 06:40:34,212 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35861
2023-12-21 06:40:34,212 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35861
2023-12-21 06:40:34,213 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42279
2023-12-21 06:40:34,213 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,213 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,213 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,213 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,213 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fcmhpw4l
2023-12-21 06:40:34,214 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e62a589d-f3b7-499f-8dd0-b01d3e8af04f
2023-12-21 06:40:34,213 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42557
2023-12-21 06:40:34,214 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42557
2023-12-21 06:40:34,215 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42093
2023-12-21 06:40:34,215 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,215 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,215 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,215 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,215 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fkr4x8ot
2023-12-21 06:40:34,215 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39793
2023-12-21 06:40:34,216 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfe69258-5194-45da-8b29-f9f617a1562a
2023-12-21 06:40:34,216 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39793
2023-12-21 06:40:34,216 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33015
2023-12-21 06:40:34,216 - distributed.worker - INFO - Starting Worker plugin PreImport-9e4d14c3-b65e-4806-84c9-99c6e6c0e70e
2023-12-21 06:40:34,216 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,216 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,216 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a487b76a-5e83-4ccc-a165-a1f5a1883879
2023-12-21 06:40:34,216 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,216 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,216 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-43hgxn3y
2023-12-21 06:40:34,217 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52081afb-c90f-4983-bdb0-4830044e81e4
2023-12-21 06:40:34,217 - distributed.worker - INFO - Starting Worker plugin PreImport-93c42b2d-0cec-4b47-a1ea-e217c0365e78
2023-12-21 06:40:34,217 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10dfa9d8-4dd3-4441-9fa6-d6269eb1bff7
2023-12-21 06:40:34,217 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43259
2023-12-21 06:40:34,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43259
2023-12-21 06:40:34,218 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35317
2023-12-21 06:40:34,218 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,218 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,218 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:34,219 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:40:34,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ou1v12et
2023-12-21 06:40:34,219 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6433192d-4e6d-470b-80fe-5aa3e94bfbe0
2023-12-21 06:40:34,259 - distributed.worker - INFO - Starting Worker plugin PreImport-cb2c0d48-db3c-496a-94bd-c3761291255f
2023-12-21 06:40:34,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be9c0969-6b60-416b-89e6-e7632733785b
2023-12-21 06:40:34,260 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,262 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,270 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,270 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,273 - distributed.worker - INFO - Starting Worker plugin PreImport-a4884112-5954-4671-81ca-8d5ff6f7fd23
2023-12-21 06:40:34,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f469f71-2c6f-420b-80cc-59864ab0f286
2023-12-21 06:40:34,273 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,284 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35861', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,284 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35861
2023-12-21 06:40:34,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49476
2023-12-21 06:40:34,285 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42557', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,285 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,286 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42557
2023-12-21 06:40:34,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49478
2023-12-21 06:40:34,286 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,286 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,287 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,287 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,287 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,295 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43259', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,296 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43259
2023-12-21 06:40:34,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49502
2023-12-21 06:40:34,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,298 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,298 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,300 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46309', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,301 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46309
2023-12-21 06:40:34,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49482
2023-12-21 06:40:34,302 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39793', status: init, memory: 0, processing: 0>
2023-12-21 06:40:34,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,302 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39793
2023-12-21 06:40:34,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49488
2023-12-21 06:40:34,303 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,303 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:34,304 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:34,304 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:34,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,312 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:34,348 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,348 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,349 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,350 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,350 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:40:34,356 - distributed.scheduler - INFO - Remove client Client-d3b4c7f9-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:34,356 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49434; closing.
2023-12-21 06:40:34,356 - distributed.scheduler - INFO - Remove client Client-d3b4c7f9-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:34,357 - distributed.scheduler - INFO - Close client connection: Client-d3b4c7f9-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:34,358 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41717'. Reason: nanny-close
2023-12-21 06:40:34,358 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,359 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36891'. Reason: nanny-close
2023-12-21 06:40:34,359 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,359 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44873. Reason: nanny-close
2023-12-21 06:40:34,359 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46717'. Reason: nanny-close
2023-12-21 06:40:34,360 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,360 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39793. Reason: nanny-close
2023-12-21 06:40:34,360 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33597'. Reason: nanny-close
2023-12-21 06:40:34,360 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,360 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43259. Reason: nanny-close
2023-12-21 06:40:34,361 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39225'. Reason: nanny-close
2023-12-21 06:40:34,361 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,361 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35861. Reason: nanny-close
2023-12-21 06:40:34,361 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45249'. Reason: nanny-close
2023-12-21 06:40:34,362 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,362 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46309. Reason: nanny-close
2023-12-21 06:40:34,362 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,362 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49450; closing.
2023-12-21 06:40:34,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36951'. Reason: nanny-close
2023-12-21 06:40:34,362 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,362 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,362 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44873', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3625288')
2023-12-21 06:40:34,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38797'. Reason: nanny-close
2023-12-21 06:40:34,362 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,362 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46395. Reason: nanny-close
2023-12-21 06:40:34,363 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:34,363 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,363 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42557. Reason: nanny-close
2023-12-21 06:40:34,363 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39467. Reason: nanny-close
2023-12-21 06:40:34,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49476; closing.
2023-12-21 06:40:34,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49502; closing.
2023-12-21 06:40:34,364 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,364 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,364 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,364 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,364 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,364 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,364 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3648412')
2023-12-21 06:40:34,365 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3652275')
2023-12-21 06:40:34,365 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49488; closing.
2023-12-21 06:40:34,365 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,365 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:34,366 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,366 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,367 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,366 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49476>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49476>: Stream is closed
2023-12-21 06:40:34,367 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:34,368 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3682241')
2023-12-21 06:40:34,368 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49482; closing.
2023-12-21 06:40:34,369 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3692317')
2023-12-21 06:40:34,369 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49478; closing.
2023-12-21 06:40:34,369 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49452; closing.
2023-12-21 06:40:34,370 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42557', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3702347')
2023-12-21 06:40:34,370 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3706417')
2023-12-21 06:40:34,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49468; closing.
2023-12-21 06:40:34,371 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39467', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140834.3713973')
2023-12-21 06:40:34,371 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:40:34,371 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49468>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-21 06:40:35,875 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:40:35,875 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:40:35,876 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:40:35,877 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:40:35,878 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-21 06:40:38,013 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:38,018 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36421 instead
  warnings.warn(
2023-12-21 06:40:38,022 - distributed.scheduler - INFO - State start
2023-12-21 06:40:38,044 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:38,045 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:40:38,046 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36421/status
2023-12-21 06:40:38,046 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:40:38,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46327'
2023-12-21 06:40:39,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:39,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:40,169 - distributed.scheduler - INFO - Receive client connection: Client-d905e4ef-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:40,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54884
2023-12-21 06:40:40,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:42,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46349
2023-12-21 06:40:42,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46349
2023-12-21 06:40:42,095 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-21 06:40:42,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:42,095 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:42,095 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:42,095 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-21 06:40:42,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v8vmprcq
2023-12-21 06:40:42,096 - distributed.worker - INFO - Starting Worker plugin PreImport-5bcbb65a-9d7a-4165-92c7-2f68f73af519
2023-12-21 06:40:42,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3ef667c-f223-4489-b1ef-abb5e210d60d
2023-12-21 06:40:42,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42f82bd3-fa8d-4e71-82ca-d17af2a7fb5c
2023-12-21 06:40:42,097 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:42,130 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46349', status: init, memory: 0, processing: 0>
2023-12-21 06:40:42,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46349
2023-12-21 06:40:42,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54904
2023-12-21 06:40:42,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:42,134 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:42,134 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:42,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:42,218 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:42,221 - distributed.scheduler - INFO - Remove client Client-d905e4ef-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:42,221 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54884; closing.
2023-12-21 06:40:42,221 - distributed.scheduler - INFO - Remove client Client-d905e4ef-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:42,222 - distributed.scheduler - INFO - Close client connection: Client-d905e4ef-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:42,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46327'. Reason: nanny-close
2023-12-21 06:40:42,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:42,225 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46349. Reason: nanny-close
2023-12-21 06:40:42,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54904; closing.
2023-12-21 06:40:42,228 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:42,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140842.2283678')
2023-12-21 06:40:42,228 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:40:42,230 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:43,490 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:40:43,490 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:40:43,491 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:40:43,492 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:40:43,492 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-21 06:40:47,722 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:47,726 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-21 06:40:47,729 - distributed.scheduler - INFO - State start
2023-12-21 06:40:47,774 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:40:47,775 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:40:47,776 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-21 06:40:47,776 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:40:47,866 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34373'
2023-12-21 06:40:49,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:40:49,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:40:50,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:40:51,298 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36687
2023-12-21 06:40:51,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36687
2023-12-21 06:40:51,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43127
2023-12-21 06:40:51,299 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:40:51,299 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:51,299 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:40:51,299 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-21 06:40:51,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jilhrgzd
2023-12-21 06:40:51,299 - distributed.worker - INFO - Starting Worker plugin PreImport-59aa2532-8374-44be-b5a6-eb4bd3182822
2023-12-21 06:40:51,301 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-917db0c2-171f-40ea-8f39-8c20a3144d24
2023-12-21 06:40:51,301 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eac44161-3a24-409d-9290-82b53298f189
2023-12-21 06:40:51,302 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:51,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36687', status: init, memory: 0, processing: 0>
2023-12-21 06:40:51,364 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36687
2023-12-21 06:40:51,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34516
2023-12-21 06:40:51,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:40:51,368 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:40:51,368 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:40:51,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:40:56,732 - distributed.scheduler - INFO - Receive client connection: Client-dec9acf5-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:56,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34530
2023-12-21 06:40:56,740 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:40:56,745 - distributed.scheduler - INFO - Remove client Client-dec9acf5-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:56,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34530; closing.
2023-12-21 06:40:56,745 - distributed.scheduler - INFO - Remove client Client-dec9acf5-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:56,746 - distributed.scheduler - INFO - Close client connection: Client-dec9acf5-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:40:56,747 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34373'. Reason: nanny-close
2023-12-21 06:40:56,747 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:40:56,748 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36687. Reason: nanny-close
2023-12-21 06:40:56,751 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34516; closing.
2023-12-21 06:40:56,751 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:40:56,751 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140856.7515705')
2023-12-21 06:40:56,751 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:40:56,753 - distributed.nanny - INFO - Worker closed
2023-12-21 06:40:58,014 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:40:58,014 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:40:58,015 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:40:58,016 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:40:58,017 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-21 06:41:00,387 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:00,393 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-21 06:41:00,397 - distributed.scheduler - INFO - State start
2023-12-21 06:41:00,420 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:00,421 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:41:00,422 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-21 06:41:00,422 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:41:05,171 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:33846'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33846>: Stream is closed
2023-12-21 06:41:05,653 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:41:05,654 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:41:05,654 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:41:05,655 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:41:05,656 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-21 06:41:08,036 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:08,041 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-21 06:41:08,044 - distributed.scheduler - INFO - State start
2023-12-21 06:41:08,067 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:08,068 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-21 06:41:08,069 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-21 06:41:08,069 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:41:08,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42989'
2023-12-21 06:41:08,372 - distributed.scheduler - INFO - Receive client connection: Client-eaca71b0-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:08,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45498
2023-12-21 06:41:10,013 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:10,014 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:10,018 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:10,994 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42541
2023-12-21 06:41:10,994 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42541
2023-12-21 06:41:10,994 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41659
2023-12-21 06:41:10,995 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-21 06:41:10,995 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:10,995 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:10,995 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-21 06:41:10,995 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9hs7hpu_
2023-12-21 06:41:10,995 - distributed.worker - INFO - Starting Worker plugin PreImport-d751a905-ac42-433c-a0a9-3c1fdf03ce25
2023-12-21 06:41:10,995 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a978e88-5545-43bb-9e8c-4df4c8361308
2023-12-21 06:41:10,995 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c07d3a4b-35de-488b-8944-9e4891616750
2023-12-21 06:41:10,996 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:11,025 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42541', status: init, memory: 0, processing: 0>
2023-12-21 06:41:11,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42541
2023-12-21 06:41:11,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52186
2023-12-21 06:41:11,027 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:11,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-21 06:41:11,029 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:11,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-21 06:41:11,036 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:11,039 - distributed.scheduler - INFO - Remove client Client-eaca71b0-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:11,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45498; closing.
2023-12-21 06:41:11,039 - distributed.scheduler - INFO - Remove client Client-eaca71b0-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:11,040 - distributed.scheduler - INFO - Close client connection: Client-eaca71b0-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:11,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42989'. Reason: nanny-close
2023-12-21 06:41:11,050 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:11,051 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42541. Reason: nanny-close
2023-12-21 06:41:11,053 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-21 06:41:11,053 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52186; closing.
2023-12-21 06:41:11,054 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140871.0539696')
2023-12-21 06:41:11,054 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:41:11,055 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:12,257 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:41:12,258 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:41:12,258 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:41:12,260 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-21 06:41:12,260 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-21 06:41:14,463 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:14,468 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43079 instead
  warnings.warn(
2023-12-21 06:41:14,472 - distributed.scheduler - INFO - State start
2023-12-21 06:41:14,618 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:14,619 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:41:14,620 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43079/status
2023-12-21 06:41:14,620 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:41:14,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33673'
2023-12-21 06:41:14,913 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40481'
2023-12-21 06:41:14,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45805'
2023-12-21 06:41:14,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38711'
2023-12-21 06:41:14,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35409'
2023-12-21 06:41:14,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39201'
2023-12-21 06:41:14,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41937'
2023-12-21 06:41:14,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34517'
2023-12-21 06:41:14,983 - distributed.scheduler - INFO - Receive client connection: Client-eeb53528-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:14,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35004
2023-12-21 06:41:16,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,835 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,837 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,886 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,889 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:16,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:16,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:16,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:19,685 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32777
2023-12-21 06:41:19,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32777
2023-12-21 06:41:19,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33129
2023-12-21 06:41:19,686 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,687 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,687 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8do2o4gn
2023-12-21 06:41:19,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db969826-cbd7-4733-b0f3-2ef8413aef64
2023-12-21 06:41:19,687 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46679
2023-12-21 06:41:19,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46679
2023-12-21 06:41:19,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34581
2023-12-21 06:41:19,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,688 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,688 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,688 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vy50i3co
2023-12-21 06:41:19,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebb9c5de-bc8e-446e-b0e8-8037cd10cb53
2023-12-21 06:41:19,689 - distributed.worker - INFO - Starting Worker plugin PreImport-80ffcf3d-a358-438c-9f8e-6099bdcf0a16
2023-12-21 06:41:19,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e060679-e87d-4bd7-9dfe-ca6a6c8bcc09
2023-12-21 06:41:19,727 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45339
2023-12-21 06:41:19,728 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45339
2023-12-21 06:41:19,728 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44859
2023-12-21 06:41:19,728 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,728 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,728 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,728 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,728 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_lxa_8uh
2023-12-21 06:41:19,729 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c2b93d5-838e-48d5-8b1f-c69b5acbdfee
2023-12-21 06:41:19,732 - distributed.worker - INFO - Starting Worker plugin PreImport-618033d1-b575-455f-b685-1b340ca4274b
2023-12-21 06:41:19,732 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86f33eff-6a05-4e98-958d-f58876a37376
2023-12-21 06:41:19,735 - distributed.worker - INFO - Starting Worker plugin PreImport-24a083b1-b298-4fd6-b969-7c6e86288d71
2023-12-21 06:41:19,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d42785be-6e07-49c7-a264-f695eb36ea4e
2023-12-21 06:41:19,735 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,745 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,758 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,760 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32777', status: init, memory: 0, processing: 0>
2023-12-21 06:41:19,762 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32777
2023-12-21 06:41:19,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35022
2023-12-21 06:41:19,763 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:19,764 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,764 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:19,798 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46679', status: init, memory: 0, processing: 0>
2023-12-21 06:41:19,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46679
2023-12-21 06:41:19,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35036
2023-12-21 06:41:19,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:19,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,801 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45339', status: init, memory: 0, processing: 0>
2023-12-21 06:41:19,806 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45339
2023-12-21 06:41:19,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35040
2023-12-21 06:41:19,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:19,808 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,809 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:19,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:19,897 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46583
2023-12-21 06:41:19,898 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46583
2023-12-21 06:41:19,898 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37353
2023-12-21 06:41:19,898 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,898 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,898 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,899 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,899 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-preqs9vx
2023-12-21 06:41:19,899 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-972e4924-189b-4f6e-b6b3-14cb6f3c814c
2023-12-21 06:41:19,900 - distributed.worker - INFO - Starting Worker plugin PreImport-e7df8f40-5228-4f75-8a24-4268857df6f2
2023-12-21 06:41:19,900 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2340aa94-eee8-4748-ae85-9cdbabeea7df
2023-12-21 06:41:19,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36571
2023-12-21 06:41:19,921 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36571
2023-12-21 06:41:19,921 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43627
2023-12-21 06:41:19,921 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,921 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,921 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,921 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,922 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xgadr_q1
2023-12-21 06:41:19,921 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42431
2023-12-21 06:41:19,922 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42431
2023-12-21 06:41:19,922 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41039
2023-12-21 06:41:19,922 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,922 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,922 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,922 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,922 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m8y0985l
2023-12-21 06:41:19,923 - distributed.worker - INFO - Starting Worker plugin PreImport-63bb7f3f-0507-42a7-ad5d-d8b18cf04037
2023-12-21 06:41:19,923 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80167079-9f17-4b60-8164-f29bfa910e3b
2023-12-21 06:41:19,923 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30f4055c-2069-4198-b439-fbcf5c3fa3a0
2023-12-21 06:41:19,925 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34753
2023-12-21 06:41:19,927 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34753
2023-12-21 06:41:19,927 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34881
2023-12-21 06:41:19,928 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,928 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,928 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,928 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,928 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-worpndt3
2023-12-21 06:41:19,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e6c3360-dae7-49b6-a677-039105e69aa5
2023-12-21 06:41:19,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ae0c3ee-8aee-49e0-834f-4c6d2c79b4b3
2023-12-21 06:41:19,939 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38915
2023-12-21 06:41:19,940 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38915
2023-12-21 06:41:19,940 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41497
2023-12-21 06:41:19,940 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:19,940 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,940 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:19,940 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-21 06:41:19,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xjc30_n_
2023-12-21 06:41:19,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40eda7ca-1872-4e6a-929e-a9d80d453419
2023-12-21 06:41:19,941 - distributed.worker - INFO - Starting Worker plugin PreImport-b7ed3f50-f9ab-4ed9-a324-3f002a9e2d65
2023-12-21 06:41:19,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b56a978e-e6e4-4751-8f1b-98265aa7fac9
2023-12-21 06:41:19,987 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,993 - distributed.worker - INFO - Starting Worker plugin PreImport-64d0df73-7d80-45d9-859d-0ed1a5e5c05e
2023-12-21 06:41:19,994 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-990f9452-9a2b-4483-b11c-9c90fedba6d4
2023-12-21 06:41:19,994 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:19,999 - distributed.worker - INFO - Starting Worker plugin PreImport-2db50d22-1ac9-487a-bd17-155a242041c5
2023-12-21 06:41:20,000 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,002 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ea46099-6af1-40c0-a08a-f4f9419c2c4c
2023-12-21 06:41:20,003 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,009 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,011 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46583', status: init, memory: 0, processing: 0>
2023-12-21 06:41:20,011 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46583
2023-12-21 06:41:20,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42194
2023-12-21 06:41:20,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:20,016 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:20,016 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,017 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42431', status: init, memory: 0, processing: 0>
2023-12-21 06:41:20,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:20,017 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42431
2023-12-21 06:41:20,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42200
2023-12-21 06:41:20,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:20,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:20,019 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:20,058 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38915', status: init, memory: 0, processing: 0>
2023-12-21 06:41:20,059 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38915
2023-12-21 06:41:20,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42236
2023-12-21 06:41:20,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36571', status: init, memory: 0, processing: 0>
2023-12-21 06:41:20,060 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:20,061 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36571
2023-12-21 06:41:20,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42220
2023-12-21 06:41:20,061 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:20,061 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34753', status: init, memory: 0, processing: 0>
2023-12-21 06:41:20,062 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34753
2023-12-21 06:41:20,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42210
2023-12-21 06:41:20,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:20,063 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:20,063 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,063 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:20,064 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:20,064 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:20,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:20,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:20,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:20,122 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,122 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,123 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,123 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,123 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,123 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,123 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,123 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-21 06:41:20,255 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,256 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:20,261 - distributed.scheduler - INFO - Remove client Client-eeb53528-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:20,261 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35004; closing.
2023-12-21 06:41:20,262 - distributed.scheduler - INFO - Remove client Client-eeb53528-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:20,262 - distributed.scheduler - INFO - Close client connection: Client-eeb53528-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:20,263 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33673'. Reason: nanny-close
2023-12-21 06:41:20,263 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,264 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40481'. Reason: nanny-close
2023-12-21 06:41:20,264 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,264 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36571. Reason: nanny-close
2023-12-21 06:41:20,264 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45805'. Reason: nanny-close
2023-12-21 06:41:20,265 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,265 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46679. Reason: nanny-close
2023-12-21 06:41:20,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38711'. Reason: nanny-close
2023-12-21 06:41:20,265 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35409'. Reason: nanny-close
2023-12-21 06:41:20,266 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,266 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42431. Reason: nanny-close
2023-12-21 06:41:20,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39201'. Reason: nanny-close
2023-12-21 06:41:20,267 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42220; closing.
2023-12-21 06:41:20,267 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,267 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,267 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45339. Reason: nanny-close
2023-12-21 06:41:20,267 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36571', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.267409')
2023-12-21 06:41:20,267 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41937'. Reason: nanny-close
2023-12-21 06:41:20,267 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,268 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32777. Reason: nanny-close
2023-12-21 06:41:20,268 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34753. Reason: nanny-close
2023-12-21 06:41:20,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34517'. Reason: nanny-close
2023-12-21 06:41:20,268 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,268 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:20,268 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38915. Reason: nanny-close
2023-12-21 06:41:20,269 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,269 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46583. Reason: nanny-close
2023-12-21 06:41:20,269 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35036; closing.
2023-12-21 06:41:20,269 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,270 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,270 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.2701194')
2023-12-21 06:41:20,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,270 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42200; closing.
2023-12-21 06:41:20,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,271 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42431', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.2710843')
2023-12-21 06:41:20,271 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:20,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35040; closing.
2023-12-21 06:41:20,271 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,271 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.272163')
2023-12-21 06:41:20,272 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,272 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35022; closing.
2023-12-21 06:41:20,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42210; closing.
2023-12-21 06:41:20,272 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:20,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42236; closing.
2023-12-21 06:41:20,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.27332')
2023-12-21 06:41:20,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.2736833')
2023-12-21 06:41:20,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38915', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.274036')
2023-12-21 06:41:20,274 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42194; closing.
2023-12-21 06:41:20,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46583', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140880.274782')
2023-12-21 06:41:20,275 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:41:21,931 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:41:21,931 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:41:21,933 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:41:21,934 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:41:21,935 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-21 06:41:24,224 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:24,228 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35981 instead
  warnings.warn(
2023-12-21 06:41:24,231 - distributed.scheduler - INFO - State start
2023-12-21 06:41:24,251 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:24,252 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:41:24,253 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35981/status
2023-12-21 06:41:24,253 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:41:24,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44219'
2023-12-21 06:41:26,115 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:26,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:26,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:26,219 - distributed.scheduler - INFO - Receive client connection: Client-f4883b36-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:26,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42364
2023-12-21 06:41:26,886 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35117
2023-12-21 06:41:26,886 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35117
2023-12-21 06:41:26,886 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38273
2023-12-21 06:41:26,886 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:26,886 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:26,886 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:26,886 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-21 06:41:26,887 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x9l9ggml
2023-12-21 06:41:26,887 - distributed.worker - INFO - Starting Worker plugin PreImport-1452b6d8-514b-4134-a7e0-f7ab8a67810a
2023-12-21 06:41:26,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c639c4bb-2571-4cda-8765-b728895a03bb
2023-12-21 06:41:26,892 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e0d7fec-b273-4b8f-a348-c385e94d93e2
2023-12-21 06:41:26,893 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:26,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35117', status: init, memory: 0, processing: 0>
2023-12-21 06:41:26,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35117
2023-12-21 06:41:26,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42378
2023-12-21 06:41:26,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:26,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:26,926 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:26,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:27,005 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:41:27,009 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:27,011 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:27,014 - distributed.scheduler - INFO - Remove client Client-f4883b36-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:27,014 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42364; closing.
2023-12-21 06:41:27,014 - distributed.scheduler - INFO - Remove client Client-f4883b36-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:27,015 - distributed.scheduler - INFO - Close client connection: Client-f4883b36-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:27,016 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44219'. Reason: nanny-close
2023-12-21 06:41:27,016 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:27,017 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35117. Reason: nanny-close
2023-12-21 06:41:27,020 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:27,020 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42378; closing.
2023-12-21 06:41:27,020 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140887.0202875')
2023-12-21 06:41:27,020 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:41:27,022 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:28,282 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:41:28,283 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:41:28,283 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:41:28,284 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:41:28,284 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-21 06:41:30,515 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:30,520 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42065 instead
  warnings.warn(
2023-12-21 06:41:30,524 - distributed.scheduler - INFO - State start
2023-12-21 06:41:30,546 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-21 06:41:30,547 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-21 06:41:30,548 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42065/status
2023-12-21 06:41:30,548 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-21 06:41:30,566 - distributed.scheduler - INFO - Receive client connection: Client-f83ad699-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:30,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38678
2023-12-21 06:41:30,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45745'
2023-12-21 06:41:32,497 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-21 06:41:32,497 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-21 06:41:32,501 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-21 06:41:33,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44805
2023-12-21 06:41:33,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44805
2023-12-21 06:41:33,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44259
2023-12-21 06:41:33,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-21 06:41:33,531 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:33,531 - distributed.worker - INFO -               Threads:                          1
2023-12-21 06:41:33,531 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-21 06:41:33,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bwya97yt
2023-12-21 06:41:33,531 - distributed.worker - INFO - Starting Worker plugin PreImport-2a4906ef-aab4-480c-bac8-dcf8eb7c2562
2023-12-21 06:41:33,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-245671fe-802d-462e-a881-c6d5a1dfc3cb
2023-12-21 06:41:33,544 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-174cbcd4-ccd6-4775-bcc2-ee9563f9c5b2
2023-12-21 06:41:33,544 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:33,567 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44805', status: init, memory: 0, processing: 0>
2023-12-21 06:41:33,568 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44805
2023-12-21 06:41:33,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38712
2023-12-21 06:41:33,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-21 06:41:33,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-21 06:41:33,569 - distributed.worker - INFO - -------------------------------------------------
2023-12-21 06:41:33,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-21 06:41:33,636 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-21 06:41:33,642 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-21 06:41:33,646 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:33,647 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-21 06:41:33,650 - distributed.scheduler - INFO - Remove client Client-f83ad699-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:33,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38678; closing.
2023-12-21 06:41:33,651 - distributed.scheduler - INFO - Remove client Client-f83ad699-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:33,651 - distributed.scheduler - INFO - Close client connection: Client-f83ad699-9fcb-11ee-b71e-d8c49764f6bb
2023-12-21 06:41:33,652 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45745'. Reason: nanny-close
2023-12-21 06:41:33,653 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-21 06:41:33,654 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44805. Reason: nanny-close
2023-12-21 06:41:33,655 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-21 06:41:33,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38712; closing.
2023-12-21 06:41:33,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1703140893.6560807')
2023-12-21 06:41:33,656 - distributed.scheduler - INFO - Lost all workers
2023-12-21 06:41:33,657 - distributed.nanny - INFO - Worker closed
2023-12-21 06:41:34,669 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-21 06:41:34,669 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-21 06:41:34,670 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-21 06:41:34,671 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-21 06:41:34,671 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39189 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37067 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45731 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37669 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44455 instead
  warnings.warn(
2023-12-21 06:43:13,696 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-21 06:43:13,696 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-21 06:43:13,699 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://10.33.225.163:34397', name: 4, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-21 06:43:13,699 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://10.33.225.163:40371', name: 5, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39213 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36781 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34219 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44927 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32945 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33601 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42927 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44455 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38729 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34409 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43219 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46369 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] [1703141426.015207] [dgx13:75990:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:52378) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36279 instead
  warnings.warn(
2023-12-21 06:52:00,513 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-21 06:52:00,523 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:42434', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32797 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39081 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34933 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36913 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34869 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39733 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42625 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39977 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40063 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38013 instead
  warnings.warn(
[1703141680.933823] [dgx13:78626:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:34246) failed: Address already in use
2023-12-21 06:54:46,980 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-bbdab5ec-d507-4a88-9f3b-9170459261b5
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7f7fea20bb80>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-21 06:54:46,981 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-36cdf490-0feb-4aac-aa72-ca5b955a7b23
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7fbfb8b3c160>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-21 06:54:46,990 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-8bb64250-93e4-4117-8bd5-d4b7f2841f8e
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7f93dc765790>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-21 06:54:47,000 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-382739f1-3f0a-42da-874a-e8a131d0cc8c
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7f3fd5c330d0>, ('explicit-comms-shuffle-71f91059afe4beb52d8c6be09d72a382', {0: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 0), ('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 4)}, 1: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 2)}, 2: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 1)}, 3: {('from_pandas-5d48eea67fb7a5813a0116c5a9918b3d', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-21 06:54:47,264 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-7c0ca411-4774-466f-aaa1-b5548b78467a
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7fbfb8b3c160>, ('explicit-comms-shuffle-730bf992603588947fed7d4dff9ec95c', {0: set(), 1: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 0)}, 2: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 2)}, 3: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-21 06:54:47,278 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-d0368eec-35a0-4211-a63e-48b25f582027
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7f93dc765790>, ('explicit-comms-shuffle-730bf992603588947fed7d4dff9ec95c', {0: set(), 1: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 0)}, 2: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 2)}, 3: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2023-12-21 06:54:47,294 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-d229319a-096c-4ec6-a911-0cc3ee08c391
Function:  _run_coroutine_on_worker
args:      (216690351337147279437698094635757753549, <function shuffle_task at 0x7f3fd5c330d0>, ('explicit-comms-shuffle-730bf992603588947fed7d4dff9ec95c', {0: set(), 1: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 0)}, 2: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 2)}, 3: {('from_pandas-c5d427c1336b8ee6af0b7f5ad7add793', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
