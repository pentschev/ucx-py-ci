============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.4
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-06 06:34:37,415 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:37,420 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42401 instead
  warnings.warn(
2024-02-06 06:34:37,425 - distributed.scheduler - INFO - State start
2024-02-06 06:34:37,449 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:37,450 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-06 06:34:37,450 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42401/status
2024-02-06 06:34:37,451 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:34:37,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34051'
2024-02-06 06:34:37,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34113'
2024-02-06 06:34:37,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36109'
2024-02-06 06:34:37,526 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45057'
2024-02-06 06:34:37,907 - distributed.scheduler - INFO - Receive client connection: Client-cb809113-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:37,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43596
2024-02-06 06:34:39,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:39,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:39,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:39,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:39,249 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:39,249 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:39,250 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35939
2024-02-06 06:34:39,250 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36161
2024-02-06 06:34:39,250 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36161
2024-02-06 06:34:39,250 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35939
2024-02-06 06:34:39,250 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39713
2024-02-06 06:34:39,250 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37603
2024-02-06 06:34:39,250 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,250 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,250 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,250 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,250 - distributed.worker - INFO -               Threads:                          4
2024-02-06 06:34:39,250 - distributed.worker - INFO -               Threads:                          4
2024-02-06 06:34:39,250 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-06 06:34:39,250 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-06 06:34:39,250 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d3mywcev
2024-02-06 06:34:39,250 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-xuvsbntr
2024-02-06 06:34:39,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b3ad72e-cd85-47be-b2cd-96112e6c0a77
2024-02-06 06:34:39,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63488f38-e409-47ec-b210-12e10abf43b9
2024-02-06 06:34:39,251 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-526550fa-94c4-46fa-b247-f41b7817c1df
2024-02-06 06:34:39,251 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3336ea4a-14a2-4fe7-b7b5-fb0bbfb66015
2024-02-06 06:34:39,251 - distributed.worker - INFO - Starting Worker plugin PreImport-527b3bec-8794-414b-8d4a-84265bfad57d
2024-02-06 06:34:39,252 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,253 - distributed.worker - INFO - Starting Worker plugin PreImport-352eda45-7c28-461c-ab57-b20cb1514afe
2024-02-06 06:34:39,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:39,253 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:39,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:39,258 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38711
2024-02-06 06:34:39,258 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38711
2024-02-06 06:34:39,258 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41653
2024-02-06 06:34:39,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,258 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,258 - distributed.worker - INFO -               Threads:                          4
2024-02-06 06:34:39,258 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-06 06:34:39,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gmmv1s_z
2024-02-06 06:34:39,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ed4efeb-5a33-45d7-b0e8-6a803dc4a34c
2024-02-06 06:34:39,258 - distributed.worker - INFO - Starting Worker plugin PreImport-dd12b16e-9bb3-4834-98cd-11068c8fa89a
2024-02-06 06:34:39,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-725d3451-46a3-40e9-b580-3ec657b8063d
2024-02-06 06:34:39,259 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:39,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:39,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:39,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43805
2024-02-06 06:34:39,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43805
2024-02-06 06:34:39,310 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35347
2024-02-06 06:34:39,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,310 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,311 - distributed.worker - INFO -               Threads:                          4
2024-02-06 06:34:39,311 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-06 06:34:39,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qoxcpu7g
2024-02-06 06:34:39,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d651d2f7-1d47-4b6d-84c4-b26c96539ba5
2024-02-06 06:34:39,311 - distributed.worker - INFO - Starting Worker plugin PreImport-7ab1af4d-7f52-44b1-b94f-e10219da4dd6
2024-02-06 06:34:39,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25cc0e33-7069-4cad-bcfd-9496d39aacd5
2024-02-06 06:34:39,311 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36161', status: init, memory: 0, processing: 0>
2024-02-06 06:34:39,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36161
2024-02-06 06:34:39,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43606
2024-02-06 06:34:39,363 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:39,364 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,365 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-06 06:34:39,366 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35939', status: init, memory: 0, processing: 0>
2024-02-06 06:34:39,367 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35939
2024-02-06 06:34:39,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43616
2024-02-06 06:34:39,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:39,369 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,369 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,370 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38711', status: init, memory: 0, processing: 0>
2024-02-06 06:34:39,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-06 06:34:39,370 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38711
2024-02-06 06:34:39,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43620
2024-02-06 06:34:39,371 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:39,372 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,372 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-06 06:34:39,387 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43805', status: init, memory: 0, processing: 0>
2024-02-06 06:34:39,388 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43805
2024-02-06 06:34:39,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43630
2024-02-06 06:34:39,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:39,389 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-06 06:34:39,390 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:39,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-06 06:34:39,456 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-06 06:34:39,456 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-06 06:34:39,456 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-06 06:34:39,456 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-06 06:34:39,462 - distributed.scheduler - INFO - Remove client Client-cb809113-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:39,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43596; closing.
2024-02-06 06:34:39,462 - distributed.scheduler - INFO - Remove client Client-cb809113-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:39,462 - distributed.scheduler - INFO - Close client connection: Client-cb809113-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:39,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34051'. Reason: nanny-close
2024-02-06 06:34:39,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:39,464 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34113'. Reason: nanny-close
2024-02-06 06:34:39,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:39,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36109'. Reason: nanny-close
2024-02-06 06:34:39,465 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35939. Reason: nanny-close
2024-02-06 06:34:39,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:39,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45057'. Reason: nanny-close
2024-02-06 06:34:39,465 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38711. Reason: nanny-close
2024-02-06 06:34:39,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:39,466 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36161. Reason: nanny-close
2024-02-06 06:34:39,466 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43805. Reason: nanny-close
2024-02-06 06:34:39,467 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43616; closing.
2024-02-06 06:34:39,467 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-06 06:34:39,467 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35939', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201279.4676409')
2024-02-06 06:34:39,467 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-06 06:34:39,468 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-06 06:34:39,468 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-06 06:34:39,468 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:39,469 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:39,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43606; closing.
2024-02-06 06:34:39,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43630; closing.
2024-02-06 06:34:39,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43620; closing.
2024-02-06 06:34:39,469 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:39,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201279.4700065')
2024-02-06 06:34:39,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201279.4703348')
2024-02-06 06:34:39,470 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:39,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201279.47078')
2024-02-06 06:34:39,471 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:34:40,279 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:34:40,280 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:34:40,280 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:34:40,281 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-06 06:34:40,282 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-06 06:34:42,283 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:42,287 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36043 instead
  warnings.warn(
2024-02-06 06:34:42,291 - distributed.scheduler - INFO - State start
2024-02-06 06:34:42,311 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:42,311 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:34:42,312 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36043/status
2024-02-06 06:34:42,312 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:34:42,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45009'
2024-02-06 06:34:42,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37033'
2024-02-06 06:34:42,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36203'
2024-02-06 06:34:42,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42165'
2024-02-06 06:34:42,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35053'
2024-02-06 06:34:42,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37887'
2024-02-06 06:34:42,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38313'
2024-02-06 06:34:42,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37745'
2024-02-06 06:34:42,594 - distributed.scheduler - INFO - Receive client connection: Client-ce6c6c34-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:42,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59618
2024-02-06 06:34:44,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,166 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,167 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46085
2024-02-06 06:34:44,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46085
2024-02-06 06:34:44,168 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40129
2024-02-06 06:34:44,168 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,168 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,168 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,168 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,168 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q25ebu51
2024-02-06 06:34:44,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2732f51-e0b3-4bb5-b69e-b0617fe1979b
2024-02-06 06:34:44,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46863
2024-02-06 06:34:44,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46863
2024-02-06 06:34:44,399 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43599
2024-02-06 06:34:44,399 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,399 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,399 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,399 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,399 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xwyfsotz
2024-02-06 06:34:44,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07e97e50-1bf7-47e1-a03f-706ab93abaf3
2024-02-06 06:34:44,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,412 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34309
2024-02-06 06:34:44,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34309
2024-02-06 06:34:44,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34627
2024-02-06 06:34:44,413 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,413 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,413 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,413 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7iv4fbco
2024-02-06 06:34:44,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2705f7c9-52a3-42e0-a996-ffa3d32ab07f
2024-02-06 06:34:44,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33425
2024-02-06 06:34:44,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33425
2024-02-06 06:34:44,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44481
2024-02-06 06:34:44,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,433 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,433 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g74p7akh
2024-02-06 06:34:44,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51113283-bb37-4eb7-9257-c7eca672fd86
2024-02-06 06:34:44,433 - distributed.worker - INFO - Starting Worker plugin PreImport-6d554de6-623e-4cba-80f3-ba243253efe9
2024-02-06 06:34:44,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-310dc380-17b2-4d5e-9533-f67fc5ad5e7f
2024-02-06 06:34:44,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,449 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,450 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38395
2024-02-06 06:34:44,450 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38395
2024-02-06 06:34:44,450 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41483
2024-02-06 06:34:44,450 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,450 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,450 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,450 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,450 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x6dj_6f0
2024-02-06 06:34:44,450 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22950a60-1dc3-4073-8ecf-1ed0462a5739
2024-02-06 06:34:44,450 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4c87d04-eb01-4ff3-9251-7f3b8fca7498
2024-02-06 06:34:44,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40925
2024-02-06 06:34:44,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40925
2024-02-06 06:34:44,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32991
2024-02-06 06:34:44,468 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,468 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,468 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,468 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,468 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s82giwut
2024-02-06 06:34:44,468 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fb4e726-7394-4553-a0ee-7b96078e7eb5
2024-02-06 06:34:44,469 - distributed.worker - INFO - Starting Worker plugin PreImport-91f76108-f7bd-4e22-816c-8331bf9e1e09
2024-02-06 06:34:44,469 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9dc7a5ef-a2f8-45d1-a16d-3cd4553d0079
2024-02-06 06:34:44,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,479 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,479 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40941
2024-02-06 06:34:44,480 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40941
2024-02-06 06:34:44,480 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43597
2024-02-06 06:34:44,480 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,480 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,480 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,480 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,480 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-azq3ue2q
2024-02-06 06:34:44,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-329c10f4-54bd-4bae-aa89-263d009bc63d
2024-02-06 06:34:44,483 - distributed.worker - INFO - Starting Worker plugin PreImport-bc118367-d07e-467b-acd2-0b19bd31e712
2024-02-06 06:34:44,483 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb62be39-8910-4817-8c46-6c1c1fcd26cf
2024-02-06 06:34:44,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:44,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:44,525 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:44,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37519
2024-02-06 06:34:44,526 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37519
2024-02-06 06:34:44,526 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43255
2024-02-06 06:34:44,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:44,526 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:44,526 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:44,527 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:44,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1hu3sida
2024-02-06 06:34:44,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8276a2cb-a471-444b-888e-b9ecaa95619f
2024-02-06 06:34:44,976 - distributed.worker - INFO - Starting Worker plugin PreImport-927a29e2-7b8f-40b6-8f77-4249e5f3fcb7
2024-02-06 06:34:44,977 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fcc1fd0-6166-485e-bc24-95aaba178d1e
2024-02-06 06:34:44,978 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:45,006 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46085', status: init, memory: 0, processing: 0>
2024-02-06 06:34:45,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46085
2024-02-06 06:34:45,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59642
2024-02-06 06:34:45,008 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:45,009 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:45,009 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:45,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,181 - distributed.worker - INFO - Starting Worker plugin PreImport-76bfb0a0-1f94-4a83-a22e-7114aa2dc5a9
2024-02-06 06:34:46,181 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c11a32e5-01dd-4497-9d52-cbee73f9d467
2024-02-06 06:34:46,182 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,194 - distributed.worker - INFO - Starting Worker plugin PreImport-97ffc6b7-d0d3-464c-a2df-e7f43c63a7b0
2024-02-06 06:34:46,194 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51331289-8835-425c-8c31-92f4a6078cb8
2024-02-06 06:34:46,195 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,215 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46863', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46863
2024-02-06 06:34:46,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59646
2024-02-06 06:34:46,217 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,218 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,218 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34309', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34309
2024-02-06 06:34:46,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59648
2024-02-06 06:34:46,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,221 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,221 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,256 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,282 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33425', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,282 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33425
2024-02-06 06:34:46,282 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59654
2024-02-06 06:34:46,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,284 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,285 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,303 - distributed.worker - INFO - Starting Worker plugin PreImport-68ab3421-77ac-49af-beb2-45b32f5934d5
2024-02-06 06:34:46,303 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,316 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,326 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38395', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,326 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,327 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38395
2024-02-06 06:34:46,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59662
2024-02-06 06:34:46,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,329 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,329 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,333 - distributed.worker - INFO - Starting Worker plugin PreImport-869191c9-cad1-4911-85c8-8bd3888dbee6
2024-02-06 06:34:46,334 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0d933f2-edb6-4a85-ab3c-d6c8c94dc3e1
2024-02-06 06:34:46,335 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,348 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40925', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,349 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40925
2024-02-06 06:34:46,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59672
2024-02-06 06:34:46,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,351 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,352 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,358 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37519', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37519
2024-02-06 06:34:46,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59690
2024-02-06 06:34:46,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40941', status: init, memory: 0, processing: 0>
2024-02-06 06:34:46,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,360 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40941
2024-02-06 06:34:46,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59682
2024-02-06 06:34:46,360 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,360 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:46,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,362 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:46,363 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:46,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:46,410 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,410 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,410 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,410 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,411 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:46,415 - distributed.scheduler - INFO - Remove client Client-ce6c6c34-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:46,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59618; closing.
2024-02-06 06:34:46,416 - distributed.scheduler - INFO - Remove client Client-ce6c6c34-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:46,417 - distributed.scheduler - INFO - Close client connection: Client-ce6c6c34-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:46,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45009'. Reason: nanny-close
2024-02-06 06:34:46,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37033'. Reason: nanny-close
2024-02-06 06:34:46,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,420 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36203'. Reason: nanny-close
2024-02-06 06:34:46,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46085. Reason: nanny-close
2024-02-06 06:34:46,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42165'. Reason: nanny-close
2024-02-06 06:34:46,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46863. Reason: nanny-close
2024-02-06 06:34:46,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35053'. Reason: nanny-close
2024-02-06 06:34:46,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33425. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37887'. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34309. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38313'. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40925. Reason: nanny-close
2024-02-06 06:34:46,422 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59642; closing.
2024-02-06 06:34:46,423 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37745'. Reason: nanny-close
2024-02-06 06:34:46,423 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40941. Reason: nanny-close
2024-02-06 06:34:46,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46085', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.4233303')
2024-02-06 06:34:46,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:46,423 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38395. Reason: nanny-close
2024-02-06 06:34:46,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37519. Reason: nanny-close
2024-02-06 06:34:46,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,424 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,425 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59646; closing.
2024-02-06 06:34:46,425 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,425 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59654; closing.
2024-02-06 06:34:46,425 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,425 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59648; closing.
2024-02-06 06:34:46,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46863', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.4262738')
2024-02-06 06:34:46,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:46,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33425', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.426622')
2024-02-06 06:34:46,427 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.4273574')
2024-02-06 06:34:46,427 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59672; closing.
2024-02-06 06:34:46,428 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,428 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:46,428 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.428674')
2024-02-06 06:34:46,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59682; closing.
2024-02-06 06:34:46,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59662; closing.
2024-02-06 06:34:46,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59690; closing.
2024-02-06 06:34:46,429 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40941', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.4297397')
2024-02-06 06:34:46,430 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.430204')
2024-02-06 06:34:46,430 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201286.4306064')
2024-02-06 06:34:46,430 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:34:47,333 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:34:47,334 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:34:47,334 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:34:47,335 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:34:47,336 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-06 06:34:49,289 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:49,293 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45589 instead
  warnings.warn(
2024-02-06 06:34:49,296 - distributed.scheduler - INFO - State start
2024-02-06 06:34:49,316 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:49,317 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:34:49,318 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45589/status
2024-02-06 06:34:49,318 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:34:49,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37853'
2024-02-06 06:34:49,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33589'
2024-02-06 06:34:49,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34635'
2024-02-06 06:34:49,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40273'
2024-02-06 06:34:49,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33467'
2024-02-06 06:34:49,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33639'
2024-02-06 06:34:49,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34507'
2024-02-06 06:34:49,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37599'
2024-02-06 06:34:49,926 - distributed.scheduler - INFO - Receive client connection: Client-d2986719-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:49,937 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46844
2024-02-06 06:34:51,386 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40121
2024-02-06 06:34:51,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37999
2024-02-06 06:34:51,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37999
2024-02-06 06:34:51,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40121
2024-02-06 06:34:51,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38389
2024-02-06 06:34:51,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45477
2024-02-06 06:34:51,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37347
2024-02-06 06:34:51,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38389
2024-02-06 06:34:51,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40895
2024-02-06 06:34:51,392 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,392 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,392 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,392 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,392 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,392 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kq4pkb0k
2024-02-06 06:34:51,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2jshpc1h
2024-02-06 06:34:51,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_bz28kwm
2024-02-06 06:34:51,392 - distributed.worker - INFO - Starting Worker plugin PreImport-18a2c412-dc29-4abd-a259-cc3a4c17c61e
2024-02-06 06:34:51,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5f1ae2e-158a-4bec-8619-b2318f0a09c8
2024-02-06 06:34:51,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bb57da6-1b65-4e15-b545-745fdf839152
2024-02-06 06:34:51,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f1699c6-1759-49ee-82cd-bf43b0c87c8b
2024-02-06 06:34:51,393 - distributed.worker - INFO - Starting Worker plugin PreImport-b9e57c1e-139d-4d54-9e63-0b59075e3127
2024-02-06 06:34:51,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb10c6f4-2afa-41d4-b52c-8c8fb83b8f34
2024-02-06 06:34:51,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,427 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37007
2024-02-06 06:34:51,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37007
2024-02-06 06:34:51,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33427
2024-02-06 06:34:51,427 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,427 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,427 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,427 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42147
2024-02-06 06:34:51,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42147
2024-02-06 06:34:51,427 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,427 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_w89v18y
2024-02-06 06:34:51,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36451
2024-02-06 06:34:51,427 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,427 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,427 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,427 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b5e0a5f-2669-4955-be41-6ae5938cd5b4
2024-02-06 06:34:51,427 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sgsfa51m
2024-02-06 06:34:51,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee603e7a-4bf2-4149-ab49-d203977faac9
2024-02-06 06:34:51,428 - distributed.worker - INFO - Starting Worker plugin PreImport-249fe917-6895-488d-930a-271b60a4d6ff
2024-02-06 06:34:51,428 - distributed.worker - INFO - Starting Worker plugin PreImport-878392a1-d591-47ac-bf26-b4560c0cdba0
2024-02-06 06:34:51,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0bcf1039-0f5f-485f-90c6-e577c52c683c
2024-02-06 06:34:51,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b63cdb44-5f7c-4e2b-81f7-271c4c393270
2024-02-06 06:34:51,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,434 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45265
2024-02-06 06:34:51,434 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45265
2024-02-06 06:34:51,434 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35233
2024-02-06 06:34:51,435 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,435 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,435 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,435 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xadu220i
2024-02-06 06:34:51,435 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d587419-3c39-4b4d-a2e7-a38a7bd4b309
2024-02-06 06:34:51,435 - distributed.worker - INFO - Starting Worker plugin PreImport-96940b65-f80f-41b8-b410-d2e53f2f8580
2024-02-06 06:34:51,435 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c58af4f-316f-4111-a2c7-16809a156327
2024-02-06 06:34:51,438 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,442 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,443 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37739
2024-02-06 06:34:51,443 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37739
2024-02-06 06:34:51,443 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37623
2024-02-06 06:34:51,443 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,443 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,443 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,443 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,443 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-awylgwek
2024-02-06 06:34:51,444 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ee4b582-9a66-4194-9bb4-351b2c719d72
2024-02-06 06:34:51,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:51,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:51,465 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:51,466 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38197
2024-02-06 06:34:51,466 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38197
2024-02-06 06:34:51,466 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39155
2024-02-06 06:34:51,466 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:51,466 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:51,466 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:51,466 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:51,466 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5vcxwpj5
2024-02-06 06:34:51,466 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43faf39c-2ce5-440b-a63b-d1d6b2e5b6fa
2024-02-06 06:34:51,466 - distributed.worker - INFO - Starting Worker plugin PreImport-e0a8e27a-3338-4642-9a92-73472ab3436b
2024-02-06 06:34:51,467 - distributed.worker - INFO - Starting Worker plugin RMMSetup-150c7519-6d83-4198-99b0-829eb15b397a
2024-02-06 06:34:53,281 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,302 - distributed.worker - INFO - Starting Worker plugin PreImport-794dd6b6-b181-43c7-92cd-097ae5c13895
2024-02-06 06:34:53,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d6f5cec-250b-4816-bab8-b32781500c34
2024-02-06 06:34:53,303 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-080044cb-4961-44f1-8ed1-23c06ca9b1c6
2024-02-06 06:34:53,306 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,314 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37999', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,315 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37999
2024-02-06 06:34:53,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46860
2024-02-06 06:34:53,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,317 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,317 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38389', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38389
2024-02-06 06:34:53,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46866
2024-02-06 06:34:53,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,327 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,336 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40121', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40121
2024-02-06 06:34:53,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46870
2024-02-06 06:34:53,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,339 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,377 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,377 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,396 - distributed.worker - INFO - Starting Worker plugin PreImport-32af6c63-4979-4ebc-bc61-d7e3fb61e111
2024-02-06 06:34:53,396 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d11e5a66-652f-49ba-8ca5-f78ff2071da3
2024-02-06 06:34:53,397 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,398 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,400 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,404 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42147', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42147
2024-02-06 06:34:53,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46886
2024-02-06 06:34:53,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,406 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,409 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37007', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,410 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37007
2024-02-06 06:34:53,410 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46898
2024-02-06 06:34:53,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,412 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,412 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,421 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38197', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,421 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38197
2024-02-06 06:34:53,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46910
2024-02-06 06:34:53,422 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45265', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,422 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45265
2024-02-06 06:34:53,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46924
2024-02-06 06:34:53,423 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,423 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,424 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,424 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37739', status: init, memory: 0, processing: 0>
2024-02-06 06:34:53,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37739
2024-02-06 06:34:53,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46916
2024-02-06 06:34:53,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:34:53,435 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:34:53,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:53,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,510 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:34:53,514 - distributed.scheduler - INFO - Remove client Client-d2986719-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:53,514 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46844; closing.
2024-02-06 06:34:53,515 - distributed.scheduler - INFO - Remove client Client-d2986719-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:53,515 - distributed.scheduler - INFO - Close client connection: Client-d2986719-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:53,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37853'. Reason: nanny-close
2024-02-06 06:34:53,516 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33589'. Reason: nanny-close
2024-02-06 06:34:53,517 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34635'. Reason: nanny-close
2024-02-06 06:34:53,518 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40121. Reason: nanny-close
2024-02-06 06:34:53,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40273'. Reason: nanny-close
2024-02-06 06:34:53,518 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37999. Reason: nanny-close
2024-02-06 06:34:53,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33467'. Reason: nanny-close
2024-02-06 06:34:53,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38389. Reason: nanny-close
2024-02-06 06:34:53,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33639'. Reason: nanny-close
2024-02-06 06:34:53,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42147. Reason: nanny-close
2024-02-06 06:34:53,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34507'. Reason: nanny-close
2024-02-06 06:34:53,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37739. Reason: nanny-close
2024-02-06 06:34:53,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37599'. Reason: nanny-close
2024-02-06 06:34:53,520 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37007. Reason: nanny-close
2024-02-06 06:34:53,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46870; closing.
2024-02-06 06:34:53,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:34:53,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45265. Reason: nanny-close
2024-02-06 06:34:53,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5212064')
2024-02-06 06:34:53,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38197. Reason: nanny-close
2024-02-06 06:34:53,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,521 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46886; closing.
2024-02-06 06:34:53,522 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5224395')
2024-02-06 06:34:53,522 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,522 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,522 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46866; closing.
2024-02-06 06:34:53,523 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:34:53,523 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5239131')
2024-02-06 06:34:53,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46860; closing.
2024-02-06 06:34:53,524 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,524 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,525 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,525 - distributed.nanny - INFO - Worker closed
2024-02-06 06:34:53,525 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46886>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-06 06:34:53,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46916; closing.
2024-02-06 06:34:53,527 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.52718')
2024-02-06 06:34:53,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46898; closing.
2024-02-06 06:34:53,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46924; closing.
2024-02-06 06:34:53,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37739', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5281832')
2024-02-06 06:34:53,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5285635')
2024-02-06 06:34:53,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5289433')
2024-02-06 06:34:53,529 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46910; closing.
2024-02-06 06:34:53,529 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201293.5296755')
2024-02-06 06:34:53,529 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:34:54,331 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:34:54,332 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:34:54,332 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:34:54,333 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:34:54,334 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-06 06:34:56,277 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:56,281 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35247 instead
  warnings.warn(
2024-02-06 06:34:56,285 - distributed.scheduler - INFO - State start
2024-02-06 06:34:56,305 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:34:56,306 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:34:56,307 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35247/status
2024-02-06 06:34:56,307 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:34:56,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38373'
2024-02-06 06:34:56,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34515'
2024-02-06 06:34:56,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33297'
2024-02-06 06:34:56,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34377'
2024-02-06 06:34:56,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35183'
2024-02-06 06:34:56,488 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40523'
2024-02-06 06:34:56,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33553'
2024-02-06 06:34:56,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46661'
2024-02-06 06:34:56,650 - distributed.scheduler - INFO - Receive client connection: Client-d6c5fe18-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:34:56,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47078
2024-02-06 06:34:58,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,349 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,352 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,353 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42409
2024-02-06 06:34:58,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36633
2024-02-06 06:34:58,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42409
2024-02-06 06:34:58,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36633
2024-02-06 06:34:58,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45179
2024-02-06 06:34:58,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41311
2024-02-06 06:34:58,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,353 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,353 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,353 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,353 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q85qtttg
2024-02-06 06:34:58,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-54oiu_l5
2024-02-06 06:34:58,354 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5bd20e0d-e308-4c63-85be-c607aa9aa3c0
2024-02-06 06:34:58,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3303a6bc-33f6-4a7f-a1ad-fed1f70f9f6a
2024-02-06 06:34:58,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,354 - distributed.worker - INFO - Starting Worker plugin PreImport-19de1cd9-937b-417a-98c6-486c48064a62
2024-02-06 06:34:58,354 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ff474c4-9382-4f6a-ab13-e76550deebf9
2024-02-06 06:34:58,355 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45619
2024-02-06 06:34:58,355 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45619
2024-02-06 06:34:58,355 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37451
2024-02-06 06:34:58,355 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,355 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,355 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,355 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,355 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-24ge_z3c
2024-02-06 06:34:58,355 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d084516e-b2a1-401b-a5fd-937a49e48bd3
2024-02-06 06:34:58,355 - distributed.worker - INFO - Starting Worker plugin PreImport-7b0bed02-49f7-4112-8ac4-3dcf677dbe1b
2024-02-06 06:34:58,355 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2310396-6e71-4c4e-b7a3-b84ba8d7f450
2024-02-06 06:34:58,357 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,358 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34313
2024-02-06 06:34:58,358 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34313
2024-02-06 06:34:58,358 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33869
2024-02-06 06:34:58,358 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,358 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,358 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,358 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,358 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8pr3kn99
2024-02-06 06:34:58,358 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e76d9e9b-4aa4-42d4-8c6f-fa7ef39c0a7d
2024-02-06 06:34:58,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43083
2024-02-06 06:34:58,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43479
2024-02-06 06:34:58,394 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43083
2024-02-06 06:34:58,394 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43479
2024-02-06 06:34:58,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40777
2024-02-06 06:34:58,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44697
2024-02-06 06:34:58,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,394 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,394 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,394 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,394 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-11opwa1u
2024-02-06 06:34:58,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wskf8htl
2024-02-06 06:34:58,394 - distributed.worker - INFO - Starting Worker plugin PreImport-3739b8be-5aed-4d11-8736-e193c200303d
2024-02-06 06:34:58,395 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9acc5ba-0e9c-4ee4-8f9b-1eed43cf2e5f
2024-02-06 06:34:58,395 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04e39832-f0a5-43a1-9800-8a7e2bf7b567
2024-02-06 06:34:58,395 - distributed.worker - INFO - Starting Worker plugin PreImport-ae4f19d0-8b6e-484a-b39e-57c6e9f2c8ad
2024-02-06 06:34:58,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4664febc-0c1a-4201-9431-01963b60d7b9
2024-02-06 06:34:58,399 - distributed.worker - INFO - Starting Worker plugin RMMSetup-decb246b-5511-4883-9090-864c45661ca7
2024-02-06 06:34:58,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43619
2024-02-06 06:34:58,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43619
2024-02-06 06:34:58,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35637
2024-02-06 06:34:58,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,418 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,418 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cdj6u9z7
2024-02-06 06:34:58,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7756b5c-ed75-4bef-aedd-4d8baa095a64
2024-02-06 06:34:58,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:34:58,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:34:58,427 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:34:58,428 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43725
2024-02-06 06:34:58,428 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43725
2024-02-06 06:34:58,428 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45207
2024-02-06 06:34:58,428 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:34:58,428 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:34:58,428 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:34:58,428 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:34:58,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_v_vo7ii
2024-02-06 06:34:58,429 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75be1930-f685-409a-a39c-d8a3083a78d6
2024-02-06 06:34:58,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0068cb84-3de3-43ed-86c3-19e33f04de85
2024-02-06 06:35:00,446 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,474 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45619', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,476 - distributed.worker - INFO - Starting Worker plugin PreImport-59d85947-8440-4757-88bd-b57af9ad0ce5
2024-02-06 06:35:00,477 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45619
2024-02-06 06:35:00,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35220
2024-02-06 06:35:00,477 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ac00c5a-3ec6-4afa-9847-f34057f948be
2024-02-06 06:35:00,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,478 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,479 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,479 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,489 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,515 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42409', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,516 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42409
2024-02-06 06:35:00,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35224
2024-02-06 06:35:00,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,519 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,520 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,520 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36633', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,521 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36633
2024-02-06 06:35:00,521 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35234
2024-02-06 06:35:00,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,522 - distributed.worker - INFO - Starting Worker plugin PreImport-c489bbac-dd37-4f12-963c-ab67eadb7d36
2024-02-06 06:35:00,523 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e98f5471-c8a4-4977-b84f-d946b5e9aefd
2024-02-06 06:35:00,523 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,523 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,524 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,524 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34313', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34313
2024-02-06 06:35:00,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35248
2024-02-06 06:35:00,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,548 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,548 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,551 - distributed.worker - INFO - Starting Worker plugin PreImport-e3b9a9c6-21fa-41ed-8710-a35d352d88b4
2024-02-06 06:35:00,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ec3b3e5-33e8-4cd8-b469-730013f00fad
2024-02-06 06:35:00,552 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,554 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,558 - distributed.worker - INFO - Starting Worker plugin PreImport-0e0734a7-4858-4e51-b4d1-26253c1369d6
2024-02-06 06:35:00,559 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,562 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,576 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43619', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,577 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43619
2024-02-06 06:35:00,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35250
2024-02-06 06:35:00,578 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,579 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,579 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43725', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,583 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43725
2024-02-06 06:35:00,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35274
2024-02-06 06:35:00,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,585 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,585 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43479', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43479
2024-02-06 06:35:00,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35260
2024-02-06 06:35:00,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,591 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,593 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43083', status: init, memory: 0, processing: 0>
2024-02-06 06:35:00,593 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43083
2024-02-06 06:35:00,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35290
2024-02-06 06:35:00,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:00,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:00,596 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:00,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:00,690 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,702 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:00,710 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:00,712 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:00,714 - distributed.scheduler - INFO - Remove client Client-d6c5fe18-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:00,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47078; closing.
2024-02-06 06:35:00,715 - distributed.scheduler - INFO - Remove client Client-d6c5fe18-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:00,715 - distributed.scheduler - INFO - Close client connection: Client-d6c5fe18-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:00,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38373'. Reason: nanny-close
2024-02-06 06:35:00,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34515'. Reason: nanny-close
2024-02-06 06:35:00,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,718 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33297'. Reason: nanny-close
2024-02-06 06:35:00,718 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43083. Reason: nanny-close
2024-02-06 06:35:00,718 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,718 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34377'. Reason: nanny-close
2024-02-06 06:35:00,718 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42409. Reason: nanny-close
2024-02-06 06:35:00,718 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35183'. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45619. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40523'. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34313. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33553'. Reason: nanny-close
2024-02-06 06:35:00,720 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36633. Reason: nanny-close
2024-02-06 06:35:00,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46661'. Reason: nanny-close
2024-02-06 06:35:00,720 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43479. Reason: nanny-close
2024-02-06 06:35:00,720 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:00,720 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35290; closing.
2024-02-06 06:35:00,721 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43725. Reason: nanny-close
2024-02-06 06:35:00,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,721 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43083', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.7212942')
2024-02-06 06:35:00,721 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43619. Reason: nanny-close
2024-02-06 06:35:00,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,722 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35224; closing.
2024-02-06 06:35:00,722 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35220; closing.
2024-02-06 06:35:00,722 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,722 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,722 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,723 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,723 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:00,723 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42409', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.7238128')
2024-02-06 06:35:00,724 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.7241693')
2024-02-06 06:35:00,724 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,724 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,724 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35248; closing.
2024-02-06 06:35:00,724 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,725 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:00,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35234; closing.
2024-02-06 06:35:00,725 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.7259135')
2024-02-06 06:35:00,726 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35260; closing.
2024-02-06 06:35:00,726 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35274; closing.
2024-02-06 06:35:00,726 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.7268524')
2024-02-06 06:35:00,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.727299')
2024-02-06 06:35:00,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43725', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.727778')
2024-02-06 06:35:00,728 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35250; closing.
2024-02-06 06:35:00,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201300.7286217')
2024-02-06 06:35:00,728 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:01,632 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:01,632 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:01,632 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:01,633 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:01,634 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-06 06:35:03,788 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:03,792 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33625 instead
  warnings.warn(
2024-02-06 06:35:03,796 - distributed.scheduler - INFO - State start
2024-02-06 06:35:03,818 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:03,819 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:03,819 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33625/status
2024-02-06 06:35:03,820 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:03,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37659'
2024-02-06 06:35:03,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34931'
2024-02-06 06:35:03,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33681'
2024-02-06 06:35:03,863 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33843'
2024-02-06 06:35:03,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44275'
2024-02-06 06:35:03,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43129'
2024-02-06 06:35:03,892 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33013'
2024-02-06 06:35:03,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38933'
2024-02-06 06:35:04,756 - distributed.scheduler - INFO - Receive client connection: Client-db27209e-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:04,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35690
2024-02-06 06:35:05,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,707 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,707 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45887
2024-02-06 06:35:05,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34867
2024-02-06 06:35:05,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45887
2024-02-06 06:35:05,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34867
2024-02-06 06:35:05,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33195
2024-02-06 06:35:05,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34017
2024-02-06 06:35:05,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,708 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,708 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,708 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,708 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1isn0q6e
2024-02-06 06:35:05,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_3c7odiw
2024-02-06 06:35:05,708 - distributed.worker - INFO - Starting Worker plugin PreImport-f6b7d640-da17-4807-be6c-6fe5c82ce186
2024-02-06 06:35:05,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6970b36-1cbf-46cf-af8e-2f270fa912d7
2024-02-06 06:35:05,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f42aeec-0d97-45dd-a14a-ed575b91c146
2024-02-06 06:35:05,711 - distributed.worker - INFO - Starting Worker plugin PreImport-ba9cec5b-e8b5-42b2-8b60-fc5dfc1b726a
2024-02-06 06:35:05,711 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1eb9acd-330a-416a-a273-10aac1efcf7f
2024-02-06 06:35:05,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0021dced-13c8-4b64-836d-13f1639d15fb
2024-02-06 06:35:05,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,783 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43377
2024-02-06 06:35:05,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43377
2024-02-06 06:35:05,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45315
2024-02-06 06:35:05,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,784 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,785 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,785 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,785 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tzscdhlj
2024-02-06 06:35:05,785 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b1fe177-d372-4746-95dd-32de15ad037a
2024-02-06 06:35:05,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41503
2024-02-06 06:35:05,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46695
2024-02-06 06:35:05,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41503
2024-02-06 06:35:05,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46695
2024-02-06 06:35:05,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39699
2024-02-06 06:35:05,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37253
2024-02-06 06:35:05,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,794 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,794 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,794 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,794 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ndzdr70w
2024-02-06 06:35:05,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5y7tqned
2024-02-06 06:35:05,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62d3060c-7975-4980-b0df-3120a68901b0
2024-02-06 06:35:05,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-266d6367-e43c-4738-a252-a04818e6d472
2024-02-06 06:35:05,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,813 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,814 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46545
2024-02-06 06:35:05,814 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46545
2024-02-06 06:35:05,814 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39877
2024-02-06 06:35:05,814 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,814 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,814 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,815 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,815 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rqw3ssq4
2024-02-06 06:35:05,815 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d21a016d-ee37-45be-b042-09faff393dba
2024-02-06 06:35:05,815 - distributed.worker - INFO - Starting Worker plugin PreImport-4003c115-8cc3-4cb5-b1b1-9da7f596afef
2024-02-06 06:35:05,815 - distributed.worker - INFO - Starting Worker plugin RMMSetup-979760dd-d03e-4dd2-b592-48a39153e38b
2024-02-06 06:35:05,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33373
2024-02-06 06:35:05,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33373
2024-02-06 06:35:05,826 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38387
2024-02-06 06:35:05,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,826 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,826 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gcb6aek2
2024-02-06 06:35:05,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-122d18ab-d735-4873-b3d5-41a1ec117e01
2024-02-06 06:35:05,827 - distributed.worker - INFO - Starting Worker plugin PreImport-13dd3e9c-20f0-44ab-8994-f7b6c0d76f22
2024-02-06 06:35:05,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74bb6fbe-92a4-4ca6-96f4-ca25fcf7d340
2024-02-06 06:35:05,980 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:05,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:05,986 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:05,988 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45199
2024-02-06 06:35:05,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45199
2024-02-06 06:35:05,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35729
2024-02-06 06:35:05,988 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:05,988 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:05,988 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:05,988 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:05,988 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tgw_59kh
2024-02-06 06:35:05,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4f7edef-086f-42ac-992b-6ec37dfdb3ab
2024-02-06 06:35:05,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51823218-4e74-4e90-88a3-24db70bed93f
2024-02-06 06:35:07,859 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34867', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34867
2024-02-06 06:35:07,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35708
2024-02-06 06:35:07,899 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,901 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:07,912 - distributed.worker - INFO - Starting Worker plugin PreImport-3cf53dfd-dc99-4412-bd5e-e759f7ed2074
2024-02-06 06:35:07,913 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef8af1fe-2b46-43b6-aa3b-8cb8ff5f7e3f
2024-02-06 06:35:07,913 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,932 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45887', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,933 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45887
2024-02-06 06:35:07,933 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35714
2024-02-06 06:35:07,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,935 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,936 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,937 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:07,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43377', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,938 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43377
2024-02-06 06:35:07,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35726
2024-02-06 06:35:07,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,940 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,940 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:07,952 - distributed.worker - INFO - Starting Worker plugin PreImport-3783106a-2414-4a34-9dbf-dae5136a7d03
2024-02-06 06:35:07,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-132006c4-da23-4957-9f6c-a0f91c7ea1bd
2024-02-06 06:35:07,954 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,955 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,967 - distributed.worker - INFO - Starting Worker plugin PreImport-60c5df78-dd44-4820-bcea-07f3dea3071a
2024-02-06 06:35:07,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4139d71-e79a-48de-9319-da5dba5f203e
2024-02-06 06:35:07,968 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,971 - distributed.worker - INFO - Starting Worker plugin PreImport-8b153016-b6f1-463c-871d-351337d7005a
2024-02-06 06:35:07,971 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,973 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,980 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33373', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,981 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33373
2024-02-06 06:35:07,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35742
2024-02-06 06:35:07,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,983 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,983 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:07,987 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41503', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,987 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41503
2024-02-06 06:35:07,987 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35750
2024-02-06 06:35:07,989 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,990 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,990 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:07,992 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45199', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,993 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45199
2024-02-06 06:35:07,993 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35774
2024-02-06 06:35:07,994 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46695', status: init, memory: 0, processing: 0>
2024-02-06 06:35:07,994 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,994 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46695
2024-02-06 06:35:07,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35766
2024-02-06 06:35:07,994 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,994 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:07,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:07,996 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:07,996 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:07,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:08,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46545', status: init, memory: 0, processing: 0>
2024-02-06 06:35:08,008 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46545
2024-02-06 06:35:08,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35782
2024-02-06 06:35:08,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:08,010 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:08,010 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:08,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:08,068 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,068 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,068 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,068 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,069 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,069 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,069 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,069 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,080 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,080 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,080 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,080 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,081 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,081 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,081 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,081 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:08,089 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,090 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:08,093 - distributed.scheduler - INFO - Remove client Client-db27209e-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:08,093 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35690; closing.
2024-02-06 06:35:08,093 - distributed.scheduler - INFO - Remove client Client-db27209e-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:08,093 - distributed.scheduler - INFO - Close client connection: Client-db27209e-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:08,094 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34931'. Reason: nanny-close
2024-02-06 06:35:08,095 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,095 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33843'. Reason: nanny-close
2024-02-06 06:35:08,095 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44275'. Reason: nanny-close
2024-02-06 06:35:08,096 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,096 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45887. Reason: nanny-close
2024-02-06 06:35:08,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33681'. Reason: nanny-close
2024-02-06 06:35:08,096 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,096 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41503. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37659'. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33373. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38933'. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43377. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33013'. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34867. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43129'. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46545. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45199. Reason: nanny-close
2024-02-06 06:35:08,098 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,098 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,099 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35742; closing.
2024-02-06 06:35:08,099 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35714; closing.
2024-02-06 06:35:08,099 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,099 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,099 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46695. Reason: nanny-close
2024-02-06 06:35:08,099 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.0995092')
2024-02-06 06:35:08,100 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45887', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.1002083')
2024-02-06 06:35:08,100 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,100 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,100 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,100 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,100 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,101 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,101 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,101 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35750; closing.
2024-02-06 06:35:08,101 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:08,101 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35726; closing.
2024-02-06 06:35:08,101 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,102 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,102 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,102 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.1026304')
2024-02-06 06:35:08,102 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:08,103 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.1030502')
2024-02-06 06:35:08,103 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35708; closing.
2024-02-06 06:35:08,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.1044457')
2024-02-06 06:35:08,104 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35774; closing.
2024-02-06 06:35:08,105 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35782; closing.
2024-02-06 06:35:08,105 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35766; closing.
2024-02-06 06:35:08,105 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.1056995')
2024-02-06 06:35:08,106 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46545', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.106068')
2024-02-06 06:35:08,106 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201308.1064196')
2024-02-06 06:35:08,106 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:09,111 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:09,111 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:09,112 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:09,113 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:09,113 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-06 06:35:11,229 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:11,233 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36345 instead
  warnings.warn(
2024-02-06 06:35:11,238 - distributed.scheduler - INFO - State start
2024-02-06 06:35:11,258 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:11,259 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:11,260 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36345/status
2024-02-06 06:35:11,260 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:11,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38759'
2024-02-06 06:35:11,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40491'
2024-02-06 06:35:11,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45177'
2024-02-06 06:35:11,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45907'
2024-02-06 06:35:11,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39889'
2024-02-06 06:35:11,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45501'
2024-02-06 06:35:11,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37045'
2024-02-06 06:35:11,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34003'
2024-02-06 06:35:12,204 - distributed.scheduler - INFO - Receive client connection: Client-dfa0c3a4-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:12,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48186
2024-02-06 06:35:12,466 - distributed.scheduler - INFO - Receive client connection: Client-e165d8f5-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:12,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48204
2024-02-06 06:35:13,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,283 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,284 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34871
2024-02-06 06:35:13,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34871
2024-02-06 06:35:13,285 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40029
2024-02-06 06:35:13,285 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,285 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,285 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,285 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ou3w3ti
2024-02-06 06:35:13,285 - distributed.worker - INFO - Starting Worker plugin RMMSetup-546f3393-7fcd-4aa8-bcce-67b4d0149330
2024-02-06 06:35:13,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41807
2024-02-06 06:35:13,301 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41807
2024-02-06 06:35:13,301 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34675
2024-02-06 06:35:13,301 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,301 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,301 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q_7e1wui
2024-02-06 06:35:13,302 - distributed.worker - INFO - Starting Worker plugin RMMSetup-abb83988-ef10-453c-b3cb-a3c49661ddc6
2024-02-06 06:35:13,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39247
2024-02-06 06:35:13,541 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39247
2024-02-06 06:35:13,541 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33181
2024-02-06 06:35:13,541 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,541 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,541 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-koyqga37
2024-02-06 06:35:13,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e37c5be7-26f3-4d2c-8408-34710f6f6eb5
2024-02-06 06:35:13,542 - distributed.worker - INFO - Starting Worker plugin PreImport-7a967444-4df8-4938-87f5-c2184a0d4f30
2024-02-06 06:35:13,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba9bba0f-6feb-4d0a-9dcd-fafdd191f6f0
2024-02-06 06:35:13,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,554 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,554 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,555 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41867
2024-02-06 06:35:13,555 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41867
2024-02-06 06:35:13,555 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39249
2024-02-06 06:35:13,555 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,555 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,555 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,556 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,556 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rk74frx6
2024-02-06 06:35:13,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,556 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3aa7c712-b9d6-4c4a-bf87-4914e8ee7e2f
2024-02-06 06:35:13,556 - distributed.worker - INFO - Starting Worker plugin PreImport-fa02c906-78a2-4789-8b3f-355ee0e8b4f4
2024-02-06 06:35:13,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6522b8e5-b9e6-4adf-8338-664efc5b9289
2024-02-06 06:35:13,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:13,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:13,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,562 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44731
2024-02-06 06:35:13,562 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44731
2024-02-06 06:35:13,562 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39909
2024-02-06 06:35:13,562 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,562 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,562 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,562 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kszjxew8
2024-02-06 06:35:13,563 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b60fb5b0-5e4b-4f60-aff0-e3b7d44d967f
2024-02-06 06:35:13,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,563 - distributed.worker - INFO - Starting Worker plugin PreImport-1ba7a759-3e47-4fe6-b7e8-78acdd3d8175
2024-02-06 06:35:13,564 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30d5d826-4099-4103-bade-f1b98f0c7ce9
2024-02-06 06:35:13,564 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44023
2024-02-06 06:35:13,564 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44023
2024-02-06 06:35:13,564 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32981
2024-02-06 06:35:13,564 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,564 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,564 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44719
2024-02-06 06:35:13,564 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,564 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44719
2024-02-06 06:35:13,564 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,564 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45651
2024-02-06 06:35:13,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jmvgebea
2024-02-06 06:35:13,565 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,565 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,565 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,565 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,565 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j6qs9oh2
2024-02-06 06:35:13,565 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ff029e5-b03c-4861-a5ab-d5d182f5271e
2024-02-06 06:35:13,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c6fd173-2733-4283-b5ee-2e27f22629b6
2024-02-06 06:35:13,565 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:13,566 - distributed.worker - INFO - Starting Worker plugin PreImport-dc06eb9c-17a5-48ec-9c6c-3c3a9e9197f9
2024-02-06 06:35:13,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d99520f-708a-4b76-8c43-fa8077526ee9
2024-02-06 06:35:13,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43727
2024-02-06 06:35:13,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43727
2024-02-06 06:35:13,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34703
2024-02-06 06:35:13,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,567 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,567 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:13,567 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:13,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p20d8_9r
2024-02-06 06:35:13,568 - distributed.worker - INFO - Starting Worker plugin PreImport-99a22caf-5473-4adb-8157-2627916d0cd3
2024-02-06 06:35:13,568 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8536b728-2dc6-418a-bd9c-946c369b4637
2024-02-06 06:35:13,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-386adcaa-9c77-42f0-af21-059f8bdfc196
2024-02-06 06:35:13,936 - distributed.worker - INFO - Starting Worker plugin PreImport-242566b0-dec7-4827-b866-54285b6dd599
2024-02-06 06:35:13,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57e23085-651c-4bb6-b0be-efd3c2ae8938
2024-02-06 06:35:13,938 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,968 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34871', status: init, memory: 0, processing: 0>
2024-02-06 06:35:13,969 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34871
2024-02-06 06:35:13,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48228
2024-02-06 06:35:13,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:13,971 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:13,972 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:13,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,148 - distributed.worker - INFO - Starting Worker plugin PreImport-9aff52b6-232c-4a11-b6ab-727c59256d83
2024-02-06 06:35:15,149 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5227f17b-c610-4f70-a7c8-0ef3d2c7863c
2024-02-06 06:35:15,150 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41807', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41807
2024-02-06 06:35:15,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48244
2024-02-06 06:35:15,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,184 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,184 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,329 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39247', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39247
2024-02-06 06:35:15,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48336
2024-02-06 06:35:15,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,367 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,393 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44731', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,393 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44731
2024-02-06 06:35:15,393 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48344
2024-02-06 06:35:15,395 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,396 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,396 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,408 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,426 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,431 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa9596ac-3025-496a-b683-a2807ab8105d
2024-02-06 06:35:15,431 - distributed.worker - INFO - Starting Worker plugin PreImport-32126096-b0e8-4cfd-9def-90cf97d6d3dd
2024-02-06 06:35:15,432 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,434 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44719', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,435 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44719
2024-02-06 06:35:15,435 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48360
2024-02-06 06:35:15,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,437 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,437 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,447 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41867', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,447 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41867
2024-02-06 06:35:15,447 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48376
2024-02-06 06:35:15,448 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,449 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,449 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,454 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43727', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,455 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43727
2024-02-06 06:35:15,455 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48396
2024-02-06 06:35:15,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,456 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,460 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44023', status: init, memory: 0, processing: 0>
2024-02-06 06:35:15,461 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44023
2024-02-06 06:35:15,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48386
2024-02-06 06:35:15,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:15,463 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:15,463 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:15,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,514 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,519 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,520 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:15,524 - distributed.scheduler - INFO - Remove client Client-e165d8f5-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:15,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48204; closing.
2024-02-06 06:35:15,524 - distributed.scheduler - INFO - Remove client Client-e165d8f5-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:15,525 - distributed.scheduler - INFO - Close client connection: Client-e165d8f5-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:15,526 - distributed.scheduler - INFO - Remove client Client-dfa0c3a4-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:15,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48186; closing.
2024-02-06 06:35:15,526 - distributed.scheduler - INFO - Remove client Client-dfa0c3a4-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:15,527 - distributed.scheduler - INFO - Close client connection: Client-dfa0c3a4-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:15,528 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38759'. Reason: nanny-close
2024-02-06 06:35:15,528 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40491'. Reason: nanny-close
2024-02-06 06:35:15,530 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43727. Reason: nanny-close
2024-02-06 06:35:15,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45177'. Reason: nanny-close
2024-02-06 06:35:15,530 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45907'. Reason: nanny-close
2024-02-06 06:35:15,531 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41867. Reason: nanny-close
2024-02-06 06:35:15,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39889'. Reason: nanny-close
2024-02-06 06:35:15,531 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41807. Reason: nanny-close
2024-02-06 06:35:15,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45501'. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44731. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48396; closing.
2024-02-06 06:35:15,532 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37045'. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5323212')
2024-02-06 06:35:15,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34871. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34003'. Reason: nanny-close
2024-02-06 06:35:15,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39247. Reason: nanny-close
2024-02-06 06:35:15,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:15,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44023. Reason: nanny-close
2024-02-06 06:35:15,533 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,533 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48376; closing.
2024-02-06 06:35:15,534 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44719. Reason: nanny-close
2024-02-06 06:35:15,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,534 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5349133')
2024-02-06 06:35:15,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48244; closing.
2024-02-06 06:35:15,535 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,535 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5358841')
2024-02-06 06:35:15,536 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48344; closing.
2024-02-06 06:35:15,536 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,536 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,536 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:15,537 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5369864')
2024-02-06 06:35:15,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48228; closing.
2024-02-06 06:35:15,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48336; closing.
2024-02-06 06:35:15,537 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5382397')
2024-02-06 06:35:15,538 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:15,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.538684')
2024-02-06 06:35:15,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48386; closing.
2024-02-06 06:35:15,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48360; closing.
2024-02-06 06:35:15,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5397172')
2024-02-06 06:35:15,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44719', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201315.5400946')
2024-02-06 06:35:15,540 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:16,494 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:16,494 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:16,495 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:16,498 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:16,499 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-06 06:35:18,732 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:18,737 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45375 instead
  warnings.warn(
2024-02-06 06:35:18,742 - distributed.scheduler - INFO - State start
2024-02-06 06:35:18,953 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:18,954 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:18,956 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45375/status
2024-02-06 06:35:18,956 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:19,000 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45847', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,014 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45847
2024-02-06 06:35:19,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48486
2024-02-06 06:35:19,030 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48486; closing.
2024-02-06 06:35:19,030 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45847', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.0306618')
2024-02-06 06:35:19,031 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:19,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38499'
2024-02-06 06:35:19,162 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46071', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,163 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46071
2024-02-06 06:35:19,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48510
2024-02-06 06:35:19,185 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35139', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,186 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35139
2024-02-06 06:35:19,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48514
2024-02-06 06:35:19,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48510; closing.
2024-02-06 06:35:19,191 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.1911335')
2024-02-06 06:35:19,233 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48514; closing.
2024-02-06 06:35:19,233 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.2334473')
2024-02-06 06:35:19,233 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:19,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34037', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,278 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34037
2024-02-06 06:35:19,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48520
2024-02-06 06:35:19,279 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33927', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,280 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33927
2024-02-06 06:35:19,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48524
2024-02-06 06:35:19,281 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38577', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38577
2024-02-06 06:35:19,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48522
2024-02-06 06:35:19,286 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46005', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,286 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46005
2024-02-06 06:35:19,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48538
2024-02-06 06:35:19,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48522; closing.
2024-02-06 06:35:19,289 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38577', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.2896128')
2024-02-06 06:35:19,307 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40047', status: init, memory: 0, processing: 0>
2024-02-06 06:35:19,307 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40047
2024-02-06 06:35:19,307 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48550
2024-02-06 06:35:19,335 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48524; closing.
2024-02-06 06:35:19,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.335611')
2024-02-06 06:35:19,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48538; closing.
2024-02-06 06:35:19,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46005', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.337426')
2024-02-06 06:35:19,337 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48550; closing.
2024-02-06 06:35:19,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.3383386')
2024-02-06 06:35:19,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48520; closing.
2024-02-06 06:35:19,339 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201319.3389776')
2024-02-06 06:35:19,339 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:20,030 - distributed.scheduler - INFO - Receive client connection: Client-e405071a-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:20,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43562
2024-02-06 06:35:20,285 - distributed.scheduler - INFO - Receive client connection: Client-e60f1688-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:20,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43580
2024-02-06 06:35:20,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:20,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:21,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:21,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43745
2024-02-06 06:35:21,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43745
2024-02-06 06:35:21,274 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-06 06:35:21,274 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:21,274 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:21,274 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:21,274 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-06 06:35:21,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jr3r04_8
2024-02-06 06:35:21,275 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cea9dac1-66ba-42d6-96f7-bf398a5026f4
2024-02-06 06:35:21,275 - distributed.worker - INFO - Starting Worker plugin PreImport-a1bfae7e-249f-42ae-a49f-0011f280d094
2024-02-06 06:35:21,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-644301d0-c200-4b7b-aa2e-5dd7308cd973
2024-02-06 06:35:21,276 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:21,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43745', status: init, memory: 0, processing: 0>
2024-02-06 06:35:21,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43745
2024-02-06 06:35:21,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43600
2024-02-06 06:35:21,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:21,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:21,339 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:21,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:21,358 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:21,361 - distributed.scheduler - INFO - Remove client Client-e405071a-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:21,361 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43562; closing.
2024-02-06 06:35:21,361 - distributed.scheduler - INFO - Remove client Client-e405071a-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:21,362 - distributed.scheduler - INFO - Close client connection: Client-e405071a-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:21,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38499'. Reason: nanny-close
2024-02-06 06:35:21,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:21,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43745. Reason: nanny-close
2024-02-06 06:35:21,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:21,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43600; closing.
2024-02-06 06:35:21,395 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201321.3955302')
2024-02-06 06:35:21,395 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:21,397 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:22,178 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:22,179 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:22,180 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:22,182 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:22,183 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-06 06:35:26,789 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:26,793 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40725 instead
  warnings.warn(
2024-02-06 06:35:26,796 - distributed.scheduler - INFO - State start
2024-02-06 06:35:26,817 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:26,817 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:26,818 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40725/status
2024-02-06 06:35:26,818 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:26,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35585'
2024-02-06 06:35:26,969 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38891', status: init, memory: 0, processing: 0>
2024-02-06 06:35:26,980 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38891
2024-02-06 06:35:26,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44664
2024-02-06 06:35:27,013 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44664; closing.
2024-02-06 06:35:27,014 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201327.0141478')
2024-02-06 06:35:27,014 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:27,032 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43679', status: init, memory: 0, processing: 0>
2024-02-06 06:35:27,032 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43679
2024-02-06 06:35:27,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44676
2024-02-06 06:35:27,065 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44676; closing.
2024-02-06 06:35:27,066 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201327.0660486')
2024-02-06 06:35:27,066 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:27,111 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37267', status: init, memory: 0, processing: 0>
2024-02-06 06:35:27,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37267
2024-02-06 06:35:27,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44694
2024-02-06 06:35:27,141 - distributed.scheduler - INFO - Receive client connection: Client-e8d9d596-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:27,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44708
2024-02-06 06:35:27,154 - distributed.scheduler - INFO - Remove client Client-e8d9d596-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:27,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44708; closing.
2024-02-06 06:35:27,154 - distributed.scheduler - INFO - Remove client Client-e8d9d596-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:27,155 - distributed.scheduler - INFO - Close client connection: Client-e8d9d596-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:27,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35585'. Reason: nanny-close
2024-02-06 06:35:27,167 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44694; closing.
2024-02-06 06:35:27,167 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201327.1675766')
2024-02-06 06:35:27,167 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:27,275 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37863', status: init, memory: 0, processing: 0>
2024-02-06 06:35:27,275 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37863
2024-02-06 06:35:27,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44722
2024-02-06 06:35:27,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:44722 has been closed.
2024-02-06 06:35:27,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37863', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201327.3346465')
2024-02-06 06:35:27,334 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:28,462 - distributed.scheduler - INFO - Receive client connection: Client-e60f1688-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:28,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44734
2024-02-06 06:35:28,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:28,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:29,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:29,091 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40247
2024-02-06 06:35:29,091 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40247
2024-02-06 06:35:29,091 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37453
2024-02-06 06:35:29,091 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:29,091 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:29,091 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:29,091 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-06 06:35:29,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bzha7q9z
2024-02-06 06:35:29,092 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a26b7cf-c031-444d-93e4-a648f346c799
2024-02-06 06:35:29,092 - distributed.worker - INFO - Starting Worker plugin PreImport-915d2195-cc8a-4fc6-97aa-a45babff8e8c
2024-02-06 06:35:29,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6346525d-8c0a-4287-9ac3-a37c67d75bc7
2024-02-06 06:35:29,094 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:29,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40247', status: init, memory: 0, processing: 0>
2024-02-06 06:35:29,152 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40247
2024-02-06 06:35:29,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44742
2024-02-06 06:35:29,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:29,155 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:29,155 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:29,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:29,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:29,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40247. Reason: nanny-close
2024-02-06 06:35:29,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44742; closing.
2024-02-06 06:35:29,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:29,175 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201329.1757073')
2024-02-06 06:35:29,175 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:29,177 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:29,826 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:29,826 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:29,827 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:29,828 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:29,829 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-06 06:35:31,763 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:31,766 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45913 instead
  warnings.warn(
2024-02-06 06:35:31,770 - distributed.scheduler - INFO - State start
2024-02-06 06:35:31,953 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:31,954 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:31,954 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45913/status
2024-02-06 06:35:31,954 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:33,289 - distributed.scheduler - INFO - Receive client connection: Client-e60f1688-c4b9-11ee-98c1-d8c49764f6bb
2024-02-06 06:35:33,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44750
2024-02-06 06:35:34,307 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44722'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44722>: Stream is closed
2024-02-06 06:35:34,541 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:34,541 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:34,542 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:34,543 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:34,543 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-06 06:35:36,561 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:36,565 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38963 instead
  warnings.warn(
2024-02-06 06:35:36,568 - distributed.scheduler - INFO - State start
2024-02-06 06:35:36,588 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:36,589 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-06 06:35:36,589 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38963/status
2024-02-06 06:35:36,590 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:36,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39411'
2024-02-06 06:35:38,257 - distributed.scheduler - INFO - Receive client connection: Client-eec74efb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:38,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54036
2024-02-06 06:35:38,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:38,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:38,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:38,377 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42701
2024-02-06 06:35:38,377 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42701
2024-02-06 06:35:38,377 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36219
2024-02-06 06:35:38,377 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-06 06:35:38,377 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:38,377 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:38,377 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-06 06:35:38,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-3xtcy567
2024-02-06 06:35:38,378 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4046cce-3509-4fc6-b604-1144962b35bc
2024-02-06 06:35:38,378 - distributed.worker - INFO - Starting Worker plugin PreImport-27fbf3dc-c8bb-4d97-8956-b99c43d9fa3f
2024-02-06 06:35:38,378 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e883e032-cddd-46e1-b4cf-b0b87aefb182
2024-02-06 06:35:38,378 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:38,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42701', status: init, memory: 0, processing: 0>
2024-02-06 06:35:38,433 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42701
2024-02-06 06:35:38,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54054
2024-02-06 06:35:38,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:38,435 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-06 06:35:38,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:38,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-06 06:35:38,479 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:38,482 - distributed.scheduler - INFO - Remove client Client-eec74efb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:38,482 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54036; closing.
2024-02-06 06:35:38,482 - distributed.scheduler - INFO - Remove client Client-eec74efb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:38,483 - distributed.scheduler - INFO - Close client connection: Client-eec74efb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:38,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39411'. Reason: nanny-close
2024-02-06 06:35:38,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:38,485 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42701. Reason: nanny-close
2024-02-06 06:35:38,486 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54054; closing.
2024-02-06 06:35:38,486 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-06 06:35:38,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201338.4869506')
2024-02-06 06:35:38,487 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:38,488 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:39,149 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:39,149 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:39,149 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:39,150 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-06 06:35:39,150 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-06 06:35:41,299 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:41,304 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44875 instead
  warnings.warn(
2024-02-06 06:35:41,307 - distributed.scheduler - INFO - State start
2024-02-06 06:35:41,330 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:41,331 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:41,331 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44875/status
2024-02-06 06:35:41,332 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:41,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40075'
2024-02-06 06:35:41,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40955'
2024-02-06 06:35:41,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36487'
2024-02-06 06:35:41,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41123'
2024-02-06 06:35:41,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39661'
2024-02-06 06:35:41,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42967'
2024-02-06 06:35:41,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38643'
2024-02-06 06:35:41,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37691'
2024-02-06 06:35:42,517 - distributed.scheduler - INFO - Receive client connection: Client-f18fcadc-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:42,530 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51880
2024-02-06 06:35:43,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,148 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46039
2024-02-06 06:35:43,148 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46039
2024-02-06 06:35:43,148 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37651
2024-02-06 06:35:43,148 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,148 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,148 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,149 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,149 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tm8r6d7s
2024-02-06 06:35:43,149 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35e3e31a-fc81-4851-a8ae-dc78ee02ac9c
2024-02-06 06:35:43,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,377 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35531
2024-02-06 06:35:43,377 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35531
2024-02-06 06:35:43,377 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41609
2024-02-06 06:35:43,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,377 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,377 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,377 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,377 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o3cns1sk
2024-02-06 06:35:43,378 - distributed.worker - INFO - Starting Worker plugin PreImport-7d8c47a9-2c35-4a0b-a503-64c49dc1ca26
2024-02-06 06:35:43,378 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67408f4f-aa5b-47d8-82d2-a060772ed6ab
2024-02-06 06:35:43,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34295
2024-02-06 06:35:43,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34295
2024-02-06 06:35:43,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34071
2024-02-06 06:35:43,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,378 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,378 - distributed.worker - INFO - Starting Worker plugin RMMSetup-793fcd75-303a-43f1-b7ab-7dfb6dea29f9
2024-02-06 06:35:43,378 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,378 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,378 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tgt2b31e
2024-02-06 06:35:43,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4a7a684-f6f3-4ef7-a30c-d4e1afbe6ce2
2024-02-06 06:35:43,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41569
2024-02-06 06:35:43,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41569
2024-02-06 06:35:43,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35573
2024-02-06 06:35:43,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,386 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,386 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,386 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yf2at600
2024-02-06 06:35:43,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40087
2024-02-06 06:35:43,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40087
2024-02-06 06:35:43,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78404086-34bc-4b04-905a-2c1a8766a750
2024-02-06 06:35:43,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39917
2024-02-06 06:35:43,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43253
2024-02-06 06:35:43,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39917
2024-02-06 06:35:43,386 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:43,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,386 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45375
2024-02-06 06:35:43,386 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:43,386 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,386 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,386 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,386 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-abkhhix6
2024-02-06 06:35:43,387 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1xm63_ll
2024-02-06 06:35:43,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-364b63a5-4db7-4aba-9738-c3c2f6e83f5d
2024-02-06 06:35:43,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-352a40a6-c486-43c4-b480-8c986edbf184
2024-02-06 06:35:43,387 - distributed.worker - INFO - Starting Worker plugin PreImport-3c6b1348-9650-4b56-953d-6a6a44ba0963
2024-02-06 06:35:43,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc329292-8f1b-498b-ad25-fff1f57fbf54
2024-02-06 06:35:43,389 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,390 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41129
2024-02-06 06:35:43,390 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41129
2024-02-06 06:35:43,390 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32863
2024-02-06 06:35:43,390 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,390 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,390 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,390 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2w_aw0sp
2024-02-06 06:35:43,390 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:43,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f11b8fb3-0f36-4bc7-85c1-d2aa50a7dd92
2024-02-06 06:35:43,391 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4de8a6c-c39a-485d-9c8c-4eecbebd644a
2024-02-06 06:35:43,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38761
2024-02-06 06:35:43,391 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38761
2024-02-06 06:35:43,391 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44339
2024-02-06 06:35:43,391 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,391 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,392 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:43,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 06:35:43,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zrtxk03c
2024-02-06 06:35:43,392 - distributed.worker - INFO - Starting Worker plugin PreImport-dd2c84a1-4c06-41d5-924e-74709c11c53a
2024-02-06 06:35:43,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf6051bd-4555-454e-af26-ce53fb607995
2024-02-06 06:35:43,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17fd9629-0520-49f4-bc9c-21073f02d59d
2024-02-06 06:35:43,393 - distributed.worker - INFO - Starting Worker plugin PreImport-4dd7c93a-0c69-4bed-a4e1-dbd59a622898
2024-02-06 06:35:43,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ab833bb-0a0b-4c56-a043-3b71c5b3c6d7
2024-02-06 06:35:43,573 - distributed.worker - INFO - Starting Worker plugin PreImport-a946fc0d-f51c-4a21-89a6-8817198fe91c
2024-02-06 06:35:43,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd75645f-dcbd-4c9a-a604-36008f7c5ab6
2024-02-06 06:35:43,574 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,602 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46039', status: init, memory: 0, processing: 0>
2024-02-06 06:35:43,603 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46039
2024-02-06 06:35:43,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51900
2024-02-06 06:35:43,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:43,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:43,606 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:43,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,089 - distributed.worker - INFO - Starting Worker plugin PreImport-84e6728c-799d-46eb-a8a7-a05494404b67
2024-02-06 06:35:45,090 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,100 - distributed.worker - INFO - Starting Worker plugin PreImport-19339bf3-a22a-4de3-8365-70479b3b4792
2024-02-06 06:35:45,100 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43077ede-5fae-49f6-a585-1541fdf93781
2024-02-06 06:35:45,102 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,114 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41129', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,115 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41129
2024-02-06 06:35:45,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51928
2024-02-06 06:35:45,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,116 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,117 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,139 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34295', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,139 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34295
2024-02-06 06:35:45,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51944
2024-02-06 06:35:45,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,142 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,143 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,143 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,163 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40087', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,163 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40087
2024-02-06 06:35:45,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51952
2024-02-06 06:35:45,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,165 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,165 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,177 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,186 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,192 - distributed.worker - INFO - Starting Worker plugin PreImport-9041cf30-0ecb-4da7-8c20-42f61306f62a
2024-02-06 06:35:45,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcb4071c-8929-44b8-bfbe-6f2d8b09f7e1
2024-02-06 06:35:45,193 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,193 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,210 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35531', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35531
2024-02-06 06:35:45,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51968
2024-02-06 06:35:45,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,214 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,214 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39917', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39917
2024-02-06 06:35:45,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51998
2024-02-06 06:35:45,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41569', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,220 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41569
2024-02-06 06:35:45,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51984
2024-02-06 06:35:45,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,223 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,223 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,225 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38761', status: init, memory: 0, processing: 0>
2024-02-06 06:35:45,226 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38761
2024-02-06 06:35:45,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52008
2024-02-06 06:35:45,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:45,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:45,228 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:45,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:45,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,294 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,294 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,294 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,294 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-06 06:35:45,308 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,308 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:45,313 - distributed.scheduler - INFO - Remove client Client-f18fcadc-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:45,314 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51880; closing.
2024-02-06 06:35:45,314 - distributed.scheduler - INFO - Remove client Client-f18fcadc-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:45,315 - distributed.scheduler - INFO - Close client connection: Client-f18fcadc-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:45,315 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40075'. Reason: nanny-close
2024-02-06 06:35:45,316 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,316 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40955'. Reason: nanny-close
2024-02-06 06:35:45,317 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,317 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36487'. Reason: nanny-close
2024-02-06 06:35:45,317 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35531. Reason: nanny-close
2024-02-06 06:35:45,317 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,317 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41123'. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34295. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39661'. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40087. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42967'. Reason: nanny-close
2024-02-06 06:35:45,318 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39917. Reason: nanny-close
2024-02-06 06:35:45,319 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,319 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38643'. Reason: nanny-close
2024-02-06 06:35:45,319 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,319 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41569. Reason: nanny-close
2024-02-06 06:35:45,319 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37691'. Reason: nanny-close
2024-02-06 06:35:45,319 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:45,320 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38761. Reason: nanny-close
2024-02-06 06:35:45,320 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,320 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41129. Reason: nanny-close
2024-02-06 06:35:45,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51952; closing.
2024-02-06 06:35:45,320 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,320 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,320 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46039. Reason: nanny-close
2024-02-06 06:35:45,320 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.3206856')
2024-02-06 06:35:45,321 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,321 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51998; closing.
2024-02-06 06:35:45,321 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51968; closing.
2024-02-06 06:35:45,321 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,321 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,321 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,321 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,322 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,322 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39917', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.3223042')
2024-02-06 06:35:45,322 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.322766')
2024-02-06 06:35:45,323 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,323 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,323 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:45,323 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,323 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,324 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51984; closing.
2024-02-06 06:35:45,324 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51944; closing.
2024-02-06 06:35:45,324 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,325 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:45,325 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51968>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-06 06:35:45,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51998>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-06 06:35:45,328 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41569', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.3284')
2024-02-06 06:35:45,328 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.3288143')
2024-02-06 06:35:45,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51928; closing.
2024-02-06 06:35:45,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52008; closing.
2024-02-06 06:35:45,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51900; closing.
2024-02-06 06:35:45,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41129', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.3299475')
2024-02-06 06:35:45,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.3303285')
2024-02-06 06:35:45,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201345.330718')
2024-02-06 06:35:45,330 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:46,131 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:46,132 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:46,132 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:46,134 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:46,134 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-06 06:35:48,227 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:48,232 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44517 instead
  warnings.warn(
2024-02-06 06:35:48,236 - distributed.scheduler - INFO - State start
2024-02-06 06:35:48,257 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:48,258 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:48,258 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44517/status
2024-02-06 06:35:48,259 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:48,286 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41067'
2024-02-06 06:35:49,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:49,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:49,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:49,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36963
2024-02-06 06:35:49,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36963
2024-02-06 06:35:49,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41475
2024-02-06 06:35:49,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:49,841 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:49,841 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:49,841 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-06 06:35:49,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rpuyk1fu
2024-02-06 06:35:49,841 - distributed.worker - INFO - Starting Worker plugin PreImport-725910bf-096c-49bd-a10b-0c81aad0b001
2024-02-06 06:35:49,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02907479-a0dc-40c0-bd95-bce5934d5e7c
2024-02-06 06:35:50,103 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1e13a7d-bee6-49fc-a03d-33ebce343e71
2024-02-06 06:35:50,103 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:50,136 - distributed.scheduler - INFO - Receive client connection: Client-f5bcc1bb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:50,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49018
2024-02-06 06:35:50,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36963', status: init, memory: 0, processing: 0>
2024-02-06 06:35:50,170 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36963
2024-02-06 06:35:50,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49032
2024-02-06 06:35:50,171 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:50,172 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:50,172 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:50,173 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:50,255 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:50,260 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:50,265 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:50,267 - distributed.scheduler - INFO - Remove client Client-f5bcc1bb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:50,267 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49018; closing.
2024-02-06 06:35:50,267 - distributed.scheduler - INFO - Remove client Client-f5bcc1bb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:50,268 - distributed.scheduler - INFO - Close client connection: Client-f5bcc1bb-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:50,269 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41067'. Reason: nanny-close
2024-02-06 06:35:50,269 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:50,270 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36963. Reason: nanny-close
2024-02-06 06:35:50,272 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:50,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49032; closing.
2024-02-06 06:35:50,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201350.272472')
2024-02-06 06:35:50,272 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:50,273 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:50,884 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:50,884 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:50,885 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:50,886 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:50,886 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-06 06:35:52,983 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:52,987 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46083 instead
  warnings.warn(
2024-02-06 06:35:52,991 - distributed.scheduler - INFO - State start
2024-02-06 06:35:53,013 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-06 06:35:53,014 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-06 06:35:53,014 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46083/status
2024-02-06 06:35:53,015 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-06 06:35:53,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37001'
2024-02-06 06:35:54,246 - distributed.scheduler - INFO - Receive client connection: Client-f887bac5-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:54,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49264
2024-02-06 06:35:54,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 06:35:54,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 06:35:54,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 06:35:54,706 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36171
2024-02-06 06:35:54,706 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36171
2024-02-06 06:35:54,706 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42747
2024-02-06 06:35:54,706 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-06 06:35:54,706 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:54,706 - distributed.worker - INFO -               Threads:                          1
2024-02-06 06:35:54,706 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-06 06:35:54,706 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ibg3c2k
2024-02-06 06:35:54,707 - distributed.worker - INFO - Starting Worker plugin PreImport-5cf337b9-ea59-4c7e-9fe5-588e1255e778
2024-02-06 06:35:54,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b62cb8fb-8f72-49f9-926e-9a4677e2da4d
2024-02-06 06:35:54,973 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17509dc3-22f1-4519-ab6e-f7f718d7f216
2024-02-06 06:35:54,973 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:55,021 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36171', status: init, memory: 0, processing: 0>
2024-02-06 06:35:55,022 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36171
2024-02-06 06:35:55,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49278
2024-02-06 06:35:55,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 06:35:55,024 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-06 06:35:55,024 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 06:35:55,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-06 06:35:55,046 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-06 06:35:55,050 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-06 06:35:55,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:55,056 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 06:35:55,058 - distributed.scheduler - INFO - Remove client Client-f887bac5-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:55,058 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49264; closing.
2024-02-06 06:35:55,059 - distributed.scheduler - INFO - Remove client Client-f887bac5-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:55,059 - distributed.scheduler - INFO - Close client connection: Client-f887bac5-c4b9-11ee-babf-d8c49764f6bb
2024-02-06 06:35:55,060 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37001'. Reason: nanny-close
2024-02-06 06:35:55,060 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-06 06:35:55,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36171. Reason: nanny-close
2024-02-06 06:35:55,063 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-06 06:35:55,063 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49278; closing.
2024-02-06 06:35:55,063 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36171', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707201355.063626')
2024-02-06 06:35:55,064 - distributed.scheduler - INFO - Lost all workers
2024-02-06 06:35:55,064 - distributed.nanny - INFO - Worker closed
2024-02-06 06:35:55,675 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-06 06:35:55,676 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-06 06:35:55,676 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-06 06:35:55,677 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-06 06:35:55,678 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45085 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44539 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46543 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43109 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46229 instead
  warnings.warn(
2024-02-06 06:38:18,566 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #006] ep: 0x7fbedd2bc180, tag: 0xb238f749685f3826, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #006] ep: 0x7fbedd2bc180, tag: 0xb238f749685f3826, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43223 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43927 instead
  warnings.warn(
[1707201521.486898] [dgx13:70450:0]            sock.c:481  UCX  ERROR bind(fd=180 addr=0.0.0.0:33944) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35611 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45383 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41785 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33021 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35661 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33789 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42055 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43513 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] 2024-02-06 06:44:40,609 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #063] ep: 0x7ff4728f00c0, tag: 0xfc4ede7adfa0f0e0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #063] ep: 0x7ff4728f00c0, tag: 0xfc4ede7adfa0f0e0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-06 06:44:40,610 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #063] ep: 0x7f57e0dfc0c0, tag: 0xb13e87f0800ff8e7, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #063] ep: 0x7f57e0dfc0c0, tag: 0xb13e87f0800ff8e7, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] [1707201886.997391] [dgx13:74803:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:40054) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] 2024-02-06 06:46:25,138 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 439, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 505, in ep
    raise CommClosedError("UCX Endpoint is closed")
distributed.comm.core.CommClosedError: UCX Endpoint is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 445, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CommClosedError('UCX Endpoint is closed')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43155 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43691 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46857 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41455 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39315 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35643 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40175 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35659 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37675 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37773 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] [1707202318.837451] [dgx13:81391:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:54785) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] [1707202342.274443] [dgx13:81681:0]            sock.c:481  UCX  ERROR bind(fd=132 addr=0.0.0.0:47771) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34503 instead
  warnings.warn(
[1707202372.063417] [dgx13:82259:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:43652) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43707 instead
  warnings.warn(
[1707202401.821916] [dgx13:82620:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:46678) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40755 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34535 instead
  warnings.warn(
[1707202564.095337] [dgx13:84312:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:46928) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38897 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43677 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43833 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39077 instead
  warnings.warn(
2024-02-06 06:57:41,558 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-7ae85d89b8163866f57303985b8a2075', 0)
Function:  subgraph_callable-d7818932-2d70-451c-a7aa-f317edfc
args:      (   key  payload1
0    0         0
1    1         1
2    2         2
3    3         3
4    4         4
5    5         5
6    6         6,     key  payload2
0    17         6
1    18         7
2    14         8
3     2         9
4     7        10
5    12        11
6     0        12
7    15        13
8     3        14
9    10        15
10   11        16
11    5        17
12    6        18
13    1        19, 'from_pandas-c52d4f0fb9bbc6e14f597d384305e904', 'from_pandas-6f7a1f58c9f0f89911a703f2fb560f26')
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-06 06:57:41,564 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-7ae85d89b8163866f57303985b8a2075', 1)
Function:  subgraph_callable-d7818932-2d70-451c-a7aa-f317edfc
args:      (    key  payload1
7     7         7
8     8         8
9     9         9
10   10        10
11   11        11
12   12        12
13   13        13,     key  payload2
0    17         6
1    18         7
2    14         8
3     2         9
4     7        10
5    12        11
6     0        12
7    15        13
8     3        14
9    10        15
10   11        16
11    5        17
12    6        18
13    1        19, 'from_pandas-c52d4f0fb9bbc6e14f597d384305e904', 'from_pandas-6f7a1f58c9f0f89911a703f2fb560f26')
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-06 06:57:41,588 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('merge_chunk-7ae85d89b8163866f57303985b8a2075', 2))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
Process SpawnProcess-61:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 281, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/sorting.py", line 46, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/core.py", line 214, in set_index
    return super().set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5551, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 262, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 61, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/multi.py", line 290, in merge_chunk
    out = lhs.merge(rhs, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 4149, in merge
    return merge_cls(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/join/join.py", line 279, in perform_merge
    left_rows, right_rows = self._gather_maps(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/join/join.py", line 205, in _gather_maps
    maps = self._joiner(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "join.pyx", line 30, in cudf._lib.join.join
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/include/cudf/table/table_device_view.cuh:269: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46431 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33215 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37197 instead
  warnings.warn(
[1707202728.558758] [dgx13:86758:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:60596) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41023 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42319 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36727 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42539 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43415 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32987 instead
  warnings.warn(
2024-02-06 07:00:47,766 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 1)
Function:  subgraph_callable-17c9efb1-e7a0-4c98-a789-527adc63
args:      (    key  payload1
8     8         8
9     9         9
10   10        10
11   11        11
12   12        12
13   13        13
14   14        14
15   15        15, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-06 07:00:47,766 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 3)
Function:  subgraph_callable-17c9efb1-e7a0-4c98-a789-527adc63
args:      (    key  payload1
24   24        24
25   25        25
26   26        26
27   27        27
28   28        28
29   29        29
30   30        30
31   31        31, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-06 07:00:47,766 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-5f75be6b611af5f0f314ef71d7b084aa', 2)
Function:  subgraph_callable-17c9efb1-e7a0-4c98-a789-527adc63
args:      (    key  payload1
16   16        16
17   17        17
18   18        18
19   19        19
20   20        20
21   21        21
22   22        22
23   23        23, '_partitions', 'getitem-283d6bb332aaae6b8c9cd0db9318a971', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-06 07:00:48,197 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-09402359-08b0-496c-88cb-443e45733973
Function:  _run_coroutine_on_worker
args:      (254274952634744760660519171254334074991, <function shuffle_task at 0x7fe9d6053b80>, ('explicit-comms-shuffle-54bae6db38178303d2e307b7feb4edb8', {0: {('assign-5f75be6b611af5f0f314ef71d7b084aa', 4), ('assign-5f75be6b611af5f0f314ef71d7b084aa', 0)}, 1: set(), 2: set(), 3: set()}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['_partitions'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('This program was not compiled for SM 70  \\n: cudaErrorInvalidDevice: invalid device ordinal')"

2024-02-06 07:00:48,282 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-37f72953-2c7f-4ff1-8522-5ba8a3723464
Function:  _run_coroutine_on_worker
args:      (254274952634744760660519171254334074991, <function shuffle_task at 0x7f6de064bdc0>, ('explicit-comms-shuffle-54bae6db38178303d2e307b7feb4edb8', {0: {('assign-5f75be6b611af5f0f314ef71d7b084aa', 4), ('assign-5f75be6b611af5f0f314ef71d7b084aa', 0)}, 1: set(), 2: set(), 3: set()}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['_partitions'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('This program was not compiled for SM 70  \\n: cudaErrorInvalidDevice: invalid device ordinal')"

2024-02-06 07:00:48,303 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-fb58fc97-7b24-4ab5-985d-0e618644df48
Function:  _run_coroutine_on_worker
args:      (254274952634744760660519171254334074991, <function shuffle_task at 0x7f34311620d0>, ('explicit-comms-shuffle-54bae6db38178303d2e307b7feb4edb8', {0: {('assign-5f75be6b611af5f0f314ef71d7b084aa', 4), ('assign-5f75be6b611af5f0f314ef71d7b084aa', 0)}, 1: set(), 2: set(), 3: set()}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['_partitions'], 5, False, 2, 1))
kwargs:    {}
Exception: "RuntimeError('This program was not compiled for SM 70  \\n: cudaErrorInvalidDevice: invalid device ordinal')"

2024-02-06 07:00:48,654 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-e1f34e0aa63cc4ff42655c5a31aecbb6', 2)
Function:  subgraph_callable-c1f77bbf-5649-4ccc-9e13-0a02abf6
args:      (    key  payload2
18    6        31
19   36        32
20   30        33
21   37        34
22    0        35
23   29        36
24    2        37
25   21        38
26   14        39, '_partitions', 'getitem-5a2ab91d683efc34eda6066adb6c47cc', ['key'])
kwargs:    {}
Exception: "RuntimeError('This program was not compiled for SM 70  \\n: cudaErrorInvalidDevice: invalid device ordinal')"

2024-02-06 07:00:48,706 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-e1f34e0aa63cc4ff42655c5a31aecbb6', 1)
Function:  subgraph_callable-c1f77bbf-5649-4ccc-9e13-0a02abf6
args:      (    key  payload2
9    39        22
10   33        23
11   18        24
12   15        25
13   27        26
14   19        27
15    4        28
16    7        29
17   26        30, '_partitions', 'getitem-5a2ab91d683efc34eda6066adb6c47cc', ['key'])
kwargs:    {}
Exception: "RuntimeError('This program was not compiled for SM 70  \\n: cudaErrorInvalidDevice: invalid device ordinal')"

Process SpawnProcess-74:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 281, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/sorting.py", line 46, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/core.py", line 214, in set_index
    return super().set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5551, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 262, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 61, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 394, in shuffle_task
    dd_concat(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/dispatch.py", line 68, in concat
    return func(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/backends.py", line 294, in concat_cudf
    return cudf.concat(dfs, axis=axis, ignore_index=ignore_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/reshape.py", line 410, in concat
    result = cudf.DataFrame._concat(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 1739, in _concat
    *libcudf.concat.concat_tables(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "concat.pyx", line 56, in cudf._lib.concat.concat_tables
RuntimeError: This program was not compiled for SM 70  
: cudaErrorInvalidDevice: invalid device ordinal
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42431 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36127 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1707202912.954932] [dgx13:64191:0]            sock.c:481  UCX  ERROR bind(fd=175 addr=0.0.0.0:58832) failed: Address already in use
[1707202912.956115] [dgx13:64191:0]            sock.c:481  UCX  ERROR bind(fd=180 addr=0.0.0.0:37047) failed: Address already in use
2024-02-06 07:02:02,256 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-02-06 07:02:02,267 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:55480', name: 3, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-02-06 07:02:02,277 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #006] ep: 0x7fc4c148e300, tag: 0xacc20fddcebc58d2, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #006] ep: 0x7fc4c148e300, tag: 0xacc20fddcebc58d2, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1707202924.919231] [dgx13:64191:1]            sock.c:481  UCX  ERROR bind(fd=248 addr=0.0.0.0:38748) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] 2024-02-06 07:02:26,022 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-02-06 07:02:26,027 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-02-06 07:02:26,035 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Process SpawnProcess-121:
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 31, in _test_initialize_ucx_tcp
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 286, in __init__
    self.sync(self._correct_state)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: IncreasedCloseTimeoutNanny failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37493 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45593 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43513 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39141 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33219 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36079 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1707203019.422308] [dgx13:90168:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:57994) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-02-06 07:04:16,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:16,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:16,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:16,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:17,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:17,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,107 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:17,107 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,107 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:17,107 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:17,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,112 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:17,112 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:17,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,314 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45937
2024-02-06 07:04:17,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45937
2024-02-06 07:04:17,314 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37177
2024-02-06 07:04:17,314 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,314 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,314 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4nahl16a
2024-02-06 07:04:17,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37115764-0314-493d-9c7f-f4d933d19e46
2024-02-06 07:04:17,315 - distributed.worker - INFO - Starting Worker plugin PreImport-aa37f0e2-3815-435c-abaf-5a508db9236b
2024-02-06 07:04:17,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f10bd394-a148-43b4-956d-72d1ed46ca50
2024-02-06 07:04:17,315 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,320 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44595
2024-02-06 07:04:17,320 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44595
2024-02-06 07:04:17,320 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33965
2024-02-06 07:04:17,320 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,320 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,320 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,320 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lcthkwqs
2024-02-06 07:04:17,320 - distributed.worker - INFO - Starting Worker plugin PreImport-9fa0bee3-7ac9-4fa5-873e-67f87f0b2dc1
2024-02-06 07:04:17,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-481e4f6f-b007-40ac-953d-53eae4b8e6e8
2024-02-06 07:04:17,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f82bf87-a35b-4a15-bdb6-50f7f5c6460d
2024-02-06 07:04:17,321 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,547 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,547 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,549 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,549 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,711 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43581
2024-02-06 07:04:17,711 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43581
2024-02-06 07:04:17,711 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38987
2024-02-06 07:04:17,711 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,712 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,712 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,712 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lzstqiw7
2024-02-06 07:04:17,712 - distributed.worker - INFO - Starting Worker plugin PreImport-0f2ee717-76f6-4c6d-9bf7-332655fbdbef
2024-02-06 07:04:17,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9a459f6-1d6e-4915-abd8-e6eb3b8f449e
2024-02-06 07:04:17,712 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f06b57c-ec43-41ec-bcca-88de6a0bb314
2024-02-06 07:04:17,714 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,735 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,735 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37567
2024-02-06 07:04:17,735 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37567
2024-02-06 07:04:17,735 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42923
2024-02-06 07:04:17,735 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,736 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,736 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,736 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ohbikcd4
2024-02-06 07:04:17,736 - distributed.worker - INFO - Starting Worker plugin PreImport-4062c74a-0bfd-4db7-907a-e52e4f987d34
2024-02-06 07:04:17,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d030d1d2-2e5d-4a69-94e3-93984c8c0776
2024-02-06 07:04:17,736 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06cd398a-e9b2-44da-be05-378e2694916f
2024-02-06 07:04:17,736 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46829
2024-02-06 07:04:17,736 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46829
2024-02-06 07:04:17,736 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45599
2024-02-06 07:04:17,736 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,736 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,736 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,737 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-196vs2u3
2024-02-06 07:04:17,737 - distributed.worker - INFO - Starting Worker plugin PreImport-c5124c46-e699-435e-85f5-1ae66443ff50
2024-02-06 07:04:17,737 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d49454d7-d2dc-48d7-8558-3fa4213ccb57
2024-02-06 07:04:17,737 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20f4008c-9b0d-44c7-adfd-2ca6a4234498
2024-02-06 07:04:17,737 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,738 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,747 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41017
2024-02-06 07:04:17,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41017
2024-02-06 07:04:17,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38179
2024-02-06 07:04:17,748 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,748 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,748 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,748 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nkatto_o
2024-02-06 07:04:17,748 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ade019bb-1943-44cb-953a-cba716b3434d
2024-02-06 07:04:17,748 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecc04ece-3fd3-436f-a53f-53ef9e2f1f89
2024-02-06 07:04:17,748 - distributed.worker - INFO - Starting Worker plugin PreImport-6be35c0e-da68-4a9f-b33c-141ee03d7502
2024-02-06 07:04:17,748 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,759 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37151
2024-02-06 07:04:17,759 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37151
2024-02-06 07:04:17,760 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42567
2024-02-06 07:04:17,760 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,760 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,760 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,760 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-husnokj_
2024-02-06 07:04:17,760 - distributed.worker - INFO - Starting Worker plugin PreImport-1a9f8d27-37dd-4b31-bbaf-027c0a836e93
2024-02-06 07:04:17,760 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a782a35d-be00-468f-8ec8-ccc8541d548c
2024-02-06 07:04:17,760 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1791ced6-d5a7-4efb-a328-78c683205dbf
2024-02-06 07:04:17,763 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:17,770 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46595
2024-02-06 07:04:17,770 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46595
2024-02-06 07:04:17,770 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37939
2024-02-06 07:04:17,770 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,770 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,770 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:17,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kokaz_9o
2024-02-06 07:04:17,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6fc509e-0125-4341-88ab-233fe6ec3876
2024-02-06 07:04:17,771 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c926fb2-7949-486c-9761-c5cecc77a4d3
2024-02-06 07:04:17,771 - distributed.worker - INFO - Starting Worker plugin PreImport-b99a9b24-930d-4aeb-8ecb-d9e0b828a597
2024-02-06 07:04:17,771 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,807 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,807 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,972 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,973 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,973 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,975 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,975 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,994 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,994 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,995 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,996 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,996 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:04:17,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:17,997 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36667
2024-02-06 07:04:17,998 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:17,999 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36667
2024-02-06 07:04:18,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,055 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,055 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,055 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,055 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,055 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-06 07:04:18,060 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44595. Reason: nanny-close
2024-02-06 07:04:18,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45937. Reason: nanny-close
2024-02-06 07:04:18,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43581. Reason: nanny-close
2024-02-06 07:04:18,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37567. Reason: nanny-close
2024-02-06 07:04:18,062 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46829. Reason: nanny-close
2024-02-06 07:04:18,063 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41017. Reason: nanny-close
2024-02-06 07:04:18,063 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,064 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46595. Reason: nanny-close
2024-02-06 07:04:18,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,064 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,064 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37151. Reason: nanny-close
2024-02-06 07:04:18,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,065 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,065 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,065 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,066 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:36667 has been closed.
2024-02-06 07:04:18,067 - distributed.nanny - INFO - Worker closed
2024-02-06 07:04:18,068 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-02-06 07:04:55,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:04:55,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:04:55,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:04:55,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44475
2024-02-06 07:04:55,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44475
2024-02-06 07:04:55,513 - distributed.worker - INFO -           Worker name:                          0
2024-02-06 07:04:55,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46679
2024-02-06 07:04:55,513 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40791
2024-02-06 07:04:55,513 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:04:55,513 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:04:55,513 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-06 07:04:55,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mv94jn9p
2024-02-06 07:04:55,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dbc778f1-50de-4e03-8a1c-f0cda30fffa8
2024-02-06 07:04:55,514 - distributed.worker - INFO - Starting Worker plugin PreImport-3c6f1a8f-ce75-4c02-8f05-f766c5738a52
2024-02-06 07:04:55,529 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-02-06 07:04:55,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1efd89fa-dacc-431a-a449-24b60ee4dca8
2024-02-06 07:04:55,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44475. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-02-06 07:04:55,530 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-02-06 07:04:55,533 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-02-06 07:05:00,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,432 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,432 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-06 07:05:00,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-06 07:05:00,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:00,896 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35425
2024-02-06 07:05:00,896 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35425
2024-02-06 07:05:00,896 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37605
2024-02-06 07:05:00,896 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:00,896 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:00,896 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:00,896 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:00,897 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iqnu8hn2
2024-02-06 07:05:00,897 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61ae4894-9758-4bbd-b1e3-14b086bd45c2
2024-02-06 07:05:00,897 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e645867-d02e-43c7-b28a-cc14bc764b5b
2024-02-06 07:05:00,897 - distributed.worker - INFO - Starting Worker plugin PreImport-b872fd5b-81c8-4ad4-8139-26d2ed1b8f25
2024-02-06 07:05:00,897 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45913
2024-02-06 07:05:01,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45913
2024-02-06 07:05:01,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32849
2024-02-06 07:05:01,024 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,024 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,024 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pwjfzzq8
2024-02-06 07:05:01,024 - distributed.worker - INFO - Starting Worker plugin RMMSetup-638897d4-2f82-4351-bf87-8e3eb2d5c608
2024-02-06 07:05:01,024 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4cd4152f-f2e7-4e24-a331-d957fe29f500
2024-02-06 07:05:01,025 - distributed.worker - INFO - Starting Worker plugin PreImport-d654209d-2170-41f8-a24c-f46df8ff468d
2024-02-06 07:05:01,025 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,052 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33427
2024-02-06 07:05:01,052 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33427
2024-02-06 07:05:01,052 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41479
2024-02-06 07:05:01,052 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,052 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,052 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,052 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,053 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-epp4cfc_
2024-02-06 07:05:01,053 - distributed.worker - INFO - Starting Worker plugin PreImport-9bfec4ab-bb21-440f-8816-0afc2aa4948b
2024-02-06 07:05:01,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51ede76d-5fc8-4e6f-8fc0-747b585c7820
2024-02-06 07:05:01,053 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8eaaece5-6ddb-4c8d-a021-b0807db4b904
2024-02-06 07:05:01,053 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,054 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43899
2024-02-06 07:05:01,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43899
2024-02-06 07:05:01,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46549
2024-02-06 07:05:01,055 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,055 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,055 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ktamuka0
2024-02-06 07:05:01,056 - distributed.worker - INFO - Starting Worker plugin PreImport-80832369-09ef-454d-b8d8-afb84cf830b2
2024-02-06 07:05:01,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81eee22d-a48d-475f-930d-971a01e41aa4
2024-02-06 07:05:01,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-821f5d0f-abbe-44b3-b197-3b93fcf7878b
2024-02-06 07:05:01,056 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,076 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,077 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33401
2024-02-06 07:05:01,077 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33401
2024-02-06 07:05:01,077 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39277
2024-02-06 07:05:01,077 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,077 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,077 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,077 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l85qg51h
2024-02-06 07:05:01,077 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4983c729-636b-4519-990f-e6918523634a
2024-02-06 07:05:01,077 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60f034d6-4c2c-482f-959f-ce8bb04717ef
2024-02-06 07:05:01,077 - distributed.worker - INFO - Starting Worker plugin PreImport-c152f48f-5b3e-406b-84bf-6fc5ec77e215
2024-02-06 07:05:01,078 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,130 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34439
2024-02-06 07:05:01,130 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34439
2024-02-06 07:05:01,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39877
2024-02-06 07:05:01,130 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,130 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,130 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,131 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-slmi4ber
2024-02-06 07:05:01,131 - distributed.worker - INFO - Starting Worker plugin PreImport-e8663a00-f7de-4ba6-942a-ccfd1e57bc7a
2024-02-06 07:05:01,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4bf16ef6-c3e8-477c-82a9-6fdf08274709
2024-02-06 07:05:01,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99b4dc34-0a2b-4b70-abee-44cdd2d8444d
2024-02-06 07:05:01,131 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:01,256 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,256 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:01,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-06 07:05:01,313 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45483
2024-02-06 07:05:01,313 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45483
2024-02-06 07:05:01,313 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33267
2024-02-06 07:05:01,313 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,313 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,313 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,313 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41029
2024-02-06 07:05:01,313 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41029
2024-02-06 07:05:01,313 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yypvc36h
2024-02-06 07:05:01,314 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45499
2024-02-06 07:05:01,314 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38617
2024-02-06 07:05:01,314 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,314 - distributed.worker - INFO -               Threads:                          1
2024-02-06 07:05:01,314 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-06 07:05:01,314 - distributed.worker - INFO - Starting Worker plugin PreImport-04afee6a-4e76-45bb-aa35-2dc5398ea95a
2024-02-06 07:05:01,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iy3ul9w1
2024-02-06 07:05:01,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c8a0dec-750d-4c5b-951f-5d70787e0876
2024-02-06 07:05:01,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a484df27-d1ef-40da-bd13-aa06cb8949fd
2024-02-06 07:05:01,314 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2aef6eec-5576-4bd8-803d-b733c1809c2f
2024-02-06 07:05:01,314 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:01,315 - distributed.worker - INFO - Starting Worker plugin PreImport-76c56ee6-eda1-410d-ab80-802d049a47de
2024-02-06 07:05:01,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec5d59a2-072c-4db3-875c-bd3e285de674
2024-02-06 07:05:01,315 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,418 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,418 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,777 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,778 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,778 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,793 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,794 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,794 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,802 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,802 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,910 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,910 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,930 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,930 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-06 07:05:02,965 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38617
2024-02-06 07:05:02,965 - distributed.worker - INFO - -------------------------------------------------
2024-02-06 07:05:02,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38617
2024-02-06 07:05:02,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35425. Reason: nanny-close
2024-02-06 07:05:02,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43899. Reason: nanny-close
2024-02-06 07:05:02,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33427. Reason: nanny-close
2024-02-06 07:05:02,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45913. Reason: nanny-close
2024-02-06 07:05:02,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34439. Reason: nanny-close
2024-02-06 07:05:02,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33401. Reason: nanny-close
2024-02-06 07:05:02,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45483. Reason: nanny-close
2024-02-06 07:05:02,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,982 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41029. Reason: nanny-close
2024-02-06 07:05:02,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,983 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,983 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,984 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:38617 has been closed.
2024-02-06 07:05:02,985 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,985 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,985 - distributed.nanny - INFO - Worker closed
2024-02-06 07:05:02,986 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-02-06 07:05:21,461 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-06 07:05:21,465 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-02-06 07:06:13,695 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34372 remote=tcp://127.0.0.1:35565>: Stream is closed
PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-02-06 07:06:27,037 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-73cabdfebe1dd4d17c2f37ae549713da', 2)
Function:  subgraph_callable-46fd3a68-85c2-449f-901e-e337b666
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7fccd293fc70 of cudf.core.dataframe.DataFrame at 0x7fccd293f100>, '_partitions', 'getitem-6bc36e7fdccbf44303bd588a4a4ec92f', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-02-06 07:06:27,063 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('assign-73cabdfebe1dd4d17c2f37ae549713da', 1))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-02-06 07:06:35,564 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-06 07:06:35,567 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
