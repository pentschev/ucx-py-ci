============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-26 06:29:40,642 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:29:40,647 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33209 instead
  warnings.warn(
2024-01-26 06:29:40,651 - distributed.scheduler - INFO - State start
2024-01-26 06:29:40,673 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:29:40,674 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-26 06:29:40,674 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33209/status
2024-01-26 06:29:40,674 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:29:40,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44797'
2024-01-26 06:29:40,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46163'
2024-01-26 06:29:40,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43063'
2024-01-26 06:29:40,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36703'
2024-01-26 06:29:42,492 - distributed.scheduler - INFO - Receive client connection: Client-4801c98a-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:42,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42648
2024-01-26 06:29:42,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:42,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:42,724 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:42,725 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42677
2024-01-26 06:29:42,725 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42677
2024-01-26 06:29:42,725 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41183
2024-01-26 06:29:42,725 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,725 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,725 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:29:42,725 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:29:42,725 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-bzlxi5x2
2024-01-26 06:29:42,726 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f38f18b2-c1b2-41a6-aced-210373f52988
2024-01-26 06:29:42,726 - distributed.worker - INFO - Starting Worker plugin PreImport-652fa499-1dce-4176-ba30-0f4994489f02
2024-01-26 06:29:42,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c57eda4b-acb3-439c-a698-c33c43c793dc
2024-01-26 06:29:42,726 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:42,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:42,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:42,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:42,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:42,741 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:42,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:42,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46715
2024-01-26 06:29:42,742 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46715
2024-01-26 06:29:42,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40095
2024-01-26 06:29:42,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,742 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,742 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:29:42,742 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:29:42,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ddb4xtt4
2024-01-26 06:29:42,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ba4da40-eb5e-4d71-a8e0-df8eb6fa9645
2024-01-26 06:29:42,743 - distributed.worker - INFO - Starting Worker plugin PreImport-34229815-3788-4d85-b6a8-239da4b4443c
2024-01-26 06:29:42,743 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e547613-e353-40f6-b576-670f94792cc1
2024-01-26 06:29:42,743 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:42,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37063
2024-01-26 06:29:42,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37063
2024-01-26 06:29:42,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44421
2024-01-26 06:29:42,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,745 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,745 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:29:42,745 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:29:42,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gpi0z9ek
2024-01-26 06:29:42,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-530df1bc-7f4b-44f4-85dd-5ca6c7852e3e
2024-01-26 06:29:42,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2d7f30a-bbbd-4401-aec8-e9880b4d4c47
2024-01-26 06:29:42,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:42,745 - distributed.worker - INFO - Starting Worker plugin PreImport-70f22557-101f-4e4f-be26-57fcead1b81e
2024-01-26 06:29:42,746 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43775
2024-01-26 06:29:42,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43775
2024-01-26 06:29:42,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33899
2024-01-26 06:29:42,746 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,746 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,746 - distributed.worker - INFO -               Threads:                          4
2024-01-26 06:29:42,746 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-26 06:29:42,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-hf5hyl_p
2024-01-26 06:29:42,747 - distributed.worker - INFO - Starting Worker plugin RMMSetup-928a99ec-3e58-4b98-975e-0577a98523d5
2024-01-26 06:29:42,747 - distributed.worker - INFO - Starting Worker plugin PreImport-125e3847-748b-4b03-8277-2b21912b63a1
2024-01-26 06:29:42,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09fdcbc5-1eb4-4da6-af12-514e9f79a626
2024-01-26 06:29:42,751 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,796 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42677', status: init, memory: 0, processing: 0>
2024-01-26 06:29:42,798 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42677
2024-01-26 06:29:42,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42664
2024-01-26 06:29:42,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:42,799 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,799 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:29:42,843 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46715', status: init, memory: 0, processing: 0>
2024-01-26 06:29:42,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46715
2024-01-26 06:29:42,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42668
2024-01-26 06:29:42,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:42,845 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,845 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:29:42,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37063', status: init, memory: 0, processing: 0>
2024-01-26 06:29:42,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37063
2024-01-26 06:29:42,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42672
2024-01-26 06:29:42,859 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43775', status: init, memory: 0, processing: 0>
2024-01-26 06:29:42,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:42,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43775
2024-01-26 06:29:42,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42674
2024-01-26 06:29:42,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,860 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:42,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:29:42,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:29:42,862 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:42,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:29:42,927 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:29:42,927 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:29:42,927 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:29:42,927 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-26 06:29:42,933 - distributed.scheduler - INFO - Remove client Client-4801c98a-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:42,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42648; closing.
2024-01-26 06:29:42,933 - distributed.scheduler - INFO - Remove client Client-4801c98a-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:42,934 - distributed.scheduler - INFO - Close client connection: Client-4801c98a-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:42,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44797'. Reason: nanny-close
2024-01-26 06:29:42,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:42,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46163'. Reason: nanny-close
2024-01-26 06:29:42,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:42,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43063'. Reason: nanny-close
2024-01-26 06:29:42,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37063. Reason: nanny-close
2024-01-26 06:29:42,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:42,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36703'. Reason: nanny-close
2024-01-26 06:29:42,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42677. Reason: nanny-close
2024-01-26 06:29:42,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:42,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43775. Reason: nanny-close
2024-01-26 06:29:42,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46715. Reason: nanny-close
2024-01-26 06:29:42,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:29:42,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:29:42,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42672; closing.
2024-01-26 06:29:42,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42674; closing.
2024-01-26 06:29:42,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:29:42,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:29:42,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37063', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250582.9395442')
2024-01-26 06:29:42,939 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:42,940 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:42,940 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42664; closing.
2024-01-26 06:29:42,940 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:42,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43775', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250582.9406068')
2024-01-26 06:29:42,940 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:42,941 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42677', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250582.9414937')
2024-01-26 06:29:42,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42668; closing.
2024-01-26 06:29:42,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:42664>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:29:42,944 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250582.944733')
2024-01-26 06:29:42,945 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:29:43,701 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:29:43,701 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:29:43,701 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:29:43,702 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-26 06:29:43,703 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-26 06:29:45,859 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:29:45,864 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40099 instead
  warnings.warn(
2024-01-26 06:29:45,868 - distributed.scheduler - INFO - State start
2024-01-26 06:29:45,891 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:29:45,892 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:29:45,892 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40099/status
2024-01-26 06:29:45,893 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:29:46,060 - distributed.scheduler - INFO - Receive client connection: Client-4b22a73b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:46,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39662
2024-01-26 06:29:46,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40725'
2024-01-26 06:29:46,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37139'
2024-01-26 06:29:46,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45595'
2024-01-26 06:29:46,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45591'
2024-01-26 06:29:46,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46347'
2024-01-26 06:29:46,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34107'
2024-01-26 06:29:46,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36083'
2024-01-26 06:29:46,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34015'
2024-01-26 06:29:48,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33377
2024-01-26 06:29:48,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33377
2024-01-26 06:29:48,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43139
2024-01-26 06:29:48,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40989
2024-01-26 06:29:48,344 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43139
2024-01-26 06:29:48,344 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34841
2024-01-26 06:29:48,344 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,345 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,345 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bja4skac
2024-01-26 06:29:48,345 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,345 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,345 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,345 - distributed.worker - INFO - Starting Worker plugin PreImport-ca298a17-c9b8-4b61-8eca-34a73e2d716e
2024-01-26 06:29:48,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-frqcv9az
2024-01-26 06:29:48,345 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c22fbae-9422-4b0b-97cb-01a8a3e865cc
2024-01-26 06:29:48,345 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81ebaf90-2cc1-4b8e-a4fe-50b0f90fbd94
2024-01-26 06:29:48,346 - distributed.worker - INFO - Starting Worker plugin PreImport-510445c6-14c2-44d9-9ff0-09a5ec87894a
2024-01-26 06:29:48,346 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90209858-8210-447c-8c81-c4ef2c71e23a
2024-01-26 06:29:48,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43149
2024-01-26 06:29:48,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43149
2024-01-26 06:29:48,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36113
2024-01-26 06:29:48,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,378 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,379 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eaa6hoj7
2024-01-26 06:29:48,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fc3c772-704f-4700-a8ba-b592949e922e
2024-01-26 06:29:48,379 - distributed.worker - INFO - Starting Worker plugin PreImport-1bc1650b-9d03-4bb4-a02f-7f7775aac6b3
2024-01-26 06:29:48,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec92f358-594c-4056-baeb-60e4fbe9570a
2024-01-26 06:29:48,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43719
2024-01-26 06:29:48,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43719
2024-01-26 06:29:48,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33691
2024-01-26 06:29:48,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,406 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,406 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y88imlgb
2024-01-26 06:29:48,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3a995f3-9442-4200-a06d-4c6c3d1c0bd9
2024-01-26 06:29:48,407 - distributed.worker - INFO - Starting Worker plugin PreImport-d29443f6-4b53-48d3-b507-8f7c72d0310a
2024-01-26 06:29:48,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce3132b1-6cf3-4c87-ac11-348f24b48ee2
2024-01-26 06:29:48,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,461 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,462 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,462 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44677
2024-01-26 06:29:48,462 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44677
2024-01-26 06:29:48,462 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41665
2024-01-26 06:29:48,462 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,462 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,462 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,463 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,463 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yazos_wg
2024-01-26 06:29:48,463 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49211dee-ee64-428a-a0db-a463d6f3b6f4
2024-01-26 06:29:48,463 - distributed.worker - INFO - Starting Worker plugin PreImport-14810ee6-3ac7-44a4-9f09-69d503a8c348
2024-01-26 06:29:48,463 - distributed.worker - INFO - Starting Worker plugin RMMSetup-665de421-409b-4d51-8686-0610b78d0425
2024-01-26 06:29:48,464 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40331
2024-01-26 06:29:48,464 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40331
2024-01-26 06:29:48,464 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33117
2024-01-26 06:29:48,464 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,464 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,464 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,464 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gk28qzhm
2024-01-26 06:29:48,465 - distributed.worker - INFO - Starting Worker plugin RMMSetup-edde8549-f92e-45a8-a9fa-7ed5a522e4f5
2024-01-26 06:29:48,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:48,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:48,836 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,836 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:48,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44591
2024-01-26 06:29:48,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34681
2024-01-26 06:29:48,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44591
2024-01-26 06:29:48,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34681
2024-01-26 06:29:48,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33911
2024-01-26 06:29:48,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43323
2024-01-26 06:29:48,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,838 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:48,838 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,838 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:48,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,838 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:48,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x6w2ejt_
2024-01-26 06:29:48,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:48,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bm6mcrsw
2024-01-26 06:29:48,838 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9e0e8b0-f513-4401-8759-7536aaa4c835
2024-01-26 06:29:48,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17c36643-4449-4e8d-994d-6bbda7adf659
2024-01-26 06:29:48,840 - distributed.worker - INFO - Starting Worker plugin PreImport-766368cb-2360-4ddd-a31c-fdba33b2707a
2024-01-26 06:29:48,840 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45fd6c47-578c-478c-800a-fda7fe7791bf
2024-01-26 06:29:50,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,081 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43139', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,082 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43139
2024-01-26 06:29:50,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54872
2024-01-26 06:29:50,083 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,084 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,219 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43149', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43149
2024-01-26 06:29:50,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54888
2024-01-26 06:29:50,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,248 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6daeb2a6-f04a-40cb-b07c-c3581829ed23
2024-01-26 06:29:50,455 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33377', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33377
2024-01-26 06:29:50,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54904
2024-01-26 06:29:50,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,484 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,645 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43719', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,646 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43719
2024-01-26 06:29:50,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54920
2024-01-26 06:29:50,647 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44677', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,647 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44677
2024-01-26 06:29:50,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54930
2024-01-26 06:29:50,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,648 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,649 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a7482a2-b72e-4e13-99c0-8bfcdbae8a8b
2024-01-26 06:29:50,651 - distributed.worker - INFO - Starting Worker plugin PreImport-651919f6-d8d7-4eb3-892f-2964b990f65a
2024-01-26 06:29:50,652 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,673 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40331', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,674 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40331
2024-01-26 06:29:50,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54932
2024-01-26 06:29:50,675 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,676 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,676 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,682 - distributed.worker - INFO - Starting Worker plugin PreImport-d9636cfa-02ac-44f2-8357-37a068321320
2024-01-26 06:29:50,682 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3574b188-bf7b-4b77-bf57-9d8996792b1f
2024-01-26 06:29:50,684 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,689 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,712 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34681', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,713 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34681
2024-01-26 06:29:50,713 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54944
2024-01-26 06:29:50,714 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,715 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,715 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,717 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44591', status: init, memory: 0, processing: 0>
2024-01-26 06:29:50,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,717 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44591
2024-01-26 06:29:50,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54934
2024-01-26 06:29:50,720 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:50,721 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:50,721 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:50,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:50,775 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,775 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,775 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,775 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,775 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,775 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,776 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,776 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:50,780 - distributed.scheduler - INFO - Remove client Client-4b22a73b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:50,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39662; closing.
2024-01-26 06:29:50,781 - distributed.scheduler - INFO - Remove client Client-4b22a73b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:50,781 - distributed.scheduler - INFO - Close client connection: Client-4b22a73b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:50,782 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40725'. Reason: nanny-close
2024-01-26 06:29:50,783 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,783 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37139'. Reason: nanny-close
2024-01-26 06:29:50,784 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45595'. Reason: nanny-close
2024-01-26 06:29:50,784 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33377. Reason: nanny-close
2024-01-26 06:29:50,784 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45591'. Reason: nanny-close
2024-01-26 06:29:50,784 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43139. Reason: nanny-close
2024-01-26 06:29:50,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,785 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46347'. Reason: nanny-close
2024-01-26 06:29:50,785 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43719. Reason: nanny-close
2024-01-26 06:29:50,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,785 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34107'. Reason: nanny-close
2024-01-26 06:29:50,785 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34681. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36083'. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44677. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54904; closing.
2024-01-26 06:29:50,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34015'. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7867212')
2024-01-26 06:29:50,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40331. Reason: nanny-close
2024-01-26 06:29:50,786 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:50,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44591. Reason: nanny-close
2024-01-26 06:29:50,787 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43149. Reason: nanny-close
2024-01-26 06:29:50,788 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,788 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,788 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54920; closing.
2024-01-26 06:29:50,788 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54872; closing.
2024-01-26 06:29:50,789 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,789 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,789 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43719', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7897942')
2024-01-26 06:29:50,790 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,790 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7901566')
2024-01-26 06:29:50,790 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:50,790 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,790 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54944; closing.
2024-01-26 06:29:50,790 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54930; closing.
2024-01-26 06:29:50,791 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34681', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7913873')
2024-01-26 06:29:50,791 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,791 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44677', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7917244')
2024-01-26 06:29:50,791 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,792 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:50,792 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54932; closing.
2024-01-26 06:29:50,792 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40331', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7926295')
2024-01-26 06:29:50,792 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54934; closing.
2024-01-26 06:29:50,793 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54888; closing.
2024-01-26 06:29:50,793 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44591', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.793534')
2024-01-26 06:29:50,794 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43149', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250590.7939527')
2024-01-26 06:29:50,794 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:29:51,648 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:29:51,649 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:29:51,649 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:29:51,650 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:29:51,650 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-26 06:29:53,733 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:29:53,737 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46521 instead
  warnings.warn(
2024-01-26 06:29:53,741 - distributed.scheduler - INFO - State start
2024-01-26 06:29:53,837 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:29:53,838 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:29:53,839 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46521/status
2024-01-26 06:29:53,839 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:29:53,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40297'
2024-01-26 06:29:53,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39659'
2024-01-26 06:29:53,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44573'
2024-01-26 06:29:54,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35401'
2024-01-26 06:29:54,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38777'
2024-01-26 06:29:54,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40363'
2024-01-26 06:29:54,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36607'
2024-01-26 06:29:54,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32825'
2024-01-26 06:29:54,626 - distributed.scheduler - INFO - Receive client connection: Client-4fd0db8c-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:54,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55122
2024-01-26 06:29:55,912 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:55,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:55,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:55,917 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32801
2024-01-26 06:29:55,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32801
2024-01-26 06:29:55,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43737
2024-01-26 06:29:55,918 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:55,918 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:55,918 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:55,918 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:55,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c0o1_mvi
2024-01-26 06:29:55,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f52e71a8-b3d7-4003-96c4-5723d7e71145
2024-01-26 06:29:55,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7aafad8-173f-4289-b318-75dd8535d773
2024-01-26 06:29:55,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:55,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:55,927 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:55,927 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:55,927 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:55,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41639
2024-01-26 06:29:55,928 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41639
2024-01-26 06:29:55,928 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41971
2024-01-26 06:29:55,928 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:55,928 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:55,928 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:55,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:55,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pduv33ao
2024-01-26 06:29:55,929 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9294f11-9086-4dc5-80b3-ae22453b2230
2024-01-26 06:29:55,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:55,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43823
2024-01-26 06:29:55,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43823
2024-01-26 06:29:55,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39495
2024-01-26 06:29:55,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:55,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:55,933 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:55,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:55,933 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:55,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:55,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kq78bhjm
2024-01-26 06:29:55,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b68e900e-d408-4f79-954b-09f61bee05c7
2024-01-26 06:29:55,935 - distributed.worker - INFO - Starting Worker plugin PreImport-4ee580c0-ec01-40c4-9970-354aafd2376b
2024-01-26 06:29:55,936 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1db1fd2-7017-4296-81a1-6b62fe4be0b5
2024-01-26 06:29:55,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:55,938 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36137
2024-01-26 06:29:55,938 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36137
2024-01-26 06:29:55,938 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40639
2024-01-26 06:29:55,938 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:55,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:55,938 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:55,939 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:55,939 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3da75cnj
2024-01-26 06:29:55,939 - distributed.worker - INFO - Starting Worker plugin PreImport-ed0a037e-cc9d-4e08-94f7-2e36a75696e5
2024-01-26 06:29:55,939 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc45a017-9d79-4791-96d0-3235b1127345
2024-01-26 06:29:55,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:55,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:55,965 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:55,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36153
2024-01-26 06:29:55,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36153
2024-01-26 06:29:55,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46511
2024-01-26 06:29:55,966 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:55,966 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:55,966 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:55,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:55,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6jzmz55z
2024-01-26 06:29:55,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cec73b32-64f7-4128-86a6-778d6609e1d2
2024-01-26 06:29:55,967 - distributed.worker - INFO - Starting Worker plugin PreImport-2ccb7be3-29a6-406e-9d6d-73d0a6001745
2024-01-26 06:29:55,967 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4eb623f2-b322-4f36-adfe-f23d71e4fbf2
2024-01-26 06:29:56,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:56,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:56,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:56,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34133
2024-01-26 06:29:56,013 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34133
2024-01-26 06:29:56,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43635
2024-01-26 06:29:56,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:56,013 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:56,013 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:56,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:56,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gigg200q
2024-01-26 06:29:56,014 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ddd39839-576e-47ba-926c-d9271ee483ea
2024-01-26 06:29:56,015 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da76971c-8811-41db-98a4-76d3bd456ce1
2024-01-26 06:29:56,031 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:56,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:56,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:29:56,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:29:56,035 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:56,036 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45965
2024-01-26 06:29:56,036 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45965
2024-01-26 06:29:56,036 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45863
2024-01-26 06:29:56,036 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:56,036 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:56,036 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:56,037 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:56,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_36pfvel
2024-01-26 06:29:56,037 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92bf40d3-4bb9-4521-9b6f-40fe4866ad83
2024-01-26 06:29:56,037 - distributed.worker - INFO - Starting Worker plugin PreImport-2187e7a0-3069-469c-8087-6e53406c4bb4
2024-01-26 06:29:56,037 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d6bcddc-0284-4c03-b8fc-68edfe352f1d
2024-01-26 06:29:56,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:29:56,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41983
2024-01-26 06:29:56,041 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41983
2024-01-26 06:29:56,041 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42531
2024-01-26 06:29:56,041 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:29:56,041 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:56,041 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:29:56,041 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:29:56,041 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-18yoe85g
2024-01-26 06:29:56,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6144004c-44bd-4b1e-9028-eb0780e48911
2024-01-26 06:29:57,729 - distributed.worker - INFO - Starting Worker plugin PreImport-b4047b49-9fb1-4f6b-b8f0-ace3e49ee545
2024-01-26 06:29:57,731 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,760 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32801', status: init, memory: 0, processing: 0>
2024-01-26 06:29:57,762 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32801
2024-01-26 06:29:57,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55148
2024-01-26 06:29:57,764 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:57,764 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:57,765 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:57,876 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,896 - distributed.worker - INFO - Starting Worker plugin PreImport-739a2a70-7dbe-4bcc-9760-b32c518e34b6
2024-01-26 06:29:57,898 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f2d56b1-f0b0-4e62-a1e3-e98a3a23a7a1
2024-01-26 06:29:57,899 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,903 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36153', status: init, memory: 0, processing: 0>
2024-01-26 06:29:57,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36153
2024-01-26 06:29:57,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55154
2024-01-26 06:29:57,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:57,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:57,913 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:57,928 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41639', status: init, memory: 0, processing: 0>
2024-01-26 06:29:57,929 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41639
2024-01-26 06:29:57,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55158
2024-01-26 06:29:57,930 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:57,930 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:57,931 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:57,934 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43823', status: init, memory: 0, processing: 0>
2024-01-26 06:29:57,935 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43823
2024-01-26 06:29:57,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55172
2024-01-26 06:29:57,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:57,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5072ef9-4cfc-4be9-9d9a-126ef1471c71
2024-01-26 06:29:57,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:57,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,939 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:57,957 - distributed.worker - INFO - Starting Worker plugin PreImport-8964ef5c-a9e0-4811-8f6b-a921a778a1a7
2024-01-26 06:29:57,958 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,968 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36137', status: init, memory: 0, processing: 0>
2024-01-26 06:29:57,969 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36137
2024-01-26 06:29:57,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55184
2024-01-26 06:29:57,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:57,971 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:57,971 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:57,983 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34133', status: init, memory: 0, processing: 0>
2024-01-26 06:29:57,984 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34133
2024-01-26 06:29:57,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55192
2024-01-26 06:29:57,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:57,985 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:57,985 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:57,987 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:58,013 - distributed.worker - INFO - Starting Worker plugin PreImport-02cd5d20-c29e-4060-9908-d2906ad91a70
2024-01-26 06:29:58,014 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cae6e3ab-62aa-4658-9d92-5a63b596f846
2024-01-26 06:29:58,014 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:58,016 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:58,036 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41983', status: init, memory: 0, processing: 0>
2024-01-26 06:29:58,036 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41983
2024-01-26 06:29:58,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55208
2024-01-26 06:29:58,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:58,038 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:58,038 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:58,040 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:58,040 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45965', status: init, memory: 0, processing: 0>
2024-01-26 06:29:58,041 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45965
2024-01-26 06:29:58,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55212
2024-01-26 06:29:58,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:29:58,043 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:29:58,043 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:29:58,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:29:58,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,115 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,115 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:29:58,120 - distributed.scheduler - INFO - Remove client Client-4fd0db8c-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:58,120 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55122; closing.
2024-01-26 06:29:58,120 - distributed.scheduler - INFO - Remove client Client-4fd0db8c-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:58,121 - distributed.scheduler - INFO - Close client connection: Client-4fd0db8c-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:29:58,121 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40297'. Reason: nanny-close
2024-01-26 06:29:58,122 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39659'. Reason: nanny-close
2024-01-26 06:29:58,123 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44573'. Reason: nanny-close
2024-01-26 06:29:58,124 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36137. Reason: nanny-close
2024-01-26 06:29:58,124 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35401'. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32801. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41983. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38777'. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40363'. Reason: nanny-close
2024-01-26 06:29:58,125 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41639. Reason: nanny-close
2024-01-26 06:29:58,126 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36607'. Reason: nanny-close
2024-01-26 06:29:58,126 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43823. Reason: nanny-close
2024-01-26 06:29:58,126 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32825'. Reason: nanny-close
2024-01-26 06:29:58,126 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,127 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,127 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36153. Reason: nanny-close
2024-01-26 06:29:58,127 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55184; closing.
2024-01-26 06:29:58,127 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:29:58,127 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45965. Reason: nanny-close
2024-01-26 06:29:58,127 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1275258')
2024-01-26 06:29:58,127 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,127 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,128 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34133. Reason: nanny-close
2024-01-26 06:29:58,128 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55208; closing.
2024-01-26 06:29:58,128 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,128 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,128 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,128 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1288786')
2024-01-26 06:29:58,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,129 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,129 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,130 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:29:58,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55148; closing.
2024-01-26 06:29:58,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55158; closing.
2024-01-26 06:29:58,130 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,130 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,131 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,131 - distributed.nanny - INFO - Worker closed
2024-01-26 06:29:58,131 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55208>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:29:58,133 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1338856')
2024-01-26 06:29:58,134 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55172; closing.
2024-01-26 06:29:58,134 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1346905')
2024-01-26 06:29:58,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55154; closing.
2024-01-26 06:29:58,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55212; closing.
2024-01-26 06:29:58,135 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43823', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.135914')
2024-01-26 06:29:58,136 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1364963')
2024-01-26 06:29:58,136 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45965', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1368887')
2024-01-26 06:29:58,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55192; closing.
2024-01-26 06:29:58,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34133', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250598.1377232')
2024-01-26 06:29:58,138 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:29:59,138 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:29:59,138 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:29:59,139 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:29:59,140 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:29:59,141 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-26 06:30:01,378 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:01,383 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41261 instead
  warnings.warn(
2024-01-26 06:30:01,386 - distributed.scheduler - INFO - State start
2024-01-26 06:30:01,676 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:01,677 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:30:01,678 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41261/status
2024-01-26 06:30:01,678 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:30:01,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43913'
2024-01-26 06:30:01,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44515'
2024-01-26 06:30:02,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35979'
2024-01-26 06:30:02,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42035'
2024-01-26 06:30:02,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39867'
2024-01-26 06:30:02,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45165'
2024-01-26 06:30:02,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34345'
2024-01-26 06:30:02,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43843'
2024-01-26 06:30:02,418 - distributed.scheduler - INFO - Receive client connection: Client-546c2a2d-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:02,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57958
2024-01-26 06:30:03,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:03,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:03,991 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:03,992 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39451
2024-01-26 06:30:03,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39451
2024-01-26 06:30:03,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43977
2024-01-26 06:30:03,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:03,992 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:03,993 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:03,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:03,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fqvjwvrh
2024-01-26 06:30:03,993 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f9a9ca2-6b66-437b-9b38-45c9023f74e5
2024-01-26 06:30:03,993 - distributed.worker - INFO - Starting Worker plugin PreImport-178eaa16-5652-4805-a4c9-5a3429bcce85
2024-01-26 06:30:03,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0bf5d4a-6241-4b50-8b0d-d29d1fa8b426
2024-01-26 06:30:04,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,037 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,038 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43567
2024-01-26 06:30:04,038 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43567
2024-01-26 06:30:04,038 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43833
2024-01-26 06:30:04,038 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,038 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,038 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,038 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,038 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tajvr7kn
2024-01-26 06:30:04,039 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1aefd4cf-16d2-43ef-a5b3-fa96ca51b884
2024-01-26 06:30:04,039 - distributed.worker - INFO - Starting Worker plugin PreImport-84c864c8-0c54-428c-a679-735c9a5389cd
2024-01-26 06:30:04,040 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cfdf3d5-4ace-4ff2-b085-73b8ed7c02f3
2024-01-26 06:30:04,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44709
2024-01-26 06:30:04,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44709
2024-01-26 06:30:04,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33813
2024-01-26 06:30:04,058 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,058 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,058 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xic0g38m
2024-01-26 06:30:04,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67d09a54-c32a-4de2-bb16-cf929ac83e77
2024-01-26 06:30:04,059 - distributed.worker - INFO - Starting Worker plugin PreImport-c273fcfd-5e07-4cb9-b390-e3246fd972d4
2024-01-26 06:30:04,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eddc975b-e3b8-4cc4-ae82-0734c672657a
2024-01-26 06:30:04,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39563
2024-01-26 06:30:04,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39563
2024-01-26 06:30:04,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40727
2024-01-26 06:30:04,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,060 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,060 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,060 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9dfey4mq
2024-01-26 06:30:04,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3969999a-9ab9-4c7b-9c59-4f892f2b53f5
2024-01-26 06:30:04,062 - distributed.worker - INFO - Starting Worker plugin PreImport-873d7fa6-6c9c-4ac1-859a-e7a65b6f3fe9
2024-01-26 06:30:04,062 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76ad8402-c24f-4b4c-a9b8-6921434378f3
2024-01-26 06:30:04,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39537
2024-01-26 06:30:04,269 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39537
2024-01-26 06:30:04,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43237
2024-01-26 06:30:04,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,269 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35035
2024-01-26 06:30:04,269 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35035
2024-01-26 06:30:04,269 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,270 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45643
2024-01-26 06:30:04,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l3654x9h
2024-01-26 06:30:04,270 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,270 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,270 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,270 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-56a5s6ej
2024-01-26 06:30:04,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b03863d-7908-4693-8378-0b54487f6250
2024-01-26 06:30:04,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34ca32b0-23b5-42b4-b418-b4b686167a55
2024-01-26 06:30:04,271 - distributed.worker - INFO - Starting Worker plugin PreImport-cc6d5843-06fc-474e-84b0-5aa83cb94b0c
2024-01-26 06:30:04,271 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35121
2024-01-26 06:30:04,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35121
2024-01-26 06:30:04,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-271ddfb7-a764-4b72-95cc-602d30e9e1a4
2024-01-26 06:30:04,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35435
2024-01-26 06:30:04,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,272 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8iyaslpj
2024-01-26 06:30:04,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91f69632-77c3-4b1b-86af-2a3ed6439260
2024-01-26 06:30:04,273 - distributed.worker - INFO - Starting Worker plugin PreImport-f6ddd0a6-59be-44f1-a9c7-41804b274520
2024-01-26 06:30:04,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92669359-0a96-40b9-ae21-b2021d9856a5
2024-01-26 06:30:04,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:04,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:04,293 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:04,294 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39579
2024-01-26 06:30:04,294 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39579
2024-01-26 06:30:04,294 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44079
2024-01-26 06:30:04,294 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:04,294 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:04,294 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:04,294 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:04,294 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qpghqbgr
2024-01-26 06:30:04,295 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1ad4d61-9b77-43e4-9bdc-df1e11792c25
2024-01-26 06:30:04,295 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d9b13d-a61d-4bd8-b6d4-9a1b766d0505
2024-01-26 06:30:04,295 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd926d91-b491-431b-a929-c2e6840bef41
2024-01-26 06:30:06,427 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39451', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,458 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39451
2024-01-26 06:30:06,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57976
2024-01-26 06:30:06,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,461 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,717 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43567', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,743 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43567
2024-01-26 06:30:06,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57990
2024-01-26 06:30:06,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,745 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,745 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,750 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,763 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,771 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44709', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,771 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44709
2024-01-26 06:30:06,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58000
2024-01-26 06:30:06,772 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,773 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,773 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,778 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,781 - distributed.worker - INFO - Starting Worker plugin PreImport-21273f9a-c252-487f-b3c9-fc8680ee9243
2024-01-26 06:30:06,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64923424-be29-466b-80aa-23a0d5c65302
2024-01-26 06:30:06,782 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,798 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39563', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,798 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39563
2024-01-26 06:30:06,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58012
2024-01-26 06:30:06,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,801 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,802 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39579', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,802 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39579
2024-01-26 06:30:06,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58030
2024-01-26 06:30:06,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,803 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35121', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,804 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35121
2024-01-26 06:30:06,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58038
2024-01-26 06:30:06,804 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,804 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39537', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,805 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,806 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39537
2024-01-26 06:30:06,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58054
2024-01-26 06:30:06,806 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,806 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,807 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35035', status: init, memory: 0, processing: 0>
2024-01-26 06:30:06,807 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,807 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35035
2024-01-26 06:30:06,807 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58022
2024-01-26 06:30:06,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:06,811 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:06,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:06,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:06,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:06,860 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,860 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,860 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,860 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,861 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,861 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,861 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,861 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:06,869 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:06,870 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:06,873 - distributed.scheduler - INFO - Remove client Client-546c2a2d-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:06,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57958; closing.
2024-01-26 06:30:06,873 - distributed.scheduler - INFO - Remove client Client-546c2a2d-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:06,873 - distributed.scheduler - INFO - Close client connection: Client-546c2a2d-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:06,874 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43913'. Reason: nanny-close
2024-01-26 06:30:06,875 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,875 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44515'. Reason: nanny-close
2024-01-26 06:30:06,876 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35979'. Reason: nanny-close
2024-01-26 06:30:06,876 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39451. Reason: nanny-close
2024-01-26 06:30:06,877 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,877 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42035'. Reason: nanny-close
2024-01-26 06:30:06,877 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43567. Reason: nanny-close
2024-01-26 06:30:06,877 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,877 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39867'. Reason: nanny-close
2024-01-26 06:30:06,877 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39579. Reason: nanny-close
2024-01-26 06:30:06,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45165'. Reason: nanny-close
2024-01-26 06:30:06,878 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44709. Reason: nanny-close
2024-01-26 06:30:06,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34345'. Reason: nanny-close
2024-01-26 06:30:06,879 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,879 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39563. Reason: nanny-close
2024-01-26 06:30:06,879 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,879 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43843'. Reason: nanny-close
2024-01-26 06:30:06,879 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57976; closing.
2024-01-26 06:30:06,879 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,879 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,879 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:06,879 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35035. Reason: nanny-close
2024-01-26 06:30:06,880 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39537. Reason: nanny-close
2024-01-26 06:30:06,880 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.880049')
2024-01-26 06:30:06,880 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,880 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35121. Reason: nanny-close
2024-01-26 06:30:06,880 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57990; closing.
2024-01-26 06:30:06,880 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58030; closing.
2024-01-26 06:30:06,881 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,881 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,881 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58000; closing.
2024-01-26 06:30:06,881 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,881 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,881 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.8816512')
2024-01-26 06:30:06,881 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,881 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,882 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39579', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.8820152')
2024-01-26 06:30:06,882 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,882 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:06,882 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,883 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44709', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.883076')
2024-01-26 06:30:06,883 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,883 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,884 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:06,884 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58030>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:30:06,885 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:57990>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-26 06:30:06,885 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58000>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58000>: Stream is closed
2024-01-26 06:30:06,886 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58012; closing.
2024-01-26 06:30:06,886 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58054; closing.
2024-01-26 06:30:06,886 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58022; closing.
2024-01-26 06:30:06,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39563', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.887087')
2024-01-26 06:30:06,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.8875318')
2024-01-26 06:30:06,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35035', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.8878884')
2024-01-26 06:30:06,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58038; closing.
2024-01-26 06:30:06,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250606.8886461')
2024-01-26 06:30:06,888 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:30:07,891 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:30:07,891 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:30:07,891 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:07,892 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:30:07,893 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-26 06:30:10,146 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:10,150 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39111 instead
  warnings.warn(
2024-01-26 06:30:10,154 - distributed.scheduler - INFO - State start
2024-01-26 06:30:10,175 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:10,176 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:30:10,177 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39111/status
2024-01-26 06:30:10,177 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:30:10,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34775'
2024-01-26 06:30:10,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36261'
2024-01-26 06:30:10,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36051'
2024-01-26 06:30:10,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43797'
2024-01-26 06:30:10,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38961'
2024-01-26 06:30:10,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41797'
2024-01-26 06:30:10,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39633'
2024-01-26 06:30:10,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37051'
2024-01-26 06:30:12,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39935
2024-01-26 06:30:12,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40321
2024-01-26 06:30:12,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40321
2024-01-26 06:30:12,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39935
2024-01-26 06:30:12,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39491
2024-01-26 06:30:12,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40677
2024-01-26 06:30:12,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,547 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,547 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,547 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,547 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hb24ddth
2024-01-26 06:30:12,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dhvl7ran
2024-01-26 06:30:12,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50fe21aa-29eb-41c1-b195-59bfa78a74be
2024-01-26 06:30:12,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66b3b84c-55be-44cc-9239-f40b5baf684d
2024-01-26 06:30:12,548 - distributed.worker - INFO - Starting Worker plugin PreImport-42e1090d-52e9-439a-a788-ad512c8aa220
2024-01-26 06:30:12,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-133d7f95-b2d1-4d63-935b-cd641122334e
2024-01-26 06:30:12,548 - distributed.worker - INFO - Starting Worker plugin PreImport-f3499909-82c6-4d4c-9943-ca46c1d687c3
2024-01-26 06:30:12,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b42ae969-d588-4bd1-b6b9-7f247e6a944a
2024-01-26 06:30:12,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,549 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34361
2024-01-26 06:30:12,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34361
2024-01-26 06:30:12,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44791
2024-01-26 06:30:12,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,549 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,549 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xtajtfza
2024-01-26 06:30:12,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22cf70f4-c3f3-47df-b329-d056db5f8375
2024-01-26 06:30:12,550 - distributed.worker - INFO - Starting Worker plugin PreImport-c9b58fbe-b7fe-4e60-b0e1-909f7d700876
2024-01-26 06:30:12,550 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90adefbf-e444-45d1-b7fa-448012a2e231
2024-01-26 06:30:12,567 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,574 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,575 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45073
2024-01-26 06:30:12,575 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45073
2024-01-26 06:30:12,575 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46207
2024-01-26 06:30:12,575 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,575 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,575 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,575 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,575 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rhtwb7l5
2024-01-26 06:30:12,576 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c36bb87-33ef-4ff7-a12b-6c69806bf2f3
2024-01-26 06:30:12,578 - distributed.worker - INFO - Starting Worker plugin PreImport-4dc65b46-5b7a-4bb8-84bf-5e763a29a95b
2024-01-26 06:30:12,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d888021f-ee27-4762-bef4-8287d4701e6a
2024-01-26 06:30:12,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,612 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,614 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33729
2024-01-26 06:30:12,614 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33729
2024-01-26 06:30:12,614 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40143
2024-01-26 06:30:12,614 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,614 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,614 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,614 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,614 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8w5737dm
2024-01-26 06:30:12,615 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-073a7a96-ea9b-40bf-9be6-f2087e4ff546
2024-01-26 06:30:12,615 - distributed.worker - INFO - Starting Worker plugin PreImport-dc53846a-a8b1-4800-8197-8f5bb23cb458
2024-01-26 06:30:12,615 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10510fe5-7ff7-4554-b211-f1b5ab84688d
2024-01-26 06:30:12,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,620 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36027
2024-01-26 06:30:12,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36027
2024-01-26 06:30:12,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36429
2024-01-26 06:30:12,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,621 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,621 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,621 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n3qwuekn
2024-01-26 06:30:12,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b15c3c66-a805-4b3c-a601-ea272d97cbaa
2024-01-26 06:30:12,622 - distributed.worker - INFO - Starting Worker plugin PreImport-4f977550-e676-43a7-a2db-7b40fdca83f4
2024-01-26 06:30:12,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48c9a2c3-eca8-4179-b881-082714ae956b
2024-01-26 06:30:12,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,831 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40511
2024-01-26 06:30:12,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40511
2024-01-26 06:30:12,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43463
2024-01-26 06:30:12,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,832 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,832 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mw0abt6t
2024-01-26 06:30:12,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d0d0069-d7ef-417f-9471-1eb248df8a58
2024-01-26 06:30:12,868 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:12,868 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:12,873 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:12,874 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44729
2024-01-26 06:30:12,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44729
2024-01-26 06:30:12,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33365
2024-01-26 06:30:12,875 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:12,875 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:12,875 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:12,875 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:12,875 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xa97m99o
2024-01-26 06:30:12,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fd4d94c-31ec-4b03-8b49-49198ccc8ee8
2024-01-26 06:30:12,875 - distributed.worker - INFO - Starting Worker plugin PreImport-4f876713-852e-47f8-8cab-0d1c7ab33b6b
2024-01-26 06:30:12,876 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d880701e-396f-4a08-a52c-b08cf69326e6
2024-01-26 06:30:14,312 - distributed.scheduler - INFO - Receive client connection: Client-598b8f6b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:14,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32904
2024-01-26 06:30:16,404 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,430 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34361', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34361
2024-01-26 06:30:16,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32910
2024-01-26 06:30:16,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,434 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,434 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,434 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,449 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36027', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,457 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36027
2024-01-26 06:30:16,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32912
2024-01-26 06:30:16,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,459 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,459 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,471 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39935', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,472 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39935
2024-01-26 06:30:16,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32924
2024-01-26 06:30:16,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,473 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,473 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,473 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,477 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,484 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,485 - distributed.worker - INFO - Starting Worker plugin PreImport-bb2d0ee1-32c2-4e11-a5a7-bcb2c862940d
2024-01-26 06:30:16,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2300d5d2-41df-443a-a46e-00152c752c22
2024-01-26 06:30:16,487 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,499 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,505 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33729', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,506 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33729
2024-01-26 06:30:16,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32940
2024-01-26 06:30:16,507 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,508 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,508 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,510 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40321', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,511 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40321
2024-01-26 06:30:16,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32928
2024-01-26 06:30:16,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,513 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,513 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,514 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45073', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,515 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45073
2024-01-26 06:30:16,515 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32936
2024-01-26 06:30:16,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,517 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,517 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,528 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40511', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,528 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40511
2024-01-26 06:30:16,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32952
2024-01-26 06:30:16,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,532 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,532 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44729', status: init, memory: 0, processing: 0>
2024-01-26 06:30:16,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44729
2024-01-26 06:30:16,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32966
2024-01-26 06:30:16,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:16,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:16,540 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:16,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:16,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,651 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-26 06:30:16,661 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,662 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:16,665 - distributed.scheduler - INFO - Remove client Client-598b8f6b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:16,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32904; closing.
2024-01-26 06:30:16,665 - distributed.scheduler - INFO - Remove client Client-598b8f6b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:16,666 - distributed.scheduler - INFO - Close client connection: Client-598b8f6b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:16,667 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34775'. Reason: nanny-close
2024-01-26 06:30:16,667 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,668 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36261'. Reason: nanny-close
2024-01-26 06:30:16,668 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,669 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36051'. Reason: nanny-close
2024-01-26 06:30:16,669 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39935. Reason: nanny-close
2024-01-26 06:30:16,669 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,669 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43797'. Reason: nanny-close
2024-01-26 06:30:16,669 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36027. Reason: nanny-close
2024-01-26 06:30:16,669 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38961'. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40321. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41797'. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45073. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,670 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,670 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32924; closing.
2024-01-26 06:30:16,670 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39633'. Reason: nanny-close
2024-01-26 06:30:16,671 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34361. Reason: nanny-close
2024-01-26 06:30:16,671 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.6710596')
2024-01-26 06:30:16,671 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37051'. Reason: nanny-close
2024-01-26 06:30:16,671 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33729. Reason: nanny-close
2024-01-26 06:30:16,671 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,671 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:16,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40511. Reason: nanny-close
2024-01-26 06:30:16,672 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,672 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,672 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32912; closing.
2024-01-26 06:30:16,672 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44729. Reason: nanny-close
2024-01-26 06:30:16,673 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,673 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,673 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,673 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36027', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.6737761')
2024-01-26 06:30:16,674 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32928; closing.
2024-01-26 06:30:16,674 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,674 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.6749053')
2024-01-26 06:30:16,675 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,675 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32936; closing.
2024-01-26 06:30:16,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32910; closing.
2024-01-26 06:30:16,675 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,676 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.676155')
2024-01-26 06:30:16,676 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.6765058')
2024-01-26 06:30:16,676 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32940; closing.
2024-01-26 06:30:16,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:16,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.6776059')
2024-01-26 06:30:16,677 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32952; closing.
2024-01-26 06:30:16,678 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32966; closing.
2024-01-26 06:30:16,678 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.678554')
2024-01-26 06:30:16,678 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250616.6789033')
2024-01-26 06:30:16,678 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:16,679 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:30:16,679 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:17,733 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:30:17,733 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:30:17,734 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:17,735 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:30:17,735 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-26 06:30:19,796 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:19,800 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35599 instead
  warnings.warn(
2024-01-26 06:30:19,804 - distributed.scheduler - INFO - State start
2024-01-26 06:30:19,825 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:19,826 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:30:19,826 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35599/status
2024-01-26 06:30:19,826 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:30:20,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43829'
2024-01-26 06:30:20,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41311'
2024-01-26 06:30:20,022 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42521'
2024-01-26 06:30:20,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33207'
2024-01-26 06:30:20,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39143'
2024-01-26 06:30:20,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44153'
2024-01-26 06:30:20,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46341'
2024-01-26 06:30:20,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35981'
2024-01-26 06:30:21,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:21,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:21,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:21,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42423
2024-01-26 06:30:21,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42423
2024-01-26 06:30:21,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41701
2024-01-26 06:30:21,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:21,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:21,812 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:21,812 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:21,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f5rtf6f3
2024-01-26 06:30:21,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c91702b4-de20-4245-8d7a-2a85593edc90
2024-01-26 06:30:21,812 - distributed.worker - INFO - Starting Worker plugin PreImport-359cead1-83bf-43e0-9cd2-a120a2c6f155
2024-01-26 06:30:21,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1586b9ae-2374-4453-a94a-d096d16ab472
2024-01-26 06:30:21,962 - distributed.scheduler - INFO - Receive client connection: Client-5f6cf6bb-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:21,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42644
2024-01-26 06:30:22,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,077 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,077 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,078 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,079 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45305
2024-01-26 06:30:22,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45305
2024-01-26 06:30:22,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38559
2024-01-26 06:30:22,080 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,080 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,080 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5td7jyqb
2024-01-26 06:30:22,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4595e3a0-ed80-452b-86e5-be6e66950b2f
2024-01-26 06:30:22,080 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,087 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,089 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38715
2024-01-26 06:30:22,089 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38715
2024-01-26 06:30:22,089 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41463
2024-01-26 06:30:22,089 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,089 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xtr5jhq4
2024-01-26 06:30:22,089 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:22,089 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:22,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db8f34ed-545c-4c4f-a754-c66c2a293e6b
2024-01-26 06:30:22,090 - distributed.worker - INFO - Starting Worker plugin PreImport-38af6ab9-7416-4abb-993e-ccbe571f06a5
2024-01-26 06:30:22,090 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b901d84-3a57-4608-88d0-ab378b25b443
2024-01-26 06:30:22,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,091 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33075
2024-01-26 06:30:22,092 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33075
2024-01-26 06:30:22,092 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37927
2024-01-26 06:30:22,092 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,092 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,092 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,092 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,092 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-viu1tbp1
2024-01-26 06:30:22,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36597
2024-01-26 06:30:22,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36597
2024-01-26 06:30:22,093 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-caf21977-19ef-4c0e-8844-5cfb46aa8ae0
2024-01-26 06:30:22,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37567
2024-01-26 06:30:22,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,093 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,093 - distributed.worker - INFO - Starting Worker plugin PreImport-802341f5-91f5-43fa-95f9-1b2301f367f1
2024-01-26 06:30:22,093 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,093 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,093 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e043108-56f8-4970-94f2-809f6d5b3340
2024-01-26 06:30:22,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yz2717i5
2024-01-26 06:30:22,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,093 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0668cbc-ee3a-49cf-a039-dfaf25e74184
2024-01-26 06:30:22,093 - distributed.worker - INFO - Starting Worker plugin PreImport-edbf999a-942f-4604-8b3b-09f037f4cb7b
2024-01-26 06:30:22,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3303b6aa-f2d2-46c9-b4d2-029155af6ca5
2024-01-26 06:30:22,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:22,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41179
2024-01-26 06:30:22,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41179
2024-01-26 06:30:22,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40741
2024-01-26 06:30:22,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,095 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,095 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zw925cps
2024-01-26 06:30:22,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57c42912-d2b8-40cc-9fa0-c1ece8b91439
2024-01-26 06:30:22,096 - distributed.worker - INFO - Starting Worker plugin PreImport-742a11f0-c3f6-4524-8fe1-afe0c0d85f4f
2024-01-26 06:30:22,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3759c0f3-0272-463a-a99c-8ccc1a1e1cca
2024-01-26 06:30:22,096 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35889
2024-01-26 06:30:22,096 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33981
2024-01-26 06:30:22,096 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35889
2024-01-26 06:30:22,097 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33981
2024-01-26 06:30:22,097 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46025
2024-01-26 06:30:22,097 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,097 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,097 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34163
2024-01-26 06:30:22,097 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,097 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,097 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,097 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,097 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:22,097 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nlqpl8xy
2024-01-26 06:30:22,097 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:22,097 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-80le1ccu
2024-01-26 06:30:22,098 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4073442b-cf76-4b37-9185-9bebd671e0ea
2024-01-26 06:30:22,098 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c11854e0-2653-405b-b4b4-cea631e4185f
2024-01-26 06:30:22,098 - distributed.worker - INFO - Starting Worker plugin PreImport-13490f20-5591-4652-ba19-59734573f478
2024-01-26 06:30:22,098 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5320bba8-235d-46e5-861a-f56a9a755f83
2024-01-26 06:30:22,100 - distributed.worker - INFO - Starting Worker plugin PreImport-09153a4a-364e-4382-bbfc-3b79e766716a
2024-01-26 06:30:22,100 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86ac522d-c9f3-486e-9803-29462d3672fd
2024-01-26 06:30:22,537 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42423', status: init, memory: 0, processing: 0>
2024-01-26 06:30:22,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42423
2024-01-26 06:30:22,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42658
2024-01-26 06:30:22,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:22,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:22,569 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:22,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:23,972 - distributed.worker - INFO - Starting Worker plugin PreImport-1ed5f3ba-38fa-4b33-ab94-0e498658e44c
2024-01-26 06:30:23,972 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4197eaee-b341-48e7-89c5-f1c6332c0d55
2024-01-26 06:30:23,973 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:23,996 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45305', status: init, memory: 0, processing: 0>
2024-01-26 06:30:23,997 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45305
2024-01-26 06:30:23,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42686
2024-01-26 06:30:23,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:23,999 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:23,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,016 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,037 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33075', status: init, memory: 0, processing: 0>
2024-01-26 06:30:24,038 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33075
2024-01-26 06:30:24,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42696
2024-01-26 06:30:24,039 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:24,040 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:24,040 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,064 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41179', status: init, memory: 0, processing: 0>
2024-01-26 06:30:24,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41179
2024-01-26 06:30:24,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42712
2024-01-26 06:30:24,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:24,090 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:24,090 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,095 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,097 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,103 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,110 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,119 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36597', status: init, memory: 0, processing: 0>
2024-01-26 06:30:24,119 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36597
2024-01-26 06:30:24,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42730
2024-01-26 06:30:24,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:24,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:24,121 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,123 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35889', status: init, memory: 0, processing: 0>
2024-01-26 06:30:24,124 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35889
2024-01-26 06:30:24,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42714
2024-01-26 06:30:24,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:24,127 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:24,127 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,134 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33981', status: init, memory: 0, processing: 0>
2024-01-26 06:30:24,135 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33981
2024-01-26 06:30:24,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42734
2024-01-26 06:30:24,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:24,138 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:24,138 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,139 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38715', status: init, memory: 0, processing: 0>
2024-01-26 06:30:24,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38715
2024-01-26 06:30:24,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42736
2024-01-26 06:30:24,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:24,142 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:24,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:24,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:24,185 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,187 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,187 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-26 06:30:24,192 - distributed.scheduler - INFO - Remove client Client-5f6cf6bb-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:24,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42644; closing.
2024-01-26 06:30:24,193 - distributed.scheduler - INFO - Remove client Client-5f6cf6bb-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:24,194 - distributed.scheduler - INFO - Close client connection: Client-5f6cf6bb-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:24,194 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43829'. Reason: nanny-close
2024-01-26 06:30:24,195 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,195 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41311'. Reason: nanny-close
2024-01-26 06:30:24,196 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,196 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42521'. Reason: nanny-close
2024-01-26 06:30:24,196 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42423. Reason: nanny-close
2024-01-26 06:30:24,196 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33207'. Reason: nanny-close
2024-01-26 06:30:24,197 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38715. Reason: nanny-close
2024-01-26 06:30:24,197 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39143'. Reason: nanny-close
2024-01-26 06:30:24,197 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36597. Reason: nanny-close
2024-01-26 06:30:24,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44153'. Reason: nanny-close
2024-01-26 06:30:24,198 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33075. Reason: nanny-close
2024-01-26 06:30:24,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46341'. Reason: nanny-close
2024-01-26 06:30:24,198 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,198 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35889. Reason: nanny-close
2024-01-26 06:30:24,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35981'. Reason: nanny-close
2024-01-26 06:30:24,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33981. Reason: nanny-close
2024-01-26 06:30:24,199 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:24,199 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42658; closing.
2024-01-26 06:30:24,199 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45305. Reason: nanny-close
2024-01-26 06:30:24,199 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42730; closing.
2024-01-26 06:30:24,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,200 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.200146')
2024-01-26 06:30:24,200 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41179. Reason: nanny-close
2024-01-26 06:30:24,200 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,201 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36597', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.201151')
2024-01-26 06:30:24,201 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,201 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:24,202 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42736; closing.
2024-01-26 06:30:24,202 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,203 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,203 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,204 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42696; closing.
2024-01-26 06:30:24,204 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:24,204 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.204286')
2024-01-26 06:30:24,206 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33075', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.2059717')
2024-01-26 06:30:24,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42714; closing.
2024-01-26 06:30:24,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42686; closing.
2024-01-26 06:30:24,207 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42734; closing.
2024-01-26 06:30:24,207 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42712; closing.
2024-01-26 06:30:24,207 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.20778')
2024-01-26 06:30:24,208 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.208315')
2024-01-26 06:30:24,208 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33981', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.2089062')
2024-01-26 06:30:24,209 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41179', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250624.2094584')
2024-01-26 06:30:24,209 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:30:25,160 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:30:25,161 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:30:25,162 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:25,163 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:30:25,164 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-26 06:30:27,464 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:27,469 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33129 instead
  warnings.warn(
2024-01-26 06:30:27,473 - distributed.scheduler - INFO - State start
2024-01-26 06:30:27,496 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:27,497 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:30:27,498 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33129/status
2024-01-26 06:30:27,498 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:30:27,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38017'
2024-01-26 06:30:28,648 - distributed.scheduler - INFO - Receive client connection: Client-63e8f3c4-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:28,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42832
2024-01-26 06:30:29,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:29,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:29,820 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:29,821 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46831
2024-01-26 06:30:29,821 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46831
2024-01-26 06:30:29,821 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-26 06:30:29,821 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:29,822 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:29,822 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:29,822 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:30:29,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0bdk41h5
2024-01-26 06:30:29,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5301f293-0a07-422d-bf4a-61c551694e75
2024-01-26 06:30:29,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff229857-3437-45b8-b4b2-d3665ffb58eb
2024-01-26 06:30:29,823 - distributed.worker - INFO - Starting Worker plugin PreImport-a361db89-e22a-447e-82fe-cc04a6a7cc71
2024-01-26 06:30:29,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:29,878 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46831', status: init, memory: 0, processing: 0>
2024-01-26 06:30:29,879 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46831
2024-01-26 06:30:29,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42854
2024-01-26 06:30:29,881 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:29,882 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:29,882 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:29,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:29,886 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:29,889 - distributed.scheduler - INFO - Remove client Client-63e8f3c4-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:29,889 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42832; closing.
2024-01-26 06:30:29,889 - distributed.scheduler - INFO - Remove client Client-63e8f3c4-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:29,890 - distributed.scheduler - INFO - Close client connection: Client-63e8f3c4-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:29,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38017'. Reason: nanny-close
2024-01-26 06:30:29,934 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:29,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46831. Reason: nanny-close
2024-01-26 06:30:29,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42854; closing.
2024-01-26 06:30:29,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:29,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250629.9384396')
2024-01-26 06:30:29,938 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:30:29,940 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:30,756 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:30:30,756 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:30:30,757 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:30,758 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:30:30,758 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-26 06:30:34,778 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:34,782 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41669 instead
  warnings.warn(
2024-01-26 06:30:34,786 - distributed.scheduler - INFO - State start
2024-01-26 06:30:34,808 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:34,809 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-26 06:30:34,810 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41669/status
2024-01-26 06:30:34,810 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:30:34,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39481'
2024-01-26 06:30:36,552 - distributed.scheduler - INFO - Receive client connection: Client-68445216-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:36,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49126
2024-01-26 06:30:36,769 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:36,769 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:37,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:37,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34651
2024-01-26 06:30:37,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34651
2024-01-26 06:30:37,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44179
2024-01-26 06:30:37,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:37,365 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:37,365 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:37,365 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:30:37,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1eh9mfwp
2024-01-26 06:30:37,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae27492f-fc50-480c-aa16-97ca89de6158
2024-01-26 06:30:37,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d2d7414-ad5b-49ca-877d-ea928fe37271
2024-01-26 06:30:37,366 - distributed.worker - INFO - Starting Worker plugin PreImport-240c191a-d7d5-46ab-a51a-7aa675b97dc4
2024-01-26 06:30:37,367 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:37,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34651', status: init, memory: 0, processing: 0>
2024-01-26 06:30:37,416 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34651
2024-01-26 06:30:37,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49146
2024-01-26 06:30:37,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:37,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:37,419 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:37,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:37,487 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:37,490 - distributed.scheduler - INFO - Remove client Client-68445216-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:37,490 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49126; closing.
2024-01-26 06:30:37,490 - distributed.scheduler - INFO - Remove client Client-68445216-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:37,490 - distributed.scheduler - INFO - Close client connection: Client-68445216-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:37,491 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39481'. Reason: nanny-close
2024-01-26 06:30:37,492 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:37,493 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34651. Reason: nanny-close
2024-01-26 06:30:37,495 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:37,495 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49146; closing.
2024-01-26 06:30:37,495 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250637.4954283')
2024-01-26 06:30:37,495 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:30:37,496 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:38,044 - distributed.scheduler - INFO - Receive client connection: Client-6b4953e7-bc14-11ee-ba82-d8c49764f6bb
2024-01-26 06:30:38,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49156
2024-01-26 06:30:38,107 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:30:38,107 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:30:38,108 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:38,109 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-26 06:30:38,109 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-26 06:30:40,221 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:40,225 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43687 instead
  warnings.warn(
2024-01-26 06:30:40,229 - distributed.scheduler - INFO - State start
2024-01-26 06:30:40,251 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:40,252 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-26 06:30:40,253 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:40,254 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-26 06:30:45,130 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:45,134 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:30:45,137 - distributed.scheduler - INFO - State start
2024-01-26 06:30:45,158 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:45,159 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-26 06:30:45,159 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-26 06:30:45,160 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-26 06:30:45,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34399'
2024-01-26 06:30:45,537 - distributed.scheduler - INFO - Receive client connection: Client-6e7da36b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:45,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42094
2024-01-26 06:30:47,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:47,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:47,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:47,012 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41555
2024-01-26 06:30:47,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41555
2024-01-26 06:30:47,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44367
2024-01-26 06:30:47,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-26 06:30:47,012 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:47,012 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:47,012 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:30:47,012 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-kfwljn1f
2024-01-26 06:30:47,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28538e66-7f47-4581-8235-dbb0d1abdd71
2024-01-26 06:30:47,013 - distributed.worker - INFO - Starting Worker plugin PreImport-f70880be-cc48-4087-a45d-c2ca36a03fa5
2024-01-26 06:30:47,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72dbc8ce-d7e2-4b65-b75a-17e19189d1cd
2024-01-26 06:30:47,013 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:47,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41555', status: init, memory: 0, processing: 0>
2024-01-26 06:30:47,063 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41555
2024-01-26 06:30:47,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42102
2024-01-26 06:30:47,063 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:47,064 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-26 06:30:47,064 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:47,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-26 06:30:47,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-26 06:30:47,085 - distributed.scheduler - INFO - Remove client Client-6e7da36b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:47,085 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42094; closing.
2024-01-26 06:30:47,086 - distributed.scheduler - INFO - Remove client Client-6e7da36b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:47,086 - distributed.scheduler - INFO - Close client connection: Client-6e7da36b-bc14-11ee-b5f7-d8c49764f6bb
2024-01-26 06:30:47,087 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34399'. Reason: nanny-close
2024-01-26 06:30:47,116 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:47,117 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41555. Reason: nanny-close
2024-01-26 06:30:47,118 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-26 06:30:47,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42102; closing.
2024-01-26 06:30:47,118 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41555', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706250647.1187904')
2024-01-26 06:30:47,119 - distributed.scheduler - INFO - Lost all workers
2024-01-26 06:30:47,119 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:47,853 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-26 06:30:47,853 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-26 06:30:47,853 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:47,854 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-26 06:30:47,855 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-26 06:30:49,947 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:49,951 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-26 06:30:49,954 - distributed.scheduler - INFO - State start
2024-01-26 06:30:50,013 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:50,014 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-26 06:30:50,015 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:50,016 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-26 06:30:50,757 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36649'
2024-01-26 06:30:50,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33207'
2024-01-26 06:30:50,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39725'
2024-01-26 06:30:50,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44649'
2024-01-26 06:30:50,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37875'
2024-01-26 06:30:50,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41351'
2024-01-26 06:30:50,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37103'
2024-01-26 06:30:50,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33661'
2024-01-26 06:30:50,940 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36649'. Reason: nanny-close
2024-01-26 06:30:50,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33207'. Reason: nanny-close
2024-01-26 06:30:50,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39725'. Reason: nanny-close
2024-01-26 06:30:50,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44649'. Reason: nanny-close
2024-01-26 06:30:50,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37875'. Reason: nanny-close
2024-01-26 06:30:50,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41351'. Reason: nanny-close
2024-01-26 06:30:50,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37103'. Reason: nanny-close
2024-01-26 06:30:50,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33661'. Reason: nanny-close
2024-01-26 06:30:52,443 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,443 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,447 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,448 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41421
2024-01-26 06:30:52,448 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41421
2024-01-26 06:30:52,448 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40061
2024-01-26 06:30:52,448 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,449 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,449 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,449 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,449 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uypknemy
2024-01-26 06:30:52,449 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e2fa212-c17e-421e-a3df-aa9f37e5a6cb
2024-01-26 06:30:52,449 - distributed.worker - INFO - Starting Worker plugin PreImport-a3ef63fd-f6c1-4a42-a8c3-fcb36cb315bf
2024-01-26 06:30:52,449 - distributed.worker - INFO - Starting Worker plugin RMMSetup-994a16d3-de48-4429-8f04-e513bf013c67
2024-01-26 06:30:52,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,667 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45989
2024-01-26 06:30:52,667 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45989
2024-01-26 06:30:52,667 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34305
2024-01-26 06:30:52,667 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,667 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,667 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,667 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o_bg2382
2024-01-26 06:30:52,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f17d04f-aed6-43b3-92b1-c1523f949d3f
2024-01-26 06:30:52,671 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,671 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34547
2024-01-26 06:30:52,672 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34547
2024-01-26 06:30:52,672 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42615
2024-01-26 06:30:52,672 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,672 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,672 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,672 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,672 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vb0s0vmy
2024-01-26 06:30:52,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,672 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c745fff9-d232-41c6-a043-8ffbcb1ec501
2024-01-26 06:30:52,675 - distributed.worker - INFO - Starting Worker plugin PreImport-abf66191-8da5-412f-8731-71797e7cb7b2
2024-01-26 06:30:52,675 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8a96966-29c3-4e7a-b391-4172afcadef8
2024-01-26 06:30:52,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,676 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40173
2024-01-26 06:30:52,676 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40173
2024-01-26 06:30:52,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,676 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39081
2024-01-26 06:30:52,676 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,677 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,677 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,677 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ka327ll
2024-01-26 06:30:52,677 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c13fcf80-7a82-44da-9d16-eeb63a9935dc
2024-01-26 06:30:52,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40751
2024-01-26 06:30:52,677 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40751
2024-01-26 06:30:52,677 - distributed.worker - INFO - Starting Worker plugin PreImport-81fde67e-6ffd-44f0-8b8f-7ac8db305ffe
2024-01-26 06:30:52,677 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40019
2024-01-26 06:30:52,677 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd47ec64-c1f4-447f-a661-b5455d7b8d97
2024-01-26 06:30:52,677 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37637
2024-01-26 06:30:52,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,677 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,677 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37637
2024-01-26 06:30:52,677 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,677 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37285
2024-01-26 06:30:52,677 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zjen9a4l
2024-01-26 06:30:52,677 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,677 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,677 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,678 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fde12080-824e-4e7b-9b29-ee9e1644c380
2024-01-26 06:30:52,678 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ujjpsv_6
2024-01-26 06:30:52,678 - distributed.worker - INFO - Starting Worker plugin PreImport-e4b664f1-1027-47e1-a332-9476f0d104f6
2024-01-26 06:30:52,678 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d66f3a1d-c11f-4f70-b74a-c4a1654452c0
2024-01-26 06:30:52,678 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7631a92b-977f-4722-8527-0a50df6e3103
2024-01-26 06:30:52,678 - distributed.worker - INFO - Starting Worker plugin PreImport-68490578-2c4f-4ff3-965b-0a21416dca68
2024-01-26 06:30:52,678 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa1dc191-19ec-4421-96b1-539d680cdf24
2024-01-26 06:30:52,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,690 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,695 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37119
2024-01-26 06:30:52,695 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37119
2024-01-26 06:30:52,695 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38913
2024-01-26 06:30:52,695 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,695 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,695 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,695 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r9udkvq8
2024-01-26 06:30:52,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5091eceb-6a1c-45c8-8c68-bef4ae05e533
2024-01-26 06:30:52,696 - distributed.worker - INFO - Starting Worker plugin PreImport-a97ca252-159a-4c1a-b112-64e0ed4ae98b
2024-01-26 06:30:52,696 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5e56f3e-71fe-4c59-b807-4ea3b8be2b3b
2024-01-26 06:30:52,699 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:52,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:52,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:52,704 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40725
2024-01-26 06:30:52,704 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40725
2024-01-26 06:30:52,704 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41859
2024-01-26 06:30:52,704 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:52,704 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:52,705 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:52,705 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-26 06:30:52,705 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zubbnr_j
2024-01-26 06:30:52,705 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e87472dc-a5eb-487f-876d-b6fa94805b73
2024-01-26 06:30:52,705 - distributed.worker - INFO - Starting Worker plugin PreImport-486a6d68-b77e-4a56-a733-c3c43d494144
2024-01-26 06:30:52,706 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da30967b-65de-4500-ba18-b43ff5ca9fe7
2024-01-26 06:30:52,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,380 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,381 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,382 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,397 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41421. Reason: nanny-close
2024-01-26 06:30:54,399 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,400 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,483 - distributed.worker - INFO - Starting Worker plugin PreImport-7c62a07e-8f0e-4549-b706-bcfeabd6f0fd
2024-01-26 06:30:54,484 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83c5de6e-b7da-45c7-aaea-36832f489567
2024-01-26 06:30:54,485 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,501 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,507 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,515 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,515 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,517 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,524 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,527 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,527 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,527 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,533 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,534 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,534 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,536 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,537 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,538 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40173. Reason: nanny-close
2024-01-26 06:30:54,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40751. Reason: nanny-close
2024-01-26 06:30:54,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45989. Reason: nanny-close
2024-01-26 06:30:54,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,552 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,552 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,553 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,562 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,562 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,565 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,565 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,570 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,573 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:30:54,574 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:30:54,574 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:54,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:30:54,599 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,599 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,600 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37119. Reason: nanny-close
2024-01-26 06:30:54,600 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40725. Reason: nanny-close
2024-01-26 06:30:54,602 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,602 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,603 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,604 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37637. Reason: nanny-close
2024-01-26 06:30:54,605 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,605 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,607 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,608 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-26 06:30:54,609 - distributed.nanny - INFO - Worker closed
2024-01-26 06:30:54,609 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34547. Reason: nanny-close
2024-01-26 06:30:54,612 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:30:54,614 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:55138 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-26 06:30:54,707 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65809 parent=65612 started daemon>
2024-01-26 06:30:54,707 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65805 parent=65612 started daemon>
2024-01-26 06:30:54,708 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65802 parent=65612 started daemon>
2024-01-26 06:30:54,708 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65798 parent=65612 started daemon>
2024-01-26 06:30:54,708 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65792 parent=65612 started daemon>
2024-01-26 06:30:54,708 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65787 parent=65612 started daemon>
2024-01-26 06:30:54,708 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65778 parent=65612 started daemon>
2024-01-26 06:30:54,930 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65805 exit status was already read will report exitcode 255
2024-01-26 06:30:54,955 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65802 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-26 06:30:57,475 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:57,479 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46747 instead
  warnings.warn(
2024-01-26 06:30:57,483 - distributed.scheduler - INFO - State start
2024-01-26 06:30:57,770 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:30:57,771 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-26 06:30:57,772 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:30:57,773 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-26 06:30:58,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43763'
2024-01-26 06:30:59,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:30:59,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:30:59,906 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:30:59,907 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33221
2024-01-26 06:30:59,907 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33221
2024-01-26 06:30:59,907 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45039
2024-01-26 06:30:59,907 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:30:59,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:30:59,907 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:30:59,907 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:30:59,907 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gkji_eyu
2024-01-26 06:30:59,908 - distributed.worker - INFO - Starting Worker plugin PreImport-f4cbb78b-c841-48bd-8ee8-1620b6cff637
2024-01-26 06:30:59,908 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78cde9d5-e2fb-478f-8d0f-b3ffd7738bd8
2024-01-26 06:31:00,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e58bb4c6-a60a-4147-90bc-4fbcd0ccf705
2024-01-26 06:31:00,165 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:31:03,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:31:03,387 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:31:03,387 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:31:03,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:31:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43763'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-26 06:31:03,402 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-26 06:31:03,404 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33221. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-26 06:31:03,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:31:03,407 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:55512 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-26 06:31:06,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42773'
2024-01-26 06:31:06,181 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:31:06,186 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37193 instead
  warnings.warn(
2024-01-26 06:31:06,190 - distributed.scheduler - INFO - State start
2024-01-26 06:31:06,293 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-26 06:31:06,295 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-26 06:31:06,296 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-26 06:31:06,297 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-26 06:31:07,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-26 06:31:07,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-26 06:31:07,595 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-26 06:31:07,596 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39623
2024-01-26 06:31:07,596 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39623
2024-01-26 06:31:07,596 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45535
2024-01-26 06:31:07,596 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-26 06:31:07,596 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:31:07,597 - distributed.worker - INFO -               Threads:                          1
2024-01-26 06:31:07,597 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-26 06:31:07,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-anefbly3
2024-01-26 06:31:07,597 - distributed.worker - INFO - Starting Worker plugin PreImport-b4b731f9-5a25-42ca-a01e-867e9821ad56
2024-01-26 06:31:07,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ec702a5-0784-4262-80b5-e9359a9ee177
2024-01-26 06:31:07,859 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-521dc28e-66b4-4a0d-9307-2b1cbd8400d1
2024-01-26 06:31:07,859 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:31:09,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-26 06:31:09,639 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-26 06:31:09,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-26 06:31:09,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-26 06:31:09,667 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42773'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-26 06:31:09,668 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-26 06:31:09,669 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39623. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-26 06:31:09,671 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-26 06:31:09,672 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:41092 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41923 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45277 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36583 instead
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 51 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
