============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-08 06:43:03,476 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:03,480 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:43:03,484 - distributed.scheduler - INFO - State start
2024-01-08 06:43:03,507 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:03,508 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-08 06:43:03,509 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:43:03,509 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:03,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33367'
2024-01-08 06:43:03,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40205'
2024-01-08 06:43:03,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38007'
2024-01-08 06:43:03,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41931'
2024-01-08 06:43:03,902 - distributed.scheduler - INFO - Receive client connection: Client-2b19aea4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:03,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54442
2024-01-08 06:43:05,404 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:05,404 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:05,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:05,409 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44839
2024-01-08 06:43:05,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44839
2024-01-08 06:43:05,409 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42797
2024-01-08 06:43:05,409 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,409 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,409 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:43:05,409 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:43:05,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-hxo37lda
2024-01-08 06:43:05,410 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1144d71c-eab5-4382-8bff-d00f5bda9fe8
2024-01-08 06:43:05,410 - distributed.worker - INFO - Starting Worker plugin PreImport-ad80b878-e80f-4daf-8a68-839b85c741ce
2024-01-08 06:43:05,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64674585-ab38-40e4-8ea5-308a3befda0e
2024-01-08 06:43:05,410 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,419 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:05,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:05,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:05,424 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38147
2024-01-08 06:43:05,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38147
2024-01-08 06:43:05,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37801
2024-01-08 06:43:05,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,424 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,424 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:43:05,424 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:43:05,424 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-0w0tjks0
2024-01-08 06:43:05,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-328c52a6-5fa4-436b-be6c-3afc213dc748
2024-01-08 06:43:05,425 - distributed.worker - INFO - Starting Worker plugin PreImport-182ccadf-a4d9-444a-9baf-710abb5a9f4f
2024-01-08 06:43:05,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c512270-730f-4d81-a7f7-b0712e0c7d27
2024-01-08 06:43:05,425 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44839', status: init, memory: 0, processing: 0>
2024-01-08 06:43:05,489 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44839
2024-01-08 06:43:05,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54458
2024-01-08 06:43:05,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:05,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:43:05,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38147', status: init, memory: 0, processing: 0>
2024-01-08 06:43:05,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38147
2024-01-08 06:43:05,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54460
2024-01-08 06:43:05,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:05,500 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,500 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:43:05,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:05,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:05,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:05,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41227
2024-01-08 06:43:05,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41227
2024-01-08 06:43:05,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45227
2024-01-08 06:43:05,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,534 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,534 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:43:05,534 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:43:05,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-c9j1sadh
2024-01-08 06:43:05,534 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ca2eb49-c869-489e-a321-22fe77e44fb9
2024-01-08 06:43:05,535 - distributed.worker - INFO - Starting Worker plugin PreImport-dd57cbda-025a-421f-abd7-c33bd9361e99
2024-01-08 06:43:05,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d978647-969a-47cd-af36-08b9ab8d26a0
2024-01-08 06:43:05,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41227', status: init, memory: 0, processing: 0>
2024-01-08 06:43:05,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:05,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:05,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41227
2024-01-08 06:43:05,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54466
2024-01-08 06:43:05,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:05,594 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,594 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:43:05,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:05,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35919
2024-01-08 06:43:05,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35919
2024-01-08 06:43:05,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39475
2024-01-08 06:43:05,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,597 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:43:05,597 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:43:05,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-1an3kpx7
2024-01-08 06:43:05,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e4b5dd8-75e0-4fdc-911c-602df9f6a409
2024-01-08 06:43:05,597 - distributed.worker - INFO - Starting Worker plugin PreImport-9f3fe0ca-08ba-4e9b-a074-f1cd80741557
2024-01-08 06:43:05,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c4881a0-e653-4a90-bf68-d8413596e3e4
2024-01-08 06:43:05,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35919', status: init, memory: 0, processing: 0>
2024-01-08 06:43:05,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35919
2024-01-08 06:43:05,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54476
2024-01-08 06:43:05,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:05,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:43:05,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:05,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:43:05,690 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:43:05,691 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:43:05,691 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:43:05,691 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:43:05,696 - distributed.scheduler - INFO - Remove client Client-2b19aea4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:05,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54442; closing.
2024-01-08 06:43:05,696 - distributed.scheduler - INFO - Remove client Client-2b19aea4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:05,696 - distributed.scheduler - INFO - Close client connection: Client-2b19aea4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:05,697 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33367'. Reason: nanny-close
2024-01-08 06:43:05,698 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:05,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40205'. Reason: nanny-close
2024-01-08 06:43:05,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:05,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38007'. Reason: nanny-close
2024-01-08 06:43:05,699 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41227. Reason: nanny-close
2024-01-08 06:43:05,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:05,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41931'. Reason: nanny-close
2024-01-08 06:43:05,699 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35919. Reason: nanny-close
2024-01-08 06:43:05,700 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:05,700 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44839. Reason: nanny-close
2024-01-08 06:43:05,701 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38147. Reason: nanny-close
2024-01-08 06:43:05,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:43:05,701 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54466; closing.
2024-01-08 06:43:05,701 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41227', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696185.7015703')
2024-01-08 06:43:05,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:43:05,702 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:05,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:43:05,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54458; closing.
2024-01-08 06:43:05,703 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:43:05,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54476; closing.
2024-01-08 06:43:05,703 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:05,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696185.703928')
2024-01-08 06:43:05,704 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:05,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35919', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696185.7043395')
2024-01-08 06:43:05,704 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:05,704 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54460; closing.
2024-01-08 06:43:05,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38147', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696185.704934')
2024-01-08 06:43:05,705 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:06,413 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:06,413 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:06,414 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:06,416 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-08 06:43:06,416 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-08 06:43:08,520 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:08,525 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:43:08,528 - distributed.scheduler - INFO - State start
2024-01-08 06:43:08,557 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:08,558 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:08,559 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:43:08,559 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:08,772 - distributed.scheduler - INFO - Receive client connection: Client-2e23bfcc-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:08,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41590
2024-01-08 06:43:08,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42899'
2024-01-08 06:43:08,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35209'
2024-01-08 06:43:08,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45021'
2024-01-08 06:43:08,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39987'
2024-01-08 06:43:08,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41723'
2024-01-08 06:43:08,908 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36073'
2024-01-08 06:43:08,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46131'
2024-01-08 06:43:08,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34645'
2024-01-08 06:43:10,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,774 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,775 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33727
2024-01-08 06:43:10,775 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33727
2024-01-08 06:43:10,775 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32783
2024-01-08 06:43:10,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,775 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,775 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,775 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x8_aqlai
2024-01-08 06:43:10,776 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9cde439-306f-468a-a93f-14805ab9ebfc
2024-01-08 06:43:10,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34111
2024-01-08 06:43:10,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34111
2024-01-08 06:43:10,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39837
2024-01-08 06:43:10,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35087
2024-01-08 06:43:10,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35087
2024-01-08 06:43:10,776 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34045
2024-01-08 06:43:10,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,776 - distributed.worker - INFO - Starting Worker plugin PreImport-eb88cc05-9372-4e52-9069-5b790466100a
2024-01-08 06:43:10,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ya0b1vs
2024-01-08 06:43:10,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,776 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc5d1cb4-c601-48aa-8ac7-8b8bec148b3a
2024-01-08 06:43:10,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zahhnwdu
2024-01-08 06:43:10,776 - distributed.worker - INFO - Starting Worker plugin PreImport-8f66a49e-05c7-4850-b17e-2de4c18ea0d8
2024-01-08 06:43:10,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60b08cfc-c435-47e2-aee6-26e45240ebaf
2024-01-08 06:43:10,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f42d7fab-0a35-4e93-9c97-b51602a00d15
2024-01-08 06:43:10,777 - distributed.worker - INFO - Starting Worker plugin PreImport-a1d141d1-d58d-487d-b513-f0098357f359
2024-01-08 06:43:10,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f77f470-adca-4eb6-a8e7-168007c32904
2024-01-08 06:43:10,779 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df1d0258-88d3-4e44-bd0f-b75717560cd0
2024-01-08 06:43:10,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,785 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,789 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,789 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,790 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35665
2024-01-08 06:43:10,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35665
2024-01-08 06:43:10,790 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41565
2024-01-08 06:43:10,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,790 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37679
2024-01-08 06:43:10,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,790 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37679
2024-01-08 06:43:10,790 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,790 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36371
2024-01-08 06:43:10,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x01xaplm
2024-01-08 06:43:10,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,790 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,791 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,791 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7c1eafa-8d4f-4ec6-ae13-a30b4c591e1a
2024-01-08 06:43:10,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eyptsx6r
2024-01-08 06:43:10,791 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44de955b-892b-4966-ba88-dfb7e31c5693
2024-01-08 06:43:10,791 - distributed.worker - INFO - Starting Worker plugin PreImport-d4ebf782-155e-441e-b750-0e2d52889c2c
2024-01-08 06:43:10,791 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dbccfdc6-12ed-4cec-9e77-77ee1ccb70ec
2024-01-08 06:43:10,791 - distributed.worker - INFO - Starting Worker plugin PreImport-47b9ce71-88e0-49ba-ab0d-2adaff07dbc6
2024-01-08 06:43:10,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a838712-03de-4053-803f-6274166c620a
2024-01-08 06:43:10,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,793 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,798 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40699
2024-01-08 06:43:10,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40699
2024-01-08 06:43:10,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40271
2024-01-08 06:43:10,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,798 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,798 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,799 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,799 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xp_v28zw
2024-01-08 06:43:10,799 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6c88175-10e3-4305-9be8-a2a215c2d9b6
2024-01-08 06:43:10,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab36259f-dcb6-4d64-8e0d-d6a82defc588
2024-01-08 06:43:10,875 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,875 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,880 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44215
2024-01-08 06:43:10,880 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44215
2024-01-08 06:43:10,880 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35619
2024-01-08 06:43:10,880 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,880 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,880 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,880 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,880 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xsn7e1dk
2024-01-08 06:43:10,881 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7f9c145-573c-45c6-9418-f2a2c7d8626c
2024-01-08 06:43:10,881 - distributed.worker - INFO - Starting Worker plugin PreImport-fffe928a-ea78-4488-9b66-ae53c0129973
2024-01-08 06:43:10,881 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c2198b9-7982-4d09-ac23-87392c718afa
2024-01-08 06:43:10,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:10,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:10,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:10,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35669
2024-01-08 06:43:10,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35669
2024-01-08 06:43:10,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34477
2024-01-08 06:43:10,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:10,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:10,910 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:10,910 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:10,910 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3wfhtw_n
2024-01-08 06:43:10,911 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f658cce-1b4d-458d-bbbc-8d95a57c8399
2024-01-08 06:43:10,911 - distributed.worker - INFO - Starting Worker plugin PreImport-eb287b4d-e2bb-45d9-9fa8-32cd09b85334
2024-01-08 06:43:10,911 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fe8d87b-fce1-45be-8c85-11e463a2b81b
2024-01-08 06:43:12,897 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:12,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34111', status: init, memory: 0, processing: 0>
2024-01-08 06:43:12,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34111
2024-01-08 06:43:12,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56092
2024-01-08 06:43:12,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:12,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:12,927 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:12,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:12,953 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:12,974 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:12,992 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37679', status: init, memory: 0, processing: 0>
2024-01-08 06:43:12,993 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37679
2024-01-08 06:43:12,993 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56104
2024-01-08 06:43:12,994 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:12,996 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:12,996 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:12,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,005 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,027 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33727', status: init, memory: 0, processing: 0>
2024-01-08 06:43:13,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33727
2024-01-08 06:43:13,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56106
2024-01-08 06:43:13,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:13,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:13,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,032 - distributed.worker - INFO - Starting Worker plugin PreImport-f8af58c4-d63c-415a-a6cb-ada8642741ff
2024-01-08 06:43:13,035 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,057 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35087', status: init, memory: 0, processing: 0>
2024-01-08 06:43:13,058 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35087
2024-01-08 06:43:13,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56122
2024-01-08 06:43:13,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,060 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:13,061 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:13,061 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,068 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35665', status: init, memory: 0, processing: 0>
2024-01-08 06:43:13,068 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35665
2024-01-08 06:43:13,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56130
2024-01-08 06:43:13,069 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40699', status: init, memory: 0, processing: 0>
2024-01-08 06:43:13,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:13,070 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40699
2024-01-08 06:43:13,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56146
2024-01-08 06:43:13,071 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:13,071 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:13,072 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:13,073 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,073 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,074 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,084 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35669', status: init, memory: 0, processing: 0>
2024-01-08 06:43:13,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35669
2024-01-08 06:43:13,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56158
2024-01-08 06:43:13,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:13,086 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:13,086 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44215', status: init, memory: 0, processing: 0>
2024-01-08 06:43:13,087 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44215
2024-01-08 06:43:13,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56162
2024-01-08 06:43:13,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,088 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:13,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:13,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:13,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:13,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,161 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,162 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,162 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,162 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,162 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:13,166 - distributed.scheduler - INFO - Remove client Client-2e23bfcc-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:13,167 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41590; closing.
2024-01-08 06:43:13,167 - distributed.scheduler - INFO - Remove client Client-2e23bfcc-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:13,167 - distributed.scheduler - INFO - Close client connection: Client-2e23bfcc-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:13,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42899'. Reason: nanny-close
2024-01-08 06:43:13,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35209'. Reason: nanny-close
2024-01-08 06:43:13,170 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45021'. Reason: nanny-close
2024-01-08 06:43:13,170 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33727. Reason: nanny-close
2024-01-08 06:43:13,170 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39987'. Reason: nanny-close
2024-01-08 06:43:13,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,171 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35087. Reason: nanny-close
2024-01-08 06:43:13,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41723'. Reason: nanny-close
2024-01-08 06:43:13,171 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34111. Reason: nanny-close
2024-01-08 06:43:13,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,172 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36073'. Reason: nanny-close
2024-01-08 06:43:13,172 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44215. Reason: nanny-close
2024-01-08 06:43:13,172 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,172 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46131'. Reason: nanny-close
2024-01-08 06:43:13,172 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40699. Reason: nanny-close
2024-01-08 06:43:13,172 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34645'. Reason: nanny-close
2024-01-08 06:43:13,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,173 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:13,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35665. Reason: nanny-close
2024-01-08 06:43:13,173 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56106; closing.
2024-01-08 06:43:13,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37679. Reason: nanny-close
2024-01-08 06:43:13,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.1737323')
2024-01-08 06:43:13,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,174 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,174 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35669. Reason: nanny-close
2024-01-08 06:43:13,174 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56092; closing.
2024-01-08 06:43:13,174 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.1747737')
2024-01-08 06:43:13,174 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,175 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,175 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,175 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,176 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56122; closing.
2024-01-08 06:43:13,176 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56162; closing.
2024-01-08 06:43:13,176 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,176 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:13,176 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,177 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,177 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,176 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56092>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:43:13,178 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:13,178 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.178663')
2024-01-08 06:43:13,179 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44215', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.1790426')
2024-01-08 06:43:13,179 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56146; closing.
2024-01-08 06:43:13,179 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56104; closing.
2024-01-08 06:43:13,180 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40699', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.180228')
2024-01-08 06:43:13,180 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.1806211')
2024-01-08 06:43:13,180 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56130; closing.
2024-01-08 06:43:13,181 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56158; closing.
2024-01-08 06:43:13,181 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35665', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.1815844')
2024-01-08 06:43:13,182 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35669', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696193.181957')
2024-01-08 06:43:13,182 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:14,234 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:14,235 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:14,235 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:14,236 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:43:14,237 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-08 06:43:16,549 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:16,554 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33123 instead
  warnings.warn(
2024-01-08 06:43:16,558 - distributed.scheduler - INFO - State start
2024-01-08 06:43:16,581 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:16,582 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:16,583 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33123/status
2024-01-08 06:43:16,583 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:16,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37681'
2024-01-08 06:43:16,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40147'
2024-01-08 06:43:16,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45693'
2024-01-08 06:43:16,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35083'
2024-01-08 06:43:16,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46247'
2024-01-08 06:43:16,866 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41641'
2024-01-08 06:43:16,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44153'
2024-01-08 06:43:16,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46173'
2024-01-08 06:43:17,900 - distributed.scheduler - INFO - Receive client connection: Client-32db93b4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:17,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56296
2024-01-08 06:43:18,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,655 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34367
2024-01-08 06:43:18,656 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34367
2024-01-08 06:43:18,656 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44769
2024-01-08 06:43:18,656 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,656 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,656 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,656 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-emv07hwj
2024-01-08 06:43:18,657 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c755d29e-b8d9-47c4-b54c-8adb51118821
2024-01-08 06:43:18,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,912 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,912 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,913 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38299
2024-01-08 06:43:18,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38299
2024-01-08 06:43:18,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43567
2024-01-08 06:43:18,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,913 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,913 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,914 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4721vo72
2024-01-08 06:43:18,914 - distributed.worker - INFO - Starting Worker plugin PreImport-8606114c-703b-4226-b6ae-2c414ea7b0ff
2024-01-08 06:43:18,914 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4dcdbda-791f-48a7-b651-92a9e72d1b73
2024-01-08 06:43:18,915 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf9548bc-801f-4504-bf62-316c059a9828
2024-01-08 06:43:18,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,917 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38107
2024-01-08 06:43:18,917 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38107
2024-01-08 06:43:18,917 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32803
2024-01-08 06:43:18,917 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,917 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,917 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,917 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ht1fo6s0
2024-01-08 06:43:18,918 - distributed.worker - INFO - Starting Worker plugin PreImport-c74e71fa-9b31-4abe-8c81-114fd3efa856
2024-01-08 06:43:18,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-805c1e19-9901-4341-86b0-a2a8eb2f63d3
2024-01-08 06:43:18,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8c7b398-faf3-4197-ab2a-25ea30a8baed
2024-01-08 06:43:18,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,922 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44729
2024-01-08 06:43:18,922 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44729
2024-01-08 06:43:18,922 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37301
2024-01-08 06:43:18,922 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,922 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,922 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,922 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,922 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qmhexwhk
2024-01-08 06:43:18,923 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-929126a3-3a33-43f1-a2e6-2b95d5a70e48
2024-01-08 06:43:18,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,923 - distributed.worker - INFO - Starting Worker plugin PreImport-5a3d6eb7-1b48-44c8-9f9f-6da98195f9f2
2024-01-08 06:43:18,923 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3a5720b-acd1-4b1d-966d-dbd83153425d
2024-01-08 06:43:18,925 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,927 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,928 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:18,928 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:18,930 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,930 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,931 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36439
2024-01-08 06:43:18,931 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36439
2024-01-08 06:43:18,931 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40107
2024-01-08 06:43:18,931 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,931 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,931 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,931 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tytdab9w
2024-01-08 06:43:18,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9508e5f-f862-4279-a3c2-f1a381b8ec04
2024-01-08 06:43:18,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41889
2024-01-08 06:43:18,932 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41889
2024-01-08 06:43:18,932 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43611
2024-01-08 06:43:18,932 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,932 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,932 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,932 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,932 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lhju85fw
2024-01-08 06:43:18,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37827
2024-01-08 06:43:18,933 - distributed.worker - INFO - Starting Worker plugin PreImport-9df2711a-2480-41bf-b2dc-ea502403049d
2024-01-08 06:43:18,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37827
2024-01-08 06:43:18,933 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:18,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40917
2024-01-08 06:43:18,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88c4c997-b2b6-4be5-a71a-ea4e3ee0f18f
2024-01-08 06:43:18,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,933 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,933 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8eddaa10-e999-45b8-9fd6-d1581311fa9b
2024-01-08 06:43:18,933 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-852ze_rp
2024-01-08 06:43:18,934 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4e6cb07-ef49-43de-beb2-d428c169a5e9
2024-01-08 06:43:18,934 - distributed.worker - INFO - Starting Worker plugin PreImport-859ebdde-0737-4577-8f36-17161983dc95
2024-01-08 06:43:18,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ba77df6-468d-43fe-b4c0-ecd87ea5430c
2024-01-08 06:43:18,934 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37805
2024-01-08 06:43:18,934 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37805
2024-01-08 06:43:18,934 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37505
2024-01-08 06:43:18,934 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:18,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:18,934 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:18,934 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:18,934 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tg156m84
2024-01-08 06:43:18,935 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7551feb4-a7b5-4f36-9078-39505bec1b6d
2024-01-08 06:43:19,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63a6c0b7-8536-4bc5-9fc0-84ba26adc9b6
2024-01-08 06:43:19,163 - distributed.worker - INFO - Starting Worker plugin PreImport-2799711c-8fe7-468e-bf66-defb627cdcca
2024-01-08 06:43:19,163 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:19,190 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34367', status: init, memory: 0, processing: 0>
2024-01-08 06:43:19,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34367
2024-01-08 06:43:19,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56314
2024-01-08 06:43:19,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:19,193 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:19,193 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:19,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,801 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,824 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,836 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38107', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38107
2024-01-08 06:43:20,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45832
2024-01-08 06:43:20,838 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,839 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,840 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,843 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-092ba162-3e9e-4eb1-8781-3594a521e90e
2024-01-08 06:43:20,844 - distributed.worker - INFO - Starting Worker plugin PreImport-4b38299c-5976-4249-87ae-635a31242b37
2024-01-08 06:43:20,844 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,849 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38299', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38299
2024-01-08 06:43:20,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45840
2024-01-08 06:43:20,860 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,861 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,862 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37805', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,869 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37805
2024-01-08 06:43:20,869 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45846
2024-01-08 06:43:20,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44729', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44729
2024-01-08 06:43:20,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45862
2024-01-08 06:43:20,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,888 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,888 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,926 - distributed.worker - INFO - Starting Worker plugin PreImport-c516b2bb-dad5-45c5-b90a-bc8f0281f8f1
2024-01-08 06:43:20,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6db3dc3d-f1b8-4b37-8c8d-d25fc5f7b143
2024-01-08 06:43:20,927 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,930 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,945 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36439', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36439
2024-01-08 06:43:20,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45876
2024-01-08 06:43:20,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,955 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41889', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41889
2024-01-08 06:43:20,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45882
2024-01-08 06:43:20,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,968 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,968 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,968 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37827', status: init, memory: 0, processing: 0>
2024-01-08 06:43:20,969 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37827
2024-01-08 06:43:20,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45894
2024-01-08 06:43:20,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:20,970 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:20,971 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:20,971 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:20,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:21,047 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,047 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,047 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,047 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,048 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,048 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,048 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,048 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:21,052 - distributed.scheduler - INFO - Remove client Client-32db93b4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:21,052 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56296; closing.
2024-01-08 06:43:21,053 - distributed.scheduler - INFO - Remove client Client-32db93b4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:21,053 - distributed.scheduler - INFO - Close client connection: Client-32db93b4-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:21,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37681'. Reason: nanny-close
2024-01-08 06:43:21,054 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40147'. Reason: nanny-close
2024-01-08 06:43:21,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,055 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45693'. Reason: nanny-close
2024-01-08 06:43:21,055 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37827. Reason: nanny-close
2024-01-08 06:43:21,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35083'. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34367. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46247'. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44729. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41641'. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38299. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45894; closing.
2024-01-08 06:43:21,057 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,057 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44153'. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37805. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0575264')
2024-01-08 06:43:21,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46173'. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36439. Reason: nanny-close
2024-01-08 06:43:21,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:21,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,058 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41889. Reason: nanny-close
2024-01-08 06:43:21,058 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,059 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38107. Reason: nanny-close
2024-01-08 06:43:21,059 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,059 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,059 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,059 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56314; closing.
2024-01-08 06:43:21,060 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45862; closing.
2024-01-08 06:43:21,060 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,060 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0603523')
2024-01-08 06:43:21,060 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,060 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,061 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,061 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0611343')
2024-01-08 06:43:21,061 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,061 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45846; closing.
2024-01-08 06:43:21,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45840; closing.
2024-01-08 06:43:21,062 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0623806')
2024-01-08 06:43:21,062 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38299', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0627236')
2024-01-08 06:43:21,062 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,062 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:21,063 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:21,063 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45876; closing.
2024-01-08 06:43:21,063 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45882; closing.
2024-01-08 06:43:21,063 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36439', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0638027')
2024-01-08 06:43:21,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.064081')
2024-01-08 06:43:21,064 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45832; closing.
2024-01-08 06:43:21,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696201.0648248')
2024-01-08 06:43:21,065 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:21,065 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:22,020 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:22,020 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:22,021 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:22,022 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:43:22,022 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-08 06:43:24,287 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:24,292 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43199 instead
  warnings.warn(
2024-01-08 06:43:24,297 - distributed.scheduler - INFO - State start
2024-01-08 06:43:24,341 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:24,343 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:24,344 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43199/status
2024-01-08 06:43:24,344 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:24,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44305'
2024-01-08 06:43:24,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39721'
2024-01-08 06:43:24,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43971'
2024-01-08 06:43:24,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35375'
2024-01-08 06:43:24,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42911'
2024-01-08 06:43:24,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42679'
2024-01-08 06:43:24,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44027'
2024-01-08 06:43:24,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39087'
2024-01-08 06:43:26,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,435 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,435 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,440 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44077
2024-01-08 06:43:26,440 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42641
2024-01-08 06:43:26,440 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42641
2024-01-08 06:43:26,440 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44077
2024-01-08 06:43:26,440 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41151
2024-01-08 06:43:26,440 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35333
2024-01-08 06:43:26,440 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,440 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,440 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,440 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,440 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,440 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,440 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,440 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ildaofud
2024-01-08 06:43:26,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n3h9saq1
2024-01-08 06:43:26,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-208ac8d3-0ce7-47b8-9ec4-f6c38bab2ce8
2024-01-08 06:43:26,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46f2819f-51ce-41bb-8bc3-4db6d33be56b
2024-01-08 06:43:26,445 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,445 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,449 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,450 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38095
2024-01-08 06:43:26,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,450 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38095
2024-01-08 06:43:26,450 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35289
2024-01-08 06:43:26,450 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,450 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,450 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,450 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,450 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b8sizku2
2024-01-08 06:43:26,451 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0178b6b3-3c40-4e53-921f-1dbbe14f91d3
2024-01-08 06:43:26,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40963
2024-01-08 06:43:26,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40963
2024-01-08 06:43:26,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33007
2024-01-08 06:43:26,451 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38881
2024-01-08 06:43:26,451 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38881
2024-01-08 06:43:26,451 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35331
2024-01-08 06:43:26,451 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,451 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,451 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ze5hs6f
2024-01-08 06:43:26,451 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,451 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,451 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,451 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9sq78khu
2024-01-08 06:43:26,451 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce24e3df-668b-4922-88f7-6024cf524e02
2024-01-08 06:43:26,452 - distributed.worker - INFO - Starting Worker plugin PreImport-78fd4f7b-1d0f-4cdd-8575-34ac0e1b1d9d
2024-01-08 06:43:26,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13aa0484-a45e-43a7-b387-f019fe0a04d4
2024-01-08 06:43:26,453 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c88b1f8-6f29-4ad5-9358-b462c46ae552
2024-01-08 06:43:26,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,532 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39727
2024-01-08 06:43:26,532 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39727
2024-01-08 06:43:26,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33051
2024-01-08 06:43:26,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,533 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,533 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,533 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,533 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b41k89ig
2024-01-08 06:43:26,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,533 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d5f3a65-f117-4fd6-a883-e1e7b0aee0c3
2024-01-08 06:43:26,537 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45233
2024-01-08 06:43:26,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45233
2024-01-08 06:43:26,538 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46871
2024-01-08 06:43:26,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,539 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,539 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9d303rvo
2024-01-08 06:43:26,539 - distributed.worker - INFO - Starting Worker plugin PreImport-03918a2b-10e2-4e6f-ae50-23f8b5161348
2024-01-08 06:43:26,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-209a39c3-f1d0-402b-a5ac-221f8badeaeb
2024-01-08 06:43:26,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f3a4640-f326-4f8f-a305-0c9a01a5081f
2024-01-08 06:43:26,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:26,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:26,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:26,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33729
2024-01-08 06:43:26,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33729
2024-01-08 06:43:26,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46107
2024-01-08 06:43:26,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:26,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:26,547 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:26,548 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:26,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yd6bcb05
2024-01-08 06:43:26,548 - distributed.worker - INFO - Starting Worker plugin PreImport-395de399-4030-4041-9e81-35f6af007bc5
2024-01-08 06:43:26,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41f3d19e-fc96-4a0e-9031-94e13a325a17
2024-01-08 06:43:26,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14cf0972-8003-43a9-8894-e53cd7fabbf2
2024-01-08 06:43:28,244 - distributed.scheduler - INFO - Receive client connection: Client-37760d21-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:28,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46064
2024-01-08 06:43:28,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72a993cf-fc70-4941-a468-bd358c6e8307
2024-01-08 06:43:28,598 - distributed.worker - INFO - Starting Worker plugin PreImport-93241eca-9a85-471c-81a9-aeb0609c678d
2024-01-08 06:43:28,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,607 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-beb67b6b-f492-4d3e-84b3-36097309ecf0
2024-01-08 06:43:28,608 - distributed.worker - INFO - Starting Worker plugin PreImport-7ebf9e13-b257-443f-a39e-691480a56a03
2024-01-08 06:43:28,609 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40963', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,625 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40963
2024-01-08 06:43:28,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46084
2024-01-08 06:43:28,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,627 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,627 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c851e615-46ea-48da-924a-04a6ab23630a
2024-01-08 06:43:28,634 - distributed.worker - INFO - Starting Worker plugin PreImport-51892d15-8367-404b-b36c-dcc5f413d9ab
2024-01-08 06:43:28,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e4a1f03-85ab-48e6-a985-6789dd7262aa
2024-01-08 06:43:28,639 - distributed.worker - INFO - Starting Worker plugin PreImport-041a0ef4-a096-4198-b06c-99c58fc4e4b1
2024-01-08 06:43:28,640 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,650 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42641', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,651 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42641
2024-01-08 06:43:28,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46090
2024-01-08 06:43:28,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,653 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,654 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38881', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,662 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38881
2024-01-08 06:43:28,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46098
2024-01-08 06:43:28,663 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44077', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,664 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44077
2024-01-08 06:43:28,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46114
2024-01-08 06:43:28,665 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,665 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,665 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,665 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38095', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,666 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,666 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,666 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38095
2024-01-08 06:43:28,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46130
2024-01-08 06:43:28,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,668 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,668 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,670 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,674 - distributed.worker - INFO - Starting Worker plugin PreImport-f8a5f655-454c-4206-ad67-f93f4d7455a5
2024-01-08 06:43:28,675 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45d0cd85-1d69-46bc-bf41-bc35de42f108
2024-01-08 06:43:28,675 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,676 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,676 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39727', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,704 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39727
2024-01-08 06:43:28,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46132
2024-01-08 06:43:28,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,706 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,706 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45233', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45233
2024-01-08 06:43:28,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46138
2024-01-08 06:43:28,713 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33729', status: init, memory: 0, processing: 0>
2024-01-08 06:43:28,713 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33729
2024-01-08 06:43:28,713 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46140
2024-01-08 06:43:28,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,714 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,714 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,714 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:28,716 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:28,716 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:28,716 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,783 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,782 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:28,793 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,794 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:28,803 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:28,805 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:28,807 - distributed.scheduler - INFO - Remove client Client-37760d21-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:28,807 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46064; closing.
2024-01-08 06:43:28,807 - distributed.scheduler - INFO - Remove client Client-37760d21-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:28,808 - distributed.scheduler - INFO - Close client connection: Client-37760d21-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:28,808 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44305'. Reason: nanny-close
2024-01-08 06:43:28,809 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,809 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39721'. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43971'. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44077. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35375'. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38095. Reason: nanny-close
2024-01-08 06:43:28,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42911'. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42641. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42679'. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33729. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44027'. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40963. Reason: nanny-close
2024-01-08 06:43:28,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46114; closing.
2024-01-08 06:43:28,812 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39087'. Reason: nanny-close
2024-01-08 06:43:28,812 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.8121934')
2024-01-08 06:43:28,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39727. Reason: nanny-close
2024-01-08 06:43:28,812 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:28,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38881. Reason: nanny-close
2024-01-08 06:43:28,813 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45233. Reason: nanny-close
2024-01-08 06:43:28,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,813 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,813 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46130; closing.
2024-01-08 06:43:28,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46090; closing.
2024-01-08 06:43:28,814 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,814 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.814837')
2024-01-08 06:43:28,815 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46084; closing.
2024-01-08 06:43:28,815 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,815 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,815 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42641', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.8157852')
2024-01-08 06:43:28,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:28,815 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,816 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.8161242')
2024-01-08 06:43:28,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46140; closing.
2024-01-08 06:43:28,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46132; closing.
2024-01-08 06:43:28,817 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.8171499')
2024-01-08 06:43:28,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.8175287')
2024-01-08 06:43:28,817 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:28,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46098; closing.
2024-01-08 06:43:28,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.818413')
2024-01-08 06:43:28,818 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46138; closing.
2024-01-08 06:43:28,819 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696208.8191268')
2024-01-08 06:43:28,819 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:29,825 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:29,825 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:29,826 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:29,827 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:43:29,827 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-08 06:43:32,191 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:32,195 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39149 instead
  warnings.warn(
2024-01-08 06:43:32,200 - distributed.scheduler - INFO - State start
2024-01-08 06:43:32,223 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:32,224 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:32,225 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39149/status
2024-01-08 06:43:32,225 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:32,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35707'
2024-01-08 06:43:32,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33735'
2024-01-08 06:43:32,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35333'
2024-01-08 06:43:32,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38709'
2024-01-08 06:43:32,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40111'
2024-01-08 06:43:32,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33685'
2024-01-08 06:43:32,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44671'
2024-01-08 06:43:32,469 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41647'
2024-01-08 06:43:32,664 - distributed.scheduler - INFO - Receive client connection: Client-3c234ac8-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:32,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39918
2024-01-08 06:43:34,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34787
2024-01-08 06:43:34,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34787
2024-01-08 06:43:34,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36865
2024-01-08 06:43:34,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,290 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,290 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,290 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,290 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7jnqug7m
2024-01-08 06:43:34,290 - distributed.worker - INFO - Starting Worker plugin PreImport-de34380a-6f94-4fda-b6b3-846a3a05724c
2024-01-08 06:43:34,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de90983f-81c1-475e-936a-4f59ff8222f2
2024-01-08 06:43:34,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71d71adc-9ef2-4da5-8635-f4827801d2d3
2024-01-08 06:43:34,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,297 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,298 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34419
2024-01-08 06:43:34,298 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34419
2024-01-08 06:43:34,298 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40595
2024-01-08 06:43:34,298 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,298 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,298 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,298 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,298 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c8kw7u1l
2024-01-08 06:43:34,299 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cc2f68f-17da-4894-94e4-58b57dcc0e7d
2024-01-08 06:43:34,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,303 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36485
2024-01-08 06:43:34,303 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36485
2024-01-08 06:43:34,303 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44071
2024-01-08 06:43:34,303 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,303 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,303 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,303 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,303 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6govhyyf
2024-01-08 06:43:34,303 - distributed.worker - INFO - Starting Worker plugin PreImport-90f2b196-ec8c-4dc9-a91b-787f01d6c75b
2024-01-08 06:43:34,303 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64d2ab3f-c204-46c4-9633-0eeb3e5477da
2024-01-08 06:43:34,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5e72fc4-bb5e-4bfa-a6d0-9646be0940b0
2024-01-08 06:43:34,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,340 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,344 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,345 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41559
2024-01-08 06:43:34,345 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41559
2024-01-08 06:43:34,345 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36309
2024-01-08 06:43:34,345 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,345 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,345 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,345 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4zo71b8e
2024-01-08 06:43:34,346 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4bd830c-9e0b-4781-9fa7-f40acf2c5041
2024-01-08 06:43:34,349 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43757
2024-01-08 06:43:34,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43757
2024-01-08 06:43:34,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37505
2024-01-08 06:43:34,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,350 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,350 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ju_draps
2024-01-08 06:43:34,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5459b9e5-fce9-42e8-af14-0dd07d805354
2024-01-08 06:43:34,352 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,352 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,356 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,357 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40923
2024-01-08 06:43:34,357 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40923
2024-01-08 06:43:34,357 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37383
2024-01-08 06:43:34,357 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,357 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,357 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,357 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,357 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l__0p0tc
2024-01-08 06:43:34,358 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68f2d7dc-fce7-4ac5-bdd8-d9f72aae9962
2024-01-08 06:43:34,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:34,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:34,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,382 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36235
2024-01-08 06:43:34,382 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36235
2024-01-08 06:43:34,383 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41683
2024-01-08 06:43:34,383 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,383 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,383 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,383 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_v0un90
2024-01-08 06:43:34,383 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f10d11f-5bc7-4f2d-beb4-b87b976b8c87
2024-01-08 06:43:34,384 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:34,385 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44867
2024-01-08 06:43:34,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44867
2024-01-08 06:43:34,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44673
2024-01-08 06:43:34,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:34,385 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:34,385 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:34,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:34,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cjcbmrs7
2024-01-08 06:43:34,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-567b0819-7f80-4b83-9cbb-415a8f59b29a
2024-01-08 06:43:36,482 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34787', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34787
2024-01-08 06:43:36,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39940
2024-01-08 06:43:36,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,520 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,542 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc7ce3b5-9d79-413d-a2f2-8d376878486a
2024-01-08 06:43:36,542 - distributed.worker - INFO - Starting Worker plugin PreImport-c84ccf17-4e61-4a91-943f-0523df0c0ebf
2024-01-08 06:43:36,544 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,567 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36485', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36485
2024-01-08 06:43:36,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39954
2024-01-08 06:43:36,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,569 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,581 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34419', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,581 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34419
2024-01-08 06:43:36,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39966
2024-01-08 06:43:36,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,583 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,583 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb7fd7db-6d57-438e-a47a-78837ee21dfa
2024-01-08 06:43:36,595 - distributed.worker - INFO - Starting Worker plugin PreImport-62aa2037-7c13-4eb9-9249-97bf23ef8dfb
2024-01-08 06:43:36,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee7f3625-3408-4519-b33a-bc24fd9dde07
2024-01-08 06:43:36,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-984d0b47-c1dd-4e8e-b028-c7f89f39df99
2024-01-08 06:43:36,598 - distributed.worker - INFO - Starting Worker plugin PreImport-9a23c9e8-2de7-4cd1-aed0-72524d31367b
2024-01-08 06:43:36,598 - distributed.worker - INFO - Starting Worker plugin PreImport-1b6be2b0-2d83-4857-9938-1bff5f26d3f9
2024-01-08 06:43:36,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,601 - distributed.worker - INFO - Starting Worker plugin PreImport-6e61af53-4fa5-4cda-b203-d1dd64a640d9
2024-01-08 06:43:36,601 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb1d6b0f-bfb1-4db5-b117-2a994b1d8cc2
2024-01-08 06:43:36,601 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebd7e415-bbfe-4648-8d88-fc00bc10ecb5
2024-01-08 06:43:36,602 - distributed.worker - INFO - Starting Worker plugin PreImport-6006ccbb-dffb-4f3b-99b7-a1f9089b34a3
2024-01-08 06:43:36,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,603 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36235', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36235
2024-01-08 06:43:36,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39978
2024-01-08 06:43:36,627 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43757', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,628 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43757
2024-01-08 06:43:36,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39994
2024-01-08 06:43:36,628 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,629 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41559', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,630 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,630 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41559
2024-01-08 06:43:36,630 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40010
2024-01-08 06:43:36,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,632 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,633 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40923', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,633 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40923
2024-01-08 06:43:36,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39970
2024-01-08 06:43:36,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,636 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,636 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,637 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44867', status: init, memory: 0, processing: 0>
2024-01-08 06:43:36,637 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44867
2024-01-08 06:43:36,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40014
2024-01-08 06:43:36,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,639 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:36,640 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:36,640 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:36,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:36,651 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,653 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,664 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:43:36,673 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,675 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:36,677 - distributed.scheduler - INFO - Remove client Client-3c234ac8-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:36,677 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39918; closing.
2024-01-08 06:43:36,677 - distributed.scheduler - INFO - Remove client Client-3c234ac8-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:36,678 - distributed.scheduler - INFO - Close client connection: Client-3c234ac8-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:36,679 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35707'. Reason: nanny-close
2024-01-08 06:43:36,679 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,679 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33735'. Reason: nanny-close
2024-01-08 06:43:36,680 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35333'. Reason: nanny-close
2024-01-08 06:43:36,680 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34419. Reason: nanny-close
2024-01-08 06:43:36,680 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38709'. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34787. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40111'. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36485. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33685'. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41559. Reason: nanny-close
2024-01-08 06:43:36,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44671'. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44867. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41647'. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,682 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40923. Reason: nanny-close
2024-01-08 06:43:36,682 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39966; closing.
2024-01-08 06:43:36,683 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,683 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34419', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.683162')
2024-01-08 06:43:36,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43757. Reason: nanny-close
2024-01-08 06:43:36,683 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,683 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36235. Reason: nanny-close
2024-01-08 06:43:36,684 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39954; closing.
2024-01-08 06:43:36,684 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,684 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,684 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,684 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,684 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,684 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,685 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,685 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.6850395')
2024-01-08 06:43:36,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40010; closing.
2024-01-08 06:43:36,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:36,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39940; closing.
2024-01-08 06:43:36,685 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,686 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,686 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,686 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.686551')
2024-01-08 06:43:36,686 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:36,687 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.6869814')
2024-01-08 06:43:36,687 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40014; closing.
2024-01-08 06:43:36,687 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39970; closing.
2024-01-08 06:43:36,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.6881745')
2024-01-08 06:43:36,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.688582')
2024-01-08 06:43:36,688 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39994; closing.
2024-01-08 06:43:36,689 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39978; closing.
2024-01-08 06:43:36,689 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.6895351')
2024-01-08 06:43:36,689 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36235', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696216.6899316')
2024-01-08 06:43:36,690 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:37,845 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:37,845 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:37,846 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:37,847 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:43:37,848 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-08 06:43:40,092 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:40,097 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37447 instead
  warnings.warn(
2024-01-08 06:43:40,101 - distributed.scheduler - INFO - State start
2024-01-08 06:43:40,130 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:40,131 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:40,132 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37447/status
2024-01-08 06:43:40,132 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:40,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46591'
2024-01-08 06:43:40,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37511'
2024-01-08 06:43:40,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36595'
2024-01-08 06:43:40,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37125'
2024-01-08 06:43:41,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34091'
2024-01-08 06:43:41,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46111'
2024-01-08 06:43:41,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44921'
2024-01-08 06:43:41,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42703'
2024-01-08 06:43:41,656 - distributed.scheduler - INFO - Receive client connection: Client-40e01334-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:41,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58756
2024-01-08 06:43:42,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:42,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:42,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:42,925 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43893
2024-01-08 06:43:42,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43893
2024-01-08 06:43:42,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35001
2024-01-08 06:43:42,926 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:42,926 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:42,926 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:42,926 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:42,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ar5tt73c
2024-01-08 06:43:42,926 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f7db3ff-4750-488e-a089-e5be4a78ec32
2024-01-08 06:43:42,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:42,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:42,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:42,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:42,986 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:42,987 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42567
2024-01-08 06:43:42,987 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42567
2024-01-08 06:43:42,987 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:42,987 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40073
2024-01-08 06:43:42,987 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:42,987 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:42,988 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:42,988 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:42,988 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4zch4tdf
2024-01-08 06:43:42,988 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c49e09f8-a708-4afe-87b3-1f5fb1de41d3
2024-01-08 06:43:42,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:42,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:42,988 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34633
2024-01-08 06:43:42,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34633
2024-01-08 06:43:42,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35019
2024-01-08 06:43:42,988 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:42,988 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:42,988 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:42,989 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:42,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d164o6s6
2024-01-08 06:43:42,989 - distributed.worker - INFO - Starting Worker plugin PreImport-90790bfe-ff32-45ff-b3f2-09454144391f
2024-01-08 06:43:42,989 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-286aaca0-4e07-417d-a03b-c3e55284d472
2024-01-08 06:43:42,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:42,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:42,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c110e7c-7fd2-49ae-a994-e6d8a606355d
2024-01-08 06:43:42,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:42,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38315
2024-01-08 06:43:42,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38315
2024-01-08 06:43:42,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33611
2024-01-08 06:43:42,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:42,993 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:42,993 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:42,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:42,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:42,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:42,994 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g408llms
2024-01-08 06:43:42,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f6c95f7-8d0f-4578-8215-a448d9cb2652
2024-01-08 06:43:42,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:42,995 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41095
2024-01-08 06:43:42,995 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41095
2024-01-08 06:43:42,995 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39599
2024-01-08 06:43:42,995 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:42,995 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:42,995 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:42,996 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:42,996 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_k6rmpzi
2024-01-08 06:43:42,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6738a6b-c681-4869-80c7-4b41ae654642
2024-01-08 06:43:42,998 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:42,999 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46243
2024-01-08 06:43:42,999 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46243
2024-01-08 06:43:42,999 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44421
2024-01-08 06:43:42,999 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:42,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:42,999 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:42,999 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:42,999 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zvuo3cji
2024-01-08 06:43:42,999 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61afecf1-8ee3-44b7-a930-20f478b9556a
2024-01-08 06:43:43,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:43,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:43,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:43,014 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43481
2024-01-08 06:43:43,014 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43481
2024-01-08 06:43:43,015 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35253
2024-01-08 06:43:43,015 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:43,015 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:43,015 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:43,015 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:43,015 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1okmf3vl
2024-01-08 06:43:43,015 - distributed.worker - INFO - Starting Worker plugin PreImport-6eb0c628-5baa-4f2c-a74f-d2210d93af76
2024-01-08 06:43:43,016 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45184f01-d533-49b8-bba4-3898211752c7
2024-01-08 06:43:43,016 - distributed.worker - INFO - Starting Worker plugin RMMSetup-924b4e1d-be97-43f3-b528-83b8458fda5b
2024-01-08 06:43:43,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:43,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:43,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:43,263 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37731
2024-01-08 06:43:43,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37731
2024-01-08 06:43:43,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36601
2024-01-08 06:43:43,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:43,263 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:43,264 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:43,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:43:43,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-go4h2xkv
2024-01-08 06:43:43,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d1296c8-f6c0-4e84-b163-c5f246ef0975
2024-01-08 06:43:45,120 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c2e1c55-e262-4f61-b4e1-9cbcaf041e46
2024-01-08 06:43:45,121 - distributed.worker - INFO - Starting Worker plugin PreImport-392e260f-bcce-43bf-bf3c-dcd6bea05aae
2024-01-08 06:43:45,122 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,156 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43893', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,158 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43893
2024-01-08 06:43:45,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58786
2024-01-08 06:43:45,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,161 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08090e4c-600e-4767-9f5c-8f65eb62f46d
2024-01-08 06:43:45,277 - distributed.worker - INFO - Starting Worker plugin PreImport-83fc2536-625b-4e6f-bce3-4b78ee3d7dcf
2024-01-08 06:43:45,278 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,287 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,315 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42567', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,315 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42567
2024-01-08 06:43:45,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58788
2024-01-08 06:43:45,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34633', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34633
2024-01-08 06:43:45,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58794
2024-01-08 06:43:45,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,327 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,344 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e54bacc4-c9c5-4c33-8675-71eb4251003d
2024-01-08 06:43:45,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a219e7b0-d872-42a4-94e3-c22fc2322d86
2024-01-08 06:43:45,352 - distributed.worker - INFO - Starting Worker plugin PreImport-da7b484a-a2b3-44e8-afea-7bb2258baf39
2024-01-08 06:43:45,352 - distributed.worker - INFO - Starting Worker plugin PreImport-7f9c64b0-1898-493b-ada1-6f19b78babc1
2024-01-08 06:43:45,352 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,352 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,371 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43481', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,371 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43481
2024-01-08 06:43:45,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58806
2024-01-08 06:43:45,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,373 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,374 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,374 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2be8dd35-81e0-499d-adb1-a2300b23cc9c
2024-01-08 06:43:45,375 - distributed.worker - INFO - Starting Worker plugin PreImport-46685f0b-f0e7-4b20-b015-bec2aa333682
2024-01-08 06:43:45,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,375 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,376 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38315', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38315
2024-01-08 06:43:45,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58816
2024-01-08 06:43:45,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41095', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,377 - distributed.worker - INFO - Starting Worker plugin PreImport-738e2bc9-17ca-42f0-a51b-518e086d5464
2024-01-08 06:43:45,378 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41095
2024-01-08 06:43:45,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58830
2024-01-08 06:43:45,378 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37c32e22-45a6-4a92-b6bd-55efea2fea24
2024-01-08 06:43:45,379 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,379 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,379 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,379 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,380 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,380 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,382 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37731', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,397 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37731
2024-01-08 06:43:45,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58840
2024-01-08 06:43:45,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,399 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,399 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,409 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46243', status: init, memory: 0, processing: 0>
2024-01-08 06:43:45,409 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46243
2024-01-08 06:43:45,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58848
2024-01-08 06:43:45,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:45,411 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:45,412 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:45,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:45,458 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,458 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,458 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,458 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,458 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,459 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,459 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,459 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:43:45,464 - distributed.scheduler - INFO - Remove client Client-40e01334-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:45,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58756; closing.
2024-01-08 06:43:45,465 - distributed.scheduler - INFO - Remove client Client-40e01334-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:45,465 - distributed.scheduler - INFO - Close client connection: Client-40e01334-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:45,466 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46591'. Reason: nanny-close
2024-01-08 06:43:45,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37511'. Reason: nanny-close
2024-01-08 06:43:45,467 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36595'. Reason: nanny-close
2024-01-08 06:43:45,467 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43893. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37125'. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34633. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34091'. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43481. Reason: nanny-close
2024-01-08 06:43:45,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46111'. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41095. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44921'. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46243. Reason: nanny-close
2024-01-08 06:43:45,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42703'. Reason: nanny-close
2024-01-08 06:43:45,470 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42567. Reason: nanny-close
2024-01-08 06:43:45,470 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:45,470 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,470 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58786; closing.
2024-01-08 06:43:45,470 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38315. Reason: nanny-close
2024-01-08 06:43:45,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4707446')
2024-01-08 06:43:45,470 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,471 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,471 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,471 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37731. Reason: nanny-close
2024-01-08 06:43:45,471 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,472 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,472 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,472 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,472 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,472 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,472 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58848; closing.
2024-01-08 06:43:45,472 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,472 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58806; closing.
2024-01-08 06:43:45,473 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58794; closing.
2024-01-08 06:43:45,473 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58830; closing.
2024-01-08 06:43:45,473 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,473 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:45,474 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,474 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46243', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.474168')
2024-01-08 06:43:45,474 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,474 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43481', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4745498')
2024-01-08 06:43:45,474 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4749079')
2024-01-08 06:43:45,475 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:45,475 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4752584')
2024-01-08 06:43:45,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58788; closing.
2024-01-08 06:43:45,476 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4762933')
2024-01-08 06:43:45,476 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58816; closing.
2024-01-08 06:43:45,476 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58840; closing.
2024-01-08 06:43:45,477 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38315', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4772656')
2024-01-08 06:43:45,477 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696225.4776368')
2024-01-08 06:43:45,477 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:46,482 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:46,482 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:46,483 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:46,484 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:43:46,484 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-08 06:43:48,867 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:48,872 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:43:48,876 - distributed.scheduler - INFO - State start
2024-01-08 06:43:48,899 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:48,900 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:48,901 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:43:48,901 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:49,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41293'
2024-01-08 06:43:50,942 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:43:50,942 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:43:51,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:43:51,582 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43993
2024-01-08 06:43:51,582 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43993
2024-01-08 06:43:51,582 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-08 06:43:51,582 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:43:51,582 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:51,582 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:43:51,582 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:43:51,582 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lw5v2ddo
2024-01-08 06:43:51,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87a78802-c33c-46c7-bbf9-da4e9ae4b380
2024-01-08 06:43:51,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-015952f3-e293-4b67-81ca-b2fa8e38b576
2024-01-08 06:43:51,583 - distributed.worker - INFO - Starting Worker plugin PreImport-57757c52-a337-462d-924e-629ee6350f31
2024-01-08 06:43:51,583 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:51,663 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43993', status: init, memory: 0, processing: 0>
2024-01-08 06:43:51,680 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43993
2024-01-08 06:43:51,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48400
2024-01-08 06:43:51,682 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:43:51,682 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:43:51,683 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:43:51,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:43:53,255 - distributed.scheduler - INFO - Receive client connection: Client-461ea4ad-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:53,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48418
2024-01-08 06:43:53,263 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:43:53,268 - distributed.scheduler - INFO - Remove client Client-461ea4ad-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:53,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48418; closing.
2024-01-08 06:43:53,269 - distributed.scheduler - INFO - Remove client Client-461ea4ad-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:53,269 - distributed.scheduler - INFO - Close client connection: Client-461ea4ad-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:43:53,270 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41293'. Reason: nanny-close
2024-01-08 06:43:53,270 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:43:53,272 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43993. Reason: nanny-close
2024-01-08 06:43:53,274 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48400; closing.
2024-01-08 06:43:53,274 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:43:53,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696233.2747872')
2024-01-08 06:43:53,275 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:43:53,276 - distributed.nanny - INFO - Worker closed
2024-01-08 06:43:53,935 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:43:53,936 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:43:53,936 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:43:53,937 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:43:53,938 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-08 06:43:58,548 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:58,552 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:43:58,557 - distributed.scheduler - INFO - State start
2024-01-08 06:43:58,582 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:43:58,583 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:43:58,584 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:43:58,584 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:43:58,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34635'
2024-01-08 06:44:00,260 - distributed.scheduler - INFO - Receive client connection: Client-4bd36a32-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:00,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44928
2024-01-08 06:44:00,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:00,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:01,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:01,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38777
2024-01-08 06:44:01,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38777
2024-01-08 06:44:01,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37919
2024-01-08 06:44:01,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:01,304 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:01,304 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:01,304 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:44:01,304 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k6vwjaog
2024-01-08 06:44:01,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-208093d5-ee43-49b3-ab39-ad38b0a56520
2024-01-08 06:44:01,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b445862-f92f-4749-868e-845806bdd21c
2024-01-08 06:44:01,305 - distributed.worker - INFO - Starting Worker plugin PreImport-f4aac5b1-2ab0-4c11-83b7-9aa0f7cf6194
2024-01-08 06:44:01,306 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:01,369 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38777', status: init, memory: 0, processing: 0>
2024-01-08 06:44:01,370 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38777
2024-01-08 06:44:01,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44942
2024-01-08 06:44:01,371 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:01,372 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:01,372 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:01,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:01,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:01,403 - distributed.scheduler - INFO - Remove client Client-4bd36a32-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:01,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44928; closing.
2024-01-08 06:44:01,403 - distributed.scheduler - INFO - Remove client Client-4bd36a32-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:01,403 - distributed.scheduler - INFO - Close client connection: Client-4bd36a32-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:01,405 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34635'. Reason: nanny-close
2024-01-08 06:44:01,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:01,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38777. Reason: nanny-close
2024-01-08 06:44:01,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:01,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44942; closing.
2024-01-08 06:44:01,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696241.4268217')
2024-01-08 06:44:01,427 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:44:01,427 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:02,120 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:44:02,120 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:44:02,121 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:44:02,122 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:44:02,122 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-08 06:44:04,416 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:04,420 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40053 instead
  warnings.warn(
2024-01-08 06:44:04,425 - distributed.scheduler - INFO - State start
2024-01-08 06:44:04,447 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:04,449 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:44:04,450 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40053/status
2024-01-08 06:44:04,450 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:44:07,289 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44944'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44944>: Stream is closed
2024-01-08 06:44:07,566 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:44:07,566 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:44:07,566 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:44:07,567 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:44:07,567 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-08 06:44:09,989 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:09,994 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33151 instead
  warnings.warn(
2024-01-08 06:44:09,998 - distributed.scheduler - INFO - State start
2024-01-08 06:44:10,027 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:10,028 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-08 06:44:10,029 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33151/status
2024-01-08 06:44:10,029 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:44:10,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34953'
2024-01-08 06:44:11,784 - distributed.scheduler - INFO - Receive client connection: Client-52a2d5be-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:11,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40394
2024-01-08 06:44:12,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:12,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:12,233 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:12,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41433
2024-01-08 06:44:12,234 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41433
2024-01-08 06:44:12,234 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33453
2024-01-08 06:44:12,234 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:44:12,234 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:12,234 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:12,234 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:44:12,234 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-r7e7uppy
2024-01-08 06:44:12,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73fb8743-c882-49d6-bf6f-c51c7a490de7
2024-01-08 06:44:12,234 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31402f2d-7758-444c-b03f-78ca62cd037a
2024-01-08 06:44:12,234 - distributed.worker - INFO - Starting Worker plugin PreImport-455d42a5-9dc8-457e-b402-0e7f21482ca0
2024-01-08 06:44:12,234 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:12,292 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41433', status: init, memory: 0, processing: 0>
2024-01-08 06:44:12,293 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41433
2024-01-08 06:44:12,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40414
2024-01-08 06:44:12,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:12,295 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:44:12,295 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:12,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:44:12,313 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:12,316 - distributed.scheduler - INFO - Remove client Client-52a2d5be-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:12,316 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40394; closing.
2024-01-08 06:44:12,316 - distributed.scheduler - INFO - Remove client Client-52a2d5be-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:12,316 - distributed.scheduler - INFO - Close client connection: Client-52a2d5be-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:12,317 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34953'. Reason: nanny-close
2024-01-08 06:44:12,342 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:12,343 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41433. Reason: nanny-close
2024-01-08 06:44:12,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:44:12,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40414; closing.
2024-01-08 06:44:12,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41433', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696252.344991')
2024-01-08 06:44:12,345 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:44:12,345 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:12,882 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:44:12,883 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:44:12,883 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:44:12,884 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-08 06:44:12,885 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-08 06:44:15,116 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:15,121 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43279 instead
  warnings.warn(
2024-01-08 06:44:15,125 - distributed.scheduler - INFO - State start
2024-01-08 06:44:15,148 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:15,149 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:44:15,150 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43279/status
2024-01-08 06:44:15,150 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:44:15,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43575'
2024-01-08 06:44:15,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36067'
2024-01-08 06:44:15,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38497'
2024-01-08 06:44:15,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36033'
2024-01-08 06:44:15,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34553'
2024-01-08 06:44:15,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40909'
2024-01-08 06:44:15,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35873'
2024-01-08 06:44:15,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44913'
2024-01-08 06:44:16,523 - distributed.scheduler - INFO - Receive client connection: Client-55ca1a53-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:16,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41466
2024-01-08 06:44:17,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33695
2024-01-08 06:44:17,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35945
2024-01-08 06:44:17,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33695
2024-01-08 06:44:17,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35945
2024-01-08 06:44:17,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37983
2024-01-08 06:44:17,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45097
2024-01-08 06:44:17,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,365 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,365 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,365 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,365 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,365 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,365 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tk605l3m
2024-01-08 06:44:17,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-84j709om
2024-01-08 06:44:17,365 - distributed.worker - INFO - Starting Worker plugin PreImport-b06e85ce-5470-4d55-99f6-50c0246773d9
2024-01-08 06:44:17,366 - distributed.worker - INFO - Starting Worker plugin PreImport-6ae31695-5d2a-4df8-81fd-d3eeb3c15730
2024-01-08 06:44:17,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d571f52-0c3b-4687-9c0b-6f1c03c7b1bf
2024-01-08 06:44:17,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-963c28dd-991f-4321-842b-e232570d4f15
2024-01-08 06:44:17,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6c003be-74f8-4ec8-9fc8-408714a9bd9a
2024-01-08 06:44:17,367 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dbf46141-94ba-460c-856c-1e821f1606dd
2024-01-08 06:44:17,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39511
2024-01-08 06:44:17,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42299
2024-01-08 06:44:17,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39511
2024-01-08 06:44:17,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42299
2024-01-08 06:44:17,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41129
2024-01-08 06:44:17,374 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33579
2024-01-08 06:44:17,374 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,374 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,374 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,374 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,374 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,374 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iwo09ty3
2024-01-08 06:44:17,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7vjq77oj
2024-01-08 06:44:17,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-018b8745-1bd8-4a11-b7b9-579cc3070949
2024-01-08 06:44:17,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-240e5700-b8c4-4400-9fd9-4d1c09f8f69e
2024-01-08 06:44:17,375 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,376 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40243
2024-01-08 06:44:17,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40243
2024-01-08 06:44:17,376 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45461
2024-01-08 06:44:17,376 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,376 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,376 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,376 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,376 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-at0ff9of
2024-01-08 06:44:17,376 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cada62e-4313-4fa3-b6f9-93c85c7a9f73
2024-01-08 06:44:17,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44391
2024-01-08 06:44:17,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44391
2024-01-08 06:44:17,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45519
2024-01-08 06:44:17,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,381 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,381 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5qm79xuq
2024-01-08 06:44:17,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3e580e8-8558-4d1a-914c-f6d98e892438
2024-01-08 06:44:17,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42605
2024-01-08 06:44:17,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42605
2024-01-08 06:44:17,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46465
2024-01-08 06:44:17,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,457 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,457 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,457 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gijjzulh
2024-01-08 06:44:17,457 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd074403-623a-4c7c-95e8-16daada3ca1a
2024-01-08 06:44:17,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:17,535 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:17,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:17,540 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42067
2024-01-08 06:44:17,540 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42067
2024-01-08 06:44:17,540 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34785
2024-01-08 06:44:17,540 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:17,540 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:17,540 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:17,540 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:44:17,540 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5bibv7ed
2024-01-08 06:44:17,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b3e37de-8be9-4e04-9698-61371f2c00fb
2024-01-08 06:44:19,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2004f509-4e42-4d7e-a9fb-f5201ddbf9e9
2024-01-08 06:44:19,429 - distributed.worker - INFO - Starting Worker plugin PreImport-b636d2f3-8499-4077-86a6-bf39df17d6d5
2024-01-08 06:44:19,430 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,434 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,458 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35945', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35945
2024-01-08 06:44:19,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41492
2024-01-08 06:44:19,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,461 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39511', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,463 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39511
2024-01-08 06:44:19,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41476
2024-01-08 06:44:19,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,466 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,466 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,524 - distributed.worker - INFO - Starting Worker plugin PreImport-c8255e8d-c18e-41e3-9fde-02aa8eac2f8a
2024-01-08 06:44:19,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1791bd8b-9829-4ebe-ac53-8dc9bd8b644f
2024-01-08 06:44:19,526 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,546 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2cdf14f7-5673-4925-bbb4-2df2dd93092a
2024-01-08 06:44:19,546 - distributed.worker - INFO - Starting Worker plugin PreImport-76d733f6-2d64-4c03-bad5-b6f87e5533af
2024-01-08 06:44:19,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,558 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40243', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,558 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40243
2024-01-08 06:44:19,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41498
2024-01-08 06:44:19,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,561 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,561 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,570 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42299', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,570 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42299
2024-01-08 06:44:19,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41508
2024-01-08 06:44:19,571 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,572 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,572 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-726d9d4a-5b9a-40f6-834b-4de3fd7b09a7
2024-01-08 06:44:19,582 - distributed.worker - INFO - Starting Worker plugin PreImport-7b1d7844-52d9-4bc2-b76c-61bc05b006ac
2024-01-08 06:44:19,582 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,587 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,592 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3923f7e3-f507-482b-a0d1-643af392ea93
2024-01-08 06:44:19,593 - distributed.worker - INFO - Starting Worker plugin PreImport-ec421437-f49d-4c6d-bcc8-fae59afa8a13
2024-01-08 06:44:19,594 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,610 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42605', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,611 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42605
2024-01-08 06:44:19,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41512
2024-01-08 06:44:19,612 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,613 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,613 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b614db4-ee36-42cb-ba6b-27861a208f3e
2024-01-08 06:44:19,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33695', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,623 - distributed.worker - INFO - Starting Worker plugin PreImport-84242373-56bf-45ef-b088-6c4a8c16f967
2024-01-08 06:44:19,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33695
2024-01-08 06:44:19,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41518
2024-01-08 06:44:19,624 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,625 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44391', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,625 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44391
2024-01-08 06:44:19,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41532
2024-01-08 06:44:19,626 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42067', status: init, memory: 0, processing: 0>
2024-01-08 06:44:19,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42067
2024-01-08 06:44:19,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41544
2024-01-08 06:44:19,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:19,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:19,657 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:19,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:19,671 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,671 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,671 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,672 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,672 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,672 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,672 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,672 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:44:19,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,689 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,689 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:19,694 - distributed.scheduler - INFO - Remove client Client-55ca1a53-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:19,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41466; closing.
2024-01-08 06:44:19,694 - distributed.scheduler - INFO - Remove client Client-55ca1a53-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:19,694 - distributed.scheduler - INFO - Close client connection: Client-55ca1a53-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:19,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43575'. Reason: nanny-close
2024-01-08 06:44:19,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36067'. Reason: nanny-close
2024-01-08 06:44:19,697 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,697 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38497'. Reason: nanny-close
2024-01-08 06:44:19,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44391. Reason: nanny-close
2024-01-08 06:44:19,697 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36033'. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33695. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34553'. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35945. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40909'. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42299. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35873'. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40243. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44913'. Reason: nanny-close
2024-01-08 06:44:19,699 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41532; closing.
2024-01-08 06:44:19,700 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,700 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39511. Reason: nanny-close
2024-01-08 06:44:19,700 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.7002242')
2024-01-08 06:44:19,700 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42605. Reason: nanny-close
2024-01-08 06:44:19,700 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,700 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,700 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,701 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41508; closing.
2024-01-08 06:44:19,701 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,702 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,702 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42299', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.702109')
2024-01-08 06:44:19,702 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,702 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41518; closing.
2024-01-08 06:44:19,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41492; closing.
2024-01-08 06:44:19,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,703 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,704 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,703 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41508>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41508>: Stream is closed
2024-01-08 06:44:19,704 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:19,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.7048984')
2024-01-08 06:44:19,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35945', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.705268')
2024-01-08 06:44:19,705 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41498; closing.
2024-01-08 06:44:19,706 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40243', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.70619')
2024-01-08 06:44:19,706 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41476; closing.
2024-01-08 06:44:19,706 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41512; closing.
2024-01-08 06:44:19,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.707083')
2024-01-08 06:44:19,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42605', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.7074666')
2024-01-08 06:44:19,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:19,709 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42067. Reason: nanny-close
2024-01-08 06:44:19,712 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41544; closing.
2024-01-08 06:44:19,712 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:19,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696259.7123575')
2024-01-08 06:44:19,712 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:44:19,714 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:20,762 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:44:20,762 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:44:20,763 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:44:20,764 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:44:20,764 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-08 06:44:23,221 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:23,226 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36257 instead
  warnings.warn(
2024-01-08 06:44:23,230 - distributed.scheduler - INFO - State start
2024-01-08 06:44:23,256 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:23,257 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:44:23,258 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36257/status
2024-01-08 06:44:23,258 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:44:23,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36945'
2024-01-08 06:44:23,820 - distributed.scheduler - INFO - Receive client connection: Client-5a86df99-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:23,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32786
2024-01-08 06:44:25,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:25,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:25,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:25,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44801
2024-01-08 06:44:25,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44801
2024-01-08 06:44:25,265 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42693
2024-01-08 06:44:25,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:25,265 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:25,265 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:25,265 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:44:25,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-09hzsja7
2024-01-08 06:44:25,265 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19a015ac-b14f-4be8-a423-8cbd57d6ef98
2024-01-08 06:44:25,676 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-993ce70b-581f-4d1c-99c8-48937797ece3
2024-01-08 06:44:25,677 - distributed.worker - INFO - Starting Worker plugin PreImport-f880625d-0b99-43b0-8bcd-ac6e3d3b4e28
2024-01-08 06:44:25,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:25,739 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44801', status: init, memory: 0, processing: 0>
2024-01-08 06:44:25,740 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44801
2024-01-08 06:44:25,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32794
2024-01-08 06:44:25,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:25,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:25,743 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:25,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:25,774 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:44:25,779 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:25,780 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:25,783 - distributed.scheduler - INFO - Remove client Client-5a86df99-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:25,783 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32786; closing.
2024-01-08 06:44:25,784 - distributed.scheduler - INFO - Remove client Client-5a86df99-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:25,784 - distributed.scheduler - INFO - Close client connection: Client-5a86df99-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:25,785 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36945'. Reason: nanny-close
2024-01-08 06:44:25,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:25,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44801. Reason: nanny-close
2024-01-08 06:44:25,788 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32794; closing.
2024-01-08 06:44:25,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:25,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696265.7891707')
2024-01-08 06:44:25,789 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:44:25,790 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:26,500 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:44:26,501 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:44:26,501 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:44:26,502 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:44:26,503 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-08 06:44:28,843 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:28,848 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:44:28,852 - distributed.scheduler - INFO - State start
2024-01-08 06:44:28,876 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:44:28,877 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:44:28,878 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:44:28,878 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:44:29,107 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45145'
2024-01-08 06:44:30,005 - distributed.scheduler - INFO - Receive client connection: Client-5df43c9d-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:30,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42236
2024-01-08 06:44:30,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:44:30,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:44:30,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:44:30,949 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34741
2024-01-08 06:44:30,949 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34741
2024-01-08 06:44:30,949 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34499
2024-01-08 06:44:30,949 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:44:30,949 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:30,949 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:44:30,949 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:44:30,950 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v1rtabvm
2024-01-08 06:44:30,950 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b8d5187-b728-4360-ac9d-513321d6903b
2024-01-08 06:44:31,242 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aca8f0d5-91ee-4517-bb91-5fc3f8aa4e2d
2024-01-08 06:44:31,243 - distributed.worker - INFO - Starting Worker plugin PreImport-db5d37d3-3102-4b59-a78c-99432bc632a8
2024-01-08 06:44:31,243 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:31,300 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34741', status: init, memory: 0, processing: 0>
2024-01-08 06:44:31,302 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34741
2024-01-08 06:44:31,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42268
2024-01-08 06:44:31,303 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:44:31,303 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:44:31,303 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:44:31,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:44:31,349 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-08 06:44:31,353 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:44:31,357 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:31,359 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:44:31,361 - distributed.scheduler - INFO - Remove client Client-5df43c9d-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:31,361 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42236; closing.
2024-01-08 06:44:31,361 - distributed.scheduler - INFO - Remove client Client-5df43c9d-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:31,362 - distributed.scheduler - INFO - Close client connection: Client-5df43c9d-adf1-11ee-a9d8-d8c49764f6bb
2024-01-08 06:44:31,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45145'. Reason: nanny-close
2024-01-08 06:44:31,363 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:44:31,364 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34741. Reason: nanny-close
2024-01-08 06:44:31,365 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42268; closing.
2024-01-08 06:44:31,365 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:44:31,366 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704696271.3660848')
2024-01-08 06:44:31,366 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:44:31,367 - distributed.nanny - INFO - Worker closed
2024-01-08 06:44:31,978 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:44:31,978 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:44:31,979 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:44:31,980 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:44:31,980 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33875 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33121 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] 2024-01-08 06:45:35,096 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:58453 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f8c94016800 error: Connection reset by remote peer')
2024-01-08 06:45:35,137 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-08 06:45:35,141 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 439, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 445, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f8c94016800 error: Connection reset by remote peer')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45069 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43581 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45469 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35825 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43467 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43705 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36807 instead
  warnings.warn(
2024-01-08 06:47:53,453 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-08 06:47:53,454 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 406, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-08 06:47:53,484 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucxx://127.0.0.1:50259', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42887 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41867 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36351 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45555 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39303 instead
  warnings.warn(
2024-01-08 06:50:39,355 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 443, in ep
    raise CommClosedError("UCX Endpoint is closed")
distributed.comm.core.CommClosedError: UCX Endpoint is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CommClosedError('UCX Endpoint is closed')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39919 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37783 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36135 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45561 instead
  warnings.warn(
[1704696766.060415] [dgx13:71779:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:48790) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45461 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40365 instead
  warnings.warn(
[1704696811.659085] [dgx13:72228:0]            sock.c:481  UCX  ERROR bind(fd=160 addr=0.0.0.0:37784) failed: Address already in use
2024-01-08 06:53:57,012 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40027 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33791 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41723 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42173 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43619 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42079 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39791 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36945 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43707 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35239 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37571 instead
  warnings.warn(
[1704697163.112433] [dgx13:77357:0]            sock.c:481  UCX  ERROR bind(fd=159 addr=0.0.0.0:42663) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44281 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45253 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37023 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37079 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38237 instead
  warnings.warn(
[1704697360.069936] [dgx13:79988:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:48526) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44693 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44253 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34335 instead
  warnings.warn(
2024-01-08 07:03:45,073 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-dbbc659e-75eb-48c1-90e8-a201b81b96c1
Function:  _run_coroutine_on_worker
args:      (168290999344799330035904815201781654221, <function shuffle_task at 0x7f8fe7e13790>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 0), ('from_pandas-eaba71b388f24e387b93842f216e4045', 4)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2024-01-08 07:03:45,075 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-684c46e6-2e9d-49fb-bd3b-c68c07156ee6
Function:  _run_coroutine_on_worker
args:      (168290999344799330035904815201781654221, <function shuffle_task at 0x7f2f455149d0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 0), ('from_pandas-eaba71b388f24e387b93842f216e4045', 4)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2024-01-08 07:03:45,107 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-6d773f6a-e52e-443d-b4b4-9eed2513d596
Function:  _run_coroutine_on_worker
args:      (168290999344799330035904815201781654221, <function shuffle_task at 0x7fc1e40af8b0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 0), ('from_pandas-eaba71b388f24e387b93842f216e4045', 4)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2024-01-08 07:03:45,117 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-6006e5d3-2625-44c5-b7de-6913383d0b34
Function:  _run_coroutine_on_worker
args:      (168290999344799330035904815201781654221, <function shuffle_task at 0x7f37942cb9d0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 0), ('from_pandas-eaba71b388f24e387b93842f216e4045', 4)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-59:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 281, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5472, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 258, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 261, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 61, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 644, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 56, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/__init__.py", line 2, in <module>
    from . import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_lowering.py", line 11, in <module>
    from cudf.core.udf.groupby_typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_typing.py", line 19, in <module>
    from cudf.core.udf.utils import Row, UDFError
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] Process SpawnProcess-60:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 250, in _test_dataframe_shuffle_merge
    cudf = pytest.importorskip("cudf")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/outcomes.py", line 292, in importorskip
    __import__(modname)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 56, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/__init__.py", line 2, in <module>
    from . import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_lowering.py", line 11, in <module>
    from cudf.core.udf.groupby_typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_typing.py", line 19, in <module>
    from cudf.core.udf.utils import Row, UDFError
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] [1704697495.017981] [dgx13:82220:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:35572) failed: Address already in use
[1704697496.365542] [dgx13:82389:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:46808) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1704697666.371426] [dgx13:59864:0]            sock.c:481  UCX  ERROR bind(fd=179 addr=0.0.0.0:34058) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] [1704697724.596557] [dgx13:85586:0]            sock.c:481  UCX  ERROR bind(fd=176 addr=0.0.0.0:59836) failed: Address already in use
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704697746.535587] [dgx13:85816:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:56689) failed: Address already in use
2024-01-08 07:09:06,947 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-08 07:09:06,950 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704697748.340107] [dgx13:59864:1]            sock.c:481  UCX  ERROR bind(fd=243 addr=0.0.0.0:57206) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-08 07:09:39,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:09:39,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:09:39,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37499
2024-01-08 07:09:39,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37499
2024-01-08 07:09:39,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46707
2024-01-08 07:09:39,908 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,908 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,908 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ny0xp2m_
2024-01-08 07:09:39,908 - distributed.worker - INFO - Starting Worker plugin PreImport-2355e0a9-aba0-429e-9037-05f234d29b52
2024-01-08 07:09:39,908 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-287ebd87-d5b7-41fa-9955-96f379f3a51b
2024-01-08 07:09:39,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9283bf30-aa50-4fbc-b007-fbde16e9598b
2024-01-08 07:09:39,909 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33563
2024-01-08 07:09:39,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33563
2024-01-08 07:09:39,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36795
2024-01-08 07:09:39,910 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,911 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,911 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kqjm53cj
2024-01-08 07:09:39,911 - distributed.worker - INFO - Starting Worker plugin RMMSetup-532ec269-ab89-4024-ab3d-ff5cd888f7ae
2024-01-08 07:09:39,911 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-898b5121-c3f1-4dd3-8c12-e73125d08cbb
2024-01-08 07:09:39,911 - distributed.worker - INFO - Starting Worker plugin PreImport-c4bf6f72-34dd-4647-92ca-a86e1c929820
2024-01-08 07:09:39,911 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,918 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32857
2024-01-08 07:09:39,918 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32857
2024-01-08 07:09:39,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39789
2024-01-08 07:09:39,918 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,919 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,919 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-czffnzwl
2024-01-08 07:09:39,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9956aaf3-3f5f-49df-8def-53fbce722f13
2024-01-08 07:09:39,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20383113-af57-45a5-b8ee-1f1f0e9c159a
2024-01-08 07:09:39,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43383
2024-01-08 07:09:39,919 - distributed.worker - INFO - Starting Worker plugin PreImport-2bce2455-beba-479e-8c70-57d11fe2728c
2024-01-08 07:09:39,919 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43383
2024-01-08 07:09:39,919 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44791
2024-01-08 07:09:39,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,919 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,919 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,919 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-25yyb59w
2024-01-08 07:09:39,920 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6416b59a-5b42-488f-983c-967591f1d331
2024-01-08 07:09:39,921 - distributed.worker - INFO - Starting Worker plugin PreImport-037b478c-08bb-4f32-bc96-2856d3aeb28c
2024-01-08 07:09:39,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-825cfbba-6e7b-4242-951a-6651c2809c81
2024-01-08 07:09:39,921 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43711
2024-01-08 07:09:39,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43711
2024-01-08 07:09:39,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45317
2024-01-08 07:09:39,933 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,933 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-__rro7_f
2024-01-08 07:09:39,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40f96968-ae89-4303-aec6-66f153e130eb
2024-01-08 07:09:39,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d7a0a24-88b1-4650-b673-cbd5582f9ac9
2024-01-08 07:09:39,934 - distributed.worker - INFO - Starting Worker plugin PreImport-2000500c-1350-462d-95ec-7323cca19b16
2024-01-08 07:09:39,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,955 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34761
2024-01-08 07:09:39,955 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34761
2024-01-08 07:09:39,955 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39997
2024-01-08 07:09:39,955 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,955 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,956 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nlllvw_p
2024-01-08 07:09:39,956 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed5c091a-a8d3-49b0-98d5-c9a340fe5787
2024-01-08 07:09:39,956 - distributed.worker - INFO - Starting Worker plugin RMMSetup-859adb25-ef1e-48ee-a956-1b90fa56239d
2024-01-08 07:09:39,956 - distributed.worker - INFO - Starting Worker plugin PreImport-674d02cc-6446-4cdf-8c58-bdaf15c28ae4
2024-01-08 07:09:39,957 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,973 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36689
2024-01-08 07:09:39,973 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36689
2024-01-08 07:09:39,973 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44711
2024-01-08 07:09:39,973 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,973 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,973 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,973 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pa7zs75b
2024-01-08 07:09:39,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87e8199b-56e9-4989-b43b-769c6f1baabe
2024-01-08 07:09:39,973 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff910102-1f4a-46d2-b837-5a5cca60652b
2024-01-08 07:09:39,973 - distributed.worker - INFO - Starting Worker plugin PreImport-ccbb1856-136f-4734-95b0-1b36d2980997
2024-01-08 07:09:39,973 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,997 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:09:39,998 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36079
2024-01-08 07:09:39,998 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36079
2024-01-08 07:09:39,998 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42651
2024-01-08 07:09:39,998 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34973
2024-01-08 07:09:39,998 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:39,998 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:09:39,998 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e8dr4f31
2024-01-08 07:09:39,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d15d177-ab77-499b-8080-842da9259847
2024-01-08 07:09:39,998 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a87d6d8c-23be-4d62-a1a2-e681cb9916de
2024-01-08 07:09:39,998 - distributed.worker - INFO - Starting Worker plugin PreImport-34df327b-3339-4d79-b561-efb614aac73f
2024-01-08 07:09:39,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,142 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,178 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,178 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,210 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,214 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,214 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,224 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,224 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,241 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,241 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,243 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,244 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:09:40,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,245 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34973
2024-01-08 07:09:40,246 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:09:40,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34973
2024-01-08 07:09:40,280 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,280 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,280 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,280 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,281 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,281 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,281 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,281 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 07:09:40,286 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34761. Reason: nanny-close
2024-01-08 07:09:40,287 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43383. Reason: nanny-close
2024-01-08 07:09:40,288 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32857. Reason: nanny-close
2024-01-08 07:09:40,289 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33563. Reason: nanny-close
2024-01-08 07:09:40,289 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,289 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,289 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37499. Reason: nanny-close
2024-01-08 07:09:40,290 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,290 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43711. Reason: nanny-close
2024-01-08 07:09:40,290 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,290 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,290 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36689. Reason: nanny-close
2024-01-08 07:09:40,291 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,291 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36079. Reason: nanny-close
2024-01-08 07:09:40,291 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,291 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,292 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,292 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,292 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,292 - distributed.core - INFO - Connection to tcp://127.0.0.1:34973 has been closed.
2024-01-08 07:09:40,293 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,293 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,293 - distributed.nanny - INFO - Worker closed
2024-01-08 07:09:40,294 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-08 07:10:15,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:15,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:15,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:15,766 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44007
2024-01-08 07:10:15,766 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44007
2024-01-08 07:10:15,766 - distributed.worker - INFO -           Worker name:                          0
2024-01-08 07:10:15,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34875
2024-01-08 07:10:15,767 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36227
2024-01-08 07:10:15,767 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:15,767 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:15,767 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 07:10:15,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_17rn2g2
2024-01-08 07:10:15,767 - distributed.worker - INFO - Starting Worker plugin PreImport-bb122456-12c6-415b-bb8b-4306e080f1fe
2024-01-08 07:10:15,770 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-08 07:10:15,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a7b473a-9f3f-4d19-8d33-4ca0360a76a2
2024-01-08 07:10:15,771 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1997af2c-c3d5-4d6a-a6c2-9723c96bb635
2024-01-08 07:10:15,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44007. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-08 07:10:15,771 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-08 07:10:15,773 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-08 07:10:19,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:19,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:19,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:19,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:20,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:20,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:20,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:20,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:20,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 07:10:20,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 07:10:20,591 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33031
2024-01-08 07:10:20,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33031
2024-01-08 07:10:20,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42247
2024-01-08 07:10:20,592 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,592 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,592 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,592 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aelfo0uu
2024-01-08 07:10:20,593 - distributed.worker - INFO - Starting Worker plugin PreImport-42c9132d-5b04-4102-9c9f-bc3be2e83101
2024-01-08 07:10:20,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5d4df52-ceff-437c-a4c2-112e25d7d25f
2024-01-08 07:10:20,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86e04dc5-6102-4cdf-bf85-96bbe46ad56e
2024-01-08 07:10:20,594 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,623 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,624 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44157
2024-01-08 07:10:20,624 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44157
2024-01-08 07:10:20,624 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38479
2024-01-08 07:10:20,624 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,624 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,624 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,624 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,624 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d0kv5orn
2024-01-08 07:10:20,625 - distributed.worker - INFO - Starting Worker plugin PreImport-3d004314-0f56-499c-844a-208aa42c1b21
2024-01-08 07:10:20,625 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37b51b79-57ba-44d9-ade8-b462c3a3786f
2024-01-08 07:10:20,625 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68ef67c8-a5d1-42e3-ba31-93896c36f026
2024-01-08 07:10:20,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,662 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,663 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,663 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,685 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39133
2024-01-08 07:10:20,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39133
2024-01-08 07:10:20,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38841
2024-01-08 07:10:20,686 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,686 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,686 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,686 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,686 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cotoyxfw
2024-01-08 07:10:20,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94a1a981-6f46-4a0c-9be4-471c2affb5fc
2024-01-08 07:10:20,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-786c546e-3793-4024-a520-b88e385436b3
2024-01-08 07:10:20,687 - distributed.worker - INFO - Starting Worker plugin PreImport-698b3250-1c99-41ff-801a-911a76edfbdd
2024-01-08 07:10:20,687 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,691 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,692 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,692 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,735 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33813
2024-01-08 07:10:20,735 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33813
2024-01-08 07:10:20,735 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41171
2024-01-08 07:10:20,735 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,735 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,735 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,736 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,736 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0rzvo_cg
2024-01-08 07:10:20,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a276a2c1-69d1-436d-9a49-dbccea34c545
2024-01-08 07:10:20,736 - distributed.worker - INFO - Starting Worker plugin PreImport-db4cce44-0da1-43fc-b6a8-ffa3b9813da1
2024-01-08 07:10:20,736 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf9ae458-5278-4fa1-9783-af92495cb58a
2024-01-08 07:10:20,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,745 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39479
2024-01-08 07:10:20,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39479
2024-01-08 07:10:20,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39309
2024-01-08 07:10:20,746 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,746 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,746 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,746 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3humkfo2
2024-01-08 07:10:20,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-789bc2e8-3b05-42ff-aa0e-4f0f1780103c
2024-01-08 07:10:20,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ec4ce42-6626-4bb5-bbd3-215a98d5c432
2024-01-08 07:10:20,746 - distributed.worker - INFO - Starting Worker plugin PreImport-543d6728-3c1b-40e9-b240-7ed9ea2739b5
2024-01-08 07:10:20,747 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,747 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41663
2024-01-08 07:10:20,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41663
2024-01-08 07:10:20,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38165
2024-01-08 07:10:20,748 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,748 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,748 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,748 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,748 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h69kxcb6
2024-01-08 07:10:20,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf3ad455-bcef-4583-aacb-aff86ede14ea
2024-01-08 07:10:20,749 - distributed.worker - INFO - Starting Worker plugin PreImport-77cd62bb-f16b-45c7-9e26-e10ee8ac35ab
2024-01-08 07:10:20,749 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42fd93d6-5fce-47f3-91dc-7777065d87e7
2024-01-08 07:10:20,749 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,751 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,751 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,753 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,754 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45631
2024-01-08 07:10:20,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45631
2024-01-08 07:10:20,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35021
2024-01-08 07:10:20,754 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,754 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,754 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7hlk05wb
2024-01-08 07:10:20,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-396c030a-4eab-48af-b9f7-cd3edaf3bda6
2024-01-08 07:10:20,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c23857f-242d-423a-a110-b59bab66b6b7
2024-01-08 07:10:20,755 - distributed.worker - INFO - Starting Worker plugin PreImport-017adb39-36f4-43fd-a630-dcf081e82553
2024-01-08 07:10:20,755 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 07:10:20,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35821
2024-01-08 07:10:20,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35821
2024-01-08 07:10:20,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37613
2024-01-08 07:10:20,810 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,810 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,810 - distributed.worker - INFO -               Threads:                          1
2024-01-08 07:10:20,810 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 07:10:20,810 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8cabjkhm
2024-01-08 07:10:20,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-567f33f7-49f7-4205-adbf-0bb7b597f984
2024-01-08 07:10:20,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecd07c70-0acb-49c4-b2d5-5cf2ac2ecb5d
2024-01-08 07:10:20,811 - distributed.worker - INFO - Starting Worker plugin PreImport-8ca342d3-625e-4146-83f6-ff0d2bb4aa4d
2024-01-08 07:10:20,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,888 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,888 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,913 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,913 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,927 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,927 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,929 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,930 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,930 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 07:10:20,942 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33583
2024-01-08 07:10:20,942 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 07:10:20,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33583
2024-01-08 07:10:20,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33031. Reason: nanny-close
2024-01-08 07:10:20,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44157. Reason: nanny-close
2024-01-08 07:10:20,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39133. Reason: nanny-close
2024-01-08 07:10:20,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39479. Reason: nanny-close
2024-01-08 07:10:20,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33813. Reason: nanny-close
2024-01-08 07:10:20,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41663. Reason: nanny-close
2024-01-08 07:10:20,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35821. Reason: nanny-close
2024-01-08 07:10:20,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45631. Reason: nanny-close
2024-01-08 07:10:20,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,992 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,992 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,993 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,993 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,993 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:33583 has been closed.
2024-01-08 07:10:20,994 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,995 - distributed.nanny - INFO - Worker closed
2024-01-08 07:10:20,995 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-08 07:11:52,537 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-08 07:11:52,791 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj0] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj1] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[dask] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[pickle] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[disk] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-send_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-send_serializers1] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
